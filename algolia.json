[{"author":"fanjinpeng","categories":"Soul","content":" HTTP 用户接入 Soul 网关注册逻辑分析 1. 注册入口 HTTP 用户接入 Soul 网关时，会调用 soul-admin 一个接口，把需要 Soul 网关管理的接口注册，今天就具体看看到底干了点儿啥。\n先看下调用的接口信息如下：\n// SpringMvcClientBeanPostProcessor.java /** * Instantiates a new Soul client bean post processor. * * @param soulSpringMvcConfig the soul spring mvc config */ public SpringMvcClientBeanPostProcessor(final SoulSpringMvcConfig soulSpringMvcConfig) { ValidateUtils.validate(soulSpringMvcConfig); this.soulSpringMvcConfig = soulSpringMvcConfig; url = soulSpringMvcConfig.getAdminUrl() + \u0026amp;quot;/soul-client/springmvc-register\u0026amp;quot;; executorService = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026amp;lt;\u0026amp;gt;()); }  2. springmvc-register 接口逻辑 全局搜索 \u0026amp;ldquo;springmvc-register\u0026amp;rdquo;，找到 soul-admin 模块下的 SoulClientController，看到这里，对于经常写 CRUD 的我们是不是很熟悉？哈哈~\n// SoulClientController.java /** * Register spring mvc string. * * @param springMvcRegisterDTO the spring mvc register dto * @return the string */ @PostMapping(\u0026amp;quot;/springmvc-register\u0026amp;quot;) public String registerSpringMvc(@RequestBody final SpringMvcRegisterDTO springMvcRegisterDTO) { return soulClientRegisterService.registerSpringMvc(springMvcRegisterDTO); }  Service 层实现类：\n// SoulClientRegisterServiceImpl.java @Override @Transactional public String registerSpringMvc(final SpringMvcRegisterDTO dto) { if (dto.isRegisterMetaData()) { MetaDataDO exist = metaDataMapper.findByPath(dto.getPath()); if (Objects.isNull(exist)) { saveSpringMvcMetaData(dto); } } String selectorId = handlerSpringMvcSelector(dto); handlerSpringMvcRule(selectorId, dto); return SoulResultMessage.SUCCESS; }  dto.isRegisterMetaData() 这个是否注册元数据信息的判断，不知道什么时候用，存疑 //TODO，先往下走。\n2.1 先看看这个方法 handlerSpringMvcSelector，处理 Selector。 // SoulClientRegisterServiceImpl.java private String handlerSpringMvcSelector(final SpringMvcRegisterDTO dto) { String contextPath = dto.getContext(); // 根据 contextPath 到数据库里查询，是否已经注册过。 SelectorDO selectorDO = selectorService.findByName(contextPath); String selectorId; String uri = String.join(\u0026amp;quot;:\u0026amp;quot;, dto.getHost(), String.valueOf(dto.getPort())); if (Objects.isNull(selectorDO)) { // 还没有注册过 selectorId = registerSelector(contextPath, dto.getRpcType(), dto.getAppName(), uri); } else { // 已经注册过，业务系统重启了会到这里 selectorId = selectorDO.getId(); //update upstream String handle = selectorDO.getHandle(); String handleAdd; DivideUpstream addDivideUpstream = buildDivideUpstream(uri); SelectorData selectorData = selectorService.buildByName(contextPath); if (StringUtils.isBlank(handle)) { handleAdd = GsonUtils.getInstance().toJson(Collections.singletonList(addDivideUpstream)); } else { List\u0026amp;lt;DivideUpstream\u0026amp;gt; exist = GsonUtils.getInstance().fromList(handle, DivideUpstream.class); for (DivideUpstream upstream : exist) { if (upstream.getUpstreamUrl().equals(addDivideUpstream.getUpstreamUrl())) { return selectorId; } } exist.add(addDivideUpstream); handleAdd = GsonUtils.getInstance().toJson(exist); } selectorDO.setHandle(handleAdd); selectorData.setHandle(handleAdd); // update db …","date":1610928000,"description":"Soul Learning (2) HTTP Client Access Source Code Parsing","dir":"blog/soul_source_learning_02_http_client_register/","fuzzywordcount":2600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"88d8f86c6742874ad561e30befddbef6","permalink":"/en/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","publishdate":"2021-01-18T00:00:00Z","readingtime":6,"relpermalink":"/en/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","summary":"HTTP 用户接入 Soul 网关注册逻辑分析 1. 注册入口 HTTP 用户接入 Soul 网关时，会调用 soul-admin 一个接口，把需要 Soul 网关管理的接口注册，今天就具体看看到底干了点儿啥。 先看下","tags":["Soul"],"title":"Soul Gateway Learning (2) HTTP Client Access Source Code Parsing","type":"blog","url":"/en/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","wordcount":2520},{"author":"范金鹏","categories":"Soul","content":" HTTP 用户接入 Soul 网关注册逻辑分析 1. 注册入口 HTTP 用户接入 Soul 网关时，会调用 soul-admin 一个接口，把需要 Soul 网关管理的接口注册，今天就具体看看到底干了点儿啥。\n先看下调用的接口信息如下：\n// SpringMvcClientBeanPostProcessor.java /** * Instantiates a new Soul client bean post processor. * * @param soulSpringMvcConfig the soul spring mvc config */ public SpringMvcClientBeanPostProcessor(final SoulSpringMvcConfig soulSpringMvcConfig) { ValidateUtils.validate(soulSpringMvcConfig); this.soulSpringMvcConfig = soulSpringMvcConfig; url = soulSpringMvcConfig.getAdminUrl() + \u0026amp;quot;/soul-client/springmvc-register\u0026amp;quot;; executorService = new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026amp;lt;\u0026amp;gt;()); }  2. springmvc-register 接口逻辑 全局搜索 \u0026amp;ldquo;springmvc-register\u0026amp;rdquo;，找到 soul-admin 模块下的 SoulClientController，看到这里，对于经常写 CRUD 的我们是不是很熟悉？哈哈~\n// SoulClientController.java /** * Register spring mvc string. * * @param springMvcRegisterDTO the spring mvc register dto * @return the string */ @PostMapping(\u0026amp;quot;/springmvc-register\u0026amp;quot;) public String registerSpringMvc(@RequestBody final SpringMvcRegisterDTO springMvcRegisterDTO) { return soulClientRegisterService.registerSpringMvc(springMvcRegisterDTO); }  Service 层实现类：\n// SoulClientRegisterServiceImpl.java @Override @Transactional public String registerSpringMvc(final SpringMvcRegisterDTO dto) { if (dto.isRegisterMetaData()) { MetaDataDO exist = metaDataMapper.findByPath(dto.getPath()); if (Objects.isNull(exist)) { saveSpringMvcMetaData(dto); } } String selectorId = handlerSpringMvcSelector(dto); handlerSpringMvcRule(selectorId, dto); return SoulResultMessage.SUCCESS; }  dto.isRegisterMetaData() 这个是否注册元数据信息的判断，不知道什么时候用，存疑 //TODO，先往下走。\n2.1 先看看这个方法 handlerSpringMvcSelector，处理 Selector。 // SoulClientRegisterServiceImpl.java private String handlerSpringMvcSelector(final SpringMvcRegisterDTO dto) { String contextPath = dto.getContext(); // 根据 contextPath 到数据库里查询，是否已经注册过。 SelectorDO selectorDO = selectorService.findByName(contextPath); String selectorId; String uri = String.join(\u0026amp;quot;:\u0026amp;quot;, dto.getHost(), String.valueOf(dto.getPort())); if (Objects.isNull(selectorDO)) { // 还没有注册过 selectorId = registerSelector(contextPath, dto.getRpcType(), dto.getAppName(), uri); } else { // 已经注册过，业务系统重启了会到这里 selectorId = selectorDO.getId(); //update upstream String handle = selectorDO.getHandle(); String handleAdd; DivideUpstream addDivideUpstream = buildDivideUpstream(uri); SelectorData selectorData = selectorService.buildByName(contextPath); if (StringUtils.isBlank(handle)) { handleAdd = GsonUtils.getInstance().toJson(Collections.singletonList(addDivideUpstream)); } else { List\u0026amp;lt;DivideUpstream\u0026amp;gt; exist = GsonUtils.getInstance().fromList(handle, DivideUpstream.class); for (DivideUpstream upstream : exist) { if (upstream.getUpstreamUrl().equals(addDivideUpstream.getUpstreamUrl())) { return selectorId; } } exist.add(addDivideUpstream); handleAdd = GsonUtils.getInstance().toJson(exist); } selectorDO.setHandle(handleAdd); selectorData.setHandle(handleAdd); // update db …","date":1610928000,"description":"Soul网关学习(2-3)Http客户端接入源码解析","dir":"blog/soul_source_learning_02_http_client_register/","fuzzywordcount":2600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"88d8f86c6742874ad561e30befddbef6","permalink":"/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","publishdate":"2021-01-18T00:00:00Z","readingtime":6,"relpermalink":"/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","summary":"HTTP 用户接入 Soul 网关注册逻辑分析 1. 注册入口 HTTP 用户接入 Soul 网关时，会调用 soul-admin 一个接口，把需要 Soul 网关管理的接口注册，今天就具体看看到底干了点儿啥。 先看下","tags":["Soul"],"title":"Soul网关学习(2-3)Http客户端接入源码解析","type":"blog","url":"/blog/soul_source_learning_02_http_client_register/source-learning-02-http-client-register/","wordcount":2520},{"author":"jipeng","categories":"Soul","content":" Divide 插件如何转发http请求 先来设想一下，网关如果收到了一个请求http://xxx.com/openapi/appname/order/findById?id=3，那么怎么将请求转发给对应的业务？\n可以想象一下大概是这几个步骤：\n 1.解析url 2.查看配置文件，看这个url是对应于哪个业务线 3.读配置文件，获取该业务线在网关注册的所有api列表 4.判断该用户的这个api请求在不在业务的api列表里面 5.进行相关的鉴权操作（用户AK/SK鉴权、用户Quota/QPS有没有超） 6.如果网关有负载均衡功能，那么需要获取业务具体给API配置的负载均衡策略 7.网关向具体的业务API发起请求 8.网关将收到的业务API的response发送给用户  这篇笔记主要来学习一下suol网关是怎么转发http请求的。\n先看一下官方文档的相关介绍http用户、Divide插件\n官方文档里面介绍到，如果网关需要支持http转发，那么需要在网关的pom里面有以下依赖：\n \u0026amp;lt;!--if you use http proxy start this--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-plugin-divide\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-plugin-httpclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--if you use http proxy end this--\u0026amp;gt;  那么可以知道http请求的代理与plugin-divide,plugin-httpclient这两个插件有关。\n插件链 官方文档中说到divide这个插件是实现http请求代理的核心，下面看一下soul-plugin/soul-plugin-divide这个模块的代码，可以看到有一个DividePlugin类，继承自AbstractPlugin，而AbstractPlugin实现了SoulPlugin接口\n可以看到SoulPlugin是DividePlugin的父类，那么猜测一下SoulPlugin是所有插件的父类。全局搜索一下SoulPlugin果然如此，它是诸多插件的父类。\n在全局搜索SoulPlugin的时候，发现soul-web/src/main/java/org/dromara/soul/web/handler里有一个类SoulWebHandler里面有一个属性是List\u0026amp;lt;SoulPlugin\u0026amp;gt;，猜测SoulWebHandler可以操作多个插件。\n看一下SoulWebHandler的继承关系图，发现它是继承了WebHandler，而WebHandler是spring框架里面的一个接口。\n由于对WebFlux不了解，上网快速搜索了一下WebHandler，得知这是WebFlux里面一个很重要的东西，它提供了一套通用的http请求处理方案。\n而soul网关的源码里面，自己实现了一个实现了WebHandler接口的SoulWebHandler类，无疑是希望框架使用soul实现的这套东西来处理请求。\n在soul-web/src/main/java/org/dromara/soul/web/configuration里的SoulConfiguration类，它在类头上声明了注解@Configuration，表明它是一个配置。SoulConfiguration类里面向spring容器注入了一个名为webHandler的bean，该bean是SoulWebHandler类型的。Application会在启动的时候扫描被@Configuration注解的类，所以通过以下代码，SoulWebHandler就被注入到spring容器中去了。\n @Bean(\u0026amp;quot;webHandler\u0026amp;quot;) public SoulWebHandler soulWebHandler(final ObjectProvider\u0026amp;lt;List\u0026amp;lt;SoulPlugin\u0026amp;gt;\u0026amp;gt; plugins) { List\u0026amp;lt;SoulPlugin\u0026amp;gt; pluginList = plugins.getIfAvailable(Collections::emptyList); final List\u0026amp;lt;SoulPlugin\u0026amp;gt; soulPlugins = pluginList.stream() .sorted(Comparator.comparingInt(SoulPlugin::getOrder)).collect(Collectors.toList()); soulPlugins.forEach(soulPlugin -\u0026amp;gt; log.info(\u0026amp;quot;load plugin:[{}] [{}]\u0026amp;quot;, soulPlugin.named(), soulPlugin.getClass().getName())); return new SoulWebHandler(soulPlugins); }  初始化SoulWebHandler的时候，将排好序的插件传入其构造函数中。各个插件都有一个order属性，可以根据这个属性来对插件进行优先级排序。以DividePlugin为例，看下它的order属性是从一个枚举类里面来的。\n @Override public int getOrder() { return PluginEnum.DIVIDE.getCode(); }  而各个插件的order的具体值是在soul-common/src/main/java/org/dromara/soul/common/enums/PluginEnums这个枚举类里面定义的。PluginEnum的code即为各个插件的order。\n插件的顺序为：global -\u0026amp;gt; sign -\u0026amp;gt; waf -\u0026amp;gt; rate-limiter -\u0026amp;gt; hystrix -\u0026amp;gt; resilience4j -\u0026amp;gt; divide -\u0026amp;gt; webClient -\u0026amp;gt; …………\n每次有一个请求的时候，WebHandler即SoulWebHandler的handle方法都会被调用，该方法里面最主要的就是初始化了一个插件链DefaultSoulPluginChain，并执行该插件链。\n看一下DefaultSoulPluginChain …","date":1610841600,"description":"How Does The Divide Plugin Forward HTTP Requests","dir":"blog/soul_source_larning_02_divide_plugin_source/","fuzzywordcount":3100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"eed9950bbc3c5eda5e2e5f6ea72d9b8d","permalink":"/en/blog/soul_source_larning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","publishdate":"2021-01-17T00:00:00Z","readingtime":7,"relpermalink":"/en/blog/soul_source_larning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","summary":"Divide 插件如何转发http请求 先来设想一下，网关如果收到了一个请求http://xxx.com/openapi/appname/order/fi","tags":["Soul"],"title":"Soul Learning(2) How Does The Divide Plugin Forward Http Requests","type":"blog","url":"/en/blog/soul_source_larning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","wordcount":3042},{"author":"季鹏","categories":"Soul","content":" Divide 插件如何转发http请求 先来设想一下，网关如果收到了一个请求http://xxx.com/openapi/appname/order/findById?id=3，那么怎么将请求转发给对应的业务？\n可以想象一下大概是这几个步骤：\n 1.解析url 2.查看配置文件，看这个url是对应于哪个业务线 3.读配置文件，获取该业务线在网关注册的所有api列表 4.判断该用户的这个api请求在不在业务的api列表里面 5.进行相关的鉴权操作（用户AK/SK鉴权、用户Quota/QPS有没有超） 6.如果网关有负载均衡功能，那么需要获取业务具体给API配置的负载均衡策略 7.网关向具体的业务API发起请求 8.网关将收到的业务API的response发送给用户  这篇笔记主要来学习一下suol网关是怎么转发http请求的。\n先看一下官方文档的相关介绍http用户、Divide插件\n官方文档里面介绍到，如果网关需要支持http转发，那么需要在网关的pom里面有以下依赖：\n \u0026amp;lt;!--if you use http proxy start this--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-plugin-divide\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-plugin-httpclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--if you use http proxy end this--\u0026amp;gt;  那么可以知道http请求的代理与plugin-divide,plugin-httpclient这两个插件有关。\n插件链 官方文档中说到divide这个插件是实现http请求代理的核心，下面看一下soul-plugin/soul-plugin-divide这个模块的代码，可以看到有一个DividePlugin类，继承自AbstractPlugin，而AbstractPlugin实现了SoulPlugin接口\n可以看到SoulPlugin是DividePlugin的父类，那么猜测一下SoulPlugin是所有插件的父类。全局搜索一下SoulPlugin果然如此，它是诸多插件的父类。\n在全局搜索SoulPlugin的时候，发现soul-web/src/main/java/org/dromara/soul/web/handler里有一个类SoulWebHandler里面有一个属性是List\u0026amp;lt;SoulPlugin\u0026amp;gt;，猜测SoulWebHandler可以操作多个插件。\n看一下SoulWebHandler的继承关系图，发现它是继承了WebHandler，而WebHandler是spring框架里面的一个接口。\n由于对WebFlux不了解，上网快速搜索了一下WebHandler，得知这是WebFlux里面一个很重要的东西，它提供了一套通用的http请求处理方案。\n而soul网关的源码里面，自己实现了一个实现了WebHandler接口的SoulWebHandler类，无疑是希望框架使用soul实现的这套东西来处理请求。\n在soul-web/src/main/java/org/dromara/soul/web/configuration里的SoulConfiguration类，它在类头上声明了注解@Configuration，表明它是一个配置。SoulConfiguration类里面向spring容器注入了一个名为webHandler的bean，该bean是SoulWebHandler类型的。Application会在启动的时候扫描被@Configuration注解的类，所以通过以下代码，SoulWebHandler就被注入到spring容器中去了。\n @Bean(\u0026amp;quot;webHandler\u0026amp;quot;) public SoulWebHandler soulWebHandler(final ObjectProvider\u0026amp;lt;List\u0026amp;lt;SoulPlugin\u0026amp;gt;\u0026amp;gt; plugins) { List\u0026amp;lt;SoulPlugin\u0026amp;gt; pluginList = plugins.getIfAvailable(Collections::emptyList); final List\u0026amp;lt;SoulPlugin\u0026amp;gt; soulPlugins = pluginList.stream() .sorted(Comparator.comparingInt(SoulPlugin::getOrder)).collect(Collectors.toList()); soulPlugins.forEach(soulPlugin -\u0026amp;gt; log.info(\u0026amp;quot;load plugin:[{}] [{}]\u0026amp;quot;, soulPlugin.named(), soulPlugin.getClass().getName())); return new SoulWebHandler(soulPlugins); }  初始化SoulWebHandler的时候，将排好序的插件传入其构造函数中。各个插件都有一个order属性，可以根据这个属性来对插件进行优先级排序。以DividePlugin为例，看下它的order属性是从一个枚举类里面来的。\n @Override public int getOrder() { return PluginEnum.DIVIDE.getCode(); }  而各个插件的order的具体值是在soul-common/src/main/java/org/dromara/soul/common/enums/PluginEnums这个枚举类里面定义的。PluginEnum的code即为各个插件的order。\n插件的顺序为：global -\u0026amp;gt; sign -\u0026amp;gt; waf -\u0026amp;gt; rate-limiter -\u0026amp;gt; hystrix -\u0026amp;gt; resilience4j -\u0026amp;gt; divide -\u0026amp;gt; webClient -\u0026amp;gt; …………\n每次有一个请求的时候，WebHandler即SoulWebHandler的handle方法都会被调用，该方法里面最主要的就是初始化了一个插件链DefaultSoulPluginChain，并执行该插件链。\n看一下DefaultSoulPluginChain …","date":1610841600,"description":"Soul网关学习(2-2)divide插件源码解析","dir":"blog/soul_source_learning_02_divide_plugin_source/","fuzzywordcount":3100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f14cba28f1c64bfdd939ccbe2e69de39","permalink":"/blog/soul_source_learning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","publishdate":"2021-01-17T00:00:00Z","readingtime":7,"relpermalink":"/blog/soul_source_learning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","summary":"Divide 插件如何转发http请求 先来设想一下，网关如果收到了一个请求http://xxx.com/openapi/appname/order/fi","tags":["Soul"],"title":"Soul网关学习(2-2)Http代理之divide插件源码解析","type":"blog","url":"/blog/soul_source_learning_02_divide_plugin_source/source-learning-02-divide-plugin-source/","wordcount":3042},{"author":"yuanjie","categories":"Soul","content":" Divide 插件使用 一、启动项目 先启动soul-bootstrap（9195）、soul-admin（9095）两个模块，我们通过bootstrap配置文件可以看到，两者是通过WebSocket协议进行数据同步：\n通过bootstrap日志也可以看到：\n所谓的数据同步是指将soul-admin中配置的数据，同步到soul集群中的JVM内存里面，是网关高性能的关键。\n我们启动两个项目之后就可以通过后台管理系统测试divide插件了。\n二、divide插件介绍 divide插件是网关处理http协议请求的核心处理插件，也是soul唯一默认开启的插件：\n我们可以想象一下网关到底是做什么的，去猜测一下处理http请求的divide插件可能具备哪些功能呢？\n首先，作为微服务网关，它的背后一定存在多条业务线的分布式微服务集群，而网关作为所有服务的统一入口，必须具备的能力就是流量分发/路由/负载均衡等，而divide这个单词顾名思义就是分配、分发的意思，所以我们可以猜测divide插件就是对http请求进行各种规则的路由转发，这也是网关最基础的能力。\n我们打开管理界面上的插件列表，可以看到所有插件都是由两部分组成：选择器（selector）和选择器规则。\n插件化设计思想是soul网关最核心的设计思想，而选择器和规则这两个概念也是soul网关的灵魂所在，理论上来说，我们掌握好它，就能对任何接入网关的流量进行管理。\n一个插件有多个选择器，一个选择器对应多种规则。选择器相当于是对流量的第一次筛选，规则就是最终的筛选。\n选择器 * **名称**：为你的选择器起一个容易分辨的名字 * **类型**：custom flow 是自定义流量。full flow 是全流量。自定义流量就是请求会走你下面的匹配方式与条件。全流量则不走。 * **匹配方式**：and 或者or 是指下面多个条件是按照and 还是or的方式来组合。 * **条件**： * uri：是指你根据uri的方式来筛选流量，match的方式支持模糊匹配（/**） * header：是指根据请求头里面的字段来筛选流量。 * query：是指根据uri的查询条件来进行筛选流量。 * ip：是指根据你请求的真实ip，来筛选流量。 * host：是指根据你请求的真实host，来筛选流量。 * post：建议不要使用。 * 条件匹配： * match : 模糊匹配，建议和uri条件搭配，支持 restful风格的匹配。（/test/**） * = : 前后值相等，才能匹配。 * regEx : 正则匹配，表示前面一个值去匹配后面的正则表达式。 * like ：字符串模糊匹配。 * **是否开启**：打开才会生效 * **打印日志**：打开的时候，当匹配上的时候，会打印匹配日志。 * **执行顺序**：当多个选择器的时候，执行顺序小的优先执行。  选择器规则 可以看到，规则的配置和选择器类似，可以理解为更细粒度的自定义配置。\n三、divide插件使用 废话少说，我们直接运行soul提供的examples模块来演示divide插件。\n注意，我们最终运行的是soul-examples-http模块。配置文件可以使用默认的，也可以自定义contextPath和appName，如上图。\n我们需要注意，contextPath这个属性非常重要，相当于是我们所有http请求的namespace，和选择器一一对齐。一般来说，我们可以配置一个业务对应一个contextPath，一个业务下面配置相同contextPath的多个服务实例会自动映射到同一个选择器进行负载均衡。\n我们启动端口为8188的这个进程后，可以发现管理控制台divide插件列表中自动配置了这个实例对应的选择器、规则：\n可以看到我启动的这个8188项目地址自动注册上去了：\n测试网关路由 通过postman先测试不经过网关转发：\nhttp://localhost:8188/order/findById?id=1  然后再测试通过网关转发到这个接口：\nhttp://localhost:9195/my-http/order/findById?id=1  看日志发现确实经过了网关转发到了8188接口地址：\n测试负载均衡 我们修改端口为8189，启动第二个进程。\n注意IDEA需要取消 Single instance only 的限制：\n我们再进入管理控制台，发现my-http选择器下出现两个配置地址：\n此时我们继续测试，发现负载均衡策略确实生效了：\n今天只是演示了divide插件最基础的配置，还有其他各种规则配置后面都可以试一试~\n","date":1610755200,"description":"Soul Learning(2) Use Divide Plugin","dir":"blog/soul_source_learning_02_divide_plugin/","fuzzywordcount":1800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"363f7dd24512031e47f1b3d95344eb6b","permalink":"/en/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","publishdate":"2021-01-16T00:00:00Z","readingtime":4,"relpermalink":"/en/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","summary":"Divide 插件使用 一、启动项目 先启动soul-bootstrap（9195）、soul-admin（9095）两个模块，我们通过bootstrap配","tags":["Soul"],"title":"Soul Learning(2) Use Divide Plugin","type":"blog","url":"/en/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","wordcount":1717},{"author":"袁杰","categories":"Soul","content":" Divide 插件使用 一、启动项目 先启动soul-bootstrap（9195）、soul-admin（9095）两个模块，我们通过bootstrap配置文件可以看到，两者是通过WebSocket协议进行数据同步：\n通过bootstrap日志也可以看到：\n所谓的数据同步是指将soul-admin中配置的数据，同步到soul集群中的JVM内存里面，是网关高性能的关键。\n我们启动两个项目之后就可以通过后台管理系统测试divide插件了。\n二、divide插件介绍 divide插件是网关处理http协议请求的核心处理插件，也是soul唯一默认开启的插件：\n我们可以想象一下网关到底是做什么的，去猜测一下处理http请求的divide插件可能具备哪些功能呢？\n首先，作为微服务网关，它的背后一定存在多条业务线的分布式微服务集群，而网关作为所有服务的统一入口，必须具备的能力就是流量分发/路由/负载均衡等，而divide这个单词顾名思义就是分配、分发的意思，所以我们可以猜测divide插件就是对http请求进行各种规则的路由转发，这也是网关最基础的能力。\n我们打开管理界面上的插件列表，可以看到所有插件都是由两部分组成：选择器（selector）和选择器规则。\n插件化设计思想是soul网关最核心的设计思想，而选择器和规则这两个概念也是soul网关的灵魂所在，理论上来说，我们掌握好它，就能对任何接入网关的流量进行管理。\n一个插件有多个选择器，一个选择器对应多种规则。选择器相当于是对流量的第一次筛选，规则就是最终的筛选。\n选择器 * **名称**：为你的选择器起一个容易分辨的名字 * **类型**：custom flow 是自定义流量。full flow 是全流量。自定义流量就是请求会走你下面的匹配方式与条件。全流量则不走。 * **匹配方式**：and 或者or 是指下面多个条件是按照and 还是or的方式来组合。 * **条件**： * uri：是指你根据uri的方式来筛选流量，match的方式支持模糊匹配（/**） * header：是指根据请求头里面的字段来筛选流量。 * query：是指根据uri的查询条件来进行筛选流量。 * ip：是指根据你请求的真实ip，来筛选流量。 * host：是指根据你请求的真实host，来筛选流量。 * post：建议不要使用。 * 条件匹配： * match : 模糊匹配，建议和uri条件搭配，支持 restful风格的匹配。（/test/**） * = : 前后值相等，才能匹配。 * regEx : 正则匹配，表示前面一个值去匹配后面的正则表达式。 * like ：字符串模糊匹配。 * **是否开启**：打开才会生效 * **打印日志**：打开的时候，当匹配上的时候，会打印匹配日志。 * **执行顺序**：当多个选择器的时候，执行顺序小的优先执行。  选择器规则 可以看到，规则的配置和选择器类似，可以理解为更细粒度的自定义配置。\n三、divide插件使用 废话少说，我们直接运行soul提供的examples模块来演示divide插件。\n注意，我们最终运行的是soul-examples-http模块。配置文件可以使用默认的，也可以自定义contextPath和appName，如上图。\n我们需要注意，contextPath这个属性非常重要，相当于是我们所有http请求的namespace，和选择器一一对齐。一般来说，我们可以配置一个业务对应一个contextPath，一个业务下面配置相同contextPath的多个服务实例会自动映射到同一个选择器进行负载均衡。\n我们启动端口为8188的这个进程后，可以发现管理控制台divide插件列表中自动配置了这个实例对应的选择器、规则：\n可以看到我启动的这个8188项目地址自动注册上去了：\n测试网关路由 通过postman先测试不经过网关转发：\nhttp://localhost:8188/order/findById?id=1  然后再测试通过网关转发到这个接口：\nhttp://localhost:9195/my-http/order/findById?id=1  看日志发现确实经过了网关转发到了8188接口地址：\n测试负载均衡 我们修改端口为8189，启动第二个进程。\n注意IDEA需要取消 Single instance only 的限制：\n我们再进入管理控制台，发现my-http选择器下出现两个配置地址：\n此时我们继续测试，发现负载均衡策略确实生效了：\n今天只是演示了divide插件最基础的配置，还有其他各种规则配置后面都可以试一试~\n","date":1610755200,"description":"Soul网关学习(2-1)divide插件使用","dir":"blog/soul_source_learning_02_divide_plugin/","fuzzywordcount":1800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"363f7dd24512031e47f1b3d95344eb6b","permalink":"/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","publishdate":"2021-01-16T00:00:00Z","readingtime":4,"relpermalink":"/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","summary":"Divide 插件使用 一、启动项目 先启动soul-bootstrap（9195）、soul-admin（9095）两个模块，我们通过bootstrap配","tags":["Soul"],"title":"Soul网关学习(2-1)Http代理之divide插件使用","type":"blog","url":"/blog/soul_source_learning_02_divide_plugin/source-learning-02-divide-plugin/","wordcount":1717},{"author":"chenxi","categories":"Soul","content":" Analysis of soul (1) Set up soul environment  soul is a High-Performance Java API Gateway\nGitHub：https://github.com/dromara/soul\ndocument：https://dromara.org/zh-cn/docs/soul/soul.html\n 1. Prepare source code 1.1. Fork dromara/soul repository to my github cchenxi/soul 1.2. Clone the repository git clone https://github.com/cchenxi/soul.git  1.3.Open the source code with idea 1.4. Compile the soul source code You can compile the project as follows.\nmvn clean package install -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -Drat.skip=true -Dcheckstyle.skip=true  2. Startup soul 2.1. Startup soul-admin module  soul-admin is the management system for soul.\n Choose to use MySQL to storage gateway data and modify the datasource config.\nRun org.dromara.soul.admin.SoulAdminBootstrap.\nWhen success, please visit the website http://localhost:9095/, then jump to the login page, and input the corresponding user name and password to log in.\nThe user name is admin and the password is 123456.\n2.2. Startup soul-bootstrap module  soul-bootstrap is the core of soul.\n Check the configuration of soul-bootstrap.\nPlease make sure the ip and the port has been configured for soul-admin.\nIf the console output as follows, it means the startup is successful.\n2021-01-14 15:01:15.832 INFO 17943 --- [ main] b.s.s.d.w.WebsocketSyncDataConfiguration : you use websocket sync soul data....... 2021-01-14 15:01:15.924 INFO 17943 --- [ main] o.d.s.p.s.d.w.WebsocketSyncDataService : websocket connection is successful..... 2021-01-14 15:01:16.113 INFO 17943 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 2 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; log4j:WARN No appenders could be found for logger (com.alibaba.dubbo.common.logger.LoggerFactory). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 2021-01-14 15:01:17.150 INFO 17943 --- [ main] o.s.b.web.embedded.netty.NettyWebServer : Netty started on port(s): 9195 2021-01-14 15:01:17.154 INFO 17943 --- [ main] o.d.s.b.SoulBootstrapApplication : Started SoulBootstrapApplication in 5.508 seconds (JVM running for 6.762)  3. Test  Add the soul-examples module to soul\u0026amp;rsquo;s pom.xml for test.\n 3.1. Startup an HTTP backend service Startup soul-examples-http\nYou can see the dependency in soul-examples-http\u0026amp;rsquo;s pom.xml.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-client-springmvc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${soul.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Configure the application.yml\nsoul: http: adminUrl: http://localhost:9095 port: 8188 contextPath: /http appName: http full: false  If soul.http.full=false, you need to add the @SoulSpringMvcClient annotation in controller or controller method.\n3.1.1. Test the service Visit http://localhost:8188/test/findByUserId?userId=1 and the result as follows.\n3.1.2. Test forward HTTP request Visit …","date":1610668800,"description":"Soul Learning(1) Environment Configuration","dir":"blog/soul_source_learning_01/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0890ce964d07f25e6130e0edcd52093e","permalink":"/en/blog/soul_source_learning_01/source-learning-01/","publishdate":"2021-01-15T00:00:00Z","readingtime":5,"relpermalink":"/en/blog/soul_source_learning_01/source-learning-01/","summary":"Analysis of soul (1) Set up soul environment  soul is a High-Performance Java API Gateway\nGitHub：https://github.com/dromara/soul\ndocument：https://dromara.org/zh-cn/docs/soul/soul.html\n 1. Prepare source code 1.1. Fork dromara/soul repository to my github cchenxi/soul 1.2. Clone the repository git clone https://github.com/cchenxi/soul.git  1.3.Open the source code with idea 1.4. Compile the soul source code You can compile the project as follows.\nmvn clean package install -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -Drat.skip=true -Dcheckstyle.skip=true  2. Startup soul 2.","tags":["Soul"],"title":"Soul Learning(1) Environment Configuration","type":"blog","url":"/en/blog/soul_source_learning_01/source-learning-01/","wordcount":933},{"author":"陈曦","categories":"Soul","content":" Soul源码分析（1） 环境配置  soul is a High-Performance Java API Gateway\nGitHub：https://github.com/dromara/soul\n官方文档：https://dromara.org/zh-cn/docs/soul/soul.html\n 1. 源代码准备 1.1. fork dromara/soul源代码至自己的仓库cchenxi/soul 1.2. clone自己仓库中的soul源代码至本地 git clone https://github.com/cchenxi/soul.git  1.3.使用idea打开soul源代码 1.4.编译soul源代码 执行以下maven命令，等待编译完成\nmvn clean package install -Dmaven.test.skip=true -Dmaven.javadoc.skip=true -Drat.skip=true -Dcheckstyle.skip=true  2. 启动 soul 2.1. 启动soul-admin模块  soul-admin是soul网关的后台管理系统\n 选择使用MySQL数据库存储网关数据，修改数据源配置为自己的数据库配置。\n运行启动类 org.dromara.soul.admin.SoulAdminBootstrap。\n启动成功后，访问地址 http://localhost:9095/ ，跳转到登录页↓\n使用用户名admin，密码 123456 登录。\n2.2. 启动soul-bootstrap模块  soul-bootstrap是网关系统的核心\n 检查soul-bootstrap的配置\n这里需要配置成 soul-admin的ip和端口\n控制台输出如下内容表示 soul-bootstrap启动成功\n2021-01-14 15:01:15.832 INFO 17943 --- [ main] b.s.s.d.w.WebsocketSyncDataConfiguration : you use websocket sync soul data....... 2021-01-14 15:01:15.924 INFO 17943 --- [ main] o.d.s.p.s.d.w.WebsocketSyncDataService : websocket connection is successful..... 2021-01-14 15:01:16.113 INFO 17943 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 2 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; log4j:WARN No appenders could be found for logger (com.alibaba.dubbo.common.logger.LoggerFactory). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 2021-01-14 15:01:17.150 INFO 17943 --- [ main] o.s.b.web.embedded.netty.NettyWebServer : Netty started on port(s): 9195 2021-01-14 15:01:17.154 INFO 17943 --- [ main] o.d.s.b.SoulBootstrapApplication : Started SoulBootstrapApplication in 5.508 seconds (JVM running for 6.762)  3. 测试http请求转发  为了方便测试，把soul-examples模块添加到soul的pom里\n 3.1. 启动一个服务 启动soul-examples-http项目\nsoul-examples-http的pom中引入了依赖\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;soul-spring-boot-starter-client-springmvc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${soul.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  在 application.yml中配置\nsoul: http: adminUrl: http://localhost:9095 port: 8188 contextPath: /http appName: http full: false  如果soul.http.full=false，则需要在具体的http接口上配置 @SoulSpringMvcClient 注解\n3.1.1. 测试http服务 执行http请求 http://localhost:8188/test/findByUserId?userId=1 结果如下图\n3.1.2. 测试网关转发 执行http请求 http://localhost:9195/http/test/findByUserId?userId=1 结果如下图\n在soul-bootstrap的控制台中输出如下信息\n2021-01-14 20:42:57.123 INFO 29812 --- [work-threads-11] o.d.soul.plugin.base.AbstractSoulPlugin : divide selector success match , selector name :/http 2021-01-14 20:42:57.125 INFO 29812 --- [work-threads-11] o.d.soul.plugin.base.AbstractSoulPlugin : divide selector success match , selector name :/http/test/** 2021-01-14 20:42:57.126 INFO 29812 --- [work-threads-11] o.d.s.plugin.httpclient.WebClientPlugin : The request urlPath is http://172.27.121.155:8188/test/findByUserId?userId=1, retryTimes is 0  可以观察到网关可以将请求正常转发。\n3.2. 启动两个服务模拟负载均衡 勾选 Allow parallel run，修改端口为8189，再次启动soul-examples-http …","date":1610668800,"description":"Soul网关学习(1)环境配置","dir":"blog/soul_source_learning_01/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0890ce964d07f25e6130e0edcd52093e","permalink":"/blog/soul_source_learning_01/source-learning-01/","publishdate":"2021-01-15T00:00:00Z","readingtime":4,"relpermalink":"/blog/soul_source_learning_01/source-learning-01/","summary":"Soul源码分析（1） 环境配置 soul is a High-Performance Java API Gateway GitHub：https://github.com/dromara/soul 官方文档：https","tags":["Soul"],"title":"Soul网关学习(1)环境配置","type":"blog","url":"/blog/soul_source_learning_01/source-learning-01/","wordcount":1625},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：Service Mesh Webinar#2：基于 MOSN 和 Istio Service Mesh 的服务治理实践\n 活动时间：7 月 22 日周四晚 8 点\n 活动形式：线上直播\n 报名方式：戳这里\n  介绍 Service Mesh Webinar Service Mesh Webinar 是由 ServiceMesher 社区和 CNCF 联合发起的线上直播活动，活动将不定期举行，邀请社区成员为大家带来 Service Mesh 领域的知识和实践分享。\nService Mesh Webinar#2 Service Mesh Webinar#2，邀请有米科技高级后端工程师姚昌宇，带来分享《基于 MOSN 和 Istio Service Mesh 的服务治理实践》。\n本期嘉宾将为大家分享从关注 ServiceMesher 社区，到参与 MOSN 开源共建的心路历程，包括 Service Mesh 技术的相关介绍、如何参与社区共建以及如何将 MOSN 结合 Istio 根据实际场景落地的示范。本期分享可以使你对 Service Mesh 技术，以及如何落地有更多的认识。\n分享主题：\n《基于 MOSN 和 Istio Service Mesh 的服务治理实践》\n分享嘉宾：\n姚昌宇 有米科技高级后端工程师、MOSN committer。多年后端开发经验、云原生爱好者。目前负责公司内部数据和算法能力接口的服务治理相关的开发工作，2018年起关注并参与 ServiceMesher 社区发起的各种活动，近期参与 MOSN v0.14.0 版本功能的开发。\n直播时间：2020年5月28日（周四）20:00-21:00\n大纲：\n Service Mesh 带来的红利 参与 MOSN 开源共建给我带来的收获 如何参与 MOSN 开源共建  找到组织：进入 MOSN 开发者群 熟悉 MOSN \u0026amp;amp; 搭建本地调试环境：现场演示（http、grpc 服务流量控制演示，指标收集，请求跟踪演示） 发现优化点/参与 Roadmap  感悟总结，以及 MOSN 近况\u0026amp;amp;未来远景介绍  用户收获：\n 了解如何参与到 MOSN 开源社区共建中； 了解如何使用 MOSN 在 Istio 场景下的服务治理实践 ; 了解 MOSN 新版本的功能以及未来远景； 结合 Istio 各个场景的 Demo，分享 MSON 的多协议/私有协议实现；  ","date":1594710000,"description":"7 月 22 日周四晚 8 点，Service Mesh Webinar#2 线上直播。","dir":"activities/service-mesh-webinar-2/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"856529bdb9666c8bad4c59a80444662f","permalink":"/activities/service-mesh-webinar-2/","publishdate":"2020-07-14T15:00:00+08:00","readingtime":2,"relpermalink":"/activities/service-mesh-webinar-2/","summary":"概要 活动主题：Service Mesh Webinar#2：基于 MOSN 和 Istio Service Mesh 的服务治理实践 活动时间：7 月 22 日周四晚 8 点 活动形式：线上直播 报名方式：戳这里","tags":["MOSN","Service Mesh Webinar"],"title":"Service Mesh Webinar#2：基于 MOSN 和 Istio Service Mesh 的服务治理实践","type":"activities","url":"/activities/service-mesh-webinar-2/","wordcount":657},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#17：网络通信框架 SOFABolt 的功能介绍及协议框架解析\n 活动时间：7 月 2 日周四晚 7 点\n 活动形式：线上直播\n 报名方式：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#16：不得不说的云原生隔离性 SOFABolt 是蚂蚁金服开源的一套基于 Netty 实现的，轻量、易用、高性能、易扩展的网络通信框架。在蚂蚁金服的分布式技术体系下，我们有大量的技术产品都需要在内网进行节点间的通信。每个产品都需要考虑高吞吐、高并发的通信，私有协议设计、连接管理、兼容性等问题。\n为了将开发人员从通信框架的实现中解放出来，专注于自己产品的能力建设上，我们将在微服务与消息中间件在网络通信上解决的问题以及积累的经验进行了总结，设计并实现了 SOFABolt。本期分享将介绍 SOFABolt 的基本功能和部分实现原理，并介绍协议框架的实现。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：30315793（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1265\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 不得不说的云原生隔离性 丞一 SOFABolt 开源负责人\n你将收获  了解 SOFABolt 的基础使用及 SOFABolt 部分功能的实现原理； 了解 SOFABolt 协议框架的设计以及如何拓展实现自定义私有协议； 了解如何设计一个通信框架；  嘉宾  SOFAGirl 主持人 丞一 SOFABolt 开源负责人  ","date":1591945200,"description":"7 月 2 日周四晚 7 点，线上直播第 17 期。","dir":"activities/sofa-channel-17/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6b021a35ee168e2bc4cb3350c6828d68","permalink":"/activities/sofa-channel-17/","publishdate":"2020-06-12T15:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-17/","summary":"概要 活动主题：SOFAChannel#17：网络通信框架 SOFABolt 的功能介绍及协议框架解析 活动时间：7 月 2 日周四晚 7 点 活动形式：线上直播 报名方式：戳","tags":["SOFAChannel","SOFABolt"],"title":"SOFAChannel#17：网络通信框架 SOFABolt 的功能介绍及协议框架解析","type":"activities","url":"/activities/sofa-channel-17/","wordcount":574},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：Service Mesh Webinar#1：多点生活在 Service Mesh 上的实践——Istio + MOSN 在 Dubbo 场景下的探索之路\n 活动时间：5 月 28 日周四晚 8 点\n 活动形式：线上直播\n 报名方式：戳这里\n  介绍 Service Mesh Webinar Service Mesh Webinar 是由 ServiceMesher 社区和 CNCF 联合发起的线上直播活动，活动将不定期举行，邀请社区成员为大家带来 Service Mesh 领域的知识和实践分享。\nService Mesh Webinar#1 Service Mesh Webinar#1，邀请多点生活平台架构组研发工程师陈鹏，带来分享《多点生活在 Service Mesh 上的实践——Istio + MOSN 在 Dubbo 场景下的探索之路》。\n随着多点生活的业务发展，传统微服务架构的面临升级困难的问题。在云原生的环境下，Service Mesh 能给我们带来什么好处。如何使用社区解决方案兼容现有业务场景，落地成符合自己的 Service Mesh 成为一个难点。服务之间主要通过 Dubbo 交互，本次分享将探索 Istio + MOSN 在 Dubbo 场景下的改造方案。\n分享主题：\n《多点生活在 Service Mesh 上的实践——Istio + MOSN 在 Dubbo 场景下的探索之路》\n分享嘉宾：\n陈鹏，多点生活平台架构组研发工程师，开源项目与云原生爱好者，有多年的网上商城、支付系统相关开发经验，2019年至今从事云原生和 Service Mesh 相关开发工作。\n直播时间：2020年5月28日（周四）20:00-21:00\n解决思路:\n从 MCP、Pilot、xDS、MOSN 技术，对 Service Mesh 的可切入点分析。\n成果：\n结合现有业务场景和可切入点，明确需要修改的场景，制定符合自己业务场景的 Service Mesh 落地方案，介绍多点生活在 Dubbo 案例的探索及改造方案。\n大纲：\n 传统微服务架构与 Service Mesh 架构  传统微服务架构在多点遇到的痛点 Service Mesh 架构能带来的福利  Istio 技术点介绍 在 Dubbo 场景下的改造分析  对比 MOSN 和 Envoy 对现有场景的支持 Istio+MOSN 和 Istio+Envoy 在 Dubbo 场景下如何改造  MOSN + Istio 具体实现探索  MOSN 配置文件介绍、从一个流量进来到转发到具体的远端的流程分析 Provider 配置信息如何下发到 Sidecar 从多点现在的实际场景对现有的 Dubbo 改造方案  Demo 演示  ","date":1589958000,"description":"5 月 28 日周四晚 8 点，Service Mesh Webinar#1 线上直播。","dir":"activities/service-mesh-webinar-1/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d146e6a876059c0f8da819f408add6fa","permalink":"/activities/service-mesh-webinar-1/","publishdate":"2020-05-20T15:00:00+08:00","readingtime":2,"relpermalink":"/activities/service-mesh-webinar-1/","summary":"概要 活动主题：Service Mesh Webinar#1：多点生活在 Service Mesh 上的实践——Istio + MOSN 在 Dubbo 场景下的探索之路 活动时间：5 月 28 日周四晚 8 点 活","tags":["MOSN","Service Mesh Webinar"],"title":"Service Mesh Webinar#1：多点生活在 Service Mesh 上的实践","type":"activities","url":"/activities/service-mesh-webinar-1/","wordcount":742},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#16：不得不说的云原生隔离性\n 活动时间：5 月 21 日周四晚 7 点\n 活动形式：线上直播\n 报名方式：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#16：不得不说的云原生隔离性 在云原生时代，容器和 Kubernetes 在日常工作和实际生产中越来越普遍，随之而来的隔离性问题在不断被提起同时也受到了越来越多的关注。\nKata Containers 是 OpenStack 基金会旗下又独立于 OpenStack 项目之外的开放基础设施顶级项目，旨在把虚拟机的安全隔离优势与容器的速度和可管理性统一起来，为用户提供安全快速易用的容器基础设施。\n本期直播，将邀请 Kata Containers 维护者彭涛（花名：巴德）带我们走近云原生的一项基础设施 \u0026amp;ndash; Kata Containers，看它是如何在云原生框架下解决容器隔离性问题的。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：30315793（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1197\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 不得不说的云原生隔离性 卫恒 SOFATracer 开源负责人\n你将收获  从 Kubernetes Pod 说起，经典容器场景下的 Pod 分享； 共享内核存在的问题以及解决办法； 上帝说，要有光；我们说，要有 Kata； The speed of containers, the security of VMs； Kata Containers 特性大放送； What? 你刚说过增加一个 VM 间接层的问题？  嘉宾  SOFAGirl 主持人 卫恒 SOFATracer 开源负责人  ","date":1588057200,"description":"5 月 21 日周四晚 7 点，线上直播第 16 期。","dir":"activities/sofa-channel-16/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4756bc76f718255ad5fc4fd172a706b4","permalink":"/activities/sofa-channel-16/","publishdate":"2020-04-28T15:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-16/","summary":"概要 活动主题：SOFAChannel#16：不得不说的云原生隔离性 活动时间：5 月 21 日周四晚 7 点 活动形式：线上直播 报名方式：戳这里 介绍 | SOFAChannel \u0026lt;SOFA:Channel/\u0026gt; 有","tags":["SOFAChannel","Kata Container"],"title":"SOFAChannel#16：不得不说的云原生隔离性","type":"activities","url":"/activities/sofa-channel-16/","wordcount":568},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#15：分布式链路组件 SOFATracer 埋点机制解析\n 活动时间：4 月 23 日周四晚 7 点\n 活动形式：线上直播\n 直播回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#15：分布式链路组件 SOFATracer 埋点机制解析 SOFATracer 是蚂蚁金服开源的基于 OpenTracing 规范 的分布式链路跟踪系统，其核心理念就是通过一个全局的 TraceId 将分布在各个服务节点上的同一次请求串联起来。通过统一的 TraceId 将调用链路中的各种网络调用情况以日志的方式记录下来同时也提供远程汇报到 Zipkin 进行展示的能力，以此达到透视化网络调用的目的。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：30315793（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1167\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 分布式链路组件 SOFATracer 埋点机制解析 卫恒 SOFATracer 开源负责人\n你将收获  带你快速上手 SOFATracer； SOFATracer 功能点详细介绍； SOFATracer 埋点机制原理详解；  嘉宾  SOFAGirl 主持人 卫恒 SOFATracer 开源负责人  ","date":1587114000,"description":"4 月 23 日周四晚 7 点，线上直播第 15 期。","dir":"activities/sofa-channel-15/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e1323556a7e34b0937a8d3c1f6815a77","permalink":"/activities/sofa-channel-15/","publishdate":"2020-04-17T17:00:00+08:00","readingtime":1,"relpermalink":"/activities/sofa-channel-15/","summary":"概要 活动主题：SOFAChannel#15：分布式链路组件 SOFATracer 埋点机制解析 活动时间：4 月 23 日周四晚 7 点 活动形式：线上直播 直播回顾：戳这里 介绍 |","tags":["SOFAChannel","SOFATracer"],"title":"SOFAChannel#15：分布式链路组件 SOFATracer 埋点机制解析","type":"activities","url":"/activities/sofa-channel-15/","wordcount":442},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#14：云原生网络代理 MOSN 的扩展机制解析\n 活动时间：4 月 9 日周四晚 7 点\n 活动形式：线上直播\n 直播回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#14：云原生网络代理 MOSN 的扩展机制解析 MOSN 是一款使用 Go 语言开发的网络代理软件，由蚂蚁金服开源并经过几十万容器的生产级验证。\nMOSN 作为云原生的网络数据平面，旨在为服务提供多协议，模块化，智能化，安全的代理能力。在实际的生产使用场景中，通用的网络代理总会与实际业务定制需求存在差异，MOSN 提供了一系列可编程的扩展机制，就是为了解决这种场景。\n本次分享将向大家介绍 MOSN 的扩展机制解析以及一些扩展实践的案例。\n欢迎先下载 Demo，提前体验 MOSN 拓展机制的使用，成功完成预习作业—— Demo 完整操作的，有机会获得小礼物哟（记得留下完成的证明，获得方式在直播中公布），我们将在直播中公布答案——进行 Demo 的详细演示。PS：在直播中也会发布闯关任务，完成闯关任务也有机会获得小礼物哟～\nDemo：https://github.com/mosn/mosn/tree/master/examples/codes/mosn-extensions\nDemo Readme：https://github.com/mosn/mosn/tree/master/examples/cn_readme/mosn-extensions\n欢迎了解 MOSN：https://github.com/mosn/mosn\n| 针对人群 对云原生、Service Mesh、网络代理有基本了解，想要了解云原生以及对云原生网络代理 MOSN 有二次开发需求的人群\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：21992058（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1131\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 蚂蚁金服分布式事务实践解析 永鹏 蚂蚁金服高级开发工程师， MOSN Committer\n你将收获  快速了解 MOSN 的多种扩展能力 3 个案例，实际体验 MOSN 扩展能力 多案例多形式，使用 MOSN 实现个性化业务需求  嘉宾  SOFAGirl 主持人 永鹏 蚂蚁金服高级开发工程师， MOSN Committer  ","date":1585299600,"description":"4 月 9 日周四晚 7 点，线上直播第 14 期。","dir":"activities/sofa-channel-14/","fuzzywordcount":1000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0e1d5c0ea19ec82218fbf675b891ee5a","permalink":"/activities/sofa-channel-14/","publishdate":"2020-03-27T17:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-14/","summary":"概要 活动主题：SOFAChannel#14：云原生网络代理 MOSN 的扩展机制解析 活动时间：4 月 9 日周四晚 7 点 活动形式：线上直播 直播回顾：戳这里 介绍","tags":["SOFAChannel","MOSN"],"title":"SOFAChannel#14：云原生网络代理 MOSN 的扩展机制解析","type":"activities","url":"/activities/sofa-channel-14/","wordcount":901},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#13：云原生网络代理 MOSN 的多协议机制解析\n 活动时间：3 月 26 日周四晚 7 点\n 活动形式：线上直播\n 直播回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#13：云原生网络代理 MOSN 的多协议机制解析 作为云原生网络代理，Service Mesh 是 MOSN 的重要应用场景。随着 Service Mesh 概念的日益推广，大家对这套体系都已经不再陌生，有了较为深入的认知。但是与理论传播相对应的是，生产级别的大规模落地实践案例却并不多见。这其中有多方面的原因，包括社区方案饱受诟病的“大规模场景性能问题”、“配套的运维监控基础设施演进速度跟不上”、“存量服务化体系的兼容方案”等等。\n现实场景中，大部分国内厂商都有一套自研 RPC 的服务化体系，属于「存量服务化体系的兼容方案」中的协议适配问题。为此，MOSN 设计了一套多协议框架，用于降低自研体系的协议适配及接入成本，加速 Service Mesh 的落地普及。本次演讲将向大家介绍 MOSN 实现多协议低成本接入的设计思路，以及相应的快速接入实践案例。\n本期为 SOFAChannel 线上直播第 13 期，将邀请蚂蚁金服技术专家\u0026amp;amp; MOSN Committer 无钩分享《云原生网络代理 MOSN 的多协议机制解析》。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：21992058（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1131\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 蚂蚁金服分布式事务实践解析 无钩 蚂蚁金服技术专家 MOSN Committer\n本期分享大纲  一个请求的 MOSN 之旅 如何在 MOSN 中接入新的协议 SOFABolt 协议接入实践 未来发展：统一路由框架  嘉宾  SOFAGirl 主持人 无钩 蚂蚁金服技术专家 MOSN Committer  ","date":1584349200,"description":"3 月 26 日周四晚 7 点，线上直播第 13 期。","dir":"activities/sofa-channel-13/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ba88bba5176867c90fb18dc045408d84","permalink":"/activities/sofa-channel-13/","publishdate":"2020-03-16T17:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-13/","summary":"概要 活动主题：SOFAChannel#13：云原生网络代理 MOSN 的多协议机制解析 活动时间：3 月 26 日周四晚 7 点 活动形式：线上直播 直播回顾：戳这里 介","tags":["SOFAChannel","MOSN"],"title":"SOFAChannel#13：云原生网络代理 MOSN 的多协议机制解析","type":"activities","url":"/activities/sofa-channel-13/","wordcount":692},{"author":"潘潘","categories":"SOFALab","content":" | SOFALab \u0026amp;lt;SOFA:Lab/\u0026amp;gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~\n\u0026amp;lt;SOFA:BootLab/\u0026amp;gt;是《剖析 | SOFABoot 框架》系列，会逐步详细介绍 SOFABoot 各个部分的代码设计和实现，欢迎领取文章进行共建。\n| SOFABoot SOFABoot 是蚂蚁金服开源的基于 Spring Boot 的研发框架，它在 Spring Boot 的基础上，提供了诸如 Readiness Check，类隔离，日志空间隔离等能力。在增强了 Spring Boot 的同时，SOFABoot 提供了让用户可以在 Spring Boot 中非常方便地使用 SOFA 中间件的能力。\nSOFABoot :https://github.com/sofastack/sofa-boot\n| SOFA:Boot Lab  认领列表：\n 【已认领】《SOFABoot 总览》 【已认领】《SOFABoot runtime 机制解析》 【已认领】《SOFABoot HealthCheck 机制解析》 【已认领】《SOFABoot 日志隔离解析》 【已认领】《SOFABoot 上下文隔离机制解析》  领取方式：关注 「金融级分布式架构」 回复可领取的文章标题，我们将会主动联系你，确认资质后，即可加入 SOFA:BootLab/，It\u0026amp;rsquo;s your show time！\n  如果有同学对以上某个主题特别感兴趣的，可以留言讨论，我们会适当根据大家的反馈调整文章的顺序，谢谢大家关注 SOFAStack ，关注 SOFABoot，我们会一直与大家一起成长的。\n除了源码解析，也欢迎提交 issue 和 PR： SOFABoot：https://github.com/sofastack/sofa-boot\n欢迎领取，参与共建~\n","date":1583208000,"description":"","dir":"activities/sofa-boot-lab/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ee08908ed6148190ca7ebcc0cdc5a3fc","permalink":"/activities/sofa-boot-lab/","publishdate":"2020-03-03T12:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-boot-lab/","summary":"| SOFALab \u0026lt;SOFA:Lab/\u0026gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~ \u0026lt;SOFA:BootLab/\u0026gt;是《剖析 | SOFABoot 框架》系列，会逐步详细","tags":["SOFALab","SOFABoot","剖析 | SOFABoot 框架"],"title":"\u003cSOFA:BootLab/\u003e","type":"activities","url":"/activities/sofa-boot-lab/","wordcount":542},{"author":"SOFA 团队","categories":"Service Mesh","content":" 2020 年 2 月 4 日到 2 月11 日，ServiceMesher 社区发起了 Service Mesh 终端用户调查，以下为问卷调查结果。\n参与问卷调查的人员情况 共收集到 516 份问卷结果，问卷填写者 94.2% 来自 ServiceMesher 社区，21.7% 的人参与过社区线上活动，27.5% 的人参与过社区 meetup，86.6% 看好 Service Mesh 的未来发展前景。\n下面是参与问卷调查人员的基本情况。\n公司所属行业\n所在公司的 Service Mesh 使用情况\n工作年限\n在公司中担任的职务\n关注 Service Mesh 技术的时长\n周围关注或了解 Service Mesh 技术的人员情况\n学习 Service Mesh 技术的方式\n关注的 Service Mesh 相关开源项目\n除了 Service Mesh 外关注的其他云原生领域\n对 Service Mesh 的了解程度\n关注 Service Mesh 技术中的哪部分\n社区参与 了解社区活动的情况\n对社区的建议\n还有很多对社区的建议，反馈比较多的如下：\n 更多落地实践和指南 发布一些入门级的文章，结合案例，让技术在中小企业中落地 组织一些线上或线下活动 对普通开发者的职业发展的建议 出系列教程  结论 从结果中可以看出，Service Mesh 在互联网公司中关注的比例最高，但是它仍然还在高速发展中，还缺乏完善的教程和案例。\n本次问卷调查旨在了解 ServiceMesher 社区成员对 Service Mesh 的了解及社区参与程度，帮助 ServiceMesher 社区做的更好，还需要社区成员们共同的努力。\n欢迎关注 Service Mesh 技术的小伙伴们加入 ServiceMesher 社区，共同交流学习和成长。\n关于本次调查问卷的最终解释权归 ServiceMesher 社区所有。\n","date":1581667200,"description":"2020 年 2 月 4 日到 2 月11 日，ServiceMesher 社区发起了 Service Mesh 终端用户调查，以下为问卷调查结果。","dir":"service-mesh-end-user-survey-report/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"894b42fb32c6c3c39986e020fd53a13e","permalink":"/service-mesh-end-user-survey-report/","publishdate":"2020-02-14T16:00:00+08:00","readingtime":2,"relpermalink":"/service-mesh-end-user-survey-report/","summary":"2020 年 2 月 4 日到 2 月11 日，ServiceMesher 社区发起了 Service Mesh 终端用户调查，以下为问卷调查结果。 参与问卷调查的人员情况 共收集到 516 份问卷结","tags":["Service Mesh"],"title":"Service Mesh 终端用户调查报告","type":"page","url":"/service-mesh-end-user-survey-report/","wordcount":530},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#12：蚂蚁金服分布式事务实践解析\n 活动时间：3 月 12 日周四晚 7 点\n 活动形式：线上直播\n 直播回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#12：蚂蚁金服分布式事务实践解析 软件开发模式从原来的单应用，到现在的微服务、分布式架构，一个大型分布式系统内，单业务链路往往需要编排多个不同的微服务，如何实现分布式场景下业务一致性，是摆在软件工程师面前的一个技术难题。\n蚂蚁金服作为一家金融科技公司，业务涉及金融核心的各个领域，从2007年开始就自主研发了分布式事务框架，解决跨数据库、跨服务的业务一致性问题；随着13来年的不断打磨和沉淀，历经双十一、双十二等大促的洗礼，如今所有核心业务都已经在使用这套框架来保障交易的完整性和最终一致性，做到知托付，让用户放心。\n本期分享将介绍蚂蚁金服内部的分布式事务实践，包括 TCC（Try-Confirm-Cancel） 模式以及 FMT （Framework-Managerment-Transaction，框架管理事务）模式。同时也会与大家分享在面对双十一大促这种世界级的流量洪峰前，我们又是如何应对这个挑战。\n本期为 SOFAChannel 线上直播第 12 期，将邀请蚂蚁金服分布式事务核心开发仁空分享《蚂蚁金服分布式事务实践解析》。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23372465（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1096\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 蚂蚁金服分布式事务实践解析 仁空，蚂蚁金服分布式事务核心开发\n本期分享大纲  分布式事务产生的背景； 蚂蚁金服分布式事务理论与实践；  TCC 模式 FMT 模式  蚂蚁金服分布式事务极致性能提升； 蚂蚁金服分布式事务开源与展望；  嘉宾  SOFAGirl 主持人 仁空，蚂蚁金服分布式事务核心开发  ","date":1581498000,"description":"3 月 12 日周四晚 7 点，线上直播第 12 期。","dir":"activities/sofa-channel-12/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c80a24fd4e76747f0ed57c139a2bfae9","permalink":"/activities/sofa-channel-12/","publishdate":"2020-02-12T17:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-12/","summary":"概要 活动主题：SOFAChannel#12：蚂蚁金服分布式事务实践解析 活动时间：3 月 12 日周四晚 7 点 活动形式：线上直播 直播回顾：戳这里 介绍 | SOFAChannel","tags":["SOFAChannel","分布式事务"],"title":"SOFAChannel#12：蚂蚁金服分布式事务实践解析","type":"activities","url":"/activities/sofa-channel-12/","wordcount":784},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#11：从一个例子开始体验轻量级类隔离容器 SOFAArk\n 活动时间：2 月 13 日周四晚 7 点\n 活动形式：线上直播\n 直播回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道，前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#11：从一个例子开始体验轻量级类隔离容器 SOFAArk SOFAArk 是一款基于 Java 实现的轻量级类隔离容器，主要提供类隔离和应用(模块)合并部署能力，由蚂蚁金服公司开源贡献。SOFAArk 提供了一套较为规范化的插件化、模块化的开发方案。截止 2019 年底，SOFAArk 已经在蚂蚁金服内部 Serverless 场景下落地实践，并已经有数家企业在生产环境使用 SOFAArk ，包括网易云音乐、挖财、溢米教育等。\n本期为 SOFAChannel 线上直播第 11 期，将邀请 SOFAArk 开源负责人玄北和大家一起解读 SOFAArk ，将从 SOFAArk 的特性出发，了解轻量级类隔离容器 SOFAArk 的主要功能，并通过一个 Demo 案例，跟讲师一起操作，实际体验 SOFAArk 具体操作以及功能实现。\nSOFAArk：https://github.com/sofastack/sofa-ark\n你将收获：\n 快速认识轻量级类隔离容器 SOFAArk； Demo 案例实操，了解如何使用 SOFAArk 实现类隔离； SOFAArk 完整功能详解；  | 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23372465（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1096\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 从一个例子开始体验轻量级类隔离容器 SOFAArk 玄北，蚂蚁金服技术专家 、SOFAArk 开源负责人\n本期分享大纲：  快速认识轻量级类隔离容器 SOFAArk； Demo 案例实操，了解如何使用 SOFAArk 实现类隔离； SOFAArk 完整功能详解；  嘉宾  SOFAGirl 主持人 玄北，蚂蚁金服技术专家 、SOFAArk 开源负责人  ","date":1579251600,"description":"2 月 13 日周四晚 7 点，线上直播第 11 期。","dir":"activities/sofa-channel-11/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b110f76087d52e5c6d8ccbe31c76a7f5","permalink":"/activities/sofa-channel-11/","publishdate":"2020-01-17T17:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-11/","summary":"概要 活动主题：SOFAChannel#11：从一个例子开始体验轻量级类隔离容器 SOFAArk 活动时间：2 月 13 日周四晚 7 点 活动形式：线上直播 直播回顾：戳这","tags":["SOFAChannel","SOFAArk"],"title":"SOFAChannel#11：从一个例子开始体验轻量级类隔离容器 SOFAArk","type":"activities","url":"/activities/sofa-channel-11/","wordcount":709},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#10：分布式事务 Seata 长事务解决方案 Saga 模式详解\n 活动时间：1 月 9 日周四晚 7 点\n 活动形式：线上直播\n 活动回顾：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#10：分布式事务 Seata 长事务解决方案 Saga 模式详解 Seata 意为：Simple Extensible Autonomous Transaction Architecture，是一套一站式分布式事务解决方案，提供了 AT、TCC、Saga 和 XA 事务模式，其中长事务解决方案 Saga 模式有着无锁高性能、异步架构高吞吐的优势。\n本期为 SOFAChannel 线上直播第 10 期，将邀请 蚂蚁金服 分布式事务核心研发 \u0026amp;amp; Seata Committer 屹远 和大家一起探讨 《分布式事务 Seata 长事务解决方案 Saga 模式详解》，将从金融分布式应用开发的痛点出发，结合 Saga 分布式事务的理论和使用场景，讲解如何使用 Seata Saga 状态机来进行服务编排和分布式事务处理，构建更有弹性的金融应用，同时也会从架构、原理、设计、高可用、最佳实践等方面剖析 Saga 状态机的实现。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23372465（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1076\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 蚂蚁金服 Service Mesh 双十一落地详解 屹远 蚂蚁金服分布式事务核心研发、Seata Committer\n本期分享大纲：  金融分布式应用开发的痛点 Seata Saga 理论基础以及使用场景 基于状态机引擎的 Saga 实现 Seata Saga 模式最佳实践以及优势  嘉宾  SOFAGirl 主持人 屹远 蚂蚁金服分布式事务核心研发、Seata Committer  ","date":1577765400,"description":"1 月 9 日周四晚 7 点，线上直播第 10 期。","dir":"activities/sofa-channel-10/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a2f7bb983d4cbad1bda1a48a3c99abb7","permalink":"/activities/sofa-channel-10/","publishdate":"2019-12-31T12:10:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-10/","summary":"概要 活动主题：SOFAChannel#10：分布式事务 Seata 长事务解决方案 Saga 模式详解 活动时间：1 月 9 日周四晚 7 点 活动形式：线上直播 活动回顾：戳这","tags":["SOFAChannel","Seata"],"title":"SOFAChannel#10：分布式事务 Seata 长事务解决方案 Saga 模式详解","type":"activities","url":"/activities/sofa-channel-10/","wordcount":627},{"author":"潘潘","categories":"Service Mesh","content":" 概要  活动主题：Service Mesh Meetup#9 杭州站：To Infinity and Beyond 活动时间：时间：2019 年 12 月 28 日（周六）13:00-17:30 活动地点：杭州西湖区紫霞路西溪谷G座8楼 活动形式：线下活动 活动回顾：请戳这里  活动介绍 关于 Service Mesh\n服务网格（Service Mesh）是致力于解决服务间通讯的基础设施层。它负责在现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，Service Mesh 通常是通过一组轻量级网络代理（Sidecar proxy），与应用程序代码部署在一起来实现，而无需感知应用程序本身。\n关于 ServiceMesher 社区\nServiceMesher 社区，专注于 Service Mesh 和云原生技术，立足开源，分享技术资讯和实践，致力于新技术的推广和普及。\nService Mesh Meetup #9 杭州站\nService Mesh Meetup 是由蚂蚁金服联合 CNCF 官方共同出品，ServiceMesher 社区主办，主题围绕服务网格、Kubernetes 及云原生，在全国各地循环举办的技术沙龙。\n本期 Meetup 与滴滴联合举办，将深入 Service Mesh 的落地实践，并带领大家探索 Service Mesh 在更广阔领域的应用。\n活动议程    时间 环节（分享主题） 分享嘉宾     13:00-13:30 签到    13:30-14:15 《蚂蚁金服 API Gateway Mesh 的思考与实践》 贾岛 蚂蚁金服 高级技术专家   14:15-15:00 《酷家乐的 Istio 与 Knative 踩坑实录》 付铖 酷家乐 容器化负责人   15:00-15:10 MOSN 社区化发布 涵畅   15:10-15:20 茶歇    15:20-16:00 圆桌环节-《Service Mesh 落地的务实与创新》 鲁直   16:00-16:45 《蚂蚁金服 Service Mesh 技术风险思考和实践》 嘉祁 蚂蚁金服 技术专家   16:45-17:30 《网易严选的 Service Mesh 实践》 王国云 网易严选 架构师    本期议题以及嘉宾详解 13:30-14:15《蚂蚁金服 API Gateway Mesh 的思考与实践》\n 讲师：贾岛 蚂蚁金服 高级技术专家 讲师介绍：靳文祥（花名贾岛），2011年毕业后加入支付宝无线团队，一直从事移动网络接入、API 网关、微服务等相关的研发工作，目前负责蚂蚁金服移动网络接入架构设计与优化。 Topic 介绍：在 Service Mesh 微服务架构中，我们常常会听到东西流量和南北流量两个术语。蚂蚁金服开源的Service Mesh Sidecar MOSN 已经多次与大家见面交流了，以往的议题重点在东西流量的服务发现与路由，那么蚂蚁金服在南北流量上的思考是怎样的？本次分享，将从蚂蚁金服 API 网关发展历程来看，Mesh 化的网关架构是怎样的，解决了什么问题，双十一的实践表现，以及我们对未来的思考。  14:15-15:00《酷家乐的 Istio 与 Knative 踩坑实录》\n 讲师：付铖 酷家乐 技术专家 讲师介绍：先后负责酷家乐户型图, 渲染引擎等模块，目前负责酷家乐国际站的研发。在业务开发过程中推动和布道云原生技术，并在部分业务落地了 Istio 服务治理和基于 Knative 的 Serverless 基础设施。 Topic 介绍：酷家乐在部分业务模块，自2018年使用了 Istio 进行服务治理，自2019年使用了 Knative 作为 FaaS 基础设施，在实践过程中解决了大量问题，也积累了不少第一手经验。本次分享，将重点讨论服务网格的性能损耗，存量业务迁移难题，函数计算的冷启动时间问题以及解决方案等。  15:20-16:00 圆桌环节《Service Mesh 落地的务实与创新》\n 主持人：鲁直 蚂蚁金服云原生落地负责人 嘉宾：  涵畅 蚂蚁金服高级技术专家 张超盟 华为云微服务平台架构师 付铖 酷家乐技术专家 王国云 网易严选架构师   16:00-16:45《蚂蚁金服 Service Mesh 技术风险思考和实践》\n 讲师：嘉祁 蚂蚁金服 技术专家 讲师介绍：黄家琦（花名：嘉祁）2012年毕业后加入阿里巴巴技术保障，2015年转入蚂蚁金服SRE，长期从事稳定性相关工作，当前重点关注于中间件，Service Mesh 与云原生基础设施的稳定性。 Topic 介绍：Servish Mesh 是微服务架构与云原生碰撞出的火花，对于传统的中间件体系与运维支撑能力是极大的挑战。本次分享的主题主要关注于在蚂蚁金服内部如何应对这些挑战，并建设相应的技术风险能力来保障其稳定。  16:45-17:30《网易严选的 Service Mesh 实践》\n 讲师：王国云 网易高级技术专家、网易严选架构师 讲师介绍：2008年加入网易，长期从事一线的研发与管理工作，曾参与或负责过网易博客、网易邮箱、网易有钱等多个核心产品的研发工作，擅长服务端架构及技术体系建设，现任严选支持业务研发部技术负责人，负责严选的容器化及 Service Mesh 演进。 Topic 介绍：网易严选在2016年选择了 Service Mesh 作为未来微服务改造的基础架构，并在过去几年支持了业务的持续快速增长。本次分享主要介绍 Service Mesh 在严选的落地和演进情况，讨论 Service Mesh 在混合云架构下落地遇到的挑战和我们的解决方案，同时也希望和大家交流一下在架构方面的一些思考。  加入 SOFA 钉钉互动群 群号：23390449，使用钉钉搜索群号即可加入，获取一手开源技术干货。\n主办方与合作伙伴  主办方：CNCF、ServiceMesher 协办方：蚂蚁金服、SOFAStack、滴滴 合作伙伴：掘金社区、开源中国、养码场、SegmentFault、麦思博、开源社  ","date":1576051200,"description":"本次 Meetup 与滴滴联合举办，将深入 Service Mesh 的落地实践，并带领大家探索 Service Mesh 在更广阔领域的应用。","dir":"activities/service-mesh-meetup-9/","fuzzywordcount":1900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a7103ed15ed823b9156082c37b7d464a","permalink":"/activities/service-mesh-meetup-9/","publishdate":"2019-12-11T16:00:00+08:00","readingtime":4,"relpermalink":"/activities/service-mesh-meetup-9/","summary":"概要 活动主题：Service Mesh Meetup#9 杭州站：To Infinity and Beyond 活动时间：时间：2019 年 12 月 28 日（周六）13:00-17:30 活动地点：杭州西湖区紫霞路","tags":["Meetup","Service Mesh"],"title":"Service Mesh Meetup#9 杭州站：To Infinity and Beyond","type":"activities","url":"/activities/service-mesh-meetup-9/","wordcount":1855},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#9：蚂蚁金服 Service Mesh 双十一落地详解\n 活动时间：12 月 5 日周四晚 7 点\n 活动形式：线上直播\n 直播回看：戳这里\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#9：蚂蚁金服 Service Mesh 双十一落地详解 Service Mesh 是蚂蚁金服下一代架构的核心，本期直播主要分享在蚂蚁金服当前的体量下，我们如何做到在奔跑的火车上换轮子，将现有的 SOA 体系快速演进至 Service Mesh 架构。\n12 月 5 日周四晚 7 点，将邀请蚂蚁金服技术专家卓与 ，聚焦 RPC 层面的设计和改造方案，分享蚂蚁金服双十一核心应用如何将现有的微服务体系平滑过渡到 Service Mesh 架构下并降低大促成本，并从核心、RPC、消息等模块展开本次双十一落地实践的实现细节分享。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23390449（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/1021\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 蚂蚁金服 Service Mesh 双十一落地详解 卓与 蚂蚁金服 Service Mesh 落地负责人\n本期分享大纲：  蚂蚁金服 Service Mesh 架构双十一大规模落地实践案例分析； 从核心、RPC、消息等模块分享蚂蚁金服 Service Mesh 落地实践细节； 欢迎先阅读Service Mesh 落地系列文章，收看直播收获更好的效果；  嘉宾  SOFAGirl 主持人 卓与 蚂蚁金服 Service Mesh 落地负责人  ","date":1574741400,"description":"12 月 5 日周四晚 7 点，线上直播第 9 期。","dir":"activities/sofa-channel-9/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"33ec75a1b0e23d96934a7ec43dfa5df5","permalink":"/activities/sofa-channel-9/","publishdate":"2019-11-26T12:10:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-9/","summary":"概要 活动主题：SOFAChannel#9：蚂蚁金服 Service Mesh 双十一落地详解 活动时间：12 月 5 日周四晚 7 点 活动形式：线上直播 直播回看：戳这里 介绍 | SOFAChannel","tags":["SOFAChannel","Service Mesh"],"title":"SOFAChannel#9：蚂蚁金服 Service Mesh 双十一落地详解","type":"activities","url":"/activities/sofa-channel-9/","wordcount":554},{"author":"潘潘","categories":"Service Mesh","content":" 概要  活动主题：Kubernetes \u0026amp;amp; Cloud Native X Service Mesh Meetup 活动时间：2019 年 11 月 24 日（星期日）9:30-16:30 活动地点：北京朝阳大望京科技商务园区宏泰东街浦项中心B座2层多功能厅 活动形式：线下活动 活动报名：请戳这里  活动介绍 Service Mesh Meetup#8 特别场 本期为 Service Mesh Meetup#8 特别场，联合 CNCF、阿里巴巴及蚂蚁金服 共同举办。\n不是任何一朵云都撑得住双 11。\n成交 2684 亿，阿里巴巴核心系统 100% 上云。\n蚂蚁金服的核心交易链路大规模上线 Service Mesh。\n这次，让双 11 狂欢继续，让云原生经得起双 11 大考，也让云原生走到开发者身边。\n你将收获 3 大经验加持：\n 双 11 洗礼下的阿里巴巴 K8s 超大规模实践经验； 蚂蚁金服首次 Service Mesh 大规模落地经验； 阿里巴巴超大规模神龙裸金属 K8s 集群运维实践经验；  错过一次，再等一年哦。\n议程    时间 环节（分享主题） 分享嘉宾     9:00-9:30 签到    9:30-10:10 《释放云原生价值，双 11 洗礼下的阿里巴巴 K8s 超大规模实践》 曾凡松（逐灵），阿里巴巴高级技术专家；汪萌海（木苏），阿里巴巴技术专家   10:10-10:50 《蚂蚁金服双十一Service Mesh超大规模落地揭秘》 黄挺（鲁直），蚂蚁金服云原生负责人；雷志远（碧远），蚂蚁金服技术专家   10:50-11:30 《阿里巴巴超大规模神龙裸金属 K8s 集群运维实践》 周涛 （广侯），阿里巴巴高级技术专家   11:30-12:10  《深入Kubernetes的“无人区” — 蚂蚁金服双十一的调度系统》 曹寅，蚂蚁金服 Kubernetes 落地负责人   12:10-13:30 午休    13:30-14:10  《服务网格在“路口”的产品思考与实践》 宋顺（齐天），蚂蚁金服高级技术专家，开源配置中心Apollo作者   14:10-14:50 《阿里集团核心应用落地 Service Mesh 的挑战与机遇》 李云（至简），阿里巴巴高级技术专家   14:50-13:10 茶歇    15:10-15:50 《蚂蚁金服云原生 PaaS 实践之路》 王成昌（晙曦），蚂蚁金服技术专家   15:50-16:30 《函数计算在双十一小程序场景的应用》 吴天龙（木吴），阿里云函数计算技术专家    加入 SOFA 钉钉互动群 群号：23390449，使用钉钉搜索群号即可加入，获取一手开源技术干货。\n","date":1573639200,"description":"11月24日，Service Mesh Meetup#8 双十一特别场邀您参加，本期联合 CNCF、阿里巴巴及蚂蚁金服共同举办。","dir":"activities/service-mesh-meetup-8/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f127ede30ff37760f648b79ecf887ffa","permalink":"/activities/service-mesh-meetup-8/","publishdate":"2019-11-13T18:00:00+08:00","readingtime":2,"relpermalink":"/activities/service-mesh-meetup-8/","summary":"概要 活动主题：Kubernetes \u0026amp; Cloud Native X Service Mesh Meetup 活动时间：2019 年 11 月 24 日（星期日）9:30-16:30 活动地点：北京朝阳大望京科技商务园","tags":["Meetup","Service Mesh","Kubernetes"],"title":"Kubernetes \u0026 Cloud Native X Service Mesh Meetup","type":"activities","url":"/activities/service-mesh-meetup-8/","wordcount":758},{"author":"潘潘","categories":"SOFALab","content":" | SOFALab \u0026amp;lt;SOFA:Lab/\u0026amp;gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~\n\u0026amp;lt;SOFA:ArkLab/\u0026amp;gt;是《剖析 | SOFAArk 源码》系列，会逐步详细介绍 SOFAArk 各个部分的代码设计和实现，欢迎领取文章进行共建。\n| SOFAArk SOFAArk 是一款基于 Java 实现的轻量级类隔离容器，主要提供类隔离和应用(模块)合并部署能力。在大型软件开发过程中，通常会推荐底层功能插件化，业务功能模块化的开发模式，以期达到低耦合、高内聚、功能复用的优点。SOFAArk 提供了一套较为规范化的插件化、模块化的开发方案，帮助解决依赖包冲突、多应用(模块)合并部署等场景问题。\nSOFAArk :https://github.com/sofastack/sofa-ark\n|\u0026amp;lt; SOFA:ArkLab/\u0026amp;gt;  认领列表：\n 【已完成】轻量级类隔离框架 SOFAArk 简介 【已认领】 SOFAArk 容器模型解析 【已认领】 SOFAArk 类加载模型机制解析 【已认领】 SOFAArk 合并部署能力解析 【已认领】 SOFAArk SPI 机制和 ClassLoaderHook 机制解析 【已认领】 SOFAArk 动态配置机制解析 【已认领】 SOFAArk maven 打包插件解析 【已认领】 （实践）SOFAArk 插件化机制解析与实践  领取方式：关注「金融级分布式架构」 回复可领取的文章标题，我们将会主动联系你，确认资质后，即可加入 SOFA:ArkLab/，It\u0026amp;rsquo;s your show time！\n  如果有同学对以上某个主题特别感兴趣的，可以留言讨论，我们会适当根据大家的反馈调整文章的顺序，谢谢大家关注 SOFAStack ，关注 SOFAArk，我们会一直与大家一起成长的。\n除了源码解析，也欢迎提交 issue 和 PR： SOFAArk :https://github.com/sofastack/sofa-ark\n欢迎领取，参与共建~\n","date":1572408000,"description":"欢迎参与 SOFAArk 源码解析系列文章共建。","dir":"activities/sofa-ark-lab/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"02869baa65a4730cea247cf1763d920c","permalink":"/activities/sofa-ark-lab/","publishdate":"2019-10-30T12:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-ark-lab/","summary":"| SOFALab \u0026lt;SOFA:Lab/\u0026gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~ \u0026lt;SOFA:ArkLab/\u0026gt;是《剖析 | SOFAArk 源码》系列，会逐步详细介","tags":["SOFALab","SOFAArk","剖析 | SOFAArk 源码"],"title":"\u003cSOFA:ArkLab/\u003e","type":"activities","url":"/activities/sofa-ark-lab/","wordcount":572},{"author":"潘潘","categories":"SOFALab","content":" | SOFALab \u0026amp;lt;SOFA:Lab/\u0026amp;gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~\n\u0026amp;lt;SOFA:RegistryLab/\u0026amp;gt;是《剖析 | SOFARegistry 实现原理》系列，会逐步详细介绍 SOFARegistry 各个部分的代码设计和实现，欢迎领取文章进行共建。\n| SOFARegistry SOFARegistry 是蚂蚁金服开源的具有承载海量服务注册和订阅能力的、高可用的服务注册中心，最早源自于淘宝的初版 ConfigServer，在支付宝/蚂蚁金服的业务发展驱动下，近十年间已经演进至第五代。\nSOFARegistry:https://github.com/sofastack/sofa-registry\n| SOFA:RegistryLab  认领列表：\n 【已完成】海量数据下的注册中心 - SOFARegistry 架构介绍 【已完成】SOFARegistry 服务发现优化之路 【已完成】SOFARegistry 数据分片和同步方案详解 【已完成】SOFARegistry MetaServer 功能介绍和实现剖析 【已完成】SOFARegistry Session 存储策略 【已领取】SOFARegistry 如何实现秒级服务上下线通知 【已领取】SOFARegistry 如何实现 DataServer 平滑扩缩容  参与方式：关注「金融级分布式架构」回复可领取的文章标题，会有相关同学联系进行确认。\n  欢迎领取，参与共建~\n","date":1567483200,"description":"","dir":"activities/sofa-registry-lab/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ebbc0e9574d88e21f39926e750c848e9","permalink":"/activities/sofa-registry-lab/","publishdate":"2019-09-03T12:00:00+08:00","readingtime":1,"relpermalink":"/activities/sofa-registry-lab/","summary":"| SOFALab \u0026lt;SOFA:Lab/\u0026gt; 源码研究实验室，由 SOFA 团队和源码爱好者们出品，欢迎你的加入~ \u0026lt;SOFA:RegistryLab/\u0026gt;是《剖析 | SOFARegistry 实现原理》系列","tags":["SOFALab","SOFARegistry"],"title":"\u003cSOFA:RegistryLab/\u003e","type":"activities","url":"/activities/sofa-registry-lab/","wordcount":435},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#8：从一个例子开始体验 SOFAJRaft\n 活动时间：8 月 29 日周四晚 7 点\n 活动形式：线上直播\n 视频回顾：https://tech.antfin.com/community/live/821\n  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#8：从一个例子开始体验 SOFAJRaft SOFAJRaft 是 Raft 算法的 Java 实现，其本质是一个工具项目。在本次直播中，我们将重点放在如何去使用这个工具上，用示例来说明如何使用 SOFAJRaft 实现自己的分布式应用。在此过程中，我们会对涉及到的一些 SOFAJRaft 经典概念进行讲解。\n为了更好地直播体验，可以在直播前，预习 SOFAJRaft 相关源码解析文章，文章集合：https://www.sofastack.tech/tags/sofajraft/\n8 月 29 日周四晚 7 点，将邀请 SOFAJRaft 开源负责人力鲲，从一个 SOFAJRaft 实例出发，带大家体验 SOFAJRaft 的应用。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/737\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 SOFAChannel#8：从一个例子开始体验 SOFAJRaft 力鲲 SOFAJRaft 开源负责人\n本期分享大纲：  如何使用 SOFAJRaft 实现自己的分布式应用 基于实例理解 SOFAJRaft 中的概念和术语  嘉宾  SOFAGirl 主持人 力鲲 SOFAJRaft 开源负责人  ","date":1565842200,"description":"8 月 29 日周四晚 7 点，线上直播第 8 期。","dir":"activities/sofa-channel-8/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"92590333a35992bc201c798645cbf7ea","permalink":"/activities/sofa-channel-8/","publishdate":"2019-08-15T12:10:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-8/","summary":"概要 活动主题：SOFAChannel#8：从一个例子开始体验 SOFAJRaft 活动时间：8 月 29 日周四晚 7 点 活动形式：线上直播 视频回顾：https://tec","tags":["SOFAChannel","SOFAJRaft"],"title":"SOFAChannel#8：从一个例子开始体验 SOFAJRaft","type":"activities","url":"/activities/sofa-channel-8/","wordcount":561},{"author":"潘潘","categories":"SOFAMeetup","content":" 概要  活动主题：SOFA Meetup#3 广州站-从开源技术到产品能力 活动时间：8 月 11 日周日下午 13 点 活动地点：广州市广电平云 B 塔 15F 活动形式：线下活动 报名方式：https://tech.antfin.com/community/activities/779  蚂蚁金服 SOFAStack SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁金服自主研发的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，历经蚂蚁金服超过十年的业务历练。SOFAStack 于 2018 年 4 月宣布开源，并逐步开源 SOFABoot、SOFARPC、SOFALookout、SOFATracer、SOFAMosn、SOFAMesh 等组件。\n欢迎 Star 我：https://github.com/sofastack\nSOFA Meetup#3 广州站-从开源技术到产品能力 适合自身企业的技术架构才是最佳的方案，SOFAStack 提供了一套的金融级解决方案，提供多种场景下需要的多种组件。\n本期 SOFA Meetup 将带来开源技术：SOFARPC、Seata 模式详解以及发展进程，并拓展分享云原生产品能力，更有无线自动化测试框架 Soloπ 的首秀分享~\n随着应用架构往云原生的方向发展，传统监控手段已经不能满足云原生时代运维的需求，因此，可观察性的理念被引入了 IT 领域。如何通过可观察性的理念，对微服务，Service Mesh 以至未来的 Serverless 架构的应用进行监控， 将是应用架构往云原生迁移过程中的一个重要命题。\n加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n点击即可报名 https://tech.antfin.com/community/activities/779\n议程    时间 环节 分享大纲 分享嘉宾     13:00-13:30 签到     13:30-14:15 《RPC 服务框架 SOFARPC 在蚂蚁金服的发展与进化》 - SOFARPC 在蚂蚁金服的应用现状\n- 协议和通信层的变化与设计\n- 跨机房与弹性的挑战\n- RPC 框架发展中的经验与教训\n- 拥抱开源，SOFARPC 的未来\n 碧远@SOFARPC 开源负责人   14:15-15:00 《蚂蚁金服在云原生架构下的可观察性的探索和实践》 \n- 为什么云原生时代需要可观察性\n- 可观察性的三大支柱\n- 现在社区方案的缺陷\n- 蚂蚁金服对云原生的可观察性的理解及实践\n 苟利@蚂蚁金服中间件团队产品专家   15:00-15:10 茶歇     15:10-15:55 《分布式事务 Seata 三种模式详解》 \n- 分布式事务产生的背景\n- 分布式事务理论基础\n- 蚂蚁金服分布式事务实践\n- 开源分布式事务 Seata 简介(AT，TCC，SAGA)\n 屹远@Seata 核心贡献者   15:55-16:40 《无线自动化测试框架 Soloπ 的跨平台实践》 \n- 移动端自动化测试转向轻量化\n- “Android+iOS”双端核心功能介绍\n- “Android+iOS”双端功能打通介绍\n- 结合云测平台、用例管理、IDE 的自动化测试解决方案\n 茅舍@Soloπ 核心作者\n不溯@Soloπ 核心作者    ","date":1564038000,"description":"SOFA Meetup#3 广州站-从开源技术到产品能力，8 月 11 日周日下午 13 点，广州市广电平云 B 塔 15F 等你。","dir":"activities/sofa-meetup-3/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0e50b11d1a8e52f8cefbac0bb4826ffe","permalink":"/activities/sofa-meetup-3/","publishdate":"2019-07-25T15:00:00+08:00","readingtime":3,"relpermalink":"/activities/sofa-meetup-3/","summary":"概要 活动主题：SOFA Meetup#3 广州站-从开源技术到产品能力 活动时间：8 月 11 日周日下午 13 点 活动地点：广州市广电平云 B 塔 15F 活动形式：线下活动 报名方式：","tags":["SOFAMeetup","SOFAStack"],"title":"SOFA Meetup#3 广州站-从开源技术到产品能力","type":"activities","url":"/activities/sofa-meetup-3/","wordcount":1067},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#7：扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进 活动时间：7 月 18 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#7：扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进 在 6 月 KubeCon 大会期间，蚂蚁金服正式宣布加入了 CNCF 成为黄金会员，同时 SOFAStack-CAFE 云应用引擎产品也通过了 K8S 一致性认证，旨在向广大金融机构提供云原生的可落地路径。\n为满足金融级云原生发布部署、变更管控场景对于可灰度、可监控、可应急的需求，SOFAStack 产品研发团队在 Kubernetes 基础上实现了自定义资源 CAFEDeployment ，它能够通过可靠而灵活的分发、风险控制的部署策略以及高性能的原地升级更新扩展部署能力。它尤其消除了金融服务行业所面临的技术障碍，使用户能够专心发展核心业务。\n与 Kubernetes 原生工作负载对象 Deployment 所提供的简洁轻量的滚动发布相比，CAFEDeployment 旨在满足金融场景对分批发布、多活容灾、原地升级等方面的诉求。\n7 月 18 日周四晚 7 点，将邀请 蚂蚁金服 SOFAStack-CAFE 云应用引擎 容器应用服务研发负责人 枫晟 为大家分享《扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进》。\n在此次分享中，将介绍对此 Kubernetes 扩展能力的相关观点主张、产品探索和实际演示。\n| 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/737\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进 枫晟 蚂蚁金服 SOFAStack-CAFE 云应用引擎 容器应用服务研发负责人\n本期分享大纲：  Kubernetes Deployment 发展历史与现状 Kubernetes Deployment 在互联网金融云场景下的问题与挑战 CafeDeployment 适配互联网金融发布的工作负载 CafeDeployment 的运行机制 CafeDeployment 功能演示  嘉宾  SOFAGirl 主持人 枫晟 蚂蚁金服 SOFAStack-CAFE 云应用引擎 容器应用服务研发负责人  ","date":1562573400,"description":"7 月 18 日周四晚 7 点，线上直播第 7 期。","dir":"activities/sofa-channel-7/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"51c929733c988190de480a3a0dc5e735","permalink":"/activities/sofa-channel-7/","publishdate":"2019-07-08T16:10:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-7/","summary":"概要 活动主题：SOFAChannel#7：扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进 活动时间：7 月 18 日周四晚 7 点 活动形式","tags":["SOFAChannel","CAFEDeployment"],"title":"SOFAChannel#7：扩展 Kubernetes 实现金融级云原生发布部署 - 自定义资源 CAFEDeployment 的背景、实现和演进","type":"activities","url":"/activities/sofa-channel-7/","wordcount":813},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#6：轻量级监控分析系统 SOFALookout 原理讲解和功能演示 活动时间：6 月 12 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#6：轻量级监控分析系统 SOFALookout 原理讲解和功能演示 6 月 27 日周四晚 7 点，将邀请 蚂蚁金服 SOFALookout 开源负责人 响风 为大家分享，通过对多个模块的剖析，详解 SOFALookout 服务端以及客户端，带你了解 SOFALookout 具体是如何支持主流 Metrics协议的数据收集、存储、查询计算和可视化的。欢迎报名参加~\nSOFALookout 是蚂蚁金服开源的一款解决系统的度量和监控问题的轻量级中间件服务，提供的服务包括：Metrics 的埋点、收集、加工、存储与查询等。该开源项目包括了两个独立部分，分别是客户端与服务器端服务。\n本期分享大纲：\n 监控预警基本概念介绍 SOFALookout 客户端使用 Gateway - 多协议数据收集与处理的设计与实现 Server - PromQL 与多种存储层的设计与实现 SOFALookout 发展路线  | 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/687\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 轻量级监控分析系统 SOFALookout 原理讲解和功能演示 响风 蚂蚁金服轻量级监控分析系统 SOFALookout 开源负责人\n本期分享大纲：  监控预警基本概念介绍 SOFALookout 客户端使用 Gateway - 多协议数据收集与处理的设计与实现 Server - PromQL 与多种存储层的设计与实现 SOFALookout 发展路线  嘉宾  SOFAGirl 主持人 响风 蚂蚁金服轻量级监控分析系统 SOFALookout 开源负责人  ","date":1560312000,"description":"6 月 12 日周四晚 7 点，线上直播第 6 期。","dir":"activities/sofa-channel-6/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"29a5673d4608770bbe7598954fcecc78","permalink":"/activities/sofa-channel-6/","publishdate":"2019-06-12T12:00:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-6/","summary":"概要 活动主题：SOFAChannel#6：轻量级监控分析系统 SOFALookout 原理讲解和功能演示 活动时间：6 月 12 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直","tags":["SOFAChannel","SOFALookout"],"title":"SOFAChannel#6：轻量级监控分析系统 SOFALookout 原理讲解和功能演示","type":"activities","url":"/activities/sofa-channel-6/","wordcount":637},{"author":"Jimmy Song","categories":null,"content":" SOFAStack Cloud Native Workshop hosted by Ant Financial (KubeCon China 2019 Co-Located Event)  Date: Monday, 24 June, 2019 Time: 9:00 – 16:00 Location: Shanghai Expo Centre Room 616 Registration Fees: Complimentary Register here: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/co-located-events/#sofastack-cloud-native-workshop Note: This event is hands-on, please bring your personal computer. The language of communication in this workshop is Chinese.  SOFAStack (Scalable Open Financial Architecture Stack) is a financial-grade distributed architecture independently developed and open sourced by Ant Financial. It contains the components required to build a financial-grade cloud native architecture. It is a best practice tempered in financial scenarios. SOFAStack official website: https://www.sofastack.tech/\nAttendees can get:\n Rapidly build microservices based on SOFAStack Best Practices for Distributed Transactions in Financial Scenarios Cloud native deployment experience based on Kubernetes Service Mesh basic usage scenario experience on the cloud Get started on Serverless apps Easily build applications on the cloud based on Serverless  How to Register: Pre-registration is required. To register for SOFAStack Cloud Native Workshop, add it on during your KubeCon + CloudNativeCon + Open Source Summit registration. You can get KubeCon half price tickets with KCCN19COMATF coupon code!\nFor questions regarding this event, please reach out to jingchao.sjc@antfin.com.\nEvent details 9:00 - 9:20 Opening speech SOFAStack Cloud Native\n9:20 - 10:10 Quickly build microservices with SOFAStack by Jie Cao\nBuilding a microservices application based on the SOFAStack. Through this workshop, you can learn how to report application monitoring data, service link data, and publish and subscribe services in the SOFAStack.\n10:15 - 11:05 SOFABoot dynamic module practice by Guolei Song\nIn this workshop, you can implement the combined deployment and dynamic module push capabilities provided by SOFAArk based on the ARK control capabilities of SOFADashboard.\n11:10 - 12:00 Using Seata to guarantee the consistency of payment by Long Chen\nUnder the microservice architecture, the distributed transaction problem is an industry problem. Through this workshop, you can understand the background of distributed transaction problems under distributed architecture, as well as common distributed transaction solutions and experience on how to use the open source distributed transaction framework - Seata\u0026amp;rsquo;s AT mode, TCC mode to solve the ultimate consistency of the business data.\n12:00 - 13:00 Lunch time\n13:00 - 13:30 Cloud Native exlporation and practice in Ant Fnancial by Renjie Yu\n13:30 - 14:40 Migrating to cloud based on Serverless by Yitao Dong\nAs one of the pioneering technologies of cloud technology, Serverless architecture allows you to further improve resource utilization and focus on business development. Through our workshop, you can experience new product …","date":1559643600,"description":"","dir":"activities/sofastack-cloud-native-workshop/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"57c6b0262af34c2010179000834d8363","permalink":"/en/activities/sofastack-cloud-native-workshop/","publishdate":"2019-06-04T10:20:00Z","readingtime":3,"relpermalink":"/en/activities/sofastack-cloud-native-workshop/","summary":"SOFAStack Cloud Native Workshop hosted by Ant Financial (KubeCon China 2019 Co-Located Event)  Date: Monday, 24 June, 2019 Time: 9:00 – 16:00 Location: Shanghai Expo Centre Room 616 Registration Fees: Complimentary Register here: https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/co-located-events/#sofastack-cloud-native-workshop Note: This event is hands-on, please bring your personal computer. The language of communication in this workshop is Chinese.  SOFAStack (Scalable Open Financial Architecture Stack) is a financial-grade distributed architecture independently developed and open sourced by Ant Financial.","tags":["KubeCon","Workshop","Cloud Native"],"title":"KubeCon China 2019 Co-Located Event SOFAStack Cloud Native Workshop","type":"activities","url":"/en/activities/sofastack-cloud-native-workshop/","wordcount":515},{"author":"宋净超","categories":null,"content":" 蚂蚁金服 SOFAStack 云原生工作坊（KubeCon China 2019 同场活动）  日期：2019年6月24日，星期一 时间：9:00 – 16:00 地点：上海世博中心 616 房间 注册费：免费 注册地址：https://www.lfasiallc.com/events/kubecon-cloudnativecon-china-2019/co-located-events/#sofastack-cloud-native-workshop 备注：本次活动为动手实践，请携带个人电脑。本沙龙沟通语言为中文。  SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁金服自主研发并开源的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，是在金融场景里锤炼出来的最佳实践。SOFAStack 官方网站：https://www.sofastack.tech/\n参加此次 Meetup 您将获得：\n 基于 SOFAStack 快速构建微服务 金融场景下的分布式事务最佳实践 基于 Kubernetes 的云原生部署体验 云上的 Service Mesh 基本使用场景体验 基于 Serverless 轻松构建云上应用  如何注册：此活动须提前注册。请将 SOFAStack Cloud Native Workshop 添加到您 KubeCon + CloudNativeCon + Open Source Summit 的注册表里。您可以使用 KCCN19COMATF 折扣码获取 KubeCon 半价门票！\n如果对此活动有任何疑问，请发送邮件至 jingchao.sjc@antfin.com。\n活动详情 9:00 - 9:20 开场演讲 SOFAStack 云原生开源体系介绍 by 余淮\n9:20 - 10:10 使用 SOFAStack 快速构建微服务 by 玄北\n基于 SOFA 技术栈构建微服务应用。通过本 workshop ，您可以了解在 SOFA 体系中如何上报应用监控数据、服务链路数据以及发布及订阅服务。\n10:15 - 11:05 SOFABoot 动态模块实践 by 卫恒\n在本 workshop 中，您可以基于 SOFADashboard 的 ARK 管控能力来实现 SOFAArk 提供的合并部署和动态模块推送的功能。\n11:10 - 12:00 使用 Seata 保障支付一致性 by 屹远\n微服务架构下，分布式事务问题是一个业界难题。通过本workshop，您可以了解到分布式架构下，分布式事务问题产生的背景，以及常见的分布式事务解决方案；并亲身体验到如何使用开源分布式事务框架Seata的AT模式、TCC模式解决业务数据的最终一致性问题。\n12:00 - 13:00 午餐时间\n13:00 - 13:30 蚂蚁金服的云原生探索与实践 by 首仁\n13:30 - 14:40 通过 Serverless 快速上云 by 隐秀\n作为云原生技术前进方向之一，Serverless 架构让您进一步提高资源利用率，更专注于业务研发。通过我们的 workshop，您可以体验到快速创建 Serveless 应用、根据业务请求秒级 0-1-N 自动伸缩、通过日志查看器快速排错、按时间触发应用等产品新功能。\n14:50 - 16:00 使用 CloudMesh 轻松实践 Service Mesh by 敖小剑\nService Mesh 将服务间通信能力下沉到基础设施，让应用解耦并轻量化。但 Service Mesh 本身的复杂度依然存在，CloudMesh 通过将 Service Mesh 托管在云上，使得您可以轻松的实践 Service Mesh 技术。通过我们的 workshop，您可以快速部署应用到 CloudMesh ，对服务进行访问，通过监控查看流量，体验服务治理、Sidecar管理和对服务的新版本进行灰度发布等实用功能。\n","date":1559643600,"description":"","dir":"activities/sofastack-cloud-native-workshop/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"57c6b0262af34c2010179000834d8363","permalink":"/activities/sofastack-cloud-native-workshop/","publishdate":"2019-06-04T10:20:00Z","readingtime":3,"relpermalink":"/activities/sofastack-cloud-native-workshop/","summary":"蚂蚁金服 SOFAStack 云原生工作坊（KubeCon China 2019 同场活动） 日期：2019年6月24日，星期一 时间：9:00 – 16:00 地点：上海世博中心 616 房间 注册费：免费","tags":["KubeCon","Workshop","Cloud Native"],"title":"KubeCon 上海同场活动 SOFAStack Cloud Native Workshop","type":"activities","url":"/activities/sofastack-cloud-native-workshop/","wordcount":1134},{"author":"花肉","categories":"SOFAMeetup","content":" 概要  活动主题：SOFA Meetup#2 上海站-使用 SOFAStack 快速构建微服务 活动时间：5 月 26 日周日下午 13 点 活动地点：上海市徐汇区田林路200号A7栋一楼 活动形式：线下活动 报名方式：https://tech.antfin.com/community/activities/576  活动介绍 蚂蚁金服 SOFAStack SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁金服自主研发的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，历经蚂蚁金服超过十年的业务历练。SOFAStack 于 2018 年 4 月宣布开源，并逐步开源 SOFABoot、SOFARPC、SOFALookout、SOFATracer、SOFAMosn、SOFAMesh 等组件。\n欢迎 Star 我：https://github.com/sofastack\nSOFA Meetup#2 上海站-使用 SOFAStack 快速构建微服务 适合自身企业的技术架构才是最佳的方案，SOFAStack 提供了一套的金融级解决方案，提供多种场景下需要的多种组件。\n5 月 26 日，SOFAMeetup#2 上海站，SOFAStack 开源核心成员集体出动。本期我们将侧重于各个落地的实际场景进行架构解析。\n分布式事务 Seata 详解、与 Spring Cloud 生态的融合案例、使用 SOFAStack 快速构建微服务 Demo 实操、更有最新开源的《让 AI 像 SQL 一样简单 — SQLFlow Demo 》首秀，期待与你不见不散~\n建议：参会者可带上电脑，现场有 Demo 实操环节~\n加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n点击即可报名 https://tech.antfin.com/community/activities/576\n议程    时间 环节 讲师     13:00 - 13:30 签到    13:30 - 14:15 《研发框架 SOFABoot 的特性及落地场景介绍》 蚂蚁金服 SOFABoot 负责人 善逝   14 :15 - 15:00 《谈注册中心 SOFARegistry 架构设计 》 蚂蚁金服 SOFARegistry 负责人 琪祥   15:00 - 15:05 茶歇休息    15:05 - 15:15 《SOFALab 社区共建分享》 SOFALab 杰出贡献者 米麒麟   15:15 - 16:00 《当 SpringCloud 遇上 SOFAStack》 蚂蚁金服 SOFAStack 开源组核心负责人 玄北   16:00 - 16:45 《分布式事务 Seata 实现原理及实践详解》 Seata Committer 绍辉   16:45 - 17:30 《使用 SOFAStack 快速构建微服务》SOFADashboard 演示 + SOFARegistry + SOFARPC + SOFAArk 蚂蚁金服 SOFADashboard 负责人 卫恒   17:30 - 17:45 《让 AI 像 SQL 一样简单 — SQLFlow Demo 演示》 蚂蚁金服深度学习引擎开源产品负责人 三平    ","date":1558437600,"description":"SOFA Meetup#2 上海站-使用 SOFAStack 快速构建微服务，5 月 26 日周日下午 13 点，上海市徐汇区田林路200号A7栋一楼。","dir":"activities/sofa-meetup-2/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5765309cb693fc8cc45958ecab5a9c3b","permalink":"/activities/sofa-meetup-2/","publishdate":"2019-05-21T11:20:00Z","readingtime":2,"relpermalink":"/activities/sofa-meetup-2/","summary":"概要 活动主题：SOFA Meetup#2 上海站-使用 SOFAStack 快速构建微服务 活动时间：5 月 26 日周日下午 13 点 活动地点：上海市徐汇区田林路200号A7栋一楼 活动形式：线","tags":["SOFAMeetup","SOFAStack"],"title":"SOFA Meetup#2 上海站——使用 SOFAStack 快速构建微服务","type":"activities","url":"/activities/sofa-meetup-2/","wordcount":849},{"author":"花肉","categories":"SOFAChannel","content":" 概要 活动主题：SOFAChannel#5：给研发工程师的代码质量利器 —— 自动化测试框架 SOFAActs\n活动时间：5 月 16 日周四晚 7 点\n活动形式：线上直播\n报名方式：https://tech.antfin.com/activities/552\n介绍 | SOFAChannel\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#5：给研发工程师的代码质量利器 —— 自动化测试框架 SOFAActs\n如何自动生成测试用例？\n如何减少测试用例设计过程中的阻力？\n如何进行精细化校验以及用例的高效管理，从而保障代码质量，有效提高测试效率？\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 线上直播第 5 期《给研发工程师的代码质量利器 —— 自动化测试框架 SOFAActs》报名开启！\n5 月 16 日周四晚 7 点，围绕蚂蚁金服的多年测试实践的积累与沉淀，自动化测试框架 SOFAActs 核心成员青勤将为大家带来精彩分享，不要错过哦。\nSOFAActs 是基于数据模型驱动测试引擎的的新一代测试框架，它的数据以 YAML 为载体，在此上构建基于数据模型的驱动引擎，适配 TestNg+SOFABoot 的测试上下文环境；支持高效、标准化构建用例，可视化编辑测试数据，精细化校验结果数据和自动清理 DB 数据，可以有效降低人工录入用例数据的成本，同时支持 API 重写提高测试代码的可扩展可复用性，提供特有注解提高测试代码编排的灵活性。\n| 加入 SOFA 钉钉互动群\n欢迎加入直播互动钉钉群：23127468（搜索群号加入即可）\n| 点击即可报名\nhttps://tech.antfin.com/activities/552\n议程 19:00-19:05 主持人开场\nSOFAGirl 主持人\n19:05-20:00 给研发工程师的代码质量利器 — 自动化测试框架 SOFAActs\n青勤 蚂蚁金服自动化测试框架 SOFAActs 核心成员\n本期分享大纲：\n 蚂蚁金服接口自动化测试理念 如何接入 SOFAActs 自动化测试框架 SOFAActs 自动化测试框架的基本使用  嘉宾  SOFAGirl 主持人 青勤 蚂蚁金服自动化测试框架 SOFAActs 核心成员  ","date":1557310800,"description":"5 月 16 日周四晚 7 点，线上直播。","dir":"activities/sofa-channel-5/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e000553c2c76f43ed8df5b8e56491a7b","permalink":"/activities/sofa-channel-5/","publishdate":"2019-05-08T10:20:00Z","readingtime":2,"relpermalink":"/activities/sofa-channel-5/","summary":"概要 活动主题：SOFAChannel#5：给研发工程师的代码质量利器 —— 自动化测试框架 SOFAActs 活动时间：5 月 16 日周四晚 7 点 活动形式：线上直播 报名方","tags":["SOFAChannel","SOFAActs"],"title":"SOFAChannel#5：给研发工程师的代码质量利器 —— 自动化测试框架 SOFAActs","type":"activities","url":"/activities/sofa-channel-5/","wordcount":733},{"author":"潘潘","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#4：分布式事务 Seata TCC 模式深度解析 活动时间：4 月 18 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 | SOFAChannel \u0026amp;lt;SOFA:Channel/\u0026amp;gt; 有趣实用的分布式架构频道：前沿技术、直播 Coding、观点“抬杠”，多种形式。\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n| SOFAChannel#4：分布式事务 Seata TCC 模式深度解析 4月初，分布式事务 Fescar 宣布进行品牌升级为 Seata，Seata 意为：Simple Extensible Autonomous Transaction Architecture，是一套一站式分布式事务解决方案。蚂蚁金服在 Seata 0.4.0 版本加入了 TCC 模式，后续也会持续输入。\n本期为 SOFAChannel 线上直播第 4 期，将邀请 蚂蚁金服 技术专家 \u0026amp;amp; Seata Committer 觉生 和大家一起探讨 《分布式事务 Seata TCC 模式深度解析》。\n本期分享大纲：\n TCC 模式基本原理解析 分布式事务并发控制解析 分布式事务空回滚与幂等解析 分布式事务防悬挂解析 分布式事务异步化解析  | 加入 SOFA 钉钉互动群 欢迎加入直播互动钉钉群：23195297（搜索群号加入即可）\n| 点击即可报名 https://tech.antfin.com/community/live/462\n议程 19:00-19:05 主持人开场 SOFAGirl 主持人\n19:05-20:00 分布式事务 Seata TCC 模式深度解析 觉生 蚂蚁金服 技术专家 / Seata Committer\n本期分享大纲：  TCC 模式基本原理解析 分布式事务并发控制解析 分布式事务空回滚与幂等解析 分布式事务防悬挂解析 分布式事务异步化解析  嘉宾  SOFAGirl 主持人 觉生 蚂蚁金服 技术专家 / Seata Committer  ","date":1554783000,"description":"4 月 18 日周四晚 7 点，线上直播第 4 期。","dir":"activities/sofa-channel-4/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0e0d4d4a75723cc408567a65aab0c3df","permalink":"/activities/sofa-channel-4/","publishdate":"2019-04-09T12:10:00+08:00","readingtime":2,"relpermalink":"/activities/sofa-channel-4/","summary":"概要 活动主题：SOFAChannel#4：分布式事务 Seata TCC 模式深度解析 活动时间：4 月 18 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章 介","tags":["SOFAChannel","Seata"],"title":"SOFAChannel#4：分布式事务 Seata TCC 模式深度解析","type":"activities","url":"/activities/sofa-channel-4/","wordcount":551},{"author":"xiaoyu","categories":"Soul","content":" Soul网关发布1.0.4-RELEASE版本  修复在1.0.3版本的后台管理中，出现的bug。 配置信息序列化方式支持自定义扩展。默认的序列化方式由kroy 改为了java序列化方式。 dubbo框架支持的更改。  对dubbo用户使用的更改。  在以前的版本中（1.0.2 or 1.0.3），dubbo的参数是通过header头上传递，在1.0.4版本中是通过body传递\n 更新了相关的文档信息。\n  关于使用1.0.4版本的建议。  1.0.4 版本支持用户自定义插件开发，支持正则表达式的匹配。\n dubbo参数传递的更改，我觉得这样会更加友好。\n  如果您之前使用的1.0.2版本，想要更新到1.0.4版本。  在插件表新增role字段。\n 重新启动1.0.4版本的管理后台。\n 执行同步所有插件（因为序列化方式的更改）\n 启动1.0.4版本的soul-web服务。\n  遇到问题？  添加qq群（429951241）\n 官网文档：https://dromara.org/website/zh-cn/docs/soul/soul.html\n github地址: https://github.com/Dromara/soul\n gitee地址： https://gitee.com/shuaiqiyu/soul\n  ","date":1554768000,"description":"Soul Gateway Released 1.0.4-RELEASE Version","dir":"blog/soul_1.0.4/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"24a9018ec5e398a9731790ef1315080f","permalink":"/en/blog/soul_1.0.4/soul_1.0.4/","publishdate":"2019-04-09T00:00:00Z","readingtime":1,"relpermalink":"/en/blog/soul_1.0.4/soul_1.0.4/","summary":"Soul网关发布1.0.4-RELEASE版本 修复在1.0.3版本的后台管理中，出现的bug。 配置信息序列化方式支持自定义扩展。默认的序列化","tags":["Soul"],"title":"Soul Gateway Released 1.0.4-RELEASE Version","type":"blog","url":"/en/blog/soul_1.0.4/soul_1.0.4/","wordcount":452},{"author":"xiaoyu","categories":"Soul","content":" Soul网关发布1.0.4-RELEASE版本  修复在1.0.3版本的后台管理中，出现的bug。 配置信息序列化方式支持自定义扩展。默认的序列化方式由kroy 改为了java序列化方式。 dubbo框架支持的更改。  对dubbo用户使用的更改。  在以前的版本中（1.0.2 or 1.0.3），dubbo的参数是通过header头上传递，在1.0.4版本中是通过body传递\n 更新了相关的文档信息。\n  关于使用1.0.4版本的建议。  1.0.4 版本支持用户自定义插件开发，支持正则表达式的匹配。\n dubbo参数传递的更改，我觉得这样会更加友好。\n  如果您之前使用的1.0.2版本，想要更新到1.0.4版本。  在插件表新增role字段。\n 重新启动1.0.4版本的管理后台。\n 执行同步所有插件（因为序列化方式的更改）\n 启动1.0.4版本的soul-web服务。\n  遇到问题？  添加qq群（429951241）\n 官网文档：https://dromara.org/website/zh-cn/docs/soul/soul.html\n github地址: https://github.com/Dromara/soul\n gitee地址： https://gitee.com/shuaiqiyu/soul\n  ","date":1554768000,"description":"Soul网关发布1.0.4-RELEASE版本","dir":"blog/soul_1.0.4/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"24a9018ec5e398a9731790ef1315080f","permalink":"/blog/soul_1.0.4/soul_1.0.4/","publishdate":"2019-04-09T00:00:00Z","readingtime":1,"relpermalink":"/blog/soul_1.0.4/soul_1.0.4/","summary":"Soul网关发布1.0.4-RELEASE版本 修复在1.0.3版本的后台管理中，出现的bug。 配置信息序列化方式支持自定义扩展。默认的序列化","tags":["Soul"],"title":"Soul网关发布1.0.4-RELEASE版本","type":"blog","url":"/blog/soul_1.0.4/soul_1.0.4/","wordcount":452},{"author":"xiaoyu","categories":"hmily","content":" Hmily 发布2.0.2-RELEASE 版本  解决SpringCloud 使用hystrix 配置线程池策略的问题。\n 新增对springcloud 内嵌事务调用的问题。\n 新增Hmily负载均衡策略。\n 其他bug的修护，与代码的优化。\n 去除不必须的第三方jar包。\n 零侵入方式的引入。\n  Hmily对现在流行RPC框架以及Spring的支持情况。  dubbo 2.7.0以下所有版本。\n Springcloud Dalston以上版本，包括支持现在的Finchley 与 Greenwich\n Motan 所有版本。\n 3.0以上所有Spring版本。\n  Hmily 在2.0.2版本对使用者RPC集群时候负载均衡策略。  hmily提供了自己实现的负载均衡策略，只是针对加了@Hmily的接口  dubbo 集群配置,配置负载方式为：loadbalance=\u0026amp;ldquo;hmily\u0026amp;rdquo;\n\u0026amp;lt;dubbo:reference timeout=\u0026amp;quot;50000\u0026amp;quot; interface=\u0026amp;quot;org.dromara.hmily.demo.dubbo.account.api.service.AccountService\u0026amp;quot; id=\u0026amp;quot;accountService\u0026amp;quot; retries=\u0026amp;quot;0\u0026amp;quot; check=\u0026amp;quot;false\u0026amp;quot; actives=\u0026amp;quot;20\u0026amp;quot; loadbalance=\u0026amp;quot;hmily\u0026amp;quot;/\u0026amp;gt;  Springcloud 在调用方的yml配置文件中新增：\nhmily ： ribbon: rule enabled : true  Hmily的具体使用文档：  官网文档 ：https://dromara.org/website/zh-cn/docs/hmily/index.html\n github地址: https://github.com/yu199195/hmily\n gitee地址： https://gitee.com/shuaiqiyu/hmily\n 欢迎大家star fork ，提供优秀的代码与建议。\n  ","date":1554422400,"description":"Hmily release 2.0.2-RELEASE version","dir":"blog/hmily_2.0.2/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7035d0f7d13567e4c4d66e7012266cc8","permalink":"/en/blog/hmily_2.0.2/hmily_2.0.2/","publishdate":"2019-04-05T00:00:00Z","readingtime":1,"relpermalink":"/en/blog/hmily_2.0.2/hmily_2.0.2/","summary":"Hmily 发布2.0.2-RELEASE 版本 解决SpringCloud 使用hystrix 配置线程池策略的问题。 新增对springcloud 内嵌事务调","tags":["hmily"],"title":"Hmily Released 2.0.2-RELEASE Version","type":"blog","url":"/en/blog/hmily_2.0.2/hmily_2.0.2/","wordcount":474},{"author":"xiaoyu","categories":"hmily","content":" Hmily 发布2.0.2-RELEASE 版本  解决SpringCloud 使用hystrix 配置线程池策略的问题。\n 新增对springcloud 内嵌事务调用的问题。\n 新增Hmily负载均衡策略。\n 其他bug的修护，与代码的优化。\n 去除不必须的第三方jar包。\n 零侵入方式的引入。\n  Hmily对现在流行RPC框架以及Spring的支持情况。  dubbo 2.7.0以下所有版本。\n Springcloud Dalston以上版本，包括支持现在的Finchley 与 Greenwich\n Motan 所有版本。\n 3.0以上所有Spring版本。\n  Hmily 在2.0.2版本对使用者RPC集群时候负载均衡策略。  hmily提供了自己实现的负载均衡策略，只是针对加了@Hmily的接口  dubbo 集群配置,配置负载方式为：loadbalance=\u0026amp;ldquo;hmily\u0026amp;rdquo;\n\u0026amp;lt;dubbo:reference timeout=\u0026amp;quot;50000\u0026amp;quot; interface=\u0026amp;quot;org.dromara.hmily.demo.dubbo.account.api.service.AccountService\u0026amp;quot; id=\u0026amp;quot;accountService\u0026amp;quot; retries=\u0026amp;quot;0\u0026amp;quot; check=\u0026amp;quot;false\u0026amp;quot; actives=\u0026amp;quot;20\u0026amp;quot; loadbalance=\u0026amp;quot;hmily\u0026amp;quot;/\u0026amp;gt;  Springcloud 在调用方的yml配置文件中新增：\nhmily ： ribbon: rule enabled : true  Hmily的具体使用文档：  官网文档 ：https://dromara.org/website/zh-cn/docs/hmily/index.html\n github地址: https://github.com/yu199195/hmily\n gitee地址： https://gitee.com/shuaiqiyu/hmily\n 欢迎大家star fork ，提供优秀的代码与建议。\n  ","date":1554422400,"description":"Hmily发布2.0.2-RELEASE版本","dir":"blog/hmily_2.0.2/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7035d0f7d13567e4c4d66e7012266cc8","permalink":"/blog/hmily_2.0.2/hmily_2.0.2/","publishdate":"2019-04-05T00:00:00Z","readingtime":1,"relpermalink":"/blog/hmily_2.0.2/hmily_2.0.2/","summary":"Hmily 发布2.0.2-RELEASE 版本 解决SpringCloud 使用hystrix 配置线程池策略的问题。 新增对springcloud 内嵌事务调","tags":["hmily"],"title":"Hmily发布2.0.2-RELEASE版本","type":"blog","url":"/blog/hmily_2.0.2/hmily_2.0.2/","wordcount":474},{"author":"潘潘","categories":"SOFAMeetup","content":" 概要  活动主题：SOFA Meetup#1 北京站——服务注册中心、分布式事务重磅发布 活动时间：3 月 24 日周日下午 13 点 活动地点：北京中关村创业大街 氪空间 活动形式：线下活动 活动视频回顾：https://tech.antfin.com/community/activities/382  活动介绍 蚂蚁金服 SOFAStack SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁金服自主研发的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，历经蚂蚁金服超过十年的业务历练。SOFAStack 于 2018 年 4 月宣布开源，并逐步开源 SOFABoot、SOFARPC、SOFALookout、SOFATracer、SOFAMosn、SOFAMesh 等组件。 欢迎 Star 我：https://github.com/alipay\nSOFA Meetup#1 北京站-服务注册中心、分布式事务重磅发布 这次的 Meetup 是 SOFAStack 第一场线下活动，也是 SOFA 开源一周年的线下庆祝会。\n我们将带来重磅发布，继续补充 SOFAStack 的开源大图，届时除了 SOFA 团队的见面交流之外，也安排了周年的庆祝环节，期待与朋友们的见面。\n重磅发布：开源蚂蚁金服分布式事务 蚂蚁金服内部的分布式事务框架已经发展十多年，广泛用于解决各类复杂业务场景的数据一致性问题，同时经受大规模业务挑战，在高性能、高可用等方面也积累了丰富的实践经验。\n这次，将带来蚂蚁金服分布式事务十多年的技术总结分享，同时也会宣布开源版本。\n重磅发布：开源蚂蚁金服注册中心 SOFARegistry SOFARegistry 是蚂蚁金服开源的服务注册中心。\nSOFARegistry： https://github.com/alipay/sofa-registry\nSOFARegistry 最早源自于淘宝的初版 ConfigServer，在支付宝/蚂蚁金服的业务发展驱动下，近十年间已经演进至第五代。目前 SOFARegistry 不仅全面应用于蚂蚁金服的自有业务，还随着蚂蚁金融科技输出，助力广大合作伙伴，同时也兼容开源生态。SOFARegistry 最新一代内部版与商业版，均以开源版为基础内核，在其上开发内部特性插件。\n更多内容，到现场来看\n加入 SOFA 钉钉互动群 群号：23127468，使用钉钉搜索群号即可加入，获取一手开源技术干货。\n议程    时间 环节 嘉宾介绍     13:00 - 13:30  签到    13:40 - 14:20  《SOFAStack 开源这一年》 蚂蚁金服技术总监 杨冰   14:20 - 15:00 《SOFARegistry \u0026amp;ndash; 蚂蚁金服高性能服务注册中心开源》 蚂蚁金服服务注册中心开源负责人 尚彧   15:00 - 15:20 庆祝一周年环节    15:20 - 16:00  《SOFAFescar \u0026amp;ndash; 蚂蚁金服分布式事务开源以及实践》 蚂蚁金服分布式事务开源负责人 绍辉   16:00 - 16:40 《SOFAJRaft \u0026amp;ndash; 蚂蚁金服基于 RAFT 一致性算法的生产级高性能 Java 实现》 蚂蚁金服 SOFAJRaft 核心成员 力鲲   16:40 - 17:00  互动交流     ","date":1552277400,"description":"SOFA Meetup#1 北京站，3 月 24 日周日下午 13 点，北京中关村创业大街氪空间。","dir":"activities/sofa-meetup-1/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dd713adb17a610bef8a0f6ed06cace55","permalink":"/activities/sofa-meetup-1/","publishdate":"2019-03-11T12:10:00+08:00","readingtime":3,"relpermalink":"/activities/sofa-meetup-1/","summary":"概要 活动主题：SOFA Meetup#1 北京站——服务注册中心、分布式事务重磅发布 活动时间：3 月 24 日周日下午 13 点 活动地点：北京中关村创业大街 氪空间 活动形式：","tags":["SOFAMeetup","SOFAStack"],"title":"SOFA Meetup#1 北京站——服务注册中心、分布式事务重磅发布","type":"activities","url":"/activities/sofa-meetup-1/","wordcount":1037},{"author":"花肉","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#3：SOFARPC 性能优化（下）—— 手把手带你性能调优（含 Demo） 活动时间：2 月 28 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 SOFA:Channel/，有趣实用的分布式架构频道\n前沿技术、直播 Coding、观点“抬杠”，多种形式\nSOFA:Channel/ 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n本期 SOFAChannel 为 SOFARPC 专场，分为上下两篇，将采用内容分享与 Demo 实际操作结合的形式进行。\n本期为上篇，下篇将在 2 月 28 日开展，记得关注哟~\n欢迎加入直播互动钉钉群：23127468（搜索群号加入即可）\n议程    SOFAChannel#2：SOFARPC 性能优化（上）—— 详解优化设计点 时间：2019-02-21      19:00-20:00 《SOFARPC 性能优化（上）—— 详解优化设计点》 蚂蚁金服 SOFA 团队 碧远    在业务规模大并发极高的情况下，RPC 对性能的追求就变得极为重要，任何一点小的优化都会累积提高业务整体性能。 本期手把手带你解读： 自定义通信协议使用有哪些注意细节？ SOFARPC 如何进行连接保持？ 在 IO 线程池中批量解包带来的性能提升有哪些？\n嘉宾 蚂蚁金服 SOFA 团队 碧远\n","date":1551349200,"description":"本次为下半场，2 月 28 日晚 7 点，线上直播。","dir":"activities/sofa-channel-3/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"202f32cfe0c8a1c3aacbf435389a956f","permalink":"/activities/sofa-channel-3/","publishdate":"2019-02-28T10:20:00Z","readingtime":1,"relpermalink":"/activities/sofa-channel-3/","summary":"概要 活动主题：SOFAChannel#3：SOFARPC 性能优化（下）—— 手把手带你性能调优（含 Demo） 活动时间：2 月 28 日周四晚 7 点 活动形","tags":["SOFAChannel","SOFARPC"],"title":"SOFAChannel#3：SOFARPC 性能优化（下）—— 手把手带你性能调优（含 Demo）","type":"activities","url":"/activities/sofa-channel-3/","wordcount":477},{"author":"花肉","categories":"SOFAChannel","content":" 概要  活动主题：SOFAChannel#2：SOFARPC 性能优化（上）—— 详解优化设计点 活动时间：2 月 21 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 SOFA:Channel/，有趣实用的分布式架构频道\n前沿技术、直播 Coding、观点“抬杠”，多种形式\nSOFA:Channel/ 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n本期 SOFAChannel 为 SOFARPC 专场，分为上下两篇，将采用内容分享与 Demo 实际操作结合的形式进行。\n本期为上篇，下篇将在 2 月 28 日开展，记得关注哟~\n欢迎加入直播互动钉钉群：23127468（搜索群号加入即可）\n议程    SOFAChannel#2：SOFARPC 性能优化（上）—— 详解优化设计点 时间：2019-02-21      19:00-20:00 《SOFARPC 性能优化（上）—— 详解优化设计点》 蚂蚁金服 SOFA 团队 碧远    在业务规模大并发极高的情况下，RPC 对性能的追求就变得极为重要，任何一点小的优化都会累积提高业务整体性能。 本期手把手带你解读： 自定义通信协议使用有哪些注意细节？ SOFARPC 如何进行连接保持？ 在 IO 线程池中批量解包带来的性能提升有哪些？\n嘉宾 蚂蚁金服 SOFA 团队 碧远\n","date":1550744400,"description":"本次为上半场，2 月 21 日晚 7 点，线上直播。","dir":"activities/sofa-channel-2/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3140990819c0601c041bf5091405220b","permalink":"/activities/sofa-channel-2/","publishdate":"2019-02-21T10:20:00Z","readingtime":1,"relpermalink":"/activities/sofa-channel-2/","summary":"概要 活动主题：SOFAChannel#2：SOFARPC 性能优化（上）—— 详解优化设计点 活动时间：2 月 21 日周四晚 7 点 活动形式：线上直播 直播视","tags":["SOFAChannel","SOFARPC"],"title":"SOFAChannel#2：SOFARPC 性能优化（上）—— 详解优化设计点","type":"activities","url":"/activities/sofa-channel-2/","wordcount":468},{"author":"花肉","categories":"SOFAChannel","content":"  活动主题：SOFAChannel#1——从蚂蚁金服微服务实践谈起 活动时间：1 月 17 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章  介绍 \u0026amp;lt;SOFA:Channel/\u0026amp;gt;，有趣实用的分布式架构频道\n前沿技术、直播 Coding、观点“抬杠”，多种形式\n\u0026amp;lt;SOFA:Channel/\u0026amp;gt; 将作为 SOFA 所有在线内容的承载，包含直播/音视频教程，集中体现 SOFAStack 的能力全景图。\n欢迎加入直播互动钉钉群：23127468（搜索群号加入即可）\n议程   SOFAChannel#1 《从蚂蚁金服微服务实践谈起》\n时间：2019-01-17 地点：浙江省杭州市西湖区线上直播   19:00-20:00   《从蚂蚁金服微服务实践谈起》 章耿 花名余淮（蚂蚁金服高级技术专家。目前在蚂蚁金服中间件服务与框架组负责应用框架及 SOFAStack 相关的工作）  嘉宾 蚂蚁金服 SOFA 团队 余淮\n视频回顾地址 https://tech.antfin.com/community/live/148\n","date":1547720400,"description":"首次 SOFAChannel 线上直播，1 月 17 日晚 7 点等你。","dir":"activities/sofa-channel-1/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5c79d2fac126784ef412f107470b2924","permalink":"/activities/sofa-channel-1/","publishdate":"2019-01-17T10:20:00Z","readingtime":1,"relpermalink":"/activities/sofa-channel-1/","summary":"活动主题：SOFAChannel#1——从蚂蚁金服微服务实践谈起 活动时间：1 月 17 日周四晚 7 点 活动形式：线上直播 直播视频回顾 直播回顾文章 介绍 \u0026","tags":["SOFAChannel","SOFARPC"],"title":"SOFAChannel#1——从蚂蚁金服微服务实践谈起","type":"activities","url":"/activities/sofa-channel-1/","wordcount":323},{"author":"xiaoyu","categories":"hmily","content":" Hmily高并发事务处理 开始先打个小小的广告 Hmily在参开源中国年度受欢迎投票 https://www.oschina.net/project/top_cn_2018?origin=zhzd 点击链接，搜索Hmily帮忙投下票,在第11横排第二个，感谢大家！ 也欢迎大家关注，或者提交pr，让Hmily变的更好，更完美。 gitHub: [https://github.com/yu199195/hmily] gitee: [https://gitee.com/shuaiqiyu/hmily]\n接下来回答一下 社区的一些问题，和大家一些疑惑的地方！\n1. Hmily的性能问题？ 答：Hmily是采用AOP切面的方式与你的RPC方法绑定，无非就是在你RPC调用的时候，保存了日志（通过异步disruptor），传递了一些参数。现在confrim，cancel也都为异步的调用，因此其性能与你的rpc性能一样。记住Hmily不生产事务，Hmily只是分布式事务的搬运工。之前Hmily在AOP切面加了一把锁，导致了性能下降，也就是Spring cloud 中国社区做的那篇文章。现在已经全部修复，并且全部异步化。其实那么测试时不合理的，因为是压测的demo，都是默认的配置。下文我会讲解，怎么样才能提高Hmiy性能。\n2. 关于RPC调用超时Hmily是怎么处理的？ 答： 我们支持在分布式环境中调用一个RPC方法，如果超时了。比如dubbo设置的超时时间是100ms,可能你的方法用了140ms,但是你的方法是执行成功了的。但是对调用方来说，你是失败的。这个时候需要回滚。所以Hmily的做法是。调用者认为你是失败的，不会将加入的回滚调用链条中。因此超时的rpc接口方，进行自身的回滚。会有一个定时任务来进行回滚，因为日志状态是try阶段，会调用cancel方法进行回滚，从而到达最终一致性！\n3.Hmily支持集群部署的问题？以及集群环境中，定时任务日志恢复的问题？ 答：Hmily是和你的应用AOP切面绑定在一起的，天然支持集群。集群环境中定时恢复问题，其实几乎没有，除非你的集群同时一下挂掉，才会有这个问题。当你集群同时挂掉，在恢复的时候，日志会有一个version字段，更新成功的，才会去进行恢复。\n4.Hmily是异步保存日志的，那么很极端情况下（代码刚好执行到这一行,然后jvm退出，断电啦什么的），日志还没保存那怎么处理呢？ 答:这种想法的，肯定是没看源码，或者是看了没怎么看懂。在AOP切面中，会先进行日志的异步保存，注意状态是PRE_TRY。在try执行完成后，更新为try。就算存在可能你说的什么断电，什么你在打断电调试，然后kill服务之类的。（Mysql我都可以让他事务失效，你信不信？）我只能说，不要花大力气去解决那些偶然的事情，最好的解决办法是不解决它。 Hmily针对高并发时候的参数配置调优。 可能这部门内容针对熟悉Hmily的人来说，不熟悉的也没关系。直接上github上看相关文档就好。 hmily支持Spring bean xml 方式的配置，同时也支持spring boot start yml方式的配置。\n\u0026amp;lt;bean id=\u0026amp;quot;hmilyTransactionBootstrap\u0026amp;quot; class=\u0026amp;quot;com.hmily.tcc.core.bootstrap.HmilyTransactionBootstrap\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;serializer\u0026amp;quot; value=\u0026amp;quot;kryo\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;recoverDelayTime\u0026amp;quot; value=\u0026amp;quot;120\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;retryMax\u0026amp;quot; value=\u0026amp;quot;3\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;loadFactor\u0026amp;quot; value=\u0026amp;quot;2\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;scheduledDelay\u0026amp;quot; value=\u0026amp;quot;120\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;scheduledThreadMax\u0026amp;quot; value=\u0026amp;quot;4\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;bufferSize\u0026amp;quot; value=\u0026amp;quot;4096\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;consumerThreads\u0026amp;quot; value=\u0026amp;quot;32\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;started\u0026amp;quot; value=\u0026amp;quot;false\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;asyncThreads\u0026amp;quot; value=\u0026amp;quot;32\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;repositorySupport\u0026amp;quot; value=\u0026amp;quot;db\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;tccDbConfig\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;bean class=\u0026amp;quot;com.hmily.tcc.common.config.TccDbConfig\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;url\u0026amp;quot; value=\u0026amp;quot;jdbc:mysql://192.168.1.98:3306/tcc?useUnicode=true\u0026amp;amp;amp;characterEncoding=utf8\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;driverClassName\u0026amp;quot; value=\u0026amp;quot;com.mysql.jdbc.Driver\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;username\u0026amp;quot; value=\u0026amp;quot;root\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;password\u0026amp;quot; value=\u0026amp;quot;123456\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/property\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt;   serializer :这里我推荐使用是kroy。当然hmily也支持hessian,protostuff,jdk。在我们测试中表现为: kroy\u0026amp;gt;hessian\u0026amp;gt;protostuff\u0026amp;gt;jdk\n recoverDelayTime :定时任务延迟时间（单位是秒，默认120。这个参数只是要大于你的rpc调用的超时时间设置。\n retryMax : 最大重复次数，默认3次。当你的服务down机，定时任务会执行retryMax次数去执行你的cancel还是confrim。 …","date":1542153600,"description":"Hmily Configuration Optimization For High Concurrent Transactions","dir":"blog/hmily_current/","fuzzywordcount":1900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"bbbc307fe80907d2722cd21dead57ed9","permalink":"/en/blog/hmily_current/hmily_current/","publishdate":"2018-11-14T00:00:00Z","readingtime":4,"relpermalink":"/en/blog/hmily_current/hmily_current/","summary":"Hmily高并发事务处理 开始先打个小小的广告 Hmily在参开源中国年度受欢迎投票 https://www.oschina.net/project/top_cn_2018?origin=zhzd 点击链接，搜索Hmily帮忙投下票,在第11横排第二个，感","tags":["hmily"],"title":"Hmily: Easy Handle Highly Concurrent Distributed Transactions","type":"blog","url":"/en/blog/hmily_current/hmily_current/","wordcount":1871},{"author":"xiaoyu","categories":"hmily","content":" Hmily高并发事务处理 开始先打个小小的广告 Hmily在参开源中国年度受欢迎投票 https://www.oschina.net/project/top_cn_2018?origin=zhzd 点击链接，搜索Hmily帮忙投下票,在第11横排第二个，感谢大家！ 也欢迎大家关注，或者提交pr，让Hmily变的更好，更完美。 gitHub: [https://github.com/yu199195/hmily] gitee: [https://gitee.com/shuaiqiyu/hmily]\n接下来回答一下 社区的一些问题，和大家一些疑惑的地方！\n1. Hmily的性能问题？ 答：Hmily是采用AOP切面的方式与你的RPC方法绑定，无非就是在你RPC调用的时候，保存了日志（通过异步disruptor），传递了一些参数。现在confrim，cancel也都为异步的调用，因此其性能与你的rpc性能一样。记住Hmily不生产事务，Hmily只是分布式事务的搬运工。之前Hmily在AOP切面加了一把锁，导致了性能下降，也就是Spring cloud 中国社区做的那篇文章。现在已经全部修复，并且全部异步化。其实那么测试时不合理的，因为是压测的demo，都是默认的配置。下文我会讲解，怎么样才能提高Hmiy性能。\n2. 关于RPC调用超时Hmily是怎么处理的？ 答： 我们支持在分布式环境中调用一个RPC方法，如果超时了。比如dubbo设置的超时时间是100ms,可能你的方法用了140ms,但是你的方法是执行成功了的。但是对调用方来说，你是失败的。这个时候需要回滚。所以Hmily的做法是。调用者认为你是失败的，不会将加入的回滚调用链条中。因此超时的rpc接口方，进行自身的回滚。会有一个定时任务来进行回滚，因为日志状态是try阶段，会调用cancel方法进行回滚，从而到达最终一致性！\n3.Hmily支持集群部署的问题？以及集群环境中，定时任务日志恢复的问题？ 答：Hmily是和你的应用AOP切面绑定在一起的，天然支持集群。集群环境中定时恢复问题，其实几乎没有，除非你的集群同时一下挂掉，才会有这个问题。当你集群同时挂掉，在恢复的时候，日志会有一个version字段，更新成功的，才会去进行恢复。\n4.Hmily是异步保存日志的，那么很极端情况下（代码刚好执行到这一行,然后jvm退出，断电啦什么的），日志还没保存那怎么处理呢？ 答:这种想法的，肯定是没看源码，或者是看了没怎么看懂。在AOP切面中，会先进行日志的异步保存，注意状态是PRE_TRY。在try执行完成后，更新为try。就算存在可能你说的什么断电，什么你在打断电调试，然后kill服务之类的。（Mysql我都可以让他事务失效，你信不信？）我只能说，不要花大力气去解决那些偶然的事情，最好的解决办法是不解决它。 Hmily针对高并发时候的参数配置调优。 可能这部门内容针对熟悉Hmily的人来说，不熟悉的也没关系。直接上github上看相关文档就好。 hmily支持Spring bean xml 方式的配置，同时也支持spring boot start yml方式的配置。\n\u0026amp;lt;bean id=\u0026amp;quot;hmilyTransactionBootstrap\u0026amp;quot; class=\u0026amp;quot;com.hmily.tcc.core.bootstrap.HmilyTransactionBootstrap\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;serializer\u0026amp;quot; value=\u0026amp;quot;kryo\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;recoverDelayTime\u0026amp;quot; value=\u0026amp;quot;120\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;retryMax\u0026amp;quot; value=\u0026amp;quot;3\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;loadFactor\u0026amp;quot; value=\u0026amp;quot;2\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;scheduledDelay\u0026amp;quot; value=\u0026amp;quot;120\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;scheduledThreadMax\u0026amp;quot; value=\u0026amp;quot;4\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;bufferSize\u0026amp;quot; value=\u0026amp;quot;4096\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;consumerThreads\u0026amp;quot; value=\u0026amp;quot;32\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;started\u0026amp;quot; value=\u0026amp;quot;false\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;asyncThreads\u0026amp;quot; value=\u0026amp;quot;32\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;repositorySupport\u0026amp;quot; value=\u0026amp;quot;db\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;tccDbConfig\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;bean class=\u0026amp;quot;com.hmily.tcc.common.config.TccDbConfig\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;url\u0026amp;quot; value=\u0026amp;quot;jdbc:mysql://192.168.1.98:3306/tcc?useUnicode=true\u0026amp;amp;amp;characterEncoding=utf8\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;driverClassName\u0026amp;quot; value=\u0026amp;quot;com.mysql.jdbc.Driver\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;username\u0026amp;quot; value=\u0026amp;quot;root\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;password\u0026amp;quot; value=\u0026amp;quot;123456\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/property\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt;   serializer :这里我推荐使用是kroy。当然hmily也支持hessian,protostuff,jdk。在我们测试中表现为: kroy\u0026amp;gt;hessian\u0026amp;gt;protostuff\u0026amp;gt;jdk\n recoverDelayTime :定时任务延迟时间（单位是秒，默认120。这个参数只是要大于你的rpc调用的超时时间设置。\n retryMax : 最大重复次数，默认3次。当你的服务down机，定时任务会执行retryMax次数去执行你的cancel还是confrim。 …","date":1542153600,"description":"Hmily针对高并发事务的配置优化","dir":"blog/hmily_current/","fuzzywordcount":1900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"bbbc307fe80907d2722cd21dead57ed9","permalink":"/blog/hmily_current/hmily_current/","publishdate":"2018-11-14T00:00:00Z","readingtime":4,"relpermalink":"/blog/hmily_current/hmily_current/","summary":"Hmily高并发事务处理 开始先打个小小的广告 Hmily在参开源中国年度受欢迎投票 https://www.oschina.net/project/top_cn_2018?origin=zhzd 点击链接，搜索Hmily帮忙投下票,在第11横排第二个，感","tags":["hmily"],"title":"Hmily: 轻松搞定高并发分布式事务","type":"blog","url":"/blog/hmily_current/hmily_current/","wordcount":1871},{"author":"xiaoyu","categories":"hmily","content":" Hmily框架特性[https://github.com/yu199195/hmily]  无缝集成Spring,Spring boot start。\n 无缝集成Dubbo,SpringCloud,Motan等rpc框架。\n 多种事务日志的存储方式（redis，mongdb,mysql等）。\n 多种不同日志序列化方式（Kryo,protostuff,hession）。\n 事务自动恢复。\n 支持内嵌事务的依赖传递。\n 代码零侵入,配置简单灵活。   Hmily为什么这么高性能？ 1.采用disruptor进行事务日志的异步读写（disruptor是一个无锁，无GC的并发编程框架） package com.hmily.tcc.core.disruptor.publisher; import com.hmily.tcc.common.bean.entity.TccTransaction; import com.hmily.tcc.common.enums.EventTypeEnum; import com.hmily.tcc.core.concurrent.threadpool.HmilyThreadFactory; import com.hmily.tcc.core.coordinator.CoordinatorService; import com.hmily.tcc.core.disruptor.event.HmilyTransactionEvent; import com.hmily.tcc.core.disruptor.factory.HmilyTransactionEventFactory; import com.hmily.tcc.core.disruptor.handler.HmilyConsumerDataHandler; import com.hmily.tcc.core.disruptor.translator.HmilyTransactionEventTranslator; import com.lmax.disruptor.BlockingWaitStrategy; import com.lmax.disruptor.IgnoreExceptionHandler; import com.lmax.disruptor.RingBuffer; import com.lmax.disruptor.dsl.Disruptor; import com.lmax.disruptor.dsl.ProducerType; import org.springframework.beans.factory.DisposableBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import java.util.concurrent.Executor; import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; import java.util.concurrent.atomic.AtomicInteger; /** * event publisher. * * @author xiaoyu(Myth) */ @Component public class HmilyTransactionEventPublisher implements DisposableBean { private Disruptor\u0026amp;lt;HmilyTransactionEvent\u0026amp;gt; disruptor; private final CoordinatorService coordinatorService; @Autowired public HmilyTransactionEventPublisher(final CoordinatorService coordinatorService) { this.coordinatorService = coordinatorService; } /** * disruptor start. * * @param bufferSize this is disruptor buffer size. * @param threadSize this is disruptor consumer thread size. */ public void start(final int bufferSize, final int threadSize) { disruptor = new Disruptor\u0026amp;lt;\u0026amp;gt;(new HmilyTransactionEventFactory(), bufferSize, r -\u0026amp;gt; { AtomicInteger index = new AtomicInteger(1); return new Thread(null, r, \u0026amp;quot;disruptor-thread-\u0026amp;quot; + index.getAndIncrement()); }, ProducerType.MULTI, new BlockingWaitStrategy()); final Executor executor = new ThreadPoolExecutor(threadSize, threadSize, 0, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026amp;lt;\u0026amp;gt;(), HmilyThreadFactory.create(\u0026amp;quot;hmily-log-disruptor\u0026amp;quot;, false), new ThreadPoolExecutor.AbortPolicy()); HmilyConsumerDataHandler[] consumers = new HmilyConsumerDataHandler[threadSize]; for (int i = 0; i \u0026amp;lt; threadSize; i++) { consumers[i] = new HmilyConsumerDataHandler(executor, coordinatorService); } disruptor.handleEventsWithWorkerPool(consumers); disruptor.setDefaultExceptionHandler(new IgnoreExceptionHandler()); disruptor.start(); } /** * publish disruptor event. * * @param tccTransaction {@linkplain …","date":1537833600,"description":"High-Performance Asynchronous Distributed Transaction TCC Framework","dir":"blog/hmily_introuduction/","fuzzywordcount":2700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4bd8179543b6449db19d6ee5824a60dd","permalink":"/en/blog/hmily_introuduction/introduction/","publishdate":"2018-09-25T00:00:00Z","readingtime":6,"relpermalink":"/en/blog/hmily_introuduction/introduction/","summary":"Hmily框架特性[https://github.com/yu199195/hmily] 无缝集成Spring,Spring boot start。 无缝","tags":["hmily"],"title":"Hmily: High-Performance Asynchronous Distributed Transaction TCC Framework","type":"blog","url":"/en/blog/hmily_introuduction/introduction/","wordcount":2682},{"author":"xiaoyu","categories":"hmily","content":" Hmily框架特性[https://github.com/yu199195/hmily]  无缝集成Spring,Spring boot start。\n 无缝集成Dubbo,SpringCloud,Motan等rpc框架。\n 多种事务日志的存储方式（redis，mongdb,mysql等）。\n 多种不同日志序列化方式（Kryo,protostuff,hession）。\n 事务自动恢复。\n 支持内嵌事务的依赖传递。\n 代码零侵入,配置简单灵活。   Hmily为什么这么高性能？ 1.采用disruptor进行事务日志的异步读写（disruptor是一个无锁，无GC的并发编程框架） package com.hmily.tcc.core.disruptor.publisher; import com.hmily.tcc.common.bean.entity.TccTransaction; import com.hmily.tcc.common.enums.EventTypeEnum; import com.hmily.tcc.core.concurrent.threadpool.HmilyThreadFactory; import com.hmily.tcc.core.coordinator.CoordinatorService; import com.hmily.tcc.core.disruptor.event.HmilyTransactionEvent; import com.hmily.tcc.core.disruptor.factory.HmilyTransactionEventFactory; import com.hmily.tcc.core.disruptor.handler.HmilyConsumerDataHandler; import com.hmily.tcc.core.disruptor.translator.HmilyTransactionEventTranslator; import com.lmax.disruptor.BlockingWaitStrategy; import com.lmax.disruptor.IgnoreExceptionHandler; import com.lmax.disruptor.RingBuffer; import com.lmax.disruptor.dsl.Disruptor; import com.lmax.disruptor.dsl.ProducerType; import org.springframework.beans.factory.DisposableBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import java.util.concurrent.Executor; import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; import java.util.concurrent.atomic.AtomicInteger; /** * event publisher. * * @author xiaoyu(Myth) */ @Component public class HmilyTransactionEventPublisher implements DisposableBean { private Disruptor\u0026amp;lt;HmilyTransactionEvent\u0026amp;gt; disruptor; private final CoordinatorService coordinatorService; @Autowired public HmilyTransactionEventPublisher(final CoordinatorService coordinatorService) { this.coordinatorService = coordinatorService; } /** * disruptor start. * * @param bufferSize this is disruptor buffer size. * @param threadSize this is disruptor consumer thread size. */ public void start(final int bufferSize, final int threadSize) { disruptor = new Disruptor\u0026amp;lt;\u0026amp;gt;(new HmilyTransactionEventFactory(), bufferSize, r -\u0026amp;gt; { AtomicInteger index = new AtomicInteger(1); return new Thread(null, r, \u0026amp;quot;disruptor-thread-\u0026amp;quot; + index.getAndIncrement()); }, ProducerType.MULTI, new BlockingWaitStrategy()); final Executor executor = new ThreadPoolExecutor(threadSize, threadSize, 0, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026amp;lt;\u0026amp;gt;(), HmilyThreadFactory.create(\u0026amp;quot;hmily-log-disruptor\u0026amp;quot;, false), new ThreadPoolExecutor.AbortPolicy()); HmilyConsumerDataHandler[] consumers = new HmilyConsumerDataHandler[threadSize]; for (int i = 0; i \u0026amp;lt; threadSize; i++) { consumers[i] = new HmilyConsumerDataHandler(executor, coordinatorService); } disruptor.handleEventsWithWorkerPool(consumers); disruptor.setDefaultExceptionHandler(new IgnoreExceptionHandler()); disruptor.start(); } /** * publish disruptor event. * * @param tccTransaction {@linkplain …","date":1537833600,"description":"高性能一部分不是事务TCC框架","dir":"blog/hmily_introduction/","fuzzywordcount":2700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4934fadfbfcab2af8ca6a7c5b0073e6c","permalink":"/blog/hmily_introduction/introduction/","publishdate":"2018-09-25T00:00:00Z","readingtime":6,"relpermalink":"/blog/hmily_introduction/introduction/","summary":"Hmily框架特性[https://github.com/yu199195/hmily] 无缝集成Spring,Spring boot start。 无缝","tags":["hmily"],"title":"Hmily: 高性能异步分布式事务TCC框架","type":"blog","url":"/blog/hmily_introduction/introduction/","wordcount":2682},{"author":null,"categories":null,"content":" What\u0026amp;rsquo;s Hmily？ Hmily is a high-performance, zero penetration, financial-level distributed transactions solution. At present, it mainly provides support for flexible transactions, including TCC, TAC (in which, it will automatically generate rollback SQL) schemes, and XA and more schemes will be supported in the future.\nFeatures  High reliability : It supports abnormal transaction rollback and transaction overtime abnormal recovery to prevent transaction suspension in distributed scenarios.\n Ease of use : It provides zero penetration Spring-Boot and Spring-Namespace schemes to integrate with business systems quickly.\n High performance : Decentralized design, fully integrated with business systems, and naturally supports cluster deployment.\n Observability : Performance monitoring of multiple metrics will be collected by Metrics, performance metrics is able to display in admin management system.\n Multiple RPC Framework support : It supports well-known RPC frameworks such as Dubbo, SpringCloud, Motan, brpc, tars, etc.\n Multiple log store medium support : It supports many mediums as log store, such as mysql, oracle, mongodb, redis, zookeeper, etc.\n Complex business scene : It supports transaction around nested RPC calls.\n  Requirements  The JDK version must be JDK8 or later.\n In TCC mode, you must use a RPC framework, such as: Dubbo, SpringCloud, Motan\n In TAC mode, you must use relational databases, such as: mysql, oracle, sqlsever\n  TCC Mode When using the TCC mode, you should provide three methods: try, confirm, and cancel according to your business requirements, and the confirm and cancel methods should be implemented by yourselves, the framework is only responsible for calling them to achieve transaction consistency.\nTAC Mode When using the TAC mode, you must use a relational database for business operations, and the framework will automatically generate a rollback SQL. When the business is abnormal, the rollback SQL will be executed to achieve transaction consistency\nAbout Hmily is a flexible distributed transaction solution that provides TCC and TAC modes.\nIt can be easily integrated by business with zero intrusion and rapid integration.\nIn terms of performance, log storage is asynchronous (optional) and uses asynchronous execution, without sacrificing business methods.\nIt was previously developed by myself personally, and it is currently restarted at JD Digital Technique Group, and it will become JD Digital Technique Group\u0026amp;rsquo;s distributed transaction solution in the future.\nSupport  If you have any questions, please join the QQ group for discussion  WeChat public account   ","date":-62135596800,"description":"","dir":"projects/hmily/overview/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"640bae80ac20018dd13658f8b7021ab1","permalink":"/en/projects/hmily/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/overview/","summary":"What\u0026rsquo;s Hmily？ Hmily is a high-performance, zero penetration, financial-level distributed transactions solution. At present, it mainly provides support for flexible transactions, including TCC, TAC (in which, it will automatically generate rollback SQL) schemes, and XA and more schemes will be supported in the future.\nFeatures  High reliability : It supports abnormal transaction rollback and transaction overtime abnormal recovery to prevent transaction suspension in distributed scenarios.\n Ease of use : It provides zero penetration Spring-Boot and Spring-Namespace schemes to integrate with business systems quickly.","tags":null,"title":"","type":"projects","url":"/en/projects/hmily/overview/","wordcount":381},{"author":null,"categories":null,"content":" Novel features:      Strong leader  Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of replicated logs and makes Raft easier to understand.    Leader election  Raft uses randomized timers to elect leaders. This reduces election conflicts simply and rapidly.    Membership change  Raft uses a new joint consensus approach.      Replicated state machines 1. Replicated state machines are implemented based on logs.  Each server stores a log. Each log entry contains a command. The state machine executes commands in order.  2. Consensus algorithms for practical systems typically have the following properties:  They ensure safety. They are highly available. They do not depend on the time sequence to ensure log consistency. A command can be completed as soon as a majority of the cluster has responded to a single round of remote procedure calls (RPCs).  Drawbacks of Paxos  Paxos is exceptionally difficult to understand. Paxos does not provide a good foundation for building practical implementations.  Raft design principles  Concept decomposition  Leader election Log replication Membership changes  Raft reduces the number of states to simplify the state space.  Raft does not allow log holes and restricts the possibilities of log inconsistency. Raft uses randomized timers to simplify the leader election.   Raft consistency algorithm State Persistent state on all servers (updated on stable storage before responding to RPCs):\n   currentTerm The latest term that the server gets (initialized to 0 on initial boot, increasing monotonically)     votedFor The candidateId that has received votes in the current term (or null if none).   Log[] Log entries. Each entry contains a command for the state machine, and the term when the entry was received by the leader.    Volatile state on all servers:\n   commitIndex The index of the highest log entry known to be committed.     lastApplied The index of the highest log entry applied to the state machine.    Volatile state on leaders:\n   nextIndex[] The index of the next log entry to be sent to each follower.     matchIndex[] The index of the highest log entry known to have been replicated on each follower.    AppendEntries RPC (log replication) Called by the leader to replicate log entries or used as heartbeats.\nArguments:\n   term leader\u0026amp;rsquo;s term     leaderId The leader\u0026amp;rsquo;s ID that can be used to redirect clients to the leader.   prevLogIndex The index of the preceding log entry.   prevLogTerm The term of the prevLogIndex entry.   entries[] The log entries to be stored (empty for heartbeat, and the leader may send more than one for efficiency).   leaderCommit The leader\u0026amp;rsquo;s commitIndex (for committed log entries).    Results:\n   term The currentTerm for the leader to update.     success True if the follower contains log entries matching prevLogIndex and prevLogTerm.    Receiver …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/raft-introduction/","fuzzywordcount":2600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b811e803d23b40da67657798801f8b51","permalink":"/en/projects/sofa-jraft/raft-introduction/","publishdate":"0001-01-01T00:00:00Z","readingtime":13,"relpermalink":"/en/projects/sofa-jraft/raft-introduction/","summary":"Novel features:      Strong leader  Raft uses a stronger form of leadership than other consensus algorithms. For example, log entries only flow from the leader to other servers. This simplifies the management of replicated logs and makes Raft easier to understand.    Leader election  Raft uses randomized timers to elect leaders. This reduces election conflicts simply and rapidly.    Membership change  Raft uses a new joint consensus approach.","tags":null,"title":"'Introduction to the Raft algorithm'","type":"projects","url":"/en/projects/sofa-jraft/raft-introduction/","wordcount":2562},{"author":null,"categories":null,"content":" Open ACTS IDE In the Packages view, right click the function name annotated by @Test, and choose ACTS Function \u0026amp;gt; Edit Test Case as shown in the following figure.\nWrite test data Prepare request parameters Prepare correct request parameter data for the request parameters (type, order, and quantity) of the tested method. The parameters are divided into simple and complex types. Simple parameters include parameter types String, Date, Integer, Float, Double, Long, Short, and Byte (including their corresponding basic types, such as int and float). Complex parameters include parameter types List, Map, Set, custom class, Java defined class, and their nested expressions.\nSimple request parameters Right click Request Parameters, choose Select Model, and click Simple Type in the pop-up to select simple parameters.\nAfter importing simple request parameters, enter their values directly in the field as shown in the preceding figure. Parameters listed top down are the first, second, and third parameters of the tested method. You can right click a parameter to adjust its order.\nComplex parameters As shown in Figure 27, you need to generate request parameter models for the AccountTransRequest class and the BusinessActionContext class. Generally, class models of the tested method\u0026amp;rsquo;s request parameters and responses are automatically generated along with the test script. You can open ACTS IDE to edit class models of request parameters as shown in Figure 28.\nFigure 28\nIf the models of the method\u0026#39;s request parameters and responses are not identified when you generate the test script, first generate models of complex request parameters and responses (for detailed operation steps, see [Generate object model](../usage-model/#generate-object-model)). Then open ACTS IDE, right click Request Parameters, choose Select Model, and click Complex Type in the pop-up to add complex objects. After this, you can view and edit complex objects under Request Parameters. ![Complex type](complex-type.png) ### List ![List example](list-example.png) ![Edit value](edit-value.png) ### Map See example 2 (the Set type is similar). In Figure 32, request parameters of the method shown in sample 2 is the `Map` type. Objects do not belong to a specific type. If you want to set an object as a complex one, edit the YAML file. For example, if you want to set an object as the AccountTransResult class, edit the YAML file as follows: ![Map examples](map-example.png) Figure 32\n![ Change type](change-type.png) ![Set property values](set-value.png) ### enum Example code: ![Example code](sample.png) 1. You can edit the values in ACTS IDE as follows: ![Edit value](change-value.png) 2. If an enum type class is nested in another class, set the value of the enum type to DEBIT in the CSV model of the class. 3. Figure 37 shows the test case data in the YAML file. ```yaml interestRecoverTypeEnum: !!com.alipay.fc.loancore.common.util.enums.InterestRecoverTypeEnum \u0026#39;ALL\u0026#39; ``` ![YAML data](yaml-data.png) …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-ide/","fuzzywordcount":2000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"697e7e6d35a2e058f3ca8b0a72032690","permalink":"/en/projects/sofa-acts/usage-ide/","publishdate":"0001-01-01T00:00:00Z","readingtime":10,"relpermalink":"/en/projects/sofa-acts/usage-ide/","summary":"Open ACTS IDE In the Packages view, right click the function name annotated by @Test, and choose ACTS Function \u0026gt; Edit Test Case as shown in the following figure.\nWrite test data Prepare request parameters Prepare correct request parameter data for the request parameters (type, order, and quantity) of the tested method. The parameters are divided into simple and complex types. Simple parameters include parameter types String, Date, Integer, Float, Double, Long, Short, and Byte (including their corresponding basic types, such as int and float).","tags":null,"title":"All-in-one editor","type":"projects","url":"/en/projects/sofa-acts/usage-ide/","wordcount":1929},{"author":null,"categories":null,"content":" Introduction To understand the usage mode of Jarslink2.0, you need to have a certain understanding of the SOFAArk framework and the packaging of Ark packages and Ark Biz packages.\nTo ensure the consistency of reading, here is a rough description of the packaging logic of the application\u0026amp;rsquo;s use of Jarslink2.0. The official recommendation is to jump to the above-mentioned link to obtain the necessary background knowledge.\nJarslink2.0 requires an application type of Spring Boot or SOFABoot. Before introducing new modes of application packaging, let\u0026amp;rsquo;s see why it is necessary for Spring Boot/SOFABoot applications to introduce new packaging modes after using Jarslink2.0.\nBackground At runtime, Jarslink2.0 works as an Ark Plugin of the SOFAArk framework, which must be introduced for the use of Jarslink2.0. The Jarslink2.0 plugin will not be loaded and started until the SOFAArk container is started. As we know, when official Spring Boot projects use plugins:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  An executable FatJar will be packaged, which contains all the dependencies, configurations, and other resources required by the running application. The FatJar entry method is the main method of the Spring Boot application. The packaging logic of a SOFABoot project is the same as that of a Spring Boot project.\nAfter the application introduces Jarslink2.0, which needs to depend on the SOFAArk framework, the FatJar that is packaged by a new packaging mode needs to contain a SOFAArk container. The FatJar entry method also needs to be replaced by the SOFAArk container startup method, because the startup of SOFAArk takes precedence over the execution of the FatJar. The SOFAArk container starts all the Ark Plugins in turn before finally starting the application. Therefore, a new packaging mode is needed, and SOFAArk provides a packaging plugin.\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  responsible for packaging Spring Boot/SOFABoot applications into an executable FatJar called Ark package.\nPackaging Type In the previous section, we have described why the use of Jarslink 2.0 needs to introduce a packaging mode that is different from the official Spring Boot packaging mode, and brought out the first packaging type—Ark package. Now let\u0026amp;rsquo;s summarize the features of Ark package:\n Ark package is an executable FatJar package format customized by the SOFAArk framework, the details of which are available in Reference Documents. The Ark package contains all the configurations, dependencies, and other resources required by the running application. Its packaging logic is similar to that of Spring Boot, and it also contains the Ark Plugin and the SOFAArk framework on which the application depends. The SOFAArk framework does not need to be introduced into the …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-repackage/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a9cb3c5d3fa32c5f6c476d9ed80c80cb","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-repackage/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-repackage/","summary":"Introduction To understand the usage mode of Jarslink2.0, you need to have a certain understanding of the SOFAArk framework and the packaging of Ark packages and Ark Biz packages.\nTo ensure the consistency of reading, here is a rough description of the packaging logic of the application\u0026rsquo;s use of Jarslink2.0. The official recommendation is to jump to the above-mentioned link to obtain the necessary background knowledge.\nJarslink2.0 requires an application type of Spring Boot or SOFABoot.","tags":null,"title":"Application packaging","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-repackage/","wordcount":749},{"author":null,"categories":null,"content":" RheaKV: an embedded, distributed, highly available, and strongly consistent KV storage class library that is implemented based on JRaft and RocksDB. AntQ Streams QCoordinator: uses JRaft to implement elections and meta information storage in the Coordinator cluster. Metadata management module of SOFARegistry: an IP address registration. The data held by all nodes must be consistent, and the normal data storage must not be affected when a minority of nodes fail. AntQ NameServer leader election  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/user-stories/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b233be7d9eed33645945293e637e28ea","permalink":"/en/projects/sofa-jraft/user-stories/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-jraft/user-stories/","summary":"RheaKV: an embedded, distributed, highly available, and strongly consistent KV storage class library that is implemented based on JRaft and RocksDB. AntQ Streams QCoordinator: uses JRaft to implement elections and meta information storage in the Coordinator cluster. Metadata management module of SOFARegistry: an IP address registration. The data held by all nodes must be consistent, and the normal data storage must not be affected when a minority of nodes fail.","tags":null,"title":"Application scenarios","type":"projects","url":"/en/projects/sofa-jraft/user-stories/","wordcount":74},{"author":null,"categories":null,"content":" ﻿## Architecture diagram Jarslink 2.0 is an Ark plugin and needs to depend on the SOFAArk container at runtime. Jarslink 2.0 is in the middle layer between the applications and the containers at runtime. Boundary interaction mode: + 1. Application boundaries: Jarslink 2.0 configures export classes that can be directly used by the applications. Such classes are loaded by Jarslink at runtime. + 2. Container boundaries: The Ark plugin can interact with the SOFAArk container by using the exposed extension points and services. Jarslink expanded the BizDeployer implementation and referenced BizManagerService and BizFactoryService container services.\nModule division The implementation classes of each module appear only in their own modules and are generally not cross-dependent. Required cross-dependencies will be moved into the core module. Detailed module descriptions can be found in the following table:\n   Module name Sub-module name Description Dependency relationship     bom  Dependent on version control None   core common Common module with log classes None   core spi SPI module, defining basic interfaces and commands None   core-impl runtime Jarslink runtime management, processing commands Core   integration  Ark plugin packaging module, implementing SOFAArk container service extension point and referencing container services All    ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-structure/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fa09c01de689002edbd0bea7f68fe66e","permalink":"/en/projects/sofa-boot/sofa-jarslink-structure/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-structure/","summary":"﻿## Architecture diagram Jarslink 2.0 is an Ark plugin and needs to depend on the SOFAArk container at runtime. Jarslink 2.0 is in the middle layer between the applications and the containers at runtime. Boundary interaction mode: + 1. Application boundaries: Jarslink 2.0 configures export classes that can be directly used by the applications. Such classes are loaded by Jarslink at runtime. + 2. Container boundaries: The Ark plugin can interact with the SOFAArk container by using the exposed extension points and services.","tags":null,"title":"Architecture","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-structure/","wordcount":188},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-biz/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d7f80ce6a00d914546e642c887d23a01","permalink":"/en/projects/sofa-boot/sofa-ark-ark-biz/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/en/projects/sofa-boot/sofa-ark-ark-biz/","summary":"","tags":null,"title":"Ark Biz","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-ark-biz/","wordcount":0},{"author":null,"categories":null,"content":" 简介 本小节将介绍 Ark Biz 目录结构，以及如何使用官方插件 sofa-ark-maven-plugin 打包并发布 Ark Biz。\nArk Biz 包和 Ark 包 都是使用 Maven 插件 sofa-ark-maven-plugin 打包生成；工程应用在配置该插件时，默认情况下只会打包发布 Ark 包， 只有在配置参数 attach 为 true 时，才会打包发布 Ark Biz：\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;/excution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;attach\u0026amp;gt;false\u0026amp;lt;/attach\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  那 Ark Biz 和 Ark 包 有什么区别呢？ 简单来说，Ark Biz 是工程应用所有资源的组织单元，它包含了应用启动所需的所有资源，详细可参考下文描述的 Ark Biz 目录格式；而工程应用打出来的 Ark 包，是一个通过 java -jar 启动，运行在 SOFAArk 容器的 Fat Jar，不仅包含应用工程对应的 Ark Biz，也包含 Ark Container，以及应用依赖的 Ark Plugin；\n通常情况，只需要发布 Ark 包 即可，但是 SOFAArk 是支持运行多个 Ark Biz的，因此如果开发者希望自己应用的 Ark Biz 包能够被其他应用直接当成 Jar 包依赖，进而运行在同一个 SOFAArk 容器之上，那么就需要打包发布 Ark Biz 包；\nArk-Biz 典型目录结构 . ├── META-INF │ ├── MANIFEST.MF │ ├── maven │ │ └── me.qlong.tech │ │ └── sofa-boot-demo3-web │ │ ├── pom.properties │ │ └── pom.xml │ └── sofa-boot-demo3 │ └── sofa-boot-demo3-web.xml ├── com │ └── alipay │ └── sofa │ └── ark │ └── biz │ └── mark ├── config │ ├── application-dev.properties │ ├── application-test.properties │ └── application.properties ├── lib │ ├── spring-beans-4.3.4.RELEASE.jar │ ├── spring-boot-1.4.2.RELEASE.jar │ ├── spring-boot-autoconfigure-1.4.2.RELEASE.jar │ ├── spring-boot-devtools-1.4.2.RELEASE.jar │ ├── spring-boot-starter-1.4.2.RELEASE.jar │ ├── spring-boot-starter-logging-1.4.2.RELEASE.jar │ ├── spring-boot-starter-tomcat-1.4.2.RELEASE.jar │ ├── spring-boot-starter-web-1.4.2.RELEASE.jar │ ├── spring-context-4.3.4.RELEASE.jar │ ├── spring-core-4.3.4.RELEASE.jar │ ├── spring-expression-4.3.4.RELEASE.jar │ ├── spring-web-4.3.4.RELEASE.jar │ ├── ... │ ├── ... │ ├── ... │ └── velocity-1.7.jar ├── logback-spring.xml ├── me │ └── qlong │ └── tech │ └── SOFABootWebSpringApplication.class └── static └── index.html  上述目录结构相关文件和目录说明如下：\n普通的 Java 工程或者 Spring Boot Core/Web 工程都可以打包成 Ark Biz；Ark Biz 没有固定的目录格式，它只是在原来 Jar 包结构基础上新增两个目录文件：\n com/alipay/sofa/ark/biz/mark : 标记文件，标记该 Jar 包是 sofa-ark-maven-plugin 打包生成的 Ark Biz 文件；\n lib/ : lib 目录存放工程应用的三方依赖，\n  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-biz/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d7f80ce6a00d914546e642c887d23a01","permalink":"/projects/sofa-boot/sofa-ark-ark-biz/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-biz/","summary":"简介 本小节将介绍 Ark Biz 目录结构，以及如何使用官方插件 sofa-ark-maven-plugin 打包并发布 Ark Biz。 Ark Biz 包和 Ark 包 都是使用 Maven 插件 sofa-ark-maven-plugin 打包生成；工程应用在配置该插件时，默认情","tags":null,"title":"Ark Biz","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-biz/","wordcount":700},{"author":null,"categories":null,"content":" SOFAArk 合并部署时，除了宿主应用，其他 Biz 允许运行时动态部署和卸载。Biz 的状态如下：\n unresolved: 未注册，此时 Biz 包未被运行时解析 resolved: Biz 包解析完成，且已注册，此时 Biz 包还没有安装或者安装中 activated: Biz 包启动完成，且处于激活状态，可以对外提供服务 deactivated: Biz 包启动完成，但出于未激活状态，模块多个版本时，只有一个版本出于激活状态(注意这个状态只对 JVM 服务生效，对 RPC 等其他中间件无效) broken: Biz 包启动失败后状态  目前 SOFAArk 提供了三种方式操作 Biz:\n 编程 API Zookeeper 动态配置 Telnet 指令  本质上，后两种都是通过编程 API 操作 Biz，所以在这里详细描述通过编程 API 控制 Biz 的生命周期。SOFAArk 提供客户端 ArkClient 操作 Biz, 主要包含三条指令：\n install: 安装 Biz，虽然有多个重载方法，本质是接受 bizFile 文件作为入参 uninstall: 卸载 Biz，运行时 Biz 是由 bizName 和 bizVersion 唯一确定的，因此需要这两个入参 switch: 激活 Biz，SOFAArk 运行部署多个相同名称不同版本的 Biz，但是运行时只有一个 Biz 被激活（JVM 服务对外可用）；当使用 switch 指令激活其他版本时，当前处于激活状态的 Biz 将切换到钝化，同样也需要 bizName 和 bizVersion 作为入参  注意：部署相同名称不同版本 Biz 时，如果已有激活的版本，后续部署的其他版本 Biz 将自动处于钝化状态\n安装 Biz 以 Spring Boot/SOFABoot 为例，应用(模块)安装包含以下流程：\n 解析模块 \u0026amp;gt; SOFAArk 容器会解析文件流，读取 Biz 配置，创建 BizClassLoader 等，生成 Biz 运行时模型\n 注册模块 \u0026amp;gt; 注册解析后的 Biz 模型，设置状态为 resolved\n 启动模块 \u0026amp;gt; 执行 Biz 的入口方法，完成上下文的刷新，如果报错则对外抛出异常\n 健康检查 \u0026amp;gt; 启动完成，此时 Biz 还没有切换至下一个状态，将会执行应用健康检查，健康检查参考 SOFABoot 文档，健康检查失败则抛出异常，如果应用没有引入 SOFABoot 健康检查依赖，则跳过\n 切换状态 \u0026amp;gt; 健康检查成功，会切换 Biz 状态；如果不存在其他版本 Biz 处于激活状态，则切换状态至 Activated，否则切换状态至 DeActivated\n  注意：启动模块时抛出异常，均导致 Biz 启动失败，可以查看 sofa-ark/common-error.log 日志\n卸载 Biz 应用（模块）卸载包含以下流程：\n 切换 Biz 状态至少 deactivated \u0026amp;gt; 钝化 Biz, 防止流量进入正在卸载的 Biz\n 关闭 ApplicationContext \u0026amp;gt; 关闭 Biz 的 Spring 上下文，如果用户需要自定义卸载操作，可以监听 ContextClosedEvent 事件\n 注销 JVM 服务 \u0026amp;gt; SOFAArk 运行时注销 Biz 发布的 JVM 服务\n 发送卸载事件 \u0026amp;gt; 通知所有 Ark Plugin 和 Ark Biz，正在卸载某个 Biz\n 清楚缓存 \u0026amp;gt; SOFAArk 运行时注销所有和该 Biz 相关的缓存\n 切换 Biz 状态为 unresolved \u0026amp;gt; Biz 执行完所有卸载操作时，将状态置为 unresolved\n  卸载面临的挑战 卸载 Biz 最大的挑战在于 ClassLoader 的卸载，如果 ClassLoader 没有卸载干净，极有可能会导致 metaspace OOM. JDK 对 Class 的回收条件非常苛刻，包含：\n 该类所有实例都已经回收 加载该类的 ClassLoader 已经回收 该类对应的 java.lang.Class 对象已经没有在任何地方被引用，无法在任何地方通过反射访问该类的方法  每个 Biz 都由独立的 BizClassLoader 加载，只要该 Biz 的加载的类或对象或 ClassLoader 被其他 Biz 或 Plugin 引用，则会导致 Biz 无法卸载成功\n激活 Biz 激活指令用于设置 Biz 状态为 Activated，如果此时已有其他版本 Biz 处于激活状态，则先设置其为 Deactivated，再激活指定的 Biz 为 Activated. 激活状态是相对 JVM 服务而言，只有被激活的 Biz，其发布的 JVM 服务才能被其他 Biz 引用\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-biz-lifecycle/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"090ee6596c6808339fa3233139903040","permalink":"/projects/sofa-boot/sofa-ark-biz-lifecycle/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-biz-lifecycle/","summary":"SOFAArk 合并部署时，除了宿主应用，其他 Biz 允许运行时动态部署和卸载。Biz 的状态如下： unresolved: 未注册，此时 Biz 包未被运行时解析 resolved: Biz 包解析完成，且已注册，此时","tags":null,"title":"Ark Biz 生命周期","type":"projects","url":"/projects/sofa-boot/sofa-ark-biz-lifecycle/","wordcount":1195},{"author":null,"categories":null,"content":" ﻿This section will introduce the directory structure of standard Ark package and how to use the maven plugin of sofa-Ark-maven-plugin to package and release an Ark package.\nMaven plugin The officially provided Maven plugin sofa-Ark-maven-plugin can package common Java projects or Spring Boot projects into standard-format Ark packages. Based on Fat Jar technology, we can directly start an Ark package with the java -jar command. The Maven plugin coordinates are:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  Goals The sofa-Ark-maven-plugin plugin provides goal: repackage, which can package the project into an executable Ark package, it can be configured as follows:\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;/excution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- Configuration information --\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  Complete configuration template Complete sofa-Ark-maven-plugin configuration template is as follows:\n\u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;0.1.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--The default packaging storage directory for Ark package and Ark biz is the project build directory--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;../target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--The name of the generated Ark package file is ${artifactId} by default--\u0026amp;gt; \u0026amp;lt;finalName\u0026amp;gt;demo-ark\u0026amp;lt;/finalName\u0026amp;gt; \u0026amp;lt;!--Whether to skip execution of goal:repackage (false by default)--\u0026amp;gt; \u0026amp;lt;skip\u0026amp;gt;false\u0026amp;lt;/skip\u0026amp;gt; \u0026amp;lt;!--Whether to package, install, and release Ark biz (false by default). Refer to the Ark Biz file for details --\u0026amp;gt; \u0026amp;lt;attach\u0026amp;gt;true\u0026amp;lt;/attach\u0026amp;gt; \u0026amp;lt;!--Set the classifier of Ark package, which is null by default--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;ark-classifier\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;!-- Set the classifier of Ark biz, which is Ark-biz by default --\u0026amp;gt; \u0026amp;lt;bizClassifier\u0026amp;gt;ark-biz-classifier\u0026amp;lt;/bizClassifier\u0026amp;gt; \u0026amp;lt;!--Exclude the specified package dependency when packaging the Ark biz. The format is: ${groupId:artifactId} or ${groupId:artifactId:classifier}--\u0026amp;gt; \u0026amp;lt;excludes\u0026amp;gt; \u0026amp;lt;exclude\u0026amp;gt;org.apache.commons:commons-lang3\u0026amp;lt;/exclude\u0026amp;gt; \u0026amp;lt;/excludes\u0026amp;gt; \u0026amp;lt;!--Exclude the package dependency that is the same as the specified groupId when packaging the Ark biz--\u0026amp;gt; \u0026amp;lt;excludeGroupIds\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-jar/","fuzzywordcount":1400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c2a9b8ad142f15b9ee82d7d9d8237850","permalink":"/en/projects/sofa-boot/sofa-ark-ark-jar/","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/en/projects/sofa-boot/sofa-ark-ark-jar/","summary":"﻿This section will introduce the directory structure of standard Ark package and how to use the maven plugin of sofa-Ark-maven-plugin to package and release an Ark package.\nMaven plugin The officially provided Maven plugin sofa-Ark-maven-plugin can package common Java projects or Spring Boot projects into standard-format Ark packages. Based on Fat Jar technology, we can directly start an Ark package with the java -jar command. The Maven plugin coordinates are:","tags":null,"title":"Ark JAR package","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-ark-jar/","wordcount":1317},{"author":null,"categories":null,"content":" ﻿This section will introduce the standard specifications and directory structure of Ark Plugin and how to use the maven plugin of sofa-ark-plugin-maven-plugin to package and release it.\nPlugin Specifications A standard Ark Plugin should meet the following specifications:\n The plugin should have a name (default is ${artifactId}). At runtime, duplicate names are not allowed. In other words, the name will be used as the unique ID of Ark Plugin;\n A plugin must be configured with a priority (default is 1,000): the lower the number, the higher the priority;\n A plugin should be configured with no more than one entry class activator, a portal for the container startup plugin used to implement the com.alipay.sofa.ark.spi.service.PluginActivator interface class in a uniform way. The plugin with higher priority will start up first;\n Import classes support both package and class levels. They are loaded first from other plugins;\n Export classes support package and class levels. Plugins with higher priority will be exported first;\n Support importing resources from the classpath (wildcard is not supported). Specified resources will be searched for from other plugins first;\n Support exporting resources from the classpath (wildcard is not supported); resources with higher priority will be exported first;\n  Maven plugins The officially provided Maven plugin sofa-ark-plugin-maven-plugin can package projects into a standard-format Ark Plugin. The coordinates of Maven plugin are:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  Goals The sofa-ark-plugin-maven-plugin plugin provides goal: ark-plugin, which can be used to package the project into a standard-format Ark Plugin. Configurations are as follows:\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;/excution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- Configuration information --\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  Complete configuration template The complete configuration template of the sofa-ark-plugin-maven-plugin plugin is shown as follows:\n\u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- Specify the directory to package ${pluginName}.ark.plugin (${project. build. directory} is the default location) --\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-plugin/","fuzzywordcount":1300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"684dc1caa0f834eec498905f95913f83","permalink":"/en/projects/sofa-boot/sofa-ark-ark-plugin/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/sofa-boot/sofa-ark-ark-plugin/","summary":"﻿This section will introduce the standard specifications and directory structure of Ark Plugin and how to use the maven plugin of sofa-ark-plugin-maven-plugin to package and release it.\nPlugin Specifications A standard Ark Plugin should meet the following specifications:\n The plugin should have a name (default is ${artifactId}). At runtime, duplicate names are not allowed. In other words, the name will be used as the unique ID of Ark Plugin;","tags":null,"title":"Ark Plugin","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-ark-plugin/","wordcount":1242},{"author":null,"categories":null,"content":" 本小节将介绍 Ark Plugin 的标准规范和目录结构，以及如何使用官方插件 sofa-ark-plugin-maven-plugin 打包发布 Ark Plugin。\n插件规范 标准的 Ark Plugin 需要满足以下规范：\n 插件必须配置插件名，默认为 ${artifactId} ；运行时，不允许存在同名的插件，可以认为它是 Ark Plugin 的唯一 ID;\n 插件必须配置优先级，默认为1000，数字越低表示优先级越高；\n 插件最多配置一个入口类 activator ，它是容器启动插件的入口，统一实现 com.alipay.sofa.ark.spi.service.PluginActivator 接口类；优先级高的插件优先启动；\n 导入类支持 package 和 class 两个级别；导入类优先从其他的插件加载；\n 导出类支持 package 和 class 两个级别；优先级高的插件优先导出；\n 支持导入 classpath 中资源，不支持通配符；优先从其他插件中查找指定资源；\n 支持导出 classpath 中资源，不支持通配符；优先级高的插件优先导出；\n  Maven 插件 官方提供 Maven 插件 sofa-ark-plugin-maven-plugin 可以将工程打包成标准格式的 Ark Plugin ； Maven 插件坐标为：\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${demo.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  Goals sofa-ark-plugin-maven-plugin 插件提供 goal: ark-plugin，可以将工程打包成标准格式的 Ark Plugin, 如下配置：\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- 配置信息 --\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  完整配置模板 完整的 sofa-ark-plugin-maven-plugin 插件配置模板如下：\n\u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- 指定打包的 ${pluginName}.ark.plugin 存放目录; 默认放在 ${project.build.directory} --\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!-- 是否把 ark plugin 安装、发布到仓库，默认为true --\u0026amp;gt; \u0026amp;lt;attach\u0026amp;gt;true\u0026amp;lt;/attach\u0026amp;gt; \u0026amp;lt;!-- ark plugin 最多仅能指定一个 com.alipay.sofa.ark.spi.service.PluginActivator 接口实现类 --\u0026amp;gt; \u0026amp;lt;activator\u0026amp;gt;com.alipay.sofa.ark.service.impl.SampleActivator\u0026amp;lt;/activator\u0026amp;gt; \u0026amp;lt;!-- 配置优先级，数字越小，优先级越高，优先启动，优先导出类，默认1000 --\u0026amp;gt; \u0026amp;lt;priority\u0026amp;gt;2000\u0026amp;lt;/priority\u0026amp;gt; \u0026amp;lt;!-- 配置插件的名字，务必配置对，运行时，是插件的唯一标识 ID。比如 sofa-rpc 插件，可以配置为 sofa-rpc; 默认为 ${artifactId} --\u0026amp;gt; \u0026amp;lt;pluginName\u0026amp;gt;${ark.plugin.name}\u0026amp;lt;/pluginName\u0026amp;gt; \u0026amp;lt;!--设置 ark plugin 的 classifier, 默认为空, 如非必要，建议不用设置--\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;!-- 配置导入类、资源 --\u0026amp;gt; \u0026amp;lt;imported\u0026amp;gt; \u0026amp;lt;!-- 配置需要优先从其他 ark plugin 加载的 package --\u0026amp;gt; \u0026amp;lt;packages\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;javax.servlet\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;org.springframework.*\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;/packages\u0026amp;gt; \u0026amp;lt;!-- 配置需要优先从其他 ark plugin 加载的 class --\u0026amp;gt; \u0026amp;lt;classes\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.rpc.config.ProviderConfig\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;/classes\u0026amp;gt; \u0026amp;lt;!-- 配置需要优先从其他 ark plugin 加载的资源 --\u0026amp;gt; \u0026amp;lt;resources\u0026amp;gt; \u0026amp;lt;resource\u0026amp;gt;META-INF/spring/bean.xml\u0026amp;lt;/resource\u0026amp;gt;\u0026amp;gt; \u0026amp;lt;/resources\u0026amp;gt; \u0026amp;lt;/imported\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-plugin/","fuzzywordcount":2000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"684dc1caa0f834eec498905f95913f83","permalink":"/projects/sofa-boot/sofa-ark-ark-plugin/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-plugin/","summary":"本小节将介绍 Ark Plugin 的标准规范和目录结构，以及如何使用官方插件 sofa-ark-plugin-maven-plugin 打包发布 Ark Plugin。 插件规范 标准的 Ark Plugin 需要满足以下规范： 插件必须配置插件名，","tags":null,"title":"Ark Plugin","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-plugin/","wordcount":1937},{"author":null,"categories":null,"content":" Ark container class loading mechanism The plugins and business modules are managed in the Ark container. The following figure describes the class loading mechanism:\nClass loading mechanism of Ark container Each Ark plugin has a separate classloader which loads a class in the following order:\n If byte codes generated by reflection are loaded, the system will throw a ClassNotFoundException to terminate the loading process. This primarily comes from our engineering practice: to avoid long time searches for the classes that can never be found. Search for the already loaded classes Search for classes in the JDK, which mainly consists of two parts: 1) the classes to be loaded by ExtClassloader; 2) the classes that are provided by the JDK but fail to be loaded from the ExtClassloader. When running locally, however, these classes will be added to the SystemClassloader\u0026amp;rsquo;s classpath or they might be put into some third-party toolkits such as sun.tools.attach.BsdVirtualMachine in tool.jar at the same time. This part also comes from our engineering practice, avoiding errors caused by loading a class more than once. See if the class is an interface from Sofa Ark, such as com.alipay.sofa.Ark.spi.service.PluginActivator. If so, the class will be delegated to the classloader of the Ark container responsible for loading. See if it is located in the plugin import (including import-classes and import-package). If so, the loading will be delegated to the plugin classloader that will export it. Load in the plugin\u0026amp;rsquo;s own classpath If the above steps have failed, it will try to load the class in SymtemClassloader to deal with the situation that the agent is used.  If the class fails to be loaded with all the above steps, the ClassNotFoundException will be thrown.\nArk business class loading mechanism Each Ark business has a separate classloader. Its class loading mechanism is basically consistent with that of Ark plugin, except for the step 5:\nFor Ark business, no import configuration is provided. Instead, it defaults to importing all classes exported by plug-ins. To deal with some particular business scenarios, however, we do provide the Deny-import configuration so that we can exclude the classes exported by some plugins.\nClass loading mechanism of Ark plugin resources The Ark plugin supports importing and exporting resources. To achieve this, we need to configure the corresponding import and export settings in sofa-Ark-plugin-maven-plugin. There are two ways to search for resources when using ClassLoader: ClassLoader.getResource(String) or ClassLoader.getResources(String);\n ClassLoader.getResource(String): When an Ark Plugin is searching for a single resource, it will delegate the Ark Plugin that will export the resource to load the class first. If multiple plugins export the resource at the same time, then the plugin with higher priority will export the resource first. If the loading fails or no other Ark Plugin has exported the resource, it will have a …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-classloader/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5803b870aae47885c37e4bbb02cb0a06","permalink":"/en/projects/sofa-boot/sofa-ark-classloader/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/sofa-ark-classloader/","summary":"Ark container class loading mechanism The plugins and business modules are managed in the Ark container. The following figure describes the class loading mechanism:\nClass loading mechanism of Ark container Each Ark plugin has a separate classloader which loads a class in the following order:\n If byte codes generated by reflection are loaded, the system will throw a ClassNotFoundException to terminate the loading process. This primarily comes from our engineering practice: to avoid long time searches for the classes that can never be found.","tags":null,"title":"Ark container class loading mechanism","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-classloader/","wordcount":549},{"author":null,"categories":null,"content":" Starting an Ark plug-in Ark provides the interface for starting a plug-in com.alipay.sofa.ark.spi.service.PluginActivator. The definition of the interface is as follows:\npublic interface PluginActivator { /** * Start Plugin * @param context plugin context * @throws ArkException */ void start(PluginContext context) throws ArkException; /** * Stop Plugin * @param context * @throws ArkException */ void stop(PluginContext context) throws ArkException; }  Once a plug-in implements this interface, and the activator attribute is configured in MANIFEST.MF, the plug-in will use the start method when it starts and the stop method when it stops.\nArk plug-in communication Ark plug-ins communicate using services. The interfaces used by the publishing and reference services are provided in the input parameter type com.alipay.sofa.ark.spi.model.PluginContext of the preceding method of starting an interface.\n/** * Publish Plugin Service * @param ifClass service interface * @param implObject service implement object * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; publishService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, T implObject); /** * Get Service publish by plugin, when there are multiple services, return the highest priority plugin service * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass); /** * Get Service publish by one specific plugin * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @param pluginName the name of the plugin which publish the service * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, String pluginName); /** * Get Service List publish by plugin * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; List\u0026amp;lt;ServiceReference\u0026amp;lt;T\u0026amp;gt;\u0026amp;gt; referenceServices(Class\u0026amp;lt;T\u0026amp;gt; ifClass);  A plug-in service is interface-specific. For one interface, the following descriptions are true: * Only one service can be published for each plug-in. When more than one service is published, the reference of the previously published service will be returned. * If you use referenceService to reference a single service when multiple plug-ins have published services, which service is returned depends on whether pluginName is specified: * When not specified, the service with the highest priority is returned. * When specified, the service published by the plugin with the specified name is returned.\nThe returned service reference ServiceReference is defined as follows:\npublic interface ServiceReference\u0026amp;lt;T\u0026amp;gt; { /** * get Service Object * @return service */ T getService(); /** * get Service Metadata * @return */ ServiceMetadata getServiceMetadata(); } public interface ServiceMetadata { /** * get Service Unique Name * @return service name */ String getServiceName(); /** * get Service Interface Class * @return interface class */ Class\u0026amp;lt;?\u0026amp;gt; getInterfaceClass(); /** * get …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-plugin/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b552fc51eb84cc0fa4c26860bd316490","permalink":"/en/projects/sofa-boot/sofa-ark-plugin/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/sofa-ark-plugin/","summary":"Starting an Ark plug-in Ark provides the interface for starting a plug-in com.alipay.sofa.ark.spi.service.PluginActivator. The definition of the interface is as follows:\npublic interface PluginActivator { /** * Start Plugin * @param context plugin context * @throws ArkException */ void start(PluginContext context) throws ArkException; /** * Stop Plugin * @param context * @throws ArkException */ void stop(PluginContext context) throws ArkException; }  Once a plug-in implements this interface, and the activator attribute is configured in MANIFEST.","tags":null,"title":"Ark container plugin mechanism","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-plugin/","wordcount":477},{"author":null,"categories":null,"content":" Ark container start process The startup process of the Ark container is illustrated as follows:\nArkService Ark Service is a service in the Ark container. The underlying layer uses Guice to manage the service. The service is provided with the lifecycle interface com.alipay.sofa.ark.spi.service.ArkService\npublic interface ArkService { /** * Ark Service init * @throws ArkException */ void init() throws ArkException; /** * Ark Service dispose * @throws ArkException */ void dispose() throws ArkException; }  After the service implements the preceding lifecycle interface, the Ark Service container invokes the interface when it starts and stops.\nPipeline service Pipeline is also a service registered in the Ark Service container. The service itself has no order or priority. The service is assembled in the Pipeline while the entire Ark container starts.\nArchive parsing At the very beginning of Pipeline, the running fatjar will be resolved into the models required for runtime, including the Ark plug-in model and the Ark business model, which are registered to the PluginManagerService and the BizManagerService in the Ark Service.\nDeploy the Ark plug-in Get all the Ark plug-ins from the PluginManagerService in the order of their priorities: * ClassloaderService prepares for the map mapping of plug-in export class * PluginDeployService starts com.alipay.sofa.Ark.spi.service.PluginActivator\nStart the Ark business Get all the Ark business from the BizManagerService, and execute the entry main function provided by the business configuration in the Main-Class attribute of MANIFEST.MF.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-startup/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ad60e803febd20607686a1b4ea98efc3","permalink":"/en/projects/sofa-boot/sofa-ark-startup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-ark-startup/","summary":"Ark container start process The startup process of the Ark container is illustrated as follows:\nArkService Ark Service is a service in the Ark container. The underlying layer uses Guice to manage the service. The service is provided with the lifecycle interface com.alipay.sofa.ark.spi.service.ArkService\npublic interface ArkService { /** * Ark Service init * @throws ArkException */ void init() throws ArkException; /** * Ark Service dispose * @throws ArkException */ void dispose() throws ArkException; }  After the service implements the preceding lifecycle interface, the Ark Service container invokes the interface when it starts and stops.","tags":null,"title":"Ark container startup process","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-startup/","wordcount":233},{"author":null,"categories":null,"content":" 使用 Ark 事件处理机制 SOFAArk 从 1.1.0 版本开始提供了全新的事件模型，囊括了 SOFAArk 中 biz 和 plugin 的各个生命周期；该版本提供的事件模型参考了 Spring 中的生命周期事件模型。本篇文档将描述如何使用 SOFAArk 的事件机制。\n事件概览 biz 生命周期事件    事件名 描述     AfterBizStartupEvent biz 启动之后发送的事件   AfterBizStopEvent biz 停止之后发送的事件   AfterBizSwitchEvent biz 切换之后发送的事件   BeforeBizStartupEvent biz 启动之前发送的事件   BeforeBizStopEvent biz 停止之前发送的事件   BeforeBizSwitchEvent biz 切换之前发送的事件    plugin 生命周期事件    事件名 描述     AfterPluginStartupEvent plugin 启动之后发送的事件   AfterPluginStopEvent plugin 停止之后发送的事件   BeforePluginStartupEvent plugin 启动之前发送的事件   BeforePluginStopEvent plugin 停止之前发送的事件    容器级别生命周期事件    事件名 描述     AfterFinishDeployEvent 执行完 DeployStage 阶段之后发送的事件   AfterFinishStartupEvent 执行完 Ark 容器启动之后发送的事件    事件监听 监听指定类型的事件 上述提到的各个阶段的事件，我们可以通过编写 EventHandler 来处理，例如，希望监听类型为 BeforeBizStartupEvent 的事件，则可以通过以下方式实现监听：\n@Component public class EventHandlerSample implements EventHandler\u0026amp;lt;BeforeBizStartupEvent\u0026amp;gt; { private static final Logger LOGGER = LoggerFactory.getLogger(\u0026amp;quot;EVENT-HANDLER-LOGGER\u0026amp;quot;); @Override public int getPriority() { return 0; } @Override public void handleEvent(BeforeBizStartupEvent event) { Biz source = event.getSource(); LOGGER.info(\u0026amp;quot;begin to startup biz, current biz is: {}\u0026amp;quot;,source.getIdentity()); } }   日志目录：target/test/logs/host-app/event-handler.log 日志输出： 2019-11-28 15:18:33,248 INFO EVENT-HANDLER-LOGGER - begin to startup biz, current biz is: provider1:2.0.0, bizState: resolved\n 在此基础上，在提供其他几个 event 的处理器：\n AfterBizStartupEvent  @Component public class AfterBizStartupEventHandler implements EventHandler\u0026amp;lt;AfterBizStartupEvent\u0026amp;gt; { private static final Logger LOGGER = LoggerFactory.getLogger(\u0026amp;quot;EVENT-HANDLER-LOGGER\u0026amp;quot;); @Override public void handleEvent(AfterBizStartupEvent event) { Biz source = event.getSource(); LOGGER.info(\u0026amp;quot;after startup biz, current biz is: {}, bizState: {}\u0026amp;quot;,source.getIdentity(),source.getBizState() ); } @Override public int getPriority() { return 0; } }  分别启动 基座 -\u0026amp;gt; 安装 ark-provider 模块 -\u0026amp;gt; 卸载 ark-provider 模块 ，然后看到日志输出如下：\n2019-11-28 15:31:42,325 INFO EVENT-HANDLER-LOGGER - after startup biz, current biz is: host-app:2.0.0, bizState: resolved 2019-11-28 15:36:23,956 INFO EVENT-HANDLER-LOGGER - begin to startup biz, current biz is: provider1:2.0.0, bizState: resolved 2019-11-28 15:36:27,216 INFO EVENT-HANDLER-LOGGER - after startup biz, current biz is: provider1:2.0.0, bizState: resolved 2019-11-28 15:53:38,225 INFO EVENT-HANDLER-LOGGER - before stop biz, current biz is: provider1:2.0.0, bizState: deactivated 2019-11-28 15:53:38,233 INFO EVENT-HANDLER-LOGGER - after biz stop, current biz is: provider1:2.0.0, bizState: unresolved  监听不指定类型的事件 某些情况下，如果期望监听所有 biz 或者 plugin 生命周期事件，可以使用以下方式：\n@Component public class AbstractArkEventHandler implements EventHandler\u0026amp;lt;AbstractArkEvent\u0026amp;gt; { @Override public int getPriority() { return 0; } @Override public void handleEvent(AbstractArkEvent event) { System.out.println(\u0026amp;quot;------------ current event topic: \u0026amp;quot; + event.getTopic()); } }   为了 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-event/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b0f233742572536edc8a517cf7547269","permalink":"/projects/sofa-boot/sofa-ark-ark-event/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-event/","summary":"使用 Ark 事件处理机制 SOFAArk 从 1.1.0 版本开始提供了全新的事件模型，囊括了 SOFAArk 中 biz 和 plugin 的各个生命周期；该版本提供的事件模型参考了 Spring 中的生命周期事件模型。本篇","tags":null,"title":"Ark 事件机制","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-event/","wordcount":1075},{"author":null,"categories":null,"content":" 本小节将介绍标准 Ark 包 的目录结构，以及如何使用官方插件 sofa-ark-maven-plugin 打包并发布 Ark 包。\nMaven 插件 官方提供 Maven 插件 sofa-ark-maven-plugin 可以将普通 Java 工程或者 Spring Boot 工程打包成标准格式 Ark 包 ；基于 Fat Jar 技术，使用 java -jar 命令可以直接启动 Ark 包 。 Maven 插件坐标为：\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  Goals sofa-ark-maven-plugin 插件提供 goal: repackage， 可以将工程打包成可执行的 Ark 包，如下配置：\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;/excution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- 配置信息 --\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  完整配置模板 完整的 sofa-ark-maven-plguin 插件配置模板如下：\n\u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--ark 包和 ark biz 的打包存放目录，默认为工程 build 目录--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--设置应用的根目录，用于读取 ${base.dir}/conf/ark/bootstrap.application 配置文件，默认为 ${project.basedir}--\u0026amp;gt; \u0026amp;lt;baseDir\u0026amp;gt;./\u0026amp;lt;/baseDir\u0026amp;gt; \u0026amp;lt;!--生成 ark 包文件名称，默认为 ${artifactId}--\u0026amp;gt; \u0026amp;lt;finalName\u0026amp;gt;demo-ark\u0026amp;lt;/finalName\u0026amp;gt; \u0026amp;lt;!--是否跳过执行 goal:repackage，默认为false--\u0026amp;gt; \u0026amp;lt;skip\u0026amp;gt;false\u0026amp;lt;/skip\u0026amp;gt; \u0026amp;lt;!--是否打包、安装和发布 ark biz，详细参考 Ark Biz 文档，默认为false--\u0026amp;gt; \u0026amp;lt;attach\u0026amp;gt;true\u0026amp;lt;/attach\u0026amp;gt; \u0026amp;lt;!--设置 ark 包的 classifier，默认为空--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;!--设置 ark biz 的 classifier，默认为 ark-biz--\u0026amp;gt; \u0026amp;lt;bizClassifier\u0026amp;gt;ark-biz\u0026amp;lt;/bizClassifier\u0026amp;gt; \u0026amp;lt;!--设置 ark biz 的 biz name，默认为 ${artifactId}--\u0026amp;gt; \u0026amp;lt;bizName\u0026amp;gt;demo-ark\u0026amp;lt;/bizName\u0026amp;gt; \u0026amp;lt;!--设置 ark biz 的 biz version，默认为 ${artifactId}--\u0026amp;gt; \u0026amp;lt;bizVersion\u0026amp;gt;0.0.1\u0026amp;lt;/bizVersion\u0026amp;gt; \u0026amp;lt;!--设置 ark biz 的 启动优先级，值越小优先级越高，${artifactId}--\u0026amp;gt; \u0026amp;lt;priority\u0026amp;gt;100\u0026amp;lt;/priority\u0026amp;gt; \u0026amp;lt;!--设置 ark biz 的启动入口，默认会搜索被打 org.springframework.boot.autoconfigure.SpringBootApplication 注解且含有 main 方法的入口类--\u0026amp;gt; \u0026amp;lt;mainClass\u0026amp;gt;com.alipay.sofa.xx.xx.MainEntry\u0026amp;lt;/mainClass\u0026amp;gt; \u0026amp;lt;!--设置是否将 scope=provided 的依赖打包，默认为 false--\u0026amp;gt; \u0026amp;lt;packageProvided\u0026amp;gt;false\u0026amp;lt;/packageProvided\u0026amp;gt; \u0026amp;lt;!--设置是否生成 Biz 包，默认为true--\u0026amp;gt; \u0026amp;lt;keepArkBizJar\u0026amp;gt;true\u0026amp;lt;/keepArkBizJar\u0026amp;gt; \u0026amp;lt;!--针对 Web 应用，设置 context path，默认为 /--\u0026amp;gt; \u0026amp;lt;webContextPath\u0026amp;gt;/\u0026amp;lt;/webContextPath\u0026amp;gt; \u0026amp;lt;!--打包 ark biz 时，排除指定的包依赖；格式为: ${groupId:artifactId} 或者 ${groupId:artifactId:classifier}--\u0026amp;gt; \u0026amp;lt;excludes\u0026amp;gt; \u0026amp;lt;exclude\u0026amp;gt;org.apache.commons:commons-lang3\u0026amp;lt;/exclude\u0026amp;gt; \u0026amp;lt;/excludes\u0026amp;gt; \u0026amp;lt;!--打包 ark biz 时，排除和指定 groupId 相同的包依赖--\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-jar/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c2a9b8ad142f15b9ee82d7d9d8237850","permalink":"/projects/sofa-boot/sofa-ark-ark-jar/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-jar/","summary":"本小节将介绍标准 Ark 包 的目录结构，以及如何使用官方插件 sofa-ark-maven-plugin 打包并发布 Ark 包。 Maven 插件 官方提供 Maven 插件 sofa-ark-maven-plugin 可以将普通 Java 工程或者 Spring Boot 工程打包成标准格式 Ark 包 ；","tags":null,"title":"Ark 包","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-jar/","wordcount":1626},{"author":null,"categories":null,"content":" Ark 应用的整体启动流程如下图所述：\n当用 java -jar 启动 Ark 包 或者 在 IDE 中通过 SofaArkBootstrap.launch 启动 Ark 应用时，相应 Launcher 入口会负责启动应用，其中会反射调用 ArkContainer 的入口，初始化 ArkService ，然后依次执行 pipeline，来完成整个 Ark 应用的启动。\nArkService Ark Serivce 是 Ark 容器中的服务，底层使用 Guice 对服务进行管理。同时针对服务，提供了生命周期接口 com.alipay.sofa.ark.spi.service.ArkService\npublic interface ArkService { /** * Ark Service init * @throws ArkException */ void init() throws ArkException; /** * Ark Service dispose * @throws ArkException */ void dispose() throws ArkException; }  当服务实现了上述接口时，在 Ark Serivce 容器启动时和停止时会调用相应的生命周期接口\nPipeline 服务 Pipeline 也是注册在 Ark Service 容器中的一个服务，服务本身是没有顺序和优先级的，在 Pipeline 中会对服务进行一些组装，同时完成整个 Ark 容器的启动\nArchive 解析 在 Pipeline 的最开始，会将运行的 fatjar 进行解析，解析成运行时需要的模型，主要包括 Ark 插件模型和 Ark 业务模型，并将这些模型注册到 Ark Service 中的 PluginManagerService 以及 BizManagerService 中\n初始化环境 设置一些运行时需要的默认参数，比如设置 log4j.ignoreTCL 为 true 让 log4j/log4j2 初始化是日志不要从 ThreadContextClassloader 中寻找配置文件(背景)\n注册容器服务 在 Ark 容器中会发布一些服务供其它的插件来使用，比如 BizDeployer 来让 SOFAArk 官方插件 sofa-jarslink 来完成 biz 的动态加载/卸载等\n部署 Ark 插件 从 PluginManagerService 中获取到所有的 Ark 插件，并按照插件优先级顺序： * ClassloaderService 准备插件 export 类的 map 映射 * PluginDeployService 启动插件的 com.alipay.sofa.ark.spi.service.PluginActivator\n启动 Ark 业务 从 BizManagerService 中获取到所有的 Ark 业务，并执行业务配置在 MANIFEST.MF 属性 Main-Class 中提供的入口 main 函数\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-startup/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ad60e803febd20607686a1b4ea98efc3","permalink":"/projects/sofa-boot/sofa-ark-startup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-startup/","summary":"Ark 应用的整体启动流程如下图所述： 当用 java -jar 启动 Ark 包 或者 在 IDE 中通过 SofaArkBootstrap.launch 启动 Ark 应用时，相应 Launcher 入口会负责启动应用，其中会反射调用 ArkContainer 的入口，初始化 ArkService ，然","tags":null,"title":"Ark 容器启动流程","type":"projects","url":"/projects/sofa-boot/sofa-ark-startup/","wordcount":523},{"author":null,"categories":null,"content":" Ark 插件启动 Ark 中提供了插件启动的接口 com.alipay.sofa.ark.spi.service.PluginActivator ，其定义如下：\npublic interface PluginActivator { /** * Start Plugin * @param context plugin context * @throws ArkException */ void start(PluginContext context) throws ArkException; /** * Stop Plugin * @param context * @throws ArkException */ void stop(PluginContext context) throws ArkException; }  插件只需要实现此接口，并在 MANIFEST.MF 中配置 activator 属性，就会在启动时执行 start 方法，停止时执行 stop 方法\nArk 插件通信 Ark 之间的通信是通过服务来完成的， 在上述启动接口方法的入参类型 com.alipay.sofa.ark.spi.model.PluginContext 中提供了发布服务和引用服务的接口\n/** * Publish Plugin Service * @param ifClass service interface * @param implObject service implement object * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; publishService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, T implObject); /** * Get Service publish by plugin, when there are multiple services, return the highest priority plugin service * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass); /** * Get Service publish by one specific plugin * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @param pluginName the name of the plugin which publish the service * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, String pluginName); /** * Get Service List publish by plugin * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; List\u0026amp;lt;ServiceReference\u0026amp;lt;T\u0026amp;gt;\u0026amp;gt; referenceServices(Class\u0026amp;lt;T\u0026amp;gt; ifClass);  插件服务是以接口为粒度的，针对同一个接口： * 每一个插件只允许发布一个服务，重复发布则会直接返回已经发布服务的引用 * 当多有个插件发布服务时，若通过 referenceService 引用单个服务 * 当不指定 pluginName 时，则返回优先级最高的服务 * 当指定 pluginName 时，则返回该插件发布的服务\n返回的服务引用 ServiceReference 定义如下:\npublic interface ServiceReference\u0026amp;lt;T\u0026amp;gt; { /** * get Service Object * @return service */ T getService(); /** * get Service Metadata * @return */ ServiceMetadata getServiceMetadata(); } public interface ServiceMetadata { /** * get Service Unique Name * @return service name */ String getServiceName(); /** * get Service Interface Class * @return interface class */ Class\u0026amp;lt;?\u0026amp;gt; getInterfaceClass(); /** * get ServiceProvider * @return */ ServiceProvider getServiceProvider(); }  其中通过 * getService() 可以获取到服务的实体 (也即发布服务时的 implObject) * getServiceMetadata() 可以获取到服务的元数据信息，包括 * 服务名：即服务的接口名 * 服务接口 * 服务的提供方：包括提供方名字(插件名等)、提供方优先级(插件优先级)\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-plugin/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b552fc51eb84cc0fa4c26860bd316490","permalink":"/projects/sofa-boot/sofa-ark-plugin/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-plugin/","summary":"Ark 插件启动 Ark 中提供了插件启动的接口 com.alipay.sofa.ark.spi.service.PluginActivator ，其定义如下： public interface PluginActivator { /** * Start Plugin * @param context plugin context * @throws ArkException */ void start(PluginContext context) throws ArkException; /** * Stop Plugin * @param context * @throws ArkException */ void stop(PluginContext context) throws ArkException; } 插件只需要实","tags":null,"title":"Ark 容器插件机制","type":"projects","url":"/projects/sofa-boot/sofa-ark-plugin/","wordcount":574},{"author":null,"categories":null,"content":" Ark 容器类加载机制 Ark 容器中会管理插件和业务，整体的类加载机制可见如下图描述：\nArk 插件类加载机制 每个 Ark 插件都拥有一个独立的类加载器，其类加载的顺序如下：\n 如果是加载反射生成的字节码，那么会直接抛出 ClassNotFoundException，终止类加载。这一部分主要是来源于我们的工程实践，避免一定找不到的类查找路径过长 查找已经被加载过的类 查找 JDK 中的类，这一块主要包含两部分：第一部分是 ExtClassloader 负责加载的类；第二部分是 JDK 提供的类但从 ExtClassloader 中加载不到，但在本地运行时会被加入到 SystemClassloader 的 classpath 中，同时这些类可能会被放到一些三方工具包中，典型的如 tool.jar 中 sun.tools.attach.BsdVirtualMachine,这一部分也主要来源于我们的工程实践，避免类被加载超过一次，从而引发报错 看类是否是由 Sofa Ark 提供的接口类，典型的如: com.alipay.sofa.ark.spi.service.PluginActivator, 如果是，则类会委托给 Ark 容器的类加载器加载 看是否在插件的 import 中(包括 import-classes 和 import-package)， 如果在，则会委托给导出该类的插件类加载器加载 在插件自身的 classpath 中加载 如果上述都失败了，则会尝试在 SymtemClassloader 中加载，这一步主要是为了解决使用 agent 时的情形  如果上述的步骤都加载失败了，则抛出 ClassNotFoundException\nArk 业务类加载机制 每个 Ark 业务都拥有一个独立的类加载器， Ark 业务类加载机制基本上与 Ark 插件保持一致，在上述的7步中，主要是第5步不一样：\n对于 Ark 业务而言，并没有提供 import 的配置，而是认为默认 import 所有插件 export 出来的类；但为了一些特殊的业务场景，我们提供了 Deny-import 的配置让业务可以排除掉某些插件导出的类\nArk 插件资源加载机制 Ark 插件支持导入导出资源，需要在 sofa-ark-plugin-maven-plugin 配置相关的导入导出配置；在使用 ClassLoader 加载资源时，存在两种方式查找资源，ClassLoader.getResource(String) 和 ClassLoader.getResources(String)；\n ClassLoader.getResource(String): Ark Plugin 在查找单个资源时，会优先委托导出该资源的 Ark Plugin 加载，如果有多个插件同时导出，则优先级高的插件优先导出；如果加载失败或者没有其他 Ark Plugin 导出，则尝试在本 Ark Plugin 查找加载；\n ClassLoader.getResources(String): Ark Plugin 在查找多个资源时，会从所有导出该资源的 Ark Plugin 加载，同时也会从本 Ark Plugin 加载资源；\n  Ark 业务资源加载机制 默认情况下，Ark Biz 会优先加载 Ark Plugin 导出的资源；如果开发者希望只在工程应用内部查找，则可以通过 sofa-ark-maven-plugin 配置 denyImportResources；如此，Ark Biz 不会从 Ark Plugin 查找该资源，只会在 Ark Biz 内部查找。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-classloader/","fuzzywordcount":1000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5803b870aae47885c37e4bbb02cb0a06","permalink":"/projects/sofa-boot/sofa-ark-classloader/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-classloader/","summary":"Ark 容器类加载机制 Ark 容器中会管理插件和业务，整体的类加载机制可见如下图描述： Ark 插件类加载机制 每个 Ark 插件都拥有一个独立的类加载器，其类加载的顺序","tags":null,"title":"Ark 容器类加载机制","type":"projects","url":"/projects/sofa-boot/sofa-ark-classloader/","wordcount":974},{"author":null,"categories":null,"content":" Ark 容器和 Ark Plugin 在运行时由不同的类加载器加载，不能使用常规的 ServiceLoader 提供 SPI 扩展，SOFAArk 自定义扩展点 SPI 机制， Ark Plugin 实现 SPI 机制，考虑到 Biz 卸载问题，Ark Biz 暂时不支持该 SPI 机制，只适用于 Ark Plugin 之间。\n声明扩展接口 使用注解 @Extensible 声明扩展接口，注解定义如下：\n@Target({ ElementType.TYPE }) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface Extensible { /** * return specify extensible file name, default value is the * full name of interface. */ String file() default \u0026amp;quot;\u0026amp;quot;; /** * return whether this a singleton, with a single, shared instance * returned on all calls, default value is true. */ boolean singleton() default true; }   file 用于声明 SPI 扩展文件名，默认为接口全类名 singleton 用于声明加载扩展类是否为单例模式  声明扩展实现 使用注解 @Extension 声明扩展实现，注解定义如下：\n@Target({ ElementType.TYPE }) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface Extension { /** * extension name */ String value(); /** * extension order, Higher values are interpreted as lower priority. * As a consequence, the object with the lowest value has the highest * priority. */ int order() default 100; }   value 用于定义扩展实现名称，例如不同的插件扩展同一个接口，可能会取不同的名字。 order 用于决定具体扩展实现的生效顺序  运行时，对于同一个接口的扩展实现生效规则如下： + 规则一：名称相同的扩展实现，只会返回优先级高的扩展实现类，order 数字越小，优先级越高 + 规则二：名称不相同的扩展实现，则返回一个对应的 List 列表，每个名称返回优先级最高的扩展实现\n加载 SPI 实现类 正常情况下，我们使用 ServiceLoader 加载 SPI 接口实现；SOFAArk 提供了工具类 ArkServiceLoader 用于加载扩展实现，工具类定义了两个简单的方法：\npublic class ArkServiceLoader { private static ExtensionLoaderService extensionLoaderService; // 方法一 public static \u0026amp;lt;T\u0026amp;gt; T loadExtension(Class\u0026amp;lt;T\u0026amp;gt; interfaceType, String extensionName) { return extensionLoaderService.getExtensionContributor(interfaceType, extensionName); } // 方法二 public static \u0026amp;lt;T\u0026amp;gt; List\u0026amp;lt;T\u0026amp;gt; loadExtension(Class\u0026amp;lt;T\u0026amp;gt; interfaceType) { return extensionLoaderService.getExtensionContributor(interfaceType); } }   方法一：用于加载指定接口和名称的扩展实现，返回单个结果。参考上述规则一 方法二：用于加载指定接口的扩展实现，返回列表结果。参考上述规则二  需要注意下，定义 SPI 接口的插件需要导出该接口，负责实现 SPI 接口的插件需要导入该接口。另外 SOFAArk 容器本身也会定义部分用于插件扩展实现的 SPI 接口，例如 ClassLoaderHook\n为什么不支持 Biz 的 SPI 扩展实现加载 考虑到 Biz 会动态的安装和卸载，如果支持 Biz 的扩展实现加载，生命周期容易引起混乱，暂时不考虑支持。如果确实存在 Ark Plugin 需要主动触发 Ark Biz 的逻辑调用，可以通过 SOFAArk 内部事件机制。\nSOFAArk 默认扩展点 SOFAArk 容器目前提供了唯一一个扩展点 ClassLoaderHook，用于其他插件提供扩展实现，自定义类/资源加载逻辑。ClassLoaderHooker 接口定义如下，用于扩展 BizClassLoader 和 PluginClassLoader 类(资源）加载逻辑：\n@Extensible public interface ClassLoaderHook\u0026amp;lt;T\u0026amp;gt; { Class\u0026amp;lt;?\u0026amp;gt; preFindClass(String name, ClassLoaderService classLoaderService, T t) throws ClassNotFoundException; Class\u0026amp;lt;?\u0026amp;gt; postFindClass(String name, ClassLoaderService classLoaderService, T t) throws ClassNotFoundException; URL preFindResource(String name, ClassLoaderService classLoaderService, T t); URL postFindResource(String name, ClassLoaderService classLoaderService, T t); Enumeration\u0026amp;lt;URL\u0026amp;gt; preFindResources(String name, ClassLoaderService classLoaderService, T t) throws IOException; Enumeration\u0026amp;lt;URL\u0026amp;gt; postFindResources(String name, ClassLoaderService classLoaderService, T t) throws IOException; }  通过在插件中扩展该 SPI 接口实现，可以自定义 PluginClassLoader 和 BizClassLoader 的类/资源的加载逻辑。 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-extension/","fuzzywordcount":2000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"685e049f442cde4f3f51fefad5453dae","permalink":"/projects/sofa-boot/sofa-ark-ark-extension/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-extension/","summary":"Ark 容器和 Ark Plugin 在运行时由不同的类加载器加载，不能使用常规的 ServiceLoader 提供 SPI 扩展，SOFAArk 自定义扩展点 SPI 机制， Ark Plugin 实现 SPI 机制，考虑到 Biz 卸载问题，A","tags":null,"title":"Ark 扩展机制","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-extension/","wordcount":1921},{"author":null,"categories":null,"content":"SOFAArk 容器使用了 logback 日志实现，并集成了 sofa-common-tools，日志相关配置可以参考 配置文档, 这里介绍 SOFAArk 三个日志文件：\n sofa-ark/common-default.log \u0026amp;gt; sofa-ark 默认日志，打印 SOFAArk 启动日志等，大概内容如下：  2019-03-12 15:08:55,758 INFO main - Begin to start ArkServiceContainer 2019-03-12 15:08:56,290 INFO main - Init Service: com.alipay.sofa.ark.container.session.StandardTelnetServerImpl 2019-03-12 15:08:56,311 INFO main - Listening on port: 1234 2019-03-12 15:08:56,313 INFO main - Init Service: com.alipay.sofa.ark.container.service.plugin.PluginDeployServiceImpl 2019-03-12 15:08:56,313 INFO main - Init Service: com.alipay.sofa.ark.container.service.biz.BizDeployServiceImpl 2019-03-12 15:08:56,313 INFO main - Init Service: com.alipay.sofa.ark.container.service.classloader.ClassLoaderServiceImpl 2019-03-12 15:08:56,317 INFO main - Finish to start ArkServiceContainer 2019-03-12 15:08:56,338 INFO main - Start to process pipeline stage: com.alipay.sofa.ark.container.pipeline.HandleArchiveStage 2019-03-12 15:08:56,349 INFO main - Finish to process pipeline stage: com.alipay.sofa.ark.container.pipeline.HandleArchiveStage 2019-03-12 15:08:56,349 INFO main - Start to process pipeline stage: com.alipay.sofa.ark.container.pipeline.RegisterServiceStage 2019-03-12 15:08:56,354 INFO main - Service: com.alipay.sofa.ark.spi.service.biz.BizManagerService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,354 INFO main - Service: com.alipay.sofa.ark.spi.service.biz.BizFactoryService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,355 INFO main - Service: com.alipay.sofa.ark.spi.service.plugin.PluginManagerService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,356 INFO main - Service: com.alipay.sofa.ark.spi.service.plugin.PluginFactoryService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,356 INFO main - Service: com.alipay.sofa.ark.spi.service.event.EventAdminService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,357 INFO main - Service: com.alipay.sofa.ark.spi.service.registry.RegistryService publish by: ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=-2147483648} succeed 2019-03-12 15:08:56,360 INFO main - Inject {field= bizManagerService} of {service= ServiceMetadata{service=\u0026#39;com.alipay.sofa.ark.spi.service.biz.BizDeployer\u0026#39;, provider=\u0026#39;ServiceProvider{provider=\u0026#39;Ark Container\u0026#39;, order=100}\u0026#39;}} success!   sofa-ark/common-error.log \u0026amp;gt; sofa-ark 错误日志，打印 SOFAArk 容器运行时错误日志，例如 biz 启动失败日志等：  2019-03-12 16:38:41,873 ERROR main - Start biz: Startup In IDE meet error java.lang.reflect.InvocationTargetException: null at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-log/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a514225510e53e9bf7173734c1f878e1","permalink":"/projects/sofa-boot/sofa-ark-ark-log/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-log/","summary":"SOFAArk 容器使用了 logback 日志实现，并集成了 sofa-common-tools，日志相关配置可以参考 配置文档, 这里介绍 SOFAArk 三个日志文件： sofa-ark/common-default.log \u0026gt; sofa-ark 默认日志，打","tags":null,"title":"Ark 日志","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-log/","wordcount":667},{"author":null,"categories":null,"content":" SOFAArk 定义了两种服务类型，用于解决应用和插件，应用和应用之间的通信问题，下面分别介绍这两种服务类型：\n插件服务 SOFAArk 允许在 Plugin 通过 PluginContext 发布和引用服务，也可以使用注解 @ArkInject 引用服务。为了方便开发高级特性，SOFAArk 容器默认将内部功能组件发布成了服务，包括 Biz 管理，Plugin 管理，事件管理，服务注册管理。目前不允许 Biz 发布服务，只能引用插件服务。下面介绍如何发布和引用插件服务，以及 SOFAArk 容器默认发布的服务。\n发布服务 每个 Plugin 都可以定义唯一的插件入口，需要实现 PluginActivator 接口并在打包插件配置中声明，先看下接口定义：\npublic interface PluginActivator { /** * Start Plugin * @param context plugin context * @throws ArkRuntimeException */ void start(PluginContext context); /** * Stop Plugin * @param context * @throws ArkRuntimeException */ void stop(PluginContext context); }  SOFAArk 容器在启动插件时，会调用插件启动入口(如果有)，因此如果插件实现方需要发布插件服务供其他插件或者 Biz 调用，可以使用入参 PluginContext 发布服务，PluginContext 提供了两个方法发布服务：\n/** * Publish Plugin Service * @param ifClass service interface * @param implObject service implement object * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; publishService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, T implObject); /** * Publish Plugin Service * @param ifClass service interface * @param implObject service implement object * @param uniqueId service implementation id * @param \u0026amp;lt;T\u0026amp;gt; * @return */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; publishService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, T implObject, String uniqueId);  这两个方法的区别在于是否指定 uniqueId，因为同一个接口，可能会有多个服务实现，因此需要使用 uniqueId 区别，同样在引用端也需要指定 uniqueId. 默认 uniqueId 为空\n引用服务 SOFAArk 提供了两种方式引用插件服务，在插件内部，可以直接使用 PluginContext 引用服务，PluginContext 提供了两个简单的方法引用服务：\n/** * Get Service publish by plugin, when there are multiple services, return the highest priority plugin service * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass); /** * Get Service publish by one specific plugin * @param ifClass service interface * @param \u0026amp;lt;T\u0026amp;gt; * @param uniqueId service implementation * @return service reference */ \u0026amp;lt;T\u0026amp;gt; ServiceReference\u0026amp;lt;T\u0026amp;gt; referenceService(Class\u0026amp;lt;T\u0026amp;gt; ifClass, String uniqueId);  在 Biz 内部，如果是 Spring Boot/SOFABoot 应用，可以直接使用注解 @ArkInject 引用服务，注解声明如下：\n@java.lang.annotation.Target(ElementType.FIELD) @java.lang.annotation.Retention(java.lang.annotation.RetentionPolicy.RUNTIME) @java.lang.annotation.Documented public @interface ArkInject { /** * ark service interface * @return */ Class\u0026amp;lt;?\u0026amp;gt; interfaceType() default void.class; /** * ark service uniqueId * * @return return reference unique-id */ String uniqueId() default \u0026amp;quot;\u0026amp;quot;; }  SOFAArk 提供集成 Spring Boot/SOFABoot 功能，在 Field 上打上 @ArkInject，指定接口类型和 uniqueId 即可完成自动注入。为了更加方便的使用，如果没有指定 interfacetType 类型，默认使用被打注解的 Field 类型。\n在插件内部，有时候也可以使用 @ArkInject 引用服务，即插件在发布某个服务时，服务内部可以直接使用 @ArkInject 引用服务；需要注意的是，被引用的服务如果是其他插件发布的，则必须满足其他插件优先当前插件启动。\n默认服务 前面提到，为了方便 Plugin 和 Biz 开发高级特性，SOFAArk 将内部功能组件发布成服务，包括：\n BizManageService \u0026amp;gt; Biz 管理器，管理、查询 Biz 信息\n BizFactoryService \u0026amp;gt; Biz 解析器，解析 Biz 文件\n PluginManagerService \u0026amp;gt; Plugin 管理器，管理、查询 Plugin 信息\n PluginFactoryService \u0026amp;gt; Plugin 解析器，解析 Plugin 文件\n EventAdminService \u0026amp;gt; SOFAArk 事件管理器， …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-service/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"81b7e697b890139c03831cdb648e094b","permalink":"/projects/sofa-boot/sofa-ark-ark-service/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-service/","summary":"SOFAArk 定义了两种服务类型，用于解决应用和插件，应用和应用之间的通信问题，下面分别介绍这两种服务类型： 插件服务 SOFAArk 允许在 Plugin 通过 PluginContext 发布和引用服务，也可","tags":null,"title":"Ark 服务机制","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-service/","wordcount":1188},{"author":null,"categories":null,"content":" 在 Ark 服务机制 中，我们详细介绍了如何引用和发布插件服务，主要是解决 Plugin 和 Biz 的通信问题；为了解决 Biz 之间的通信问题，SOFAArk 引入了 SOFABoot 提供的 SofaService/SofaReference 编程界面；下面介绍其使用方法。\n引入依赖 引入 runtime-sofa-boot-plugin 依赖，如果应用基于 Spring Boot 1.x 开发，推荐使用 v2.6.1 版本；如果应用基于 Spring Boot 2.x 开发，推荐使用 v3.1.3 版本；\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  发布和引用 JVM 服务 SOFAArk 引入了 SOFABoot 提供的 SofaService/SofaReference JVM 服务概念(参考文档)，为了方便文档统一，重复其介绍。\nSOFABoot 提供三种方式给开发人员发布和引用 JVM 服务\n XML 方式 Annotation 方式 编程 API 方式  XML 方式 服务发布 首先需要定义一个 Bean：\n\u0026amp;lt;bean id=\u0026amp;quot;sampleService\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleServiceImpl\u0026amp;quot;\u0026amp;gt;  然后通过 SOFA 提供的 Spring 扩展标签来将上面的 Bean 发布成一个 SOFA JVM 服务。\n\u0026amp;lt;sofa:service interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; ref=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  上面的配置中的 interface 指的是需要发布成服务的接口，ref 指向的是需要发布成 JVM 服务的 Bean，至此，我们就已经完成了一个 JVM 服务的发布。\n服务引用 使用 SOFA 提供的 Spring 扩展标签引用服务:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleServiceRef\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  上面的配置中的 interface 是服务的接口，需要和发布服务时配置的 interface 一致。id 属性的含义同 Spring BeanId。上面的配置会生成一个 id 为 sampleServiceRef 的 Spring Bean，你可以将 sampleServiceRef 这个 Bean 注入到当前 SOFABoot 模块 Spring 上下文的任意地方。\n service/reference 标签还支持 RPC 服务发布，相关文档: RPC 服务发布与引用\n Annotation 方式  警告\n如果一个服务已经被加上了 @SofaService 的注解，它就不能再用 XML 的方式去发布服务了，选择一种方式发布服务，而不是两种混用。\n 除了通过 XML 方式发布 JVM 服务和引用之外，SOFABoot 还提供了 Annotation 的方式来发布和引用 JVM 服务。通过 Annotation 方式发布 JVM 服务，只需要在实现类上加一个 @SofaService 注解即可，如下：\n@SofaService public class SampleImpl implements SampleInterface { public void test() { } }   提示\n@SofaService 的作用是将一个 Bean 发布成一个 JVM 服务，这意味着虽然你可以不用再写 \u0026amp;lt;sofa:service/\u0026amp;gt; 的配置，但是还是需要事先将 @SofaService 所注解的类配置成一个 Spring Bean。\n 在使用 XML 配置 \u0026amp;lt;sofa:service/\u0026amp;gt; 的时候，我们配置了一个 interface 属性，但是在使用 @SofaService 注解的时候，却没有看到有配置服务接口的地方。这是因为当被 @SofaService 注解的类只有一个接口的时候，框架会直接采用这个接口作为服务的接口。当被 @SofaService 注解的类实现了多个接口时，可以设置 @SofaService 的 interfaceType 字段来指定服务接口，比如下面这样：\n@SofaService(interfaceType=SampleInterface.class) public class SampleImpl implements SampleInterface, Serializable { public void test() { } }  和 @SofaService 对应，Sofa 提供了 @SofaReference 来引用一个 JVM 服务。假设我们需要在一个 Spring Bean 中使用 SampleJvmService 这个 JVM 服务，那么只需要在字段上加上一个 @SofaReference 的注解即可：\npublic class SampleServiceRef { @SofaReference private SampleService sampleService; }  和 @SofaService 类似，我们也没有在 @SofaReference 上指定服务接口，这是因为 @SofaReference 在不指定服务接口的时候，会采用被注解字段的类型作为服务接口，你也可以通过设定 @SofaReference 的 interfaceType 属性来指定：\npublic class SampleServiceRef { @SofaReference(interfaceType=SampleService.class) private SampleService sampleService; }  使用 @SofaService 注解发布服务时，需要在实现类上打上 @SofaService 注解；在 Spring Boot 使用 Bean Method 创建 Bean 时，会导致 @Bean 和 @SofaService 分散在两处，而且无法对同一个实现类使用不同的 unique id。 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-jvm/","fuzzywordcount":2500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1bf80b24428da0f91edc6af7b63f6047","permalink":"/projects/sofa-boot/sofa-ark-ark-jvm/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-jvm/","summary":"在 Ark 服务机制 中，我们详细介绍了如何引用和发布插件服务，主要是解决 Plugin 和 Biz 的通信问题；为了解决 Biz 之间的通信问题，SOFAArk 引入了 SOFABoot 提供的 SofaService/SofaReference 编","tags":null,"title":"Ark 服务通信","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-jvm/","wordcount":2436},{"author":null,"categories":null,"content":" Use java.lang.Runnable in thread If you start a thread via java.lang.Runnable in the code or use a thread pool to process some businesses asynchronously, SOFATracer log context needs to be passed from the parent thread to the child thread. com.alipay.common.tracer.core.async.SofaTracerRunnable provided by SOFATracer is reponsible for completing this operation by default. You can use it as follows:\nThread thread = new Thread(new SofaTracerRunnable(new Runnable() { @Override public void run() { //do something your business code } })); thread.start();  Use java.util.concurrent.Callable in thread If you start a thread via java.util.concurrent.Callable in the code or use a thread pool to process some businesses asynchronously, SOFATracer log context needs to be passed from the parent thread to the child thread. com.alipay.common.tracer.core.async.SofaTracerCallable provided by SOFATracer is reponsible for completing this operation by default. You can use it as follows:\nExecutorService executor = Executors.newCachedThreadPool(); SofaTracerCallable\u0026amp;lt;Object\u0026amp;gt; sofaTracerSpanSofaTracerCallable = new SofaTracerCallable\u0026amp;lt;Object\u0026amp;gt;(new Callable\u0026amp;lt;Object\u0026amp;gt;() { @Override public Object call() throws Exception { return new Object(); } }); Future\u0026amp;lt;Object\u0026amp;gt; futureResult = executor.submit(sofaTracerSpanSofaTracerCallable); //do something in current thread Thread.sleep(1000); //another thread execute success and get result Object objectReturn = futureResult.get();  This example assumes that the object type returned by java.util.concurrent.Callable is java.lang.Object. You can replace it with the expected type based on actual situation.\nSOFATracer support for thread pool and asynchronous call scenarios Asynchronous  Asynchronous invocation, in RPC calls, for example,each time the rpc call request goes out, it will not wait until the result is returned before initiating the next call. There is a time difference here, before the callback of the previous rpc call comes back, another new one begin. At this time, the TracerContext in the current thread is not cleaned up, the spanId will be incremented, and the tracerId is the same.\n For the above situation, when the SOFATracer is processed for the asynchronous situation, it will not wait for the callback to execute, and then the cr phase will be cleaned up. Instead, the current thread\u0026amp;rsquo;s tracerContext context will be cleaned up in advance to ensure the correctness of the link.\nThread Pool For now, whether it\u0026amp;rsquo;s SOFARPC or Dubbo\u0026amp;rsquo;s trace implementation, the situation is the same when using single-thread or thread pools:\n Synchronous call. A thread in the thread pool is allocated to handle the RPC request. This does not cause the next RPC request to take the tracerContext data of the previous request by mistake Asynchronous calls, since the asynchronous callback is not in the callback to clean up the context, but in advance, there is no dirty data problem. callback, which is essentially an …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/async/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e755346c441115663c101638667fe4c0","permalink":"/en/projects/sofa-tracer/async/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-tracer/async/","summary":"Use java.lang.Runnable in thread If you start a thread via java.lang.Runnable in the code or use a thread pool to process some businesses asynchronously, SOFATracer log context needs to be passed from the parent thread to the child thread. com.alipay.common.tracer.core.async.SofaTracerRunnable provided by SOFATracer is reponsible for completing this operation by default. You can use it as follows:\nThread thread = new Thread(new SofaTracerRunnable(new Runnable() { @Override public void run() { //do something your business code } })); thread.","tags":null,"title":"Asynchronous thread processing","type":"projects","url":"/en/projects/sofa-tracer/async/","wordcount":436},{"author":null,"categories":null,"content":" ﻿## Model\nApplications Jarslink manages the life cycle of multiple applications. During runtime dynamic deployment, it usually converts a Jar file entity into an abstract model Biz. + Biz: abstract model of the application at runtime\nInstruction Currently, Jarslink supports the telnet protocol and accepts the entered instructions. In the future, it will also support instruction execution through APIs. Acceptable instruction types: + InstallCommand: install the application + UninstallCommand: uninstall the application + CheckCommand: check the application state + SwitchCommand: switch the application state\nService The Jarslink plugin has expanded the SOFAArk container\u0026amp;rsquo;s services of BizDeployer and CommandProvider and referenced the SOFAArk container\u0026amp;rsquo;s exposed services of BizManagerService and BizFactoryService. + BizDeployer is the application deployment extension point provided by the SOFAArk container, and it is used to control the Biz startup in the Ark package. Jarslink has registered its own implementation with the SOFAArk container. + CommandProvider is the command processing extension point provided by the SOFAArk container, and it is used to process the commands received by the SOFAArk container through a telnet link. + BizManagerService is the Biz manager exposed by the SOFAArk container, and it is used for registration, deregistration, and other operations. + BizFactoryService is the Biz generator exposed by the SOFAArk container, and it is used to abstract static Biz package files into runtime Biz models.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-model/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"991cb277d50874fa27095b920ac9b736","permalink":"/en/projects/sofa-boot/sofa-jarslink-model/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-model/","summary":"﻿## Model\nApplications Jarslink manages the life cycle of multiple applications. During runtime dynamic deployment, it usually converts a Jar file entity into an abstract model Biz. + Biz: abstract model of the application at runtime\nInstruction Currently, Jarslink supports the telnet protocol and accepts the entered instructions. In the future, it will also support instruction execution through APIs. Acceptable instruction types: + InstallCommand: install the application + UninstallCommand: uninstall the application + CheckCommand: check the application state + SwitchCommand: switch the application state","tags":null,"title":"Basic model","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-model/","wordcount":221},{"author":null,"categories":null,"content":" Messages All messages are internally passed through SofaRequest and SofaResponse.\nTo convert to other protocols, you need to transform the messages to the objects that are to be actually transferred when calling and receiving requests.\nThe modules that can write SofaRequest and SofaResponse are as follows:\n Invoker Filter ServerHandler Serialization  The modules that can only read message bodies are as follows:\n Cluster Router LoadBalance  Logs The log initialization is based on the extension mechanism. Since the log loading should be done earliest, there is a separate key in rpc-config.json.\n{ / / Log implementation is done earlier than configuration loading, so it cannot adapt to the extension mechanism \u0026amp;quot;logger.impl\u0026amp;quot;: \u0026amp;quot;com.alipay.sofa.rpc.log.MiddlewareLoggerImpl\u0026amp;quot; }  Configuration items RPC configuration for users The user configuration includes port configuration (although the fields for setting port in the objects have been opened, SOFA gets those fields from the configuration file by default), thread pool size configuration, and other configurations.\n Load the configuration via SofaConfigs and call ExternalConfigLoader to read external properties. Get the configuration through the API provided by SofaConfigs. All the internally configured keys are in the SofaOptions class. Priority: System.property \u0026amp;gt; sofa-config.properties (one for each application) \u0026amp;gt; rpc-config.properties.  RPC framework configuration The configuration of the framework itself, such as default serialization and default timeout. In the future, SOFARPC may support the configuration for multiple ClassLoaders.\n Load the configuration file via RpcConfigs. Get and listen to data changes via the API provided by RpcConfigs All internally configured keys are in the RpcOptions class Priority: System.property \u0026amp;gt; custom rpc-config.json (There may be multiple custom configuration files which are sorted in certain order) \u0026amp;gt; rpc-config-default.json.  Constants  The global basic constants are in RpcConstants. For example:  Calling method (sync/oneway) Protocol (bolt/grpc/rest); serialization (hessian/java/protobuf) Context key  If the extension implements its own constants, you need to maintain the constants yourself. For example:\n The constants of BOLT protocol  SERIALIZE_CODE_HESSIAN = 1 PROTOCOL_TR = 13  The constants related with DSR Configuration Center  The keys specific for DSR Configuration Center, such as _WEIGHT and _CONNECTTIMEOUT    Address  The address information of provider is placed in the ProviderInfo class. The value of ProviderInfo is mainly divided into three parts:  Fields, which are generally required items, such as IP, port and status. Static fields, such as application name. Dynamic fields, such as warm-up weight.  Field enumerations are maintained in the ProviderInfoAttrs class.  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/common-model/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b2cc3f7ed134408d6adc25e418e1978b","permalink":"/en/projects/sofa-rpc/common-model/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/common-model/","summary":"Messages All messages are internally passed through SofaRequest and SofaResponse.\nTo convert to other protocols, you need to transform the messages to the objects that are to be actually transferred when calling and receiving requests.\nThe modules that can write SofaRequest and SofaResponse are as follows:\n Invoker Filter ServerHandler Serialization  The modules that can only read message bodies are as follows:\n Cluster Router LoadBalance  Logs The log initialization is based on the extension mechanism.","tags":null,"title":"Basic model","type":"projects","url":"/en/projects/sofa-rpc/common-model/","wordcount":386},{"author":null,"categories":null,"content":" Publish Service To use SOFARPC to publish a Bolt-protocol service, you only need to add a Binding named bolt. The ways to add Bolt Binding are as follows:\nXML To publish a Bolt service using XML, simply add the \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; tag to the \u0026amp;lt;sofa:service\u0026amp;gt; tag:\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.sample.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Annotation To publish a Bolt service using Annotation, you only need to set the bindingType of @SofaServiceBinding to bolt:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)}) Public class SampleServiceImpl implements SampleService { }  API in Spring environment To publish a Bolt-protocol service in Spring or Spring Boot environment, just add BoltBindingParam to ServiceParam:\nServiceParam serviceParam = new ServiceParam(); serviceParam.setInterfaceType(SampleService.class); // Set the service interface serviceParam.setInstance(new SampleServiceImpl()); // Set the implementation of the service interface List\u0026amp;lt;BindingParam\u0026amp;gt; params = new ArrayList\u0026amp;lt;BindingParam\u0026amp;gt;(); BindingParam serviceBindingParam = new BoltBindingParam(); Params.add(serviceBindingParam); serviceParam.setBindingParams(params);  API in non-Spring environment To provide the Bolt-protocol service using the bare API of SOFARPC in a non-Spring environment, just set the ServerConfig whose protocol is Bolt to the corresponding ProviderConfig:\nRegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181\u0026amp;quot;); // Create a new ServerConfig with Bolt protocol ServerConfig serverConfig = new ServerConfig() .setPort(8803) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;); ProviderConfig\u0026amp;lt;SampleService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRef(new SampleServiceImpl()) .setServer(serverConfig) // Set ServerConfig to ProviderConfig to indicate that the protocol published by this service is Bolt. .setRegistry(registryConfig); providerConfig.export();  Reference Service To reference a Bolt-protocol service using SOFARPC, just add a Binding named bolt. The ways to use Bolt Binding are as follows:\nXML To reference a Bolt-protocol service using XML, simply add the \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; tag to the \u0026amp;lt;sofa:reference\u0026amp;gt; tag:\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.sample.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation To reference a Bolt-protocol service using Annotation, just set the bindingType of @SofaReferenceBinding to bolt:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)) Private SampleService sampleService;  API in Spring environment To reference a Bolt-protocol service in a Spring or Spring Boot environment, simply add a BoltBindingParam to ReferenceParam:\nReferenceClient …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt-usage/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dd929c0a3cd2f4620ddf0d30d98ba85d","permalink":"/en/projects/sofa-rpc/bolt-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/bolt-usage/","summary":"Publish Service To use SOFARPC to publish a Bolt-protocol service, you only need to add a Binding named bolt. The ways to add Bolt Binding are as follows:\nXML To publish a Bolt service using XML, simply add the \u0026lt;sofa:binding.bolt\u0026gt; tag to the \u0026lt;sofa:service\u0026gt; tag:\n\u0026lt;sofa:service ref=\u0026quot;sampleService\u0026quot; interface=\u0026quot;com.alipay.sofa.rpc.sample.SampleService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;/sofa:service\u0026gt;  Annotation To publish a Bolt service using Annotation, you only need to set the bindingType of @SofaServiceBinding to bolt:","tags":null,"title":"Basic usage of Bolt protocol","type":"projects","url":"/en/projects/sofa-rpc/bolt-usage/","wordcount":364},{"author":null,"categories":null,"content":" In SOFARPC, to use different communication protocols, it is only required to use different Bindings. If you need to use the Dubbo protocol, just set Binding to Dubbo. The following shows an example using Annotation. For other usage methods, refer to Basic usage of Bolt protocol.\nPublish Service To publish a Dubbo service, just set the bindingType of @SofaServiceBinding to dubbo:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  Reference Service To reference a Dubbo service, just set the bindingType of @SofaReferenceBinding to dubbo:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;), jvmFirst = false) private SampleService sampleService;  Set the Group of Dubbo Service In the SOFARPC program model, there is no field called Group, but there is a model of uniqueId, which can be directly mapped to the Group in Dubbo model. For example, the following code is to publish a service whose Group is groupDemo:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;)}, uniqueId = \u0026amp;quot;groupDemo\u0026amp;quot;) public class SampleServiceImpl implements SampleService { }  The following code is to reference a service whose Group is groupDemo:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;), uniqueId = \u0026amp;quot;groupDemo\u0026amp;quot;, jvmFirst = false) private SampleService sampleService;   Note that Dubbo protocol currently only supports Zookeeper as service registry center.\n ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/dubbo-usage/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1bf72c194a20a5dccea70423690191f4","permalink":"/en/projects/sofa-rpc/dubbo-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/dubbo-usage/","summary":"In SOFARPC, to use different communication protocols, it is only required to use different Bindings. If you need to use the Dubbo protocol, just set Binding to Dubbo. The following shows an example using Annotation. For other usage methods, refer to Basic usage of Bolt protocol.\nPublish Service To publish a Dubbo service, just set the bindingType of @SofaServiceBinding to dubbo:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026quot;dubbo\u0026quot;)}) public class SampleServiceImpl implements SampleService { }  Reference Service To reference a Dubbo service, just set the bindingType of @SofaReferenceBinding to dubbo:","tags":null,"title":"Basic usage of Dubbo protocol","type":"projects","url":"/en/projects/sofa-rpc/dubbo-usage/","wordcount":203},{"author":null,"categories":null,"content":" In SOFARPC, to use different communication protocols, it is only required to use different Bindings. If you need to use the H2C protocol, just set Binding to H2C. The following shows an example using Annotation. For other usage methods, refer to Basic usage of Bolt protocol.\nPublish Service To publish an H2C service, just set the bindingType of @SofaServiceBinding to h2c:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;h2c\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  Reference Service To reference a H2C service, just set the bindingType of @SofaReferenceBinding to h2c:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;h2c\u0026amp;quot;), jvmFirst = false) private SampleService sampleService;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/h2c-usage/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fa75eff1e99b3acad5087160a1b44a09","permalink":"/en/projects/sofa-rpc/h2c-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/h2c-usage/","summary":"In SOFARPC, to use different communication protocols, it is only required to use different Bindings. If you need to use the H2C protocol, just set Binding to H2C. The following shows an example using Annotation. For other usage methods, refer to Basic usage of Bolt protocol.\nPublish Service To publish an H2C service, just set the bindingType of @SofaServiceBinding to h2c:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026quot;h2c\u0026quot;)}) public class SampleServiceImpl implements SampleService { }  Reference Service To reference a H2C service, just set the bindingType of @SofaReferenceBinding to h2c:","tags":null,"title":"Basic usage of H2C protocol","type":"projects","url":"/en/projects/sofa-rpc/h2c-usage/","wordcount":100},{"author":null,"categories":null,"content":" In SOFARPC (Not In SOFABoot/SpringBoot)，when use Http as a protocol of server，we can use Json as the way of serialization，for some basic test scenes.\nSOFARPC API Usage Service Publish ServerConfig serverConfig = new ServerConfig() .setStopTimeout(60000) .setPort(12300) .setProtocol(RpcConstants.PROTOCOL_TYPE_HTTP) .setDaemon(true); ProviderConfig\u0026amp;lt;HttpService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HttpService\u0026amp;gt;() .setInterfaceId(HttpService.class.getName()) .setRef(new HttpServiceImpl()) .setApplication(new ApplicationConfig().setAppName(\u0026amp;quot;serverApp\u0026amp;quot;)) .setServer(serverConfig) .setUniqueId(\u0026amp;quot;uuu\u0026amp;quot;) .setRegister(false); providerConfig.export();  Service Consume Because of the Http+Json，So users can use HttpClient to start a normal call, this is a piece of code in test.\nprivate ObjectMapper mapper = new ObjectMapper(); HttpClient httpclient = HttpClientBuilder.create().build(); // POST normal request String url = \u0026amp;quot;http://127.0.0.1:12300/com.alipay.sofa.rpc.server.http.HttpService:uuu/object\u0026amp;quot;; HttpPost httpPost = new HttpPost(url); httpPost.setHeader(RemotingConstants.HEAD_SERIALIZE_TYPE, \u0026amp;quot;json\u0026amp;quot;); ExampleObj obj = new ExampleObj(); obj.setId(1); obj.setName(\u0026amp;quot;xxx\u0026amp;quot;); byte[] bytes = mapper.writeValueAsBytes(obj); ByteArrayEntity entity = new ByteArrayEntity(bytes, ContentType.create(\u0026amp;quot;application/json\u0026amp;quot;)); httpPost.setEntity(entity); HttpResponse httpResponse = httpclient.execute(httpPost); Assert.assertEquals(200, httpResponse.getStatusLine().getStatusCode()); byte[] data = EntityUtils.toByteArray(httpResponse.getEntity()); ExampleObj result = mapper.readValue(data, ExampleObj.class); Assert.assertEquals(\u0026amp;quot;xxxxx\u0026amp;quot;, result.getName());  You will get some result.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/http-json/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"28abdf6369247346bad670c639a422b8","permalink":"/en/projects/sofa-rpc/http-json/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/http-json/","summary":"In SOFARPC (Not In SOFABoot/SpringBoot)，when use Http as a protocol of server，we can use Json as the way of serialization，for some basic test scenes.\nSOFARPC API Usage Service Publish ServerConfig serverConfig = new ServerConfig() .setStopTimeout(60000) .setPort(12300) .setProtocol(RpcConstants.PROTOCOL_TYPE_HTTP) .setDaemon(true); ProviderConfig\u0026lt;HttpService\u0026gt; providerConfig = new ProviderConfig\u0026lt;HttpService\u0026gt;() .setInterfaceId(HttpService.class.getName()) .setRef(new HttpServiceImpl()) .setApplication(new ApplicationConfig().setAppName(\u0026quot;serverApp\u0026quot;)) .setServer(serverConfig) .setUniqueId(\u0026quot;uuu\u0026quot;) .setRegister(false); providerConfig.export();  Service Consume Because of the Http+Json，So users can use HttpClient to start a normal call, this is a piece of code in test.","tags":null,"title":"Basic usage of HTTP protocol","type":"projects","url":"/en/projects/sofa-rpc/http-json/","wordcount":140},{"author":null,"categories":null,"content":" In SOFARPC, using different communication protocols is equal to using different Bindings. If you need to use the RESTful protocol, just set Binding to REST.\nPublish Service When defining a RESTful service interface, you need to add meta information to the interface using the annotations in JAXRS standard, such as the following interface:\n@Path(\u0026amp;quot;sample\u0026amp;quot;) public interface SampleService { @GET @Path(\u0026amp;quot;hello\u0026amp;quot;) String hello(); }   The annotations in JAXRS standard can be found in RESTEasy documentation.\n After the interface is defined, you can publish the implementation of the interface as a service, for example, by means of Annotation:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)}) public class RestfulSampleServiceImpl implements SampleService { @Override public String hello() { return \u0026amp;quot;Hello\u0026amp;quot;; } }  If you want to publish the service by other methods, please refer to Basic usage of Bolt protocol.\nAccess services through browser After the service is published, you can directly access the service through the browser. For the above service, the access address is as follows:\nhttp://localhost:8341/sample/hello  The default port for SOFARPC RESTful service is 8341.\nReference Service In addition to accessing RESTful services published by SOFARPC through a browser, you can also reference services through the standard SOFARPC service reference methods, such as Annotation:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)) private SampleService sampleService;  If you want to reference the service by other methods, please refer to Basic usage of Bolt protocol.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-basic/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d41f976864ba8f8221f5b5d26f354d1c","permalink":"/en/projects/sofa-rpc/restful-basic/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/restful-basic/","summary":"In SOFARPC, using different communication protocols is equal to using different Bindings. If you need to use the RESTful protocol, just set Binding to REST.\nPublish Service When defining a RESTful service interface, you need to add meta information to the interface using the annotations in JAXRS standard, such as the following interface:\n@Path(\u0026quot;sample\u0026quot;) public interface SampleService { @GET @Path(\u0026quot;hello\u0026quot;) String hello(); }   The annotations in JAXRS standard can be found in RESTEasy documentation.","tags":null,"title":"Basic usage of RESTful protocol","type":"projects","url":"/en/projects/sofa-rpc/restful-basic/","wordcount":228},{"author":null,"categories":null,"content":" Test code\nTest environment and conditions  Three 16-core 20 GB memory Docker containers as the server nodes (3 replicas) Two to eight 8-core Docker containers as clients 24 Raft groups. Each server node has eight leaders responsible for processing read/right requests. Followers do not have the permission to read. The target of stress testing is the RheaKV module of JRaft. Only the put and get APIs are subject to stress testing. Linearizable reads are guaranteed for the get API. The key size and value size are both 16 bytes. The read percentage is 10% and the write percentage is 90%.  Currently, the test scenarios are relatively simple. We will add more test scenarios in the future.\nTest scenario 1 Scenario 1: Test conditions    Number of clients Client batching Storage type Read/write ratio Replicator pipeline Key size Value size     8 Enabled MemoryDB 1:9 Enabled 16 bytes 16 bytes    Scenario 1: Result summary  Eight clients achieved 400,000+ ops, and the p95 RT is within 8 ms. Three server nodes didn\u0026amp;rsquo;t reach their maximum load. The load is about 15, and the CPU usage is about 40%.  Scenario 1: Load of three servers Scenario 1: Server 1 top - 20:11:14 up 10 days, 23:09, 1 user, load average: 12.29, 6.92, 4.00 Tasks: 36 total, 1 running, 35 sleeping, 0 stopped, 0 zombie %Cpu0 : 24.3 us, 17.7 sy, 0.0 ni, 50.0 id, 2.0 wa, 0.0 hi, 0.0 si, 6.0 st %Cpu1 : 21.9 us, 18.5 sy, 0.0 ni, 49.5 id, 2.0 wa, 0.0 hi, 0.0 si, 8.1 st %Cpu2 : 20.6 us, 18.6 sy, 0.0 ni, 53.2 id, 2.0 wa, 0.0 hi, 0.0 si, 5.6 st %Cpu3 : 23.3 us, 20.0 sy, 0.0 ni, 50.3 id, 1.3 wa, 0.0 hi, 0.0 si, 5.0 st %Cpu4 : 24.1 us, 19.1 sy, 0.0 ni, 49.8 id, 2.3 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu5 : 21.3 us, 18.9 sy, 0.0 ni, 53.2 id, 2.0 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu6 : 24.7 us, 18.4 sy, 0.0 ni, 50.2 id, 2.0 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu7 : 24.8 us, 17.8 sy, 0.0 ni, 50.0 id, 1.7 wa, 0.0 hi, 0.0 si, 5.7 st %Cpu8 : 26.0 us, 18.3 sy, 0.0 ni, 51.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.0 st %Cpu9 : 26.6 us, 16.9 sy, 0.0 ni, 52.2 id, 2.0 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu10 : 31.7 us, 17.7 sy, 0.0 ni, 46.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.0 st %Cpu11 : 23.2 us, 18.9 sy, 0.0 ni, 53.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu12 : 25.6 us, 18.3 sy, 0.0 ni, 51.5 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu13 : 22.6 us, 18.3 sy, 0.0 ni, 54.5 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu14 : 24.7 us, 17.3 sy, 0.0 ni, 54.0 id, 1.7 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu15 : 61.8 us, 8.3 sy, 0.0 ni, 28.2 id, 0.3 wa, 0.0 hi, 0.0 si, 1.3 st KiB Mem : 62914560 total, 6854596 free, 39128016 used, 16931948 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 6854596 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 15682 root 20 0 12.853g 8.859g 24064 S 708.7 14.8 26:49.38 java  Scenario 1: Server 2 top - 20:11:47 up 10 days, 23:03, 1 user, load average: 17.68, 8.50, 4.56 Tasks: 33 total, 1 running, 31 sleeping, 0 stopped, 1 zombie %Cpu0 : 22.7 us, 17.3 sy, 0.0 ni, 35.0 id, 8.3 wa, 0.0 hi, 0.0 si, 16.7 st %Cpu1 : 20.1 us, 19.4 …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/benchmark-performance/","fuzzywordcount":8900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4530827525ca87732eaf76ae34f03603","permalink":"/en/projects/sofa-jraft/benchmark-performance/","publishdate":"0001-01-01T00:00:00Z","readingtime":42,"relpermalink":"/en/projects/sofa-jraft/benchmark-performance/","summary":"Test code\nTest environment and conditions  Three 16-core 20 GB memory Docker containers as the server nodes (3 replicas) Two to eight 8-core Docker containers as clients 24 Raft groups. Each server node has eight leaders responsible for processing read/right requests. Followers do not have the permission to read. The target of stress testing is the RheaKV module of JRaft. Only the put and get APIs are subject to stress testing.","tags":null,"title":"Benchmark data","type":"projects","url":"/en/projects/sofa-jraft/benchmark-performance/","wordcount":8817},{"author":null,"categories":null,"content":" 测试代码\n测试环境\u0026amp;amp;条件  3 台 16C 20G 内存的 docker 容器作为 server node (3 副本) 2 ~ 8 台 8C docker 容器 作为 client 24 个 raft 复制组，平均每台 server node 上各自有 8 个 leader 负责读写请求，不开启 follower 读 压测目标为 JRaft 中的 RheaKV 模块，只压测 put、get 两个接口，其中 get 是保证线性一致读的，key 和 value 大小均为 16 字节 读比例 10%，写比例 90%  目前的测试场景比较简单，以后会增加更多测试场景\n测试场景1 场景1: 测试条件    Client 数量 Client-Batching Storage-Type 读写比例 Replicator-Pipeline key 大小 value 大小     8 开启 MemoryDB 1:9 开启 16 字节 16字节    场景1: 结果汇总：  8 个 client 一共达到 40w+ ops，p95 RT 在 8ms 以内 3 个 server 节点负载没达到极限 load 15 左右，cpu 40% 左右  场景1: 3 个 server 机器负载： 场景1: server1 top - 20:11:14 up 10 days, 23:09, 1 user, load average: 12.29, 6.92, 4.00 Tasks: 36 total, 1 running, 35 sleeping, 0 stopped, 0 zombie %Cpu0 : 24.3 us, 17.7 sy, 0.0 ni, 50.0 id, 2.0 wa, 0.0 hi, 0.0 si, 6.0 st %Cpu1 : 21.9 us, 18.5 sy, 0.0 ni, 49.5 id, 2.0 wa, 0.0 hi, 0.0 si, 8.1 st %Cpu2 : 20.6 us, 18.6 sy, 0.0 ni, 53.2 id, 2.0 wa, 0.0 hi, 0.0 si, 5.6 st %Cpu3 : 23.3 us, 20.0 sy, 0.0 ni, 50.3 id, 1.3 wa, 0.0 hi, 0.0 si, 5.0 st %Cpu4 : 24.1 us, 19.1 sy, 0.0 ni, 49.8 id, 2.3 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu5 : 21.3 us, 18.9 sy, 0.0 ni, 53.2 id, 2.0 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu6 : 24.7 us, 18.4 sy, 0.0 ni, 50.2 id, 2.0 wa, 0.0 hi, 0.0 si, 4.7 st %Cpu7 : 24.8 us, 17.8 sy, 0.0 ni, 50.0 id, 1.7 wa, 0.0 hi, 0.0 si, 5.7 st %Cpu8 : 26.0 us, 18.3 sy, 0.0 ni, 51.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.0 st %Cpu9 : 26.6 us, 16.9 sy, 0.0 ni, 52.2 id, 2.0 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu10 : 31.7 us, 17.7 sy, 0.0 ni, 46.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.0 st %Cpu11 : 23.2 us, 18.9 sy, 0.0 ni, 53.3 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu12 : 25.6 us, 18.3 sy, 0.0 ni, 51.5 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu13 : 22.6 us, 18.3 sy, 0.0 ni, 54.5 id, 2.3 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu14 : 24.7 us, 17.3 sy, 0.0 ni, 54.0 id, 1.7 wa, 0.0 hi, 0.0 si, 2.3 st %Cpu15 : 61.8 us, 8.3 sy, 0.0 ni, 28.2 id, 0.3 wa, 0.0 hi, 0.0 si, 1.3 st KiB Mem : 62914560 total, 6854596 free, 39128016 used, 16931948 buff/cache KiB Swap: 2097148 total, 2097148 free, 0 used. 6854596 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 15682 root 20 0 12.853g 8.859g 24064 S 708.7 14.8 26:49.38 java  场景1: server2 top - 20:11:47 up 10 days, 23:03, 1 user, load average: 17.68, 8.50, 4.56 Tasks: 33 total, 1 running, 31 sleeping, 0 stopped, 1 zombie %Cpu0 : 22.7 us, 17.3 sy, 0.0 ni, 35.0 id, 8.3 wa, 0.0 hi, 0.0 si, 16.7 st %Cpu1 : 20.1 us, 19.4 sy, 0.0 ni, 43.8 id, 9.4 wa, 0.0 hi, 0.0 si, 7.4 st %Cpu2 : 23.3 us, 20.0 sy, 0.0 ni, 39.7 id, 10.3 wa, 0.0 hi, 0.0 si, 6.7 st %Cpu3 : 24.1 us, 20.1 sy, 0.0 ni, 40.8 id, 9.4 wa, 0.0 hi, 0.0 si, 5.7 st %Cpu4 : 21.4 us, 17.7 sy, 0.0 ni, 37.1 id, 9.0 wa, 0.0 hi, 0.0 si, 14.7 st %Cpu5 : 22.6 us, 19.6 sy, 0.0 ni, 40.5 id, 10.6 wa, 0.0 hi, 0.0 si, 6.6 st %Cpu6 : 23.6 us, 19.9 sy, 0.0 ni, 40.2 id, 10.3 wa, 0.0 hi, 0.0 si, 6.0 st %Cpu7 : 20.5 us, 19.9 sy, 0.0 ni, 44.4 id, 9.9 wa, 0.0 hi, 0.0 si, 5.3 st %Cpu8 : 40.7 us, 13.3 sy, 0.0 ni, 34.3 id, 9.0 wa, 0.0 hi, 0.0 si, 2.7 st %Cpu9 : 39.9 us, 14.0 sy, 0.0 ni, 35.2 …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/benchmark-performance/","fuzzywordcount":9300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4530827525ca87732eaf76ae34f03603","permalink":"/projects/sofa-jraft/benchmark-performance/","publishdate":"0001-01-01T00:00:00Z","readingtime":19,"relpermalink":"/projects/sofa-jraft/benchmark-performance/","summary":"测试代码 测试环境\u0026amp;条件 3 台 16C 20G 内存的 docker 容器作为 server node (3 副本) 2 ~ 8 台 8C docker 容器 作为 client 24 个 raft 复制组，平均每台 server node 上各自有 8 个 leader 负责读写请求","tags":null,"title":"Benchmark 数据","type":"projects","url":"/projects/sofa-jraft/benchmark-performance/","wordcount":9269},{"author":null,"categories":null,"content":"Bolt protocol is a TCP-based custom protocol that performs better than HTTP. Within Ant Financial, a large number of RPCs use the Bolt protocol to communicate: * Basic usage * Calling type * Timeout control * Generic call * Serialization protocol * Custom thread pool\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b812f6aadadf38b79140a711ff6aa6cd","permalink":"/en/projects/sofa-rpc/bolt/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/bolt/","summary":"Bolt protocol is a TCP-based custom protocol that performs better than HTTP. Within Ant Financial, a large number of RPCs use the Bolt protocol to communicate: * Basic usage * Calling type * Timeout control * Generic call * Serialization protocol * Custom thread pool","tags":null,"title":"Bolt protocol","type":"projects","url":"/en/projects/sofa-rpc/bolt/","wordcount":45},{"author":null,"categories":null,"content":"Bolt 协议一个基于 TCP 的自定义的协议，相比 HTTP 来说，性能更好，在蚂蚁金服内部，大量的 RPC 都是采用 Bolt 协议来进行通信： * 基本使用 * 调用方式 * 超时控制 * 泛化调用 * 序列化协议 * 自定义线程池\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b812f6aadadf38b79140a711ff6aa6cd","permalink":"/projects/sofa-rpc/bolt/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/bolt/","summary":"Bolt 协议一个基于 TCP 的自定义的协议，相比 HTTP 来说，性能更好，在蚂蚁金服内部，大量的 RPC 都是采用 Bolt 协议来进行通信： * 基本使用 * 调用方式 * 超时控制 * 泛化","tags":null,"title":"Bolt 协议","type":"projects","url":"/projects/sofa-rpc/bolt/","wordcount":85},{"author":null,"categories":null,"content":" Bolt 协议基本使用 发布服务 使用 SOFARPC 发布一个 Bolt 协议的服务，只需要增加名称为 Bolt 的 Binding 即可，不同的使用方式添加 Bolt Binding 的方式如下：\nXML 使用 XML 发布一个 Bolt 协议只需要在 \u0026amp;lt;sofa:service\u0026amp;gt; 标签下增加 \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; 标签即可：\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.sample.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Annotation 使用 Annotation 发布一个 Bolt 协议的服务只需要设置 @SofaServiceBinding 的 bindingType 为 bolt 即可：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  Spring 环境下 API 方式 在 Spring 或者 Spring Boot 环境下发布一个 Bolt 协议的服务只需要往 ServiceParam 里面增加一个 BoltBindingParam 即可：\nServiceParam serviceParam = new ServiceParam(); serviceParam.setInterfaceType(SampleService.class); // 设置服务接口 serviceParam.setInstance(new SampleServiceImpl()); // 设置服务接口的实现 List\u0026amp;lt;BindingParam\u0026amp;gt; params = new ArrayList\u0026amp;lt;BindingParam\u0026amp;gt;(); BindingParam serviceBindingParam = new BoltBindingParam(); params.add(serviceBindingParam); serviceParam.setBindingParams(params);  非 Spring 环境下的 API 方式 在非 Spring 环境下使用 SOFARPC 的裸 API 提供 Bolt 协议的服务，只需要将 Protocol 为 Bolt 的 ServerConfig 设置给对应的 ProviderConfig：\nRegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181\u0026amp;quot;); // 新建一个协议为 Bolt 的 ServerConfig ServerConfig serverConfig = new ServerConfig() .setPort(8803) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;); ProviderConfig\u0026amp;lt;SampleService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRef(new SampleServiceImpl()) .setServer(serverConfig) // 将 ServerConfig 设置给 ProviderConfig，表示这个服务发布的协议为 Bolt。 .setRegistry(registryConfig); providerConfig.export();  引用服务 使用 SOFARPC 引用一个 Bolt 服务，只需要增加名称为 Bolt 的 Binding 即可，不同的使用方式添加 Bolt Binding 的方式如下：\nXML 使用 XML 引用一个 Bolt 协议的服务只需要在 \u0026amp;lt;sofa:reference\u0026amp;gt; 标签下增加 \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; 标签即可：\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.sample.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 使用 Annotation 引用一个 Bolt 协议的服务只需要设置 @SofaReferenceBinding 的 bindingType 为 bolt 即可：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)) private SampleService sampleService;  Spring 环境下 API 方式 在 Spring 或者 Spring Boot 环境下引用一个 Bolt 协议的服务只需要往 ReferenceParam 里面增加一个 BoltBindingParam 即可：\nReferenceClient referenceClient = clientFactory.getClient(ReferenceClient.class); ReferenceParam\u0026amp;lt;SampleService\u0026amp;gt; referenceParam = new ReferenceParam\u0026amp;lt;SampleService\u0026amp;gt;(); referenceParam.setInterfaceType(SampleService.class); BindingParam refBindingParam = new BoltBindingParam(); referenceParam.setBindingParam(refBindingParam);  非 Spring 环境下的 API 方式 在非 Spring 环境下使用 SOFARPC 的裸 API 引用一个 Bolt 协议的服务，只需要设置 ConsumerConfig 的 Protocol 为 bolt 即可：\nConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt-usage/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dd929c0a3cd2f4620ddf0d30d98ba85d","permalink":"/projects/sofa-rpc/bolt-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/bolt-usage/","summary":"Bolt 协议基本使用 发布服务 使用 SOFARPC 发布一个 Bolt 协议的服务，只需要增加名称为 Bolt 的 Binding 即可，不同的使用方式添加 Bolt Binding 的方式如下： XML 使用 XML 发布一个 Bolt 协议只需要","tags":null,"title":"Bolt 协议基本使用","type":"projects","url":"/projects/sofa-rpc/bolt-usage/","wordcount":570},{"author":null,"categories":null,"content":" 泛化调用提供了让客户端在不需要依赖服务端的接口情况下就能够发起调用的能力。目前 SOFARPC 的泛化调用仅支持在 Bolt 通信协议下使用 Hessian2 作为序列化协议，其他的方式并不支持。\nSOFABoot 环境 发布服务 发布服务没有什么特殊的,正常发布服务即可.比如\n\u0026amp;lt;!-- generic --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;sampleGenericServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleGenericServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  引用服务 \u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;sampleGenericServiceReference\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.api.GenericService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs generic-interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericService\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  其中,jvm-first根据实际情况,默认可以不写,接口写泛化调用的通用接口,generic-interface中写上自己要调用的接口名称即可.\n发起调用 GenericService sampleGenericServiceReference = (GenericService) applicationContext .getBean(\u0026amp;quot;sampleGenericServiceReference\u0026amp;quot;); GenericObject genericResult = (GenericObject) sampleGenericServiceReference.$genericInvoke(\u0026amp;quot;sayGeneric\u0026amp;quot;, new String[] { \u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericParamModel\u0026amp;quot; }, new Object[] { genericObject });  RPC API ConsumerConfig\u0026amp;lt;GenericService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;GenericService\u0026amp;gt;() .setInterfaceId(\u0026amp;quot;com.alipay.sofa.rpc.quickstart.HelloService\u0026amp;quot;) .setGeneric(true); GenericService testService = consumerConfig.refer(); String result = (String) testService.$invoke(\u0026amp;quot;sayHello\u0026amp;quot;, new String[] { \u0026amp;quot;java.lang.String\u0026amp;quot; },new Object[] { \u0026amp;quot;1111\u0026amp;quot; });  如上通过 setGeneric 设置该服务为泛化服务，设置服务方的接口名。以 GenericService 作为泛化服务，通过 GenericService 就能够发起泛化调用了。发起调用时，需要传入方法名，方法类型，方法参数。\n如果参数或者返回结果在客户端也需要泛化表示。可以通过 GenericObject 来实现。\nGenericObject genericObject = new GenericObject(\u0026amp;quot;com.alipay.sofa.rpc.invoke.generic.TestObj\u0026amp;quot;); genericObject.putField(\u0026amp;quot;str\u0026amp;quot;, \u0026amp;quot;xxxx\u0026amp;quot;); genericObject.putField(\u0026amp;quot;num\u0026amp;quot;, 222); GenericObject result = (GenericObject) testService.$genericInvoke(\u0026amp;quot;echoObj\u0026amp;quot;, new String[] { \u0026amp;quot;com.alipay.sofa.rpc.invoke.generic.TestObj\u0026amp;quot; }, new Object[] { genericObject }); String str = result.getField(\u0026amp;quot;str\u0026amp;quot;); String num = result.getField(\u0026amp;quot;num\u0026amp;quot;);  如上就能获得序列化结果。完整的泛化调用方式使用说明如下：\n/** * Java Bean */ public class People { private String name; private int age; // getters and setters } /** * 服务方提供的接口 */ interface SampleService { String hello(String arg); People hello(People people); String[] hello(String[] args); } /** * 客户端 */ public class ConsumerClass { GenericService genericService; public void do() { // 1. $invoke仅支持方法参数类型在当前应用的 ClassLoader 中存在的情况 genericService.$invoke(\u0026amp;quot;hello\u0026amp;quot;, new String[]{ String.class.getName() }, new Object[]{\u0026amp;quot;I\u0026#39;m an arg\u0026amp;quot;}); // 2. $genericInvoke支持方法参数类型在当前应用的 ClassLoader 中不存在的情况。 // 2.1 构造参数 GenericObject genericObject = new …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/generic-invoke/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"84ac624dc99a42a8f89489aa10304ef7","permalink":"/projects/sofa-rpc/generic-invoke/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/generic-invoke/","summary":"泛化调用提供了让客户端在不需要依赖服务端的接口情况下就能够发起调用的能力。目前 SOFARPC 的泛化调用仅支持在 Bolt 通信协议下使用 Hessian2 作为序列化协议，其他的方","tags":null,"title":"Bolt 协议泛化调用","type":"projects","url":"/projects/sofa-rpc/generic-invoke/","wordcount":710},{"author":null,"categories":null,"content":" 调用方式 SOFARPC 在 Bolt 协议下提供了多种调用方式满足不同的场景。\n同步 在同步的调用方式下，客户端发起调用后会等待服务端返回结果再进行后续的操作。这是 SOFARPC 的默认调用方式，无需进行任何设置即可。\n异步 异步调用的方式下，客户端发起调用后不会等到服务端的结果，继续执行后面的业务逻辑。服务端返回的结果会被 SOFARPC 缓存，当客户端需要结果的时候，再主动调用 API 获取。如果需要将一个服务设置为异步的调用方式，在对应的使用方式下设置 type 属性即可：\nXML 方式 在 XML 方式下，设置 \u0026amp;lt;sofa:global-attrs\u0026amp;gt; 标签的 type 属性为 future 即可：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs type=\u0026amp;quot;future\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 方式 在 Annotation 方式下，设置 @SofaReferenceBinding 的 invokeType 属性为 future 即可：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, invokeType = \u0026amp;quot;future\u0026amp;quot;)) private SampleService sampleService;  Spring 环境下 API 方式 在 Spring 环境下使用 API，设置 BoltBindingParam 的 type 属性即可：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setType(\u0026amp;quot;future\u0026amp;quot;);  在非 Spring 环境下 API 方式 在非 Spring 环境下使用 SOFARPC 裸 API，设置 ConsumerConfig 的 invokeType 属性即可：\nConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRegistry(registryConfig) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) .setInvokeType(\u0026amp;quot;future\u0026amp;quot;);  获取调用结果 使用异步调用的方式，目前提供了两种方式来获取异步调用的结果：\n直接获取结果 用户可以通过以下的方式来直接获取异步调用的结果：\nString result = (String)SofaResponseFuture.getResponse(0, true);  其中第一个参数是获取结果的超时时间，第二个参数表示是否清除线程上下文中的结果。\n获取 JDK 原生 Future 用户可以通过以下的方式来获取 JDK 的原生的 Future 对象，再可以从任意地方去调用这个 Future 对象来获取结果：\nFuture future = SofaResponseFuture.getFuture(true);  其中的第一个参数表示是否清除线程上下文中的结果。\n回调 SOFARPC Bolt 协议的回调方式可以让 SOFARPC 在发起调用后不等待结果，在客户端收到服务端返回的结果后，自动回调用户实现的一个回调接口。\n使用 SOFARPC Bolt 协议的回调方式，首先需要实现一个回调接口，并且在对应的配置中设置回调接口，再将调用方式设置为 callback。\n实现回调接口 SOFARPC 提供了一个回调的接口 com.alipay.sofa.rpc.core.invoke.SofaResponseCallback，用户使用 SOFARPC Bolt 协议的回调方式，首先需要实现这个接口，该接口提供了三个方法：\n onAppResponse：当客户端接收到服务端的正常返回的时候，SOFARPC 会回调这个方法。 onAppException：当客户端接收到服务端的异常响应的时候，SOFARPC 会回调这个方法。 onSofaException：当 SOFARPC 本身出现一些错误，比如路由错误的时候，SOFARPC 会回调这个方法。  设置回调接口 实现回调接口之后，用户需要将实现类设置到对应的服务引用配置中，并且将调用方式设置为 callback。\nSOFARPC 为设置回调接口提供了两种方式，分别为 Callback Class 和 Callback Ref。Callback Class 的方式直接设置回调的类名，SOFARPC 会通过调用回调类的默认构造函数的方式生成回调类的实例。Callback Ref 的方式则为用户直接提供回调类的实例。\nXML 方式 如果通过 XML 的方式引用服务，将 \u0026amp;lt;sofa:global-attrs\u0026amp;gt; 标签的 type 属性设置为 callback，并且设置 callback-ref 或者 callback-class 属性即可：\n\u0026amp;lt;bean id=\u0026amp;quot;sampleCallback\u0026amp;quot; class=\u0026amp;quot;com.example.demo.SampleCallback\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs type=\u0026amp;quot;callback\u0026amp;quot; callback-ref=\u0026amp;quot;sampleCallback\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  在 XML 的方式下，callback-ref 的值需要是回调类的 Bean 名称。\nAnnotation 方式 如果通过 Annotation 的方式引用服务，设置 @SofaReferenceBinding 注解的 invokeType 属性为 callback，并且设置 callbackClass 或者 callbackRef 属性即可：\n@SofaReference(binding = …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/invoke-type/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e296d9344b6aa9f550e6213b97da084b","permalink":"/projects/sofa-rpc/invoke-type/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-rpc/invoke-type/","summary":"调用方式 SOFARPC 在 Bolt 协议下提供了多种调用方式满足不同的场景。 同步 在同步的调用方式下，客户端发起调用后会等待服务端返回结果再进行后续的操作。这是 SOFARPC 的","tags":null,"title":"Bolt 协议调用方式","type":"projects","url":"/projects/sofa-rpc/invoke-type/","wordcount":1619},{"author":null,"categories":null,"content":" 超时控制 使用 Bolt 协议进行通信的时候，SOFARPC 的超时时间默认为 3 秒，用户可以在引用服务的时候去设置超时时间，又分别可以在服务以及方法的维度设置超时时间，SOFARPC 的超时时间的设置的单位都为毫秒。\n服务维度 如果需要在发布服务的时候在服务维度设置超时时间，设置对应的 timeout 参数到对应的值即可。\nXML 方式 如果使用 XML 的方式引用服务，设置 \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; 标签下的 \u0026amp;lt;sofa:global-attrs\u0026amp;gt; 标签的 timeout 属性的值即可：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs timeout=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 方式 如果使用 Annotation 引用服务，设置 @SofaReferenceBinding 的 timeout 属性的值即可：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, timeout = 2000)) private SampleService sampleService;  Spring 环境 API 方式 如果在 Spring 或者 Spring Boot 的环境下引用服务，设置 BoltBindingParam 的 timeout 属性的值即可：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setTimeout(2000)  非 Spring 环境下 API 方式 如果在非 Spring 环境下直接使用 SOFARPC 的裸 API 引用服务，设置 ConsumerConfig 的 timeout 属性即可：\nConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRegistry(registryConfig) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) .setTimeout(2000);  方法维度 如果想要单独调整一个服务中某一个方法的超时时间，可以通过在方法维度上设置超时时间来实现。\n对于某一个方法来说，优先方法维度的超时时间，如果没有设置，则使用服务维度的超时时间。\nXML 方式 如果使用 XML 的方式引用一个服务，设置对应的 \u0026amp;lt;sofa:method\u0026amp;gt; 标签的 timeout 属性即可：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;hello\u0026amp;quot; timeout=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 方式 目前暂未提供通过 Annotation 的方式来设置方法级别的超时时间。\nSpring 环境 API 方式 如果在 Spring 或者 Spring Boot 的环境下引用服务，设置 RpcBindingMethodInfo 的 timeout 属性的值即可：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); RpcBindingMethodInfo rpcBindingMethodInfo = new RpcBindingMethodInfo(); rpcBindingMethodInfo.setName(\u0026amp;quot;hello\u0026amp;quot;); rpcBindingMethodInfo.setTimeout(2000); List\u0026amp;lt;RpcBindingMethodInfo\u0026amp;gt; rpcBindingMethodInfos = new ArrayList\u0026amp;lt;\u0026amp;gt;(); rpcBindingMethodInfos.add(rpcBindingMethodInfo); boltBindingParam.setMethodInfos(rpcBindingMethodInfos);  非 Spring 环境 API 方式 如果在非 Spring 环境下使用 SOFARPC 的裸 API 引用服务，设置 MethodConfig 的 timeout 属性即可：\nMethodConfig methodConfig = new MethodConfig(); methodConfig.setName(\u0026amp;quot;hello\u0026amp;quot;); methodConfig.setTimeout(2000); List\u0026amp;lt;MethodConfig\u0026amp;gt; methodConfigs = new ArrayList\u0026amp;lt;MethodConfig\u0026amp;gt;(); methodConfigs.add(methodConfig); ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRegistry(registryConfig) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) .setMethods(methodConfigs);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt-timeout/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cf14f73dc0c4672a9255ef55b56de419","permalink":"/projects/sofa-rpc/bolt-timeout/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/bolt-timeout/","summary":"超时控制 使用 Bolt 协议进行通信的时候，SOFARPC 的超时时间默认为 3 秒，用户可以在引用服务的时候去设置超时时间，又分别可以在服务以及方法的维度","tags":null,"title":"Bolt 协议超时控制","type":"projects","url":"/projects/sofa-rpc/bolt-timeout/","wordcount":584},{"author":null,"categories":null,"content":" As one of the developing directions of cloud native technology, Serverless architecture enables you to further improve resource utilization and focus on service development. With our workshop, you can experience new features such as quick creation of Serveless applications, automatic second-level 0-1-N scaling based on service requests, and quick troubleshooting via log viewer.\nWorkshop procedure Flow diagram Preview Preparation  Access to Serverless application service address Login with account and password Git clone this project to local  Step 1-1: Publish backend Java application  Select Create quickly Select Java Runtime Upload the code package balance-mng.jar The entry method can be automatically recognized Port: 8080 Copy and save the backend service address after creation View the number of computing instances of backend service: 0  Step 1-2: Publish frontend NodeJS application  Select create an application Select the buildpack NodeJS Upload the code package stock-mng.zip The entry method can be automatically recognized Select nodejs-0.0.1.1-pre at runtime Port: 3000 Set the environment variable BALANCEMNG_URL as the backend service address  Step 2: 0-1 cold boot capability  Access frontend service View the changes in the number of the computing instances of backend service  Step 3: Log and monitoring  View application service logs via Log Shell View usage amount via monitoring  Step 4: Configure time trigger  Configure timing trigger to call application at fixed time View the triggering results via operation records  Step 5: Create versions and control traffic  Clone the frontend application and create a new version Upload the code package stock-mng-v2.zip Configure router to visit V1 and V2 at 1:1 ratio View the effect in the browser  Step 6: Quick M-n capability for high-concurrency  Simulate high concurrency situtation via scripts and access the frontend application service Check how the Serverless application perform quick M-N computing instance changes  ","date":-62135596800,"description":"With this guide, you can experience new features such as quick creation of Serveless applications, automatic second-level 0-1-N scaling based on service requests, quick troubleshooting via log viewer, and application startup at fixed time.","dir":"guides/kc-serverless-demo/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f355d1b598fed47b730bd74ad25f3683","permalink":"/en/guides/kc-serverless-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/guides/kc-serverless-demo/","summary":"As one of the developing directions of cloud native technology, Serverless architecture enables you to further improve resource utilization and focus on service development. With our workshop, you can experience new features such as quick creation of Serveless applications, automatic second-level 0-1-N scaling based on service requests, and quick troubleshooting via log viewer.\nWorkshop procedure Flow diagram Preview Preparation  Access to Serverless application service address Login with account and password Git clone this project to local  Step 1-1: Publish backend Java application  Select Create quickly Select Java Runtime Upload the code package balance-mng.","tags":null,"title":"Build applications on the cloud based on Serverless","type":"guides","url":"/en/guides/kc-serverless-demo/","wordcount":290},{"author":null,"categories":null,"content":" Procedure This guide introduces how to quickly build a microservice based on SOFAStack. It mainly includes the following steps.\n Publish service using SOFABoot and SOFARPC Call service using SOFABoot and SOFARPC View Tracer information reported by SOFATracer via ZipKin View Metrics information via SOFALookout  Architecture Tasks 1. Preparation Clone the project demo from GitHub to local\ngit clone https://github.com/sofastack-guides/kc-sofastack-demo.git  Import the project into IDEA or Eclipse. After import, the interface is as follows:\n balance-mng: account management system, providing deduction balance service stock-mng: account system, providing deduction inventory service  2. Introduce dependencies Add the following dependencies into the pom.xml files of balance-mng and stock-mng project modules.\n\u0026amp;lt;!--SOFARPC dependency--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--SOFATracer dependency--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--SOFARegistry dependency--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--runtime dependency--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!--SOFALookout dependency--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  For balance-mng project, you need to introduce the dependencies into the pom file of balance-mng-imp module.\nFor stock-mng project, you need to introduce the dependencies into the pom file of stock-mng module.\n3. Add configurations Copy the following configurations into the application.properties file of balance-mng and stock-mng project module.\n# 1、Add Service Registry address com.alipay.sofa.rpc.registry.address=sofa://118.31.43.62:9603 # 2、Add the zipkin address where tracer data is reported com.alipay.sofa.tracer.zipkin.base-url=http://139.224.123.199:9411 # 3、Add the server-side address where the metrics data is reported com.alipay.sofa.lookout.agent-host-address=139.224.123.35  For balance-mng project, you need to add configurations to the application.properties file in balance-mng-bootstrap module.\nFor stock-mng project, you need to add configurations to the application.properties file in stock-mng module.\n4. Modify unique id Since everyone shares a set of service discoveries, to differentiate the services published by different users, it is required to add a unique id to the service.\nKubeCon workshop will prepare a SOFAStack account for each user in the format ofuser0@sofastack.io to user99@sofastack.io. The first half of the account, namely …","date":-62135596800,"description":"This guide introduces how to quickly build a microservice based on SOFAStack. ","dir":"guides/sofastack-quick-start/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"78bfd4806a86dc15ac86eee16fb85c82","permalink":"/en/guides/sofastack-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/guides/sofastack-quick-start/","summary":"Procedure This guide introduces how to quickly build a microservice based on SOFAStack. It mainly includes the following steps.\n Publish service using SOFABoot and SOFARPC Call service using SOFABoot and SOFARPC View Tracer information reported by SOFATracer via ZipKin View Metrics information via SOFALookout  Architecture Tasks 1. Preparation Clone the project demo from GitHub to local\ngit clone https://github.com/sofastack-guides/kc-sofastack-demo.git  Import the project into IDEA or Eclipse. After import, the interface is as follows:","tags":null,"title":"Build microservices with SOFAStack","type":"guides","url":"/en/guides/sofastack-quick-start/","wordcount":586},{"author":null,"categories":null,"content":" SOFARPC provides a variety of calling types under the Bolt protocol to meet different scenarios.\nSynchronous In the synchronous calling type, after the client initiates a call, it will wait for the server to return the result and then perform subsequent operations. This is the default calling type of SOFARPC.\nAsynchronous In the asynchronous calling type, after the client initiates a call, it will not wait for the result from the server but continue to execute the subsequent business logic. The result returned by the server will be cached by SOFARPC. When the client needs the result, it can call the API to get the result. To set a service to be asynchronous, you can configure the type attribute in the corresponding usage mode:\nXML In the XML mode, set the type attribute of the \u0026amp;lt;sofa:global-attrs\u0026amp;gt; tag to future:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs type=\u0026amp;quot;future\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation In Annotation mode, set the invokeType attribute of @SofaReferenceBinding to future:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, invokeType = \u0026amp;quot;future\u0026amp;quot;)) private SampleService sampleService;  API in Spring environment When using the API in Spring environment, you just need to set the type attribute of BoltBindingParam:\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setType(\u0026amp;quot;future\u0026amp;quot;);  API in non-Spring environment When using the bare API of SOFARPC in a non-Spring environment, you just need to set the invokeType attribute of ConsumerConfig:\nConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRegistry(registryConfig) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) .setInvokeType(\u0026amp;quot;future\u0026amp;quot;);  Get the calling result Currently, there are two ways to get the result of an asynchronous call:\nGet result directly You can directly get the result of an asynchronous call in the following way:\nString result = (String)SofaResponseFuture.getResponse(0, true);  The first parameter is the timeout period for getting the result, and the second parameter indicates whether to clear the result in the thread context.\nGet JDK native Future You can get the JDK native Future object in the following way, and then call the Future object from anywhere to get the result:\nFuture future = SofaResponseFuture.getFuture(true);  The first parameter indicates whether to clear the result in the thread context.\nCallback By using the callback type of the SOFARPC Bolt protocol, SOFARPC doesn\u0026amp;rsquo;t need to wait for the result after initiating a call. After the client receives the result returned from the server, it can automatically call back a callback interface implemented by the user.\nTo use the callback type of the SOFARPC Bolt protocol, you first need to …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/invoke-type/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e296d9344b6aa9f550e6213b97da084b","permalink":"/en/projects/sofa-rpc/invoke-type/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-rpc/invoke-type/","summary":"SOFARPC provides a variety of calling types under the Bolt protocol to meet different scenarios.\nSynchronous In the synchronous calling type, after the client initiates a call, it will wait for the server to return the result and then perform subsequent operations. This is the default calling type of SOFARPC.\nAsynchronous In the asynchronous calling type, after the client initiates a call, it will not wait for the result from the server but continue to execute the subsequent business logic.","tags":null,"title":"Calling type","type":"projects","url":"/en/projects/sofa-rpc/invoke-type/","wordcount":908},{"author":null,"categories":null,"content":" Client built-in extension metrics The extension modules currently in effect by default are lookout-ext-jvm and lookout-ext-os (from v1.5.0).\nJVM thread    metric name metric tags specification     jvm.threads.totalStarted  \u0026amp;mdash;   jvm.threads.active  \u0026amp;mdash;   jvm.threads.peak  \u0026amp;mdash;   jvm.threads.daemon  \u0026amp;mdash;    JVM class loading    metric name metric tags specification     jvm.classes.unloaded  \u0026amp;mdash;   jvm.classes.loaded  \u0026amp;mdash;   jvm.classes.total  \u0026amp;mdash;    JVM memory    metric name metric tags specification     jvm.memory.heap.init  \u0026amp;mdash;   jvm.memory.heap.used  \u0026amp;mdash;   jvm.memory.heap.max  \u0026amp;mdash;   jvm.memory.heap.committed  \u0026amp;mdash;    JVM garbage recycling    metric name metric tags specification     jvm.gc.young.time  \u0026amp;mdash;   jvm.gc.young.count  \u0026amp;mdash;   jvm.gc.old.time  \u0026amp;mdash;   jvm.gc.old.count  \u0026amp;mdash;    Machine file system information    metric name metric tags specification     instance.file.system.free.space root（the available filesystem roots） \u0026amp;mdash;   instance.file.system.total.space root \u0026amp;mdash;   instance.file.system.usabe.space root \u0026amp;mdash;    Machine information    metric name metric tags specification     instance.mem.free  \u0026amp;mdash;   instance.mem.total  \u0026amp;mdash;   instance.processors  \u0026amp;mdash;   instance.uptime  \u0026amp;mdash;   instance.systemload.average  \u0026amp;mdash;    Linux operating system information (enabled by default after version 1.5.0)    metric name metric tags specification     os.systemload.average.1min  \u0026amp;mdash;   os.systemload.average.5min  \u0026amp;mdash;   os.systemload.average.15min  \u0026amp;mdash;   os.cpu.idle  \u0026amp;mdash;   os.cpu.iowait  \u0026amp;mdash;   os.cpu.irq  \u0026amp;mdash;   os.cpu.nice  \u0026amp;mdash;   os.cpu.softirq  \u0026amp;mdash;   os.cpu.system  \u0026amp;mdash;   os.cpu.user  \u0026amp;mdash;   os.disk.usage.percent.used device,root,type \u0026amp;mdash;   os.disk.usage.total.bytes device,root,type \u0026amp;mdash;   os.disk.usage.used.bytes device,root,type \u0026amp;mdash;   os.net.stats.in.bytes intfc \u0026amp;mdash;   os.net.stats.in.compressed intfc \u0026amp;mdash;   os.net.stats.in.dropped intfc \u0026amp;mdash;   os.net.stats.in.errs intfc \u0026amp;mdash;   os.net.stats.in.fifo.errs intfc \u0026amp;mdash;   os.net.stats.in.frame.errs intfc \u0026amp;mdash;   os.net.stats.in.multicast intfc \u0026amp;mdash;   os.net.stats.in.packets intfc \u0026amp;mdash;   os.net.stats.out.bytes intfc \u0026amp;mdash;   os.net.stats.out.carrier.errs intfc \u0026amp;mdash;   os.net.stats.out.collisions intfc \u0026amp;mdash;   os.net.stats.out.compressed intfc \u0026amp;mdash;   os.net.stats.out.dropped intfc \u0026amp;mdash;   os.net.stats.out.errs intfc \u0026amp;mdash;   os.net.stats.out.fifo.errs intfc \u0026amp;mdash;   os.net.stats.out.packets intfc \u0026amp;mdash;   os.memory.stats.buffers.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.cached.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.free.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.total.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3    ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/client-ext-metrics/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c8a4fb3d904e359e99db9d4e81e60812","permalink":"/en/projects/sofa-lookout/client-ext-metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-lookout/client-ext-metrics/","summary":"Client built-in extension metrics The extension modules currently in effect by default are lookout-ext-jvm and lookout-ext-os (from v1.5.0).\nJVM thread    metric name metric tags specification     jvm.threads.totalStarted  \u0026mdash;   jvm.threads.active  \u0026mdash;   jvm.threads.peak  \u0026mdash;   jvm.threads.daemon  \u0026mdash;    JVM class loading    metric name metric tags specification     jvm.classes.unloaded  \u0026mdash;   jvm.","tags":null,"title":"Client built-in extension metrics","type":"projects","url":"/en/projects/sofa-lookout/client-ext-metrics/","wordcount":224},{"author":null,"categories":null,"content":"The client module is a complex module which contains cluster, router, address holder，connection holder, and load balancer, and interacts with proxy, registry center and other modules.\nSee the following flow chart:\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/client-invoke-flow/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"310d99d64b808a3b526563e92c699952","permalink":"/en/projects/sofa-rpc/client-invoke-flow/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/client-invoke-flow/","summary":"The client module is a complex module which contains cluster, router, address holder，connection holder, and load balancer, and interacts with proxy, registry center and other modules.\nSee the following flow chart:","tags":null,"title":"Client call flow","type":"projects","url":"/en/projects/sofa-rpc/client-invoke-flow/","wordcount":31},{"author":null,"categories":null,"content":" Client configuration example lookoutConfig.setProperty(LookoutConfig.LOOKOUT_AGENT_HOST_ADDRESS,\u0026amp;quot;127.0.0.1\u0026amp;quot;);  Description of server configuration item    Configuration item Corresponding SpringBoot configuration item Default value Description     lookout.enable com.alipay.sofa.lookout.enable true Function switch, it defaults to true. If you change it to false (empty objects and empty methods), then all metrics comsume almost no memory and computing resource   lookout.max.metrics.num com.alipay.sofa.lookout.max-metrics-num 5000 Maximum number limit of metrics, over which will be automatically ignored   lookout.prometheus.exporter.server.port com.alipay.sofa.lookout.prometheus-exporter-server-port 9494 The port got by Prometheus   Lookout.exporter.enable com.alipay.sofa.lookout.exporter-enable false Whether or not to enable services that support passive collection   Lookout.agent.host.address com.alipay.sofa.lookout.agent-host-address - Proactively report the annotation address of the Agent server. Multiple addresses are separated by commas    Description of client log configuration    Configuration item of system property Corresponding SpringBoot configuration item Default Value Description     -Dlogging.level.com.alipay.lookout=? logging.level.com.alipay.lookout warn The log level of Lookout client. Debug to see the details of the report data   -Dlogging.path=? logging.path Directory of the current user Modify SpringBoot V1 log directory, including \u0026amp;ldquo;lookout/\u0026amp;rdquo; log subdirectory    Custom client configuration (suitable for SpringBoot technology stack mode) Use configuration custom extensions: MetricConfigCustomizerConfig\n@Configuration public class MetricConfigCustomizerConfig { @Bean public MetricConfigCustomizer metricConfigCustomizer() { return new MetricConfigCustomizer() { @Override public void customize(MetricConfig metricConfig) { metricConfig.addProperty(\u0026amp;quot;testaa\u0026amp;quot;, \u0026amp;quot;testbb\u0026amp;quot;); } }; } }  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/client-configuration/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5fd84950d4d565d3fb20781337792bf1","permalink":"/en/projects/sofa-lookout/client-configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-lookout/client-configuration/","summary":"Client configuration example lookoutConfig.setProperty(LookoutConfig.LOOKOUT_AGENT_HOST_ADDRESS,\u0026quot;127.0.0.1\u0026quot;);  Description of server configuration item    Configuration item Corresponding SpringBoot configuration item Default value Description     lookout.enable com.alipay.sofa.lookout.enable true Function switch, it defaults to true. If you change it to false (empty objects and empty methods), then all metrics comsume almost no memory and computing resource   lookout.max.metrics.num com.alipay.sofa.lookout.max-metrics-num 5000 Maximum number limit of metrics, over which will be automatically ignored   lookout.","tags":null,"title":"Client configuration","type":"projects","url":"/en/projects/sofa-lookout/client-configuration/","wordcount":192},{"author":null,"categories":null,"content":" 1. Create a Maven project After deploying the servers, we can create a new Maven project to use services provided by SOFARegistry. Create a new Maven project, and then import the following dependency:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${registry.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2. Publish data // Create a client instance. RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); // Create a publisher registry. String dataId = \u0026amp;quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026amp;quot;; PublisherRegistration registration = new PublisherRegistration(dataId); // Register the registry with the client and publish data. registryClient.register(registration, \u0026amp;quot;10.10.1.1:12200?xx=yy\u0026amp;quot;);  Perform the following steps to publish data by using SOFARegistry:\n Create a client instance. Create a publisher registry. Register the registry with the client and publish data.  2.1 Create a client instance The key to creating a client instance is to create a RegistryClientConfig object. When creating a RegistryClientConfig object, you need to specify the RegistryEndpoint and RegistryEndpointPort.\n RegistryEndpoint: the endpoint of any session node of SOFARegistry RegistryEndpointPort: the session.server.httpServerPort port number configured for a session node  2.2 Create a publisher registry To create a publisher registry, you only need to create a PublisherRegistration object and specify the dataId, which is the unique identifier of the publisher service.\n2.3 Publish data You can call the register method of the RegistryClient to publish data. This method requires two parameters: the first is a publisher registry with the specified dataId of a service, and the second is a string type data value.\n3. Subscribe to the data // Create a client instance. RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); // Create SubscriberDataObserver. SubscriberDataObserver subscriberDataObserver = new SubscriberDataObserver() { public void handleData(String dataId, UserData userData) { System.out.println(\u0026amp;quot;receive data success, dataId: \u0026amp;quot; + dataId + \u0026amp;quot;, data: \u0026amp;quot; + userData); } }; // Create a subscriber registry and specify the subscription level. ScopeEnum covers three subscription levels: zone, dataCenter, and global. String dataId = \u0026amp;quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026amp;quot;; SubscriberRegistration registration = new SubscriberRegistration(dataId, subscriberDataObserver); registration.setScopeEnum(ScopeEnum.global); // Register the registry with …","date":-62135596800,"description":"","dir":"projects/sofa-registry/client-quick-start/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"66e300d44b2f2a903d976bf83eb7c16e","permalink":"/en/projects/sofa-registry/client-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-registry/client-quick-start/","summary":"1. Create a Maven project After deploying the servers, we can create a new Maven project to use services provided by SOFARegistry. Create a new Maven project, and then import the following dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;registry-client-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${registry.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  2. Publish data // Create a client instance. RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026quot;127.0.0.1\u0026quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); // Create a publisher registry. String dataId = \u0026quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026quot;; PublisherRegistration registration = new PublisherRegistration(dataId); // Register the registry with the client and publish data.","tags":null,"title":"Client usage","type":"projects","url":"/en/projects/sofa-registry/client-quick-start/","wordcount":558},{"author":null,"categories":null,"content":"  Project address\n Introduction During merged deployment, Biz packages can communicate with each other by releasing and referencing JVM services apart from using the RPC framework. This sample project is intended to demonstrate how two Biz packages communicate by JVM services.\nWithin the biz-jvm-invocation-sample project, there are three sub-projects whose functions are as follows: + facade: A common Java module that defines the SampleJvmService interface.\npackage me.qlong.tech.service; public interface SampleJvmService { String service(); }   app-one: A SOFABoot Web application that defines a simple rest request and use the @SofaReference annotation to reference the SampleJvmService. When a page request is triggered, an attempt is made to call the JVM service. The key code is:  package me.qlong.controller; import com.alipay.sofa.runtime.api.annotation.SofaReference; import me.qlong.tech.service.SampleJvmService; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @RestController public class HelloController { @SofaReference private SampleJvmService sampleJvmService; @RequestMapping(\u0026amp;quot;/hello\u0026amp;quot;) public String hello() { return sampleJvmService.service(); } }   app-two: A non-web application in SOFABoot that uses the @SofaService annotation to publish the SampleJvmService. ```java package me.qlong.tech.service.impl;  import com.alipay.sofa.runtime.api.annotation.SofaService; import me.qlong.tech.service.SampleJvmService; import org.springframework.stereotype.Component;\n@SofaService @Component public class AppTwoSampleService implements SampleJvmService{ public String service() { return \u0026amp;ldquo;App Two\u0026amp;rdquo;; } }\n ## Dependency To communicate between Biz packages through JVM services, you must add dependencies on the SOFARuntime package and the corresponding Ark Plugin as follows: ```xml \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  For detailed information about publishing and referencing JVM services, see the SOFABoot Documentation. You are advised to use annotations in Jarslink2.0.\nDemo  cd biz-jvm-invocation-sample/facade \u0026amp;amp;\u0026amp;amp; mvn clean install Execute the mvn clean install command in the facade root directory, and install the facade package in the local Maven repository so that you can add a facade dependency in app-one and app-two:  \u0026amp;lt;!--service facade--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;me.qlong.tech\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;facade\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   cd biz-jvm-invocation-sample/app-one \u0026amp;amp;\u0026amp;amp; mvn clean package Execute the mvn clean package command in the app-one root directory and …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-invocation-demo/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"59778c5223dc6267bd537be6c79b658a","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-invocation-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-invocation-demo/","summary":"Project address\n Introduction During merged deployment, Biz packages can communicate with each other by releasing and referencing JVM services apart from using the RPC framework. This sample project is intended to demonstrate how two Biz packages communicate by JVM services.\nWithin the biz-jvm-invocation-sample project, there are three sub-projects whose functions are as follows: + facade: A common Java module that defines the SampleJvmService interface.\npackage me.qlong.tech.service; public interface SampleJvmService { String service(); }   app-one: A SOFABoot Web application that defines a simple rest request and use the @SofaReference annotation to reference the SampleJvmService.","tags":null,"title":"Communicate across applications","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-invocation-demo/","wordcount":527},{"author":null,"categories":null,"content":"SOFARPC supports different communication protocols and currently supports Bolt, RESTful and Dubbo. For details, please refer to the corresponding document of each protocol: * Bolt Protocol * Basic usage * Calling type * Timeout control * Generic call * Serialization protocol * Custom thread pool * RESTful * Basic usage * Custom filter * Integrate Swagger * Dubbo * Basic usage * H2C * Basic usage\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/protocol/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"18f51cb12f7a0384a71ab22349292a08","permalink":"/en/projects/sofa-rpc/protocol/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/protocol/","summary":"SOFARPC supports different communication protocols and currently supports Bolt, RESTful and Dubbo. For details, please refer to the corresponding document of each protocol: * Bolt Protocol * Basic usage * Calling type * Timeout control * Generic call * Serialization protocol * Custom thread pool * RESTful * Basic usage * Custom filter * Integrate Swagger * Dubbo * Basic usage * H2C * Basic usage","tags":null,"title":"Communication protocols","type":"projects","url":"/en/projects/sofa-rpc/protocol/","wordcount":66},{"author":null,"categories":null,"content":" How to compile  Install JDK7 and above, and Maven 3.2.5 and above.\n Directly download the code and then execute the following command:\ncd sofa-jarslink mvn clean install  Note: you cannot compile the code under a sub-directory (i.e., sub-module). Since there are many modules, the configuration is restricted to the root directory only to avoid repetitive configuration of some packaging plugins such as the formatting plugin and License plugin. There will be an error message if you execute the packaging command in a sub-module.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-compile/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"23c8cae6050d7a772f45d3ff2b4ce889","permalink":"/en/projects/sofa-boot/sofa-jarslink-compile/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-compile/","summary":"How to compile  Install JDK7 and above, and Maven 3.2.5 and above.\n Directly download the code and then execute the following command:\ncd sofa-jarslink mvn clean install  Note: you cannot compile the code under a sub-directory (i.e., sub-module). Since there are many modules, the configuration is restricted to the root directory only to avoid repetitive configuration of some packaging plugins such as the formatting plugin and License plugin.","tags":null,"title":"Compile Jarslink project","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-compile/","wordcount":83},{"author":null,"categories":null,"content":" Install JDK7 or later and Maven 3.2.5 or later.\n Download the codes directly and execute the following commands:\ncd sofa-rpc mvn clean install  Note: You can not build under a subdirectory (namely the submodule). Because there are too many SOFARPC modules, if every submodule needs to be installed and deployed, there will be much useless records in the repository. This issue is considered in the process of designing the SOFARPC project structure. The current structure saves you the trouble of installing and deploying all submodules, and you just have to install and deploy one module, namely the sofa-rpc-all (all) module.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/how-to-build/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"52ad3debb35be8743c97bb4b6b77f22b","permalink":"/en/projects/sofa-rpc/how-to-build/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/how-to-build/","summary":"Install JDK7 or later and Maven 3.2.5 or later.\n Download the codes directly and execute the following commands:\ncd sofa-rpc mvn clean install  Note: You can not build under a subdirectory (namely the submodule). Because there are too many SOFARPC modules, if every submodule needs to be installed and deployed, there will be much useless records in the repository. This issue is considered in the process of designing the SOFARPC project structure.","tags":null,"title":"Compile SOFARPC project","type":"projects","url":"/en/projects/sofa-rpc/how-to-build/","wordcount":100},{"author":null,"categories":null,"content":"Provide all the parameters that can be configured. * Service publishing and reference configuration * Warm-up forwarding configuration * Fault tolerance configuration\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b1a8a8c426beab292165716f1dff1ae4","permalink":"/en/projects/sofa-rpc/configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/configuration/","summary":"Provide all the parameters that can be configured. * Service publishing and reference configuration * Warm-up forwarding configuration * Fault tolerance configuration","tags":null,"title":"Configuration parameters","type":"projects","url":"/en/projects/sofa-rpc/configuration/","wordcount":22},{"author":null,"categories":null,"content":"To use Consul as service registry center, you need to add this dependency\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.ecwid.consul\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;consul-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  and need to configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500  The value after consul: is the connection address of the consul. If you need to set some other parameters, you can also configure as follows:\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500?a=1\u0026amp;amp;b=2  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-consul/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e6b0aa843ea0ad401c3184f6ce87649b","permalink":"/en/projects/sofa-rpc/registry-consul/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-consul/","summary":"To use Consul as service registry center, you need to add this dependency\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.ecwid.consul\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;consul-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  and need to configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500  The value after consul: is the connection address of the consul. If you need to set some other parameters, you can also configure as follows:\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500?a=1\u0026amp;b=2  ","tags":null,"title":"Consul","type":"projects","url":"/en/projects/sofa-rpc/registry-consul/","wordcount":54},{"author":null,"categories":null,"content":"  You can visit Development Route first to learn more about development tasks and future planning.\n Preparations Before contributing any code, we need to know how to use the Git tool and the GitHub website.\n Refer to the Git official books for the Git tool usage. The first few chapters will help you get a quick start. Read Git collaboration process through  GitHub Code Contribution Process Submitting an issue Whether you want to fix a bug of SOFAArk or add a new feature of SOFAArk, you have to submit an issue to describe your demand before you submit the code on GitHub address in SOFAArk. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project to result in repetitive work. The maintenance personnel of SOFAArk will discuss about the bug or new function you submitted, to determine if the modification is necessary, or if there is any room for improvement or any better solution. Start developing and submitting code after agreement to reduce the cost of communication between both parties as well as the number of rejected pull requests.  Getting the source code To modify or add a function, click the fork button in the upper left corner to copy a SOFAArk trunk code to your code repository, after submitting an issue.\nPulling a branch Perform all the SOFAArk modifications on the branch, and submit a pull request after the modifications, which will be merged to the trunk by the project maintenance personnel after Code Review.\nTherefore, after getting the introduction to source code steps, you need to:\n Download the code locally. You may select the git/https mode in this step.  git clone https://github.com/{your account}/sofa-ark.git   Pull a branch to prepare for code modification.  git branch add_xxx_feature  After the preceding command is executed, your code repository will switch to the corresponding branch. To view the current branch, execute the following command:\ngit branch -a  If you want to switch back to the trunk, execute the following command:\ngit checkout -b master  If you want to switch back to the branch, execute the following command:\ngit checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally. After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent. SOFAArk uses the Maven plug-in to keep the code style consistent. Before submitting the code, execute the following commands locally:   mvn clean compile   Supplement unit test code. New modifications should have passed existing unit tests. You should provide the new unit test to prove that the previous code has bugs and the new code has fixed such bugs. Execute the following command to run all tests:\nmvn clean test  Other do\u0026amp;rsquo;s and don\u0026amp;rsquo;ts  Retain the original style of the code you are editing, especially the spaces and line feeds in the code. Delete useless annotations. Annotate the places where the logic and functionality …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-contribution/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dbf77f98884a71c5c7a3fbb4dd189cfe","permalink":"/en/projects/sofa-boot/sofa-ark-contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-ark-contribution/","summary":"You can visit Development Route first to learn more about development tasks and future planning.\n Preparations Before contributing any code, we need to know how to use the Git tool and the GitHub website.\n Refer to the Git official books for the Git tool usage. The first few chapters will help you get a quick start. Read Git collaboration process through  GitHub Code Contribution Process Submitting an issue Whether you want to fix a bug of SOFAArk or add a new feature of SOFAArk, you have to submit an issue to describe your demand before you submit the code on GitHub address in SOFAArk.","tags":null,"title":"Contribution","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-contribution/","wordcount":795},{"author":null,"categories":null,"content":"  You can go into the development route to learn more about development tasks and future planning.\n Preparations Before contributing any code, we need to know how to use the Git tool and the GitHub website.\n For the use of git tools, refer to official books on git and get familiarized by reading the first few chapters. For the git collaboration process, refer to the article named Git Collaboration Process.  GitHub Code Contribution Process Submitting an issue No Matter whether you are fixing a Jarslink bug or adding a Jarslink feature, submit an issue on the Jarslink GitHub address to describe the bug you are going to fix or the feature you intend to add before you submit the code. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project to result in repetitive work. The Jarslink maintenance personnel will discuss the bug or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start developing and submitting code after agreement to reduce the cost of communication between both parties as well as the number of rejected pull requests.  Getting the source code To modify or add a feature, click the fork button in the upper left corner to copy Jarslink trunk code to your code repository, after submitting an issue.\nPulling a branch Perform all the Jarslink modifications on the branch, and submit a pull request after the modifications, which will be merged into the trunk by the project maintenance personnel after the code review.\nTherefore, after getting the introduction to source code steps, you need to:\n Download the code locally. You may select the git/https mode in this step.\ngit clone https://github.com/your account name/sofa-jarslink.git  Pull a branch to prepare for code modification.\ngit branch add_xxx_feature   After the preceding command is executed, your code repository will switch to the corresponding branch. To view the current branch, execute the following command:\n git branch -a  If you want to switch back to the trunk, execute the following command:\n git checkout -b master  If you want to switch back to the branch, execute the following command:\n git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally. After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent.  Jarslink uses the Maven plugin to keep the code style consistent. Before submitting the code, be sure to execute the following commands locally\nmvn clean compile   Supplement unit test code. New modifications should have passed existing unit tests. You should provide the new unit test to prove that the previous code has bugs and the new code has fixed such bugs.  Execute the following command to run all tests:\n mvn clean test  You can also use the IDE to help execute a command.\nOther do\u0026amp;rsquo;s and don\u0026amp;rsquo;ts  Retain the original …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-contribution/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d4c1185c6a691679f6dc9dba033550ce","permalink":"/en/projects/sofa-boot/sofa-jarslink-contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-contribution/","summary":"You can go into the development route to learn more about development tasks and future planning.\n Preparations Before contributing any code, we need to know how to use the Git tool and the GitHub website.\n For the use of git tools, refer to official books on git and get familiarized by reading the first few chapters. For the git collaboration process, refer to the article named Git Collaboration Process.","tags":null,"title":"Contribution","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-contribution/","wordcount":827},{"author":null,"categories":null,"content":"  Before you read this document, you are suggested to read the SOFARPC development roadmap to learn about development tasks and future plans.\n Preparations Before you contribute code, you need to learn the basic use of Git tool and GitHub website.\n For how to use the Git tool, see Git official documenation and pay attention to the first few chapters. For Git collaboration process, see Git collaboration process.  GitHub code contribution process Submit an issue No matter to fix SOFARPC bugs or add SOFARPC features, before submitting the codes, you must submit an issue on SOFARPC\u0026amp;rsquo;s GitHub to describe the bugs to be fixed and the functions you want to add.\nThere are several benefits of doing this:\n Avoid the conflict with other developers or their plans for this project, thus eliminating repetitive work. SOFARPC operations staff discuss your bugs or new features to determine if the changes are necessary and whether there is space for improvement or a better approach. Reduce communication cost between you and the SOFARPC operations staff, thus reducing the cases that pull request is rejected.  Get source codes To modify or add features, after you submit the issue, you can click Fork in the upper left corner to copy the SOFARPC trunk code to your code repository.\nPull branches All SOFARPC modifications are made on the branch. After modification, submit the pull request. After the code review, the project operations staff merge the branches to the trunk.\nTherefore, you must complete the following steps after getting the source codes:\n Download the codes locally through Git or HTTPs.\n git clone https://github.com/your account name/sofa-rpc.git  Pull branches for code modifications.\n  \t git branch add_xxx_feature \nAfter executing the above command, your code repository switches to the corresponding branch. Execute the following command to see your current branch:\n\t git branch -a \nIf you want to switch back to the trunk, execute the following command:\n git checkout -b master \nIf you want to switch back to the branch, execute the following command:\n git checkout -b \u0026amp;quot;branchName\u0026amp;quot; \nModify and submit codes locally Once the branch is pulled, you can modify the code.\nAttentions for modifying codes  Keep a consistent code style.\nSOFARPC keeps the code format consistent through the Maven plugin. You must execute the following command locally before committing the code:   mvn clean compile   Supplemental unit test code. New modifications should have passed the existing unit tests. Provide new unit tests to prove that the previous code has bugs, and the new code has fixed these bugs.\nYou can run all tests with the following command:\nmvn clean test  You can also use IDE to assist the test running.\n  Other attentions  Keep the original code style, especially the spacing and alignment. Delete the useless comments directly. Add comments for the logics and functions that cannot be easily understood. Update documentation timely.  After modification, …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/contributing/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"448a7b9a949bd2d9e2e71ac6c237f9df","permalink":"/en/projects/sofa-rpc/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-rpc/contributing/","summary":"Before you read this document, you are suggested to read the SOFARPC development roadmap to learn about development tasks and future plans.\n Preparations Before you contribute code, you need to learn the basic use of Git tool and GitHub website.\n For how to use the Git tool, see Git official documenation and pay attention to the first few chapters. For Git collaboration process, see Git collaboration process.","tags":null,"title":"Contribution","type":"projects","url":"/en/projects/sofa-rpc/contributing/","wordcount":751},{"author":null,"categories":null,"content":"SOFARPC uses some third-party open-source components which include but not limited to:\n Major dependencies\n Netty under Apache License 2.0 SLF4j under MIT License SOFA Bolt under Apache License 2.0 Javassist under Apache License 2.0 Resteasy under Apache License 2.0 SOFA Hessian under Apache License 2.0  Extended dependencies\n protobuf under New BSD License Snappy under New BSD License dubbo under Apache License 2.0   ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/notice/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b6c87388d5c1462f13d92012639a08b2","permalink":"/en/projects/sofa-rpc/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/notice/","summary":"SOFARPC uses some third-party open-source components which include but not limited to:\n Major dependencies\n Netty under Apache License 2.0 SLF4j under MIT License SOFA Bolt under Apache License 2.0 Javassist under Apache License 2.0 Resteasy under Apache License 2.0 SOFA Hessian under Apache License 2.0  Extended dependencies\n protobuf under New BSD License Snappy under New BSD License dubbo under Apache License 2.0   ","tags":null,"title":"Copyright","type":"projects","url":"/en/projects/sofa-rpc/notice/","wordcount":62},{"author":null,"categories":null,"content":" Copyright statement of dependent components SOFADashboard uses some third-party open-source components, including but not limited to:\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License SOFABolt under Apache License 2.0 SOFABolt under Apache License 2.0 Curator under Apache License 2.0  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/notice/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a9ebe38d245302f94ab7bfa793329926","permalink":"/en/projects/sofa-dashboard/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/notice/","summary":" Copyright statement of dependent components SOFADashboard uses some third-party open-source components, including but not limited to:\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License SOFABolt under Apache License 2.0 SOFABolt under Apache License 2.0 Curator under Apache License 2.0  ","tags":null,"title":"Copyright statement","type":"projects","url":"/en/projects/sofa-dashboard/notice/","wordcount":47},{"author":null,"categories":null,"content":" Copyright statement of dependent components SOFARegistry uses some third-party open-source components, including but not limited to:\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license Netty under Apache License 2.0 SLF4j under the MIT License jersey under CDDL Version 1.1\n SOFAJRaft under Apache License 2.0 SOFABolt under Apache License 2.0 SOFAHessian under Apache License 2.0  If you find anything we have missed, please let us know.\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/notice/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c40263ffd56a2f1292756c9fafea55e2","permalink":"/en/projects/sofa-registry/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-registry/notice/","summary":"Copyright statement of dependent components SOFARegistry uses some third-party open-source components, including but not limited to:\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license Netty under Apache License 2.0 SLF4j under the MIT License jersey under CDDL Version 1.1\n SOFAJRaft under Apache License 2.0 SOFABolt under Apache License 2.0 SOFAHessian under Apache License 2.0  If you find anything we have missed, please let us know.","tags":null,"title":"Copyright statement","type":"projects","url":"/en/projects/sofa-registry/notice/","wordcount":68},{"author":null,"categories":null,"content":" 本文档主要介绍一个基于 jraft 的分布式计数器的例子。\n场景 在多个节点（机器）组成的一个 raft group 中保存一个分布式计数器，该计数器可以递增和获取，并且在所有节点之间保持一致，任何少数节点的挂掉都不会影响对外提供的两个服务：\n incrmentAndGet(delta) 递增 delta 数值并返回递增后的值。 get() 获取最新的值  RPC 请求 jraft 底层使用 bolt 作为通讯框架，定义两个请求\n IncrementAndGetRequest，用于递增  public class IncrementAndGetRequest implements Serializable { private static final long serialVersionUID = -5623664785560971849L; private long delta; public long getDelta() { return this.delta; } public void setDelta(long delta) { this.delta = delta; } }   GetValueRequest，用于获取最新值：  public class GetValueRequest implements Serializable { private static final long serialVersionUID = 9218253805003988802L; public GetValueRequest() { super(); } }  应答结果 ValueResponse，包括：\n success　是否成功 value 成功情况下返回的最新值 errorMsg 失败情况下的错误信息 redirect　发生了重新选举，需要跳转的新的leader节点。  public class ValueResponse implements Serializable { private static final long serialVersionUID = -4220017686727146773L; private long value; private boolean success; /** * redirect peer id */ private String redirect; private String errorMsg; public String getErrorMsg() { return this.errorMsg; } public void setErrorMsg(String errorMsg) { this.errorMsg = errorMsg; } ...... }   IncrementAndGetRequest 用于 Leader 服务端接收 IncrementAndAddClosure 请求后的回调处理：  public class IncrementAndAddClosure implements Closure { private CounterServer counterServer; private IncrementAndGetRequest request; private ValueResponse response; private Closure done; // 网络应答callback public IncrementAndAddClosure(CounterServer counterServer, IncrementAndGetRequest request, ValueResponse response, Closure done) { super(); this.counterServer = counterServer; this.request = request; this.response = response; this.done = done; } @Override public void run(Status status) { // 返回应答给客户端 if (this.done != null) { done.run(status); } } public IncrementAndGetRequest getRequest() { return this.request; } public void setRequest(IncrementAndGetRequest request) { this.request = request; } public ValueResponse getResponse() { return this.response; } }  服务端 状态机 CounterStateMachine 首先持有一个初始值：\npublic class CounterStateMachine extends StateMachineAdapter { /** * counter value */ private AtomicLong value = new AtomicLong(0);  实现核心的 onApply(iterator) 方法，应用用户提交的请求到状态机：\n@Override public void onApply(Iterator iter) { // 遍历日志 while (iter.hasNext()) { long delta = 0; IncrementAndAddClosure closure = null; // done 回调不为null，必须在应用日志后调用，如果不为 null，说明当前是leader。 if (iter.done() != null) { // 当前是leader，可以直接从 IncrementAndAddClosure 中获取 delta，避免反序列化 closure = (IncrementAndAddClosure) iter.done(); delta = closure.getRequest().getDelta(); } else { // 其他节点应用此日志，需要反序列化 IncrementAndGetRequest，获取 delta ByteBuffer data = iter.getData(); try { IncrementAndGetRequest request = Codecs.getSerializer(Codecs.Hessian2).decode(data.array(), IncrementAndGetRequest.class.getName()); delta = request.getDelta(); } catch (CodecException e) { LOG.error(\u0026amp;quot;Fail to decode IncrementAndGetRequest\u0026amp;quot;, e); } } long prev = this.value.get(); // 更新状态机 long …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/counter-example/","fuzzywordcount":2500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f9c54b9f7883ccb1d7c259b7101f4674","permalink":"/projects/sofa-jraft/counter-example/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/projects/sofa-jraft/counter-example/","summary":"本文档主要介绍一个基于 jraft 的分布式计数器的例子。 场景 在多个节点（机器）组成的一个 raft group 中保存一个分布式计数器，该计数器可以递增和获取，并且在所有","tags":null,"title":"Counter 例子详解","type":"projects","url":"/projects/sofa-jraft/counter-example/","wordcount":2404},{"author":null,"categories":null,"content":"  Project address\n Introduction Jarslink 2.0 is available for both Spring Boot and SOFABoot; we just need to add the specified dependencies. To be convenient, it is recommended to use Jarslink 2.0 in the form of SOFABoot projects. This sample project is intended to demonstrate how to quickly reform a Spring Boot project into a SOFABoot project.\nReform After creating a Spring Boot project in the official Spring Boot website, we only need to introduce the SOFABoot dependencies. First, modify the configuration file pom.xml of the Maven project.\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Replace as\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.5.0-SNAPSHOT\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Note: Currently Jarslink 2.0.0 is still at its snapshot version, and the SOFABoot 2.5.0 that it depends on will be released in the near future, so for the moment, the SOFABoot 2.5.0-SNAPSHOT version has to be introduced as the dependency. The pull of the SNAPSHOT package requires special configuration, for which you can refer to FAQ: How do I configure for pulling a SNAPSHOT dependency package?\nThen, add a Spring Boot or SOFABoot official Starter, such as:\n\u0026amp;lt;dependencies\u0026amp;gt; \u0026amp;lt;!-- Jarslink2.0 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-jarslink-ark-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.0.0-SNAPSHOT\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- Web --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-web\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;/dependencies\u0026amp;gt;  To package the application into an Ark package or Biz package, we need to configure the sofa-Ark-maven-plugin packaging plugin in the main pom.xml file, and delete the native packaging plugin the Spring Boot configuration.\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--specify destination where executable-ark-jar will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  Finally, add a parameter that SOFABoot must use under the application.properties file for the project, including spring.application.name (used to mark the …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-app-demo/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"46cb9153d039f01a375a569c2a9a5535","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-app-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-app-demo/","summary":"Project address\n Introduction Jarslink 2.0 is available for both Spring Boot and SOFABoot; we just need to add the specified dependencies. To be convenient, it is recommended to use Jarslink 2.0 in the form of SOFABoot projects. This sample project is intended to demonstrate how to quickly reform a Spring Boot project into a SOFABoot project.\nReform After creating a Spring Boot project in the official Spring Boot website, we only need to introduce the SOFABoot dependencies.","tags":null,"title":"Create a SOFABoot application","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-app-demo/","wordcount":419},{"author":null,"categories":null,"content":" SOFARPC provides a good extensibility mechanism, which provide SPI capabilities for each module. SOFARPC uses multiple filters to intercept requests and responses. These filters can be customized and extended by users. The execution order of custom filters is after the built-in filters. The procedure is as follows:\nBolt filter 1 Create a new custom filter.\npublic class CustomFilter extends Filter { @Override public boolean needToLoad(FilterInvoker invoker) { return true; } @Override public SofaResponse invoke(FilterInvoker invoker, SofaRequest request) throws SofaRpcException { SofaResponse response = invoker.invoke(request); return response; } }  2 The custom filter will be added into the interceptor chain. There are three specific ways to do this step.\n Method 1: In API. In this way, the custom filter can take effect in the specified provider or consumer.  // Service provider providerConfig.setFilterRef(Arrays.asList(new CustomFilter())); // Service caller consumerConfig.setFilterRef(Arrays.asList(new CustomFilter()));   Method 2: Add @Extension annotation + configuration extension file to the class.   @Extension(\u0026amp;quot;customer\u0026amp;quot;) public class CustomFilter extends Filter { @Override public boolean needToLoad(FilterInvoker invoker) { return true; } @Override public SofaResponse invoke(FilterInvoker invoker, SofaRequest request) throws SofaRpcException { SofaResponse response = invoker.invoke(request); return response; } }  Create a new extension file META-INF/services/sofa-rpc/com.alipay.sofa.rpc.filter.Filter with the following content:\ncustomer=com.alipay.sofa.rpc.custom.CustomFilter  Code injection.\n// Service provider providerConfig.setFilter(Arrays.asList(\u0026amp;quot;customer\u0026amp;quot;)); // Service caller consumerConfig.setFilter(Arrays.asList(\u0026amp;quot;customer\u0026amp;quot;));   Method 3: Add @Extension annotation + @AutoActive annotation + configuration extension file to the class. In this way, the code injection step in method 2 is replaced with the @AutoActive annotation. The custom filter can take effect in all providers or consumers. The providerSide parameter indicates whether it takes effect on the server, and the consumerSide parameter indicates whether it takes effect on the client.  @Extension(\u0026amp;quot;customer\u0026amp;quot;) @AutoActive(providerSide = true, consumerSide = true) public class customerFilter extends Filter { // ... }  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-filter/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"30ff5937b52a7c2dd8028e878979a33d","permalink":"/en/projects/sofa-rpc/custom-filter/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/custom-filter/","summary":"SOFARPC provides a good extensibility mechanism, which provide SPI capabilities for each module. SOFARPC uses multiple filters to intercept requests and responses. These filters can be customized and extended by users. The execution order of custom filters is after the built-in filters. The procedure is as follows:\nBolt filter 1 Create a new custom filter.\npublic class CustomFilter extends Filter { @Override public boolean needToLoad(FilterInvoker invoker) { return true; } @Override public SofaResponse invoke(FilterInvoker invoker, SofaRequest request) throws SofaRpcException { SofaResponse response = invoker.","tags":null,"title":"Custom filter","type":"projects","url":"/en/projects/sofa-rpc/custom-filter/","wordcount":285},{"author":null,"categories":null,"content":"The route service address in SOFARPC is abstracted into a processing chain, and is processed by each router. Like filter, SOFARPC provides the same extensibility for router.\n@Extension(value = \u0026amp;quot;customerRouter\u0026amp;quot;) @AutoActive(consumerSide = true) public class CustomerRouter extends Router { @Override public void init(ConsumerBootstrap consumerBootstrap) { } @Override public boolean needToLoad(ConsumerBootstrap consumerBootstrap) { return true; } @Override public List\u0026amp;lt;ProviderInfo\u0026amp;gt; route(SofaRequest request, List\u0026amp;lt;ProviderInfo\u0026amp;gt; providerInfos) { return providerInfos; }  Create a extension file META-INF/services/sofa-rpc/com.alipay.sofa.rpc.client.Router with the following content:\ncustomerRouter=com.alipay.sofa.rpc.custom.CustomRouter  This file customized a CustomerRouter, which takes effect in all consumers. The parameter ConsumerBootstrap in init method is a wrapper class of the referenced service, and can get objects such as ConsumerConfig, proxy class, and service address pool. needToLoad indicates whether the Router is valid, and the route method is the method for filtering addresses.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-router/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"236e8d4bda3e856267a3575853aa900c","permalink":"/en/projects/sofa-rpc/custom-router/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/custom-router/","summary":"The route service address in SOFARPC is abstracted into a processing chain, and is processed by each router. Like filter, SOFARPC provides the same extensibility for router.\n@Extension(value = \u0026quot;customerRouter\u0026quot;) @AutoActive(consumerSide = true) public class CustomerRouter extends Router { @Override public void init(ConsumerBootstrap consumerBootstrap) { } @Override public boolean needToLoad(ConsumerBootstrap consumerBootstrap) { return true; } @Override public List\u0026lt;ProviderInfo\u0026gt; route(SofaRequest request, List\u0026lt;ProviderInfo\u0026gt; providerInfos) { return providerInfos; }  Create a extension file META-INF/services/sofa-rpc/com.","tags":null,"title":"Custom router","type":"projects","url":"/en/projects/sofa-rpc/custom-router/","wordcount":131},{"author":null,"categories":null,"content":" SOFARPC supports custom business thread pools. A separate business thread pool can be set up for the specified service, isolated from SOFARPC\u0026amp;rsquo;s global business thread pool. Multiple services can share a single thread pool.\nSOFARPC requires that the type of custom thread pool must be com.alipay.sofa.rpc.server.UserThreadPool.\nUse XML If you publish the service using XML, you can first set the bean of the thread pool whose class is com.alipay.sofa.rpc.server.UserThreadPool, and then set the bean in the thread-pool-ref attribute of \u0026amp;lt;sofa:global-attrs\u0026amp;gt; tag.\n\u0026amp;lt;bean id=\u0026amp;quot;helloService\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.quickstart.HelloService\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Customize a thread pool --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;customExecutor\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.server.UserThreadPool\u0026amp;quot; init-method=\u0026amp;quot;init\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;corePoolSize\u0026amp;quot; value=\u0026amp;quot;10\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;maximumPoolSize\u0026amp;quot; value=\u0026amp;quot;10\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;queueSize\u0026amp;quot; value=\u0026amp;quot;0\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;helloService\u0026amp;quot; interface=\u0026amp;quot;XXXService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;!-- Set the thread pool to a Service --\u0026amp;gt; \u0026amp;lt;sofa:global-attrs thread-pool-ref=\u0026amp;quot;customExecutor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Use Annotation If you publish the service using Annotation, you can set the bean of the custom thread pool by setting the userThreadPool attribute of @SofaServiceBinding:\n@SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, userThreadPool = \u0026amp;quot;customThreadPool\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  Use API in Spring environment If you publish a service using the API in Spring environment, you can configure a custom thread pool by calling the setUserThreadPool method of BoltBindingParam:\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setUserThreadPool(new UserThreadPool());  Use API in non-Spring environment If you publish service using the API in non-Spring environment, you can set a custom thread pool as follows:\nUserThreadPool threadPool = new UserThreadPool(); threadPool.setCorePoolSize(10); threadPool.setMaximumPoolSize(100); threadPool.setKeepAliveTime(200); threadPool.setPrestartAllCoreThreads(false); threadPool.setAllowCoreThreadTimeOut(false); threadPool.setQueueSize(200); UserThreadPoolManager.registerUserThread(ConfigUniqueNameGenerator.getUniqueName(providerConfig), threadPool);  As above, a custom thread pool is set up for the HelloService service.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-thread-pool/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4d582a7894b2381248522f3a1fc400c9","permalink":"/en/projects/sofa-rpc/custom-thread-pool/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/custom-thread-pool/","summary":"SOFARPC supports custom business thread pools. A separate business thread pool can be set up for the specified service, isolated from SOFARPC\u0026rsquo;s global business thread pool. Multiple services can share a single thread pool.\nSOFARPC requires that the type of custom thread pool must be com.alipay.sofa.rpc.server.UserThreadPool.\nUse XML If you publish the service using XML, you can first set the bean of the thread pool whose class is com.alipay.sofa.rpc.server.UserThreadPool, and then set the bean in the thread-pool-ref attribute of \u0026lt;sofa:global-attrs\u0026gt; tag.","tags":null,"title":"Custom thread pool","type":"projects","url":"/en/projects/sofa-rpc/custom-thread-pool/","wordcount":252},{"author":null,"categories":null,"content":" You can view basic information of your application on SOFADashboard, including the IP address, ports, and health check status. This feature is dependent on the SOFADashboard client. If you want to display the information about an application on the SOFADashboard control page, import the sofa-dashboard-client dependency.\n\u0026amp;lt;denpendency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-dashboard-client\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/denpendency\u0026amp;gt;  Function display ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/dashboard-client/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"60586c6dfee1f2afcdac88cbe7a36b83","permalink":"/en/projects/sofa-dashboard/dashboard-client/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/dashboard-client/","summary":" You can view basic information of your application on SOFADashboard, including the IP address, ports, and health check status. This feature is dependent on the SOFADashboard client. If you want to display the information about an application on the SOFADashboard control page, import the sofa-dashboard-client dependency.\n\u0026lt;denpendency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sofa-dashboard-client\u0026lt;/artifactId\u0026gt; \u0026lt;/denpendency\u0026gt;  Function display ","tags":null,"title":"Dashboard client","type":"projects","url":"/en/projects/sofa-dashboard/dashboard-client/","wordcount":52},{"author":null,"categories":null,"content":" In this document will demonstrate how to use SOFATracer to track of Datasource.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce SOFATracer Introduce SOFATracer dependency in the new Spring Boot project:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.2.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce h2database dependencies For convenience, we use the h2database memory database for test. So, we need to introduce the following dependencies:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.h2database\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;h2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;scope\u0026amp;gt;runtime\u0026amp;lt;/scope\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;mysql\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;mysql-connector-java\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce the required connection pool dependencies Introduce the required connection pool dependency packages, such as druid, c3p0, tomcat, dbcp, Hikari, and so on.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;druid\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.0.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;c3p0\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;c3p0\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;0.9.1.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.tomcat\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tomcat-jdbc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;8.5.31\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-dbcp\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-dbcp\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.zaxxer\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;HikariCP-java6\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.3.8\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Configure data source Taking HikariCP as the example, we create a new Spring configuration file named datasource.xml, which defines the followings:\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;!-- dataSource pool --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;simpleDataSource\u0026amp;quot; class=\u0026amp;quot;com.zaxxer.hikari.HikariDataSource\u0026amp;quot; destroy-method=\u0026amp;quot;close\u0026amp;quot; primary=\u0026amp;quot;true\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;driverClassName\u0026amp;quot; value=\u0026amp;quot;org.h2.Driver\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;jdbcUrl\u0026amp;quot; value=\u0026amp;quot;jdbc:h2:~/test\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;username\u0026amp;quot; value=\u0026amp;quot;sofa\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;password\u0026amp;quot; value=\u0026amp;quot;123456\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt;  Application configuration  Required configuration  It should be noted that it is …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-datasource/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d9d8f6756a294104647067eaa7827f61","permalink":"/en/projects/sofa-tracer/usage-of-datasource/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-tracer/usage-of-datasource/","summary":"In this document will demonstrate how to use SOFATracer to track of Datasource.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce SOFATracer Introduce SOFATracer dependency in the new Spring Boot project:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Introduce h2database dependencies For convenience, we use the h2database memory database for test. So, we need to introduce the following dependencies:","tags":null,"title":"DataSource Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-datasource/","wordcount":642},{"author":null,"categories":null,"content":" Datasource Log Format SOFATracer tracks the standard JDBC data source and outputs the chain data of SQL statement execution, in the default JSON format.\nDataSource digest log (datasource-client-digest.log) The data is output in JSON format. Each key meaning is as follows:\n   Key Meaning     Time log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Database.name Database name   Sql SQL execution statement   Result.code SQL execution status code   Total.time SQL statement execution total time   Connection.establish.span SQL execution connection establishment time   Db.execute.cost SQL execution time   Database.type Database type   Database.endpoint Database url   Current.thread.name Current thread name   Baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-09-28 01:11:56.715\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerDataSource\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;1e1bcab91538068316462100111113\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1.2\u0026amp;quot;,\u0026amp;quot;database.name\u0026amp;quot;:\u0026amp;quot;test\u0026amp;quot;,\u0026amp;quot;sql\u0026amp;quot;:\u0026amp;quot;CREATE TABLE TEST(ID INT PRIMARY KEY%2C NAME VARCHAR(255));\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;success\u0026amp;quot;,\u0026amp;quot;total.time\u0026amp;quot;:\u0026amp;quot;228ms\u0026amp;quot;,\u0026amp;quot;connection.establish.span\u0026amp;quot;:\u0026amp;quot;220ms\u0026amp;quot;,\u0026amp;quot;db.execute.cost\u0026amp;quot;:\u0026amp;quot;3ms\u0026amp;quot;,\u0026amp;quot;database.type\u0026amp;quot;:\u0026amp;quot;h2\u0026amp;quot;,\u0026amp;quot;database.endpoint\u0026amp;quot;:\u0026amp;quot;jdbc:h2:~/test:-1\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-1\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  DataSource statistical Log (datasource-client-stat.log) stat.key is the set of statistical keywords in this period, which uniquely determines a set of statistical data, including local.app, database.name, and SQL field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   database.name Database name   sql SQL execution statement   count SQL execution count in this period   total.cost.milliseconds Total duration (ms) for SQL execution in this period   success Request result: Y for success; N for failure   load.test Pressure mark: T for pressure test; F for non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-09-28 01:12:43.647\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerDataSource\u0026amp;quot;,\u0026amp;quot;database.name\u0026amp;quot;:\u0026amp;quot;test\u0026amp;quot;, \u0026amp;quot;sql\u0026amp;quot;:\u0026amp;quot;CREATE TABLE TEST(ID INT PRIMARY KEY%2C NAME VARCHAR(255));\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:228,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-datasource/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"647de768aff8ececc8276d247c5afee1","permalink":"/en/projects/sofa-tracer/log-format-datasource/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/log-format-datasource/","summary":"Datasource Log Format SOFATracer tracks the standard JDBC data source and outputs the chain data of SQL statement execution, in the default JSON format.\nDataSource digest log (datasource-client-digest.log) The data is output in JSON format. Each key meaning is as follows:\n   Key Meaning     Time log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Database.","tags":null,"title":"DataSource log","type":"projects","url":"/en/projects/sofa-tracer/log-format-datasource/","wordcount":202},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 DataSource 进行埋点。\n SOFATracer 2.2.0 基于标准的 JDBC 接口实现，支持对标准的数据库连接池（如 DBCP、Druid、c3p0、tomcat、HikariCP、BoneCP）埋点。下面演示如何接入 SOFATracer 埋点能力。\n 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n依赖引入 引入 SOFATracer 依赖 \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入 h2database 依赖 为了方便，我们使用 h2database 内存数据库测试，引入如下依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.h2database\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;h2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;scope\u0026amp;gt;runtime\u0026amp;lt;/scope\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;mysql\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;mysql-connector-java\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入所需的连接池依赖 用户引入所需的连接池依赖包，如 druid, c3p0, tomcat, dbcp, Hikari 等。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;druid\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.0.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;c3p0\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;c3p0\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;0.9.1.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.tomcat\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tomcat-jdbc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;8.5.31\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-dbcp\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-dbcp\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.zaxxer\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;HikariCP-java6\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.3.8\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置数据源 我们以 HikariCP 为例，新建一个名为datasource.xml Spring 配置文件，定义如下内容:\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;!-- dataSource pool --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;simpleDataSource\u0026amp;quot; class=\u0026amp;quot;com.zaxxer.hikari.HikariDataSource\u0026amp;quot; destroy-method=\u0026amp;quot;close\u0026amp;quot; primary=\u0026amp;quot;true\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;driverClassName\u0026amp;quot; value=\u0026amp;quot;org.h2.Driver\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;jdbcUrl\u0026amp;quot; value=\u0026amp;quot;jdbc:h2:~/test\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;username\u0026amp;quot; value=\u0026amp;quot;sofa\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;password\u0026amp;quot; value=\u0026amp;quot;123456\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt;  应用配置  必要配置  需要注意一点，引入 SOFATracer 需要强制配置应用名，否则应用启动失败。这一属性和 SOFABoot 框架要求一致，配置如下：\nspring.application.name=SOFATracerDataSource   非必要配置  为了该演示工程正常运行，需要配置 h2database 属性；另为了方便查看日志，配置日志路径。如下：\n# logging path logging.path=./logs # h2 web consloe 路径 spring.h2.console.path=/h2-console # 开启 h2 web consloe，默认为 false spring.h2.console.enabled=true #允许远程访问 h2 web consloe spring.h2.console.settings.web-allow-others=true spring.datasource.username=sofa …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-datasource/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d9d8f6756a294104647067eaa7827f61","permalink":"/projects/sofa-tracer/usage-of-datasource/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-tracer/usage-of-datasource/","summary":"在本文档将演示如何使用 SOFATracer 对 DataSource 进行埋点。 SOFATracer 2.2.0 基于标准的 JDBC 接口实现，支持对标准的数据库连接池（如 DBCP、Druid、c3p0、tomcat、H","tags":null,"title":"DataSource 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-datasource/","wordcount":1014},{"author":null,"categories":null,"content":" SOFATracer 对标准的 JDBC 数据源进行埋点，输出 SQL 语句执行链路数据，默认日志输出为 JSON 数据格式。\nDataSource 摘要日志（datasource-client-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   database.name 数据库名称   sql sql执行语句   connection.establish.span sql执行建连时间   db.execute.cost sql执行时间   database.type 数据库类型   database.endpoint 数据库url   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 21:31:31.566\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerDataSource\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe91d156743109138810017302\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-1\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;15ms\u0026amp;quot;,\u0026amp;quot;database.name\u0026amp;quot;:\u0026amp;quot;test\u0026amp;quot;,\u0026amp;quot;sql\u0026amp;quot;:\u0026amp;quot;DROP TABLE IF EXISTS TEST; CREATE TABLE TEST(ID INT PRIMARY KEY%2C NAME VARCHAR(255));\u0026amp;quot;,\u0026amp;quot;connection.establish.span\u0026amp;quot;:\u0026amp;quot;128ms\u0026amp;quot;,\u0026amp;quot;db.execute.cost\u0026amp;quot;:\u0026amp;quot;15ms\u0026amp;quot;,\u0026amp;quot;database.type\u0026amp;quot;:\u0026amp;quot;h2\u0026amp;quot;,\u0026amp;quot;database.endpoint\u0026amp;quot;:\u0026amp;quot;jdbc:h2:~/test:-1\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  DataSource 统计日志（datasource-client-stat.log） stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、database.name、和 sql 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   database.name 数据库名称   sql sql执行语句   count 本段时间内sql执行次数   total.cost.milliseconds 本段时间内sql执行总耗时（ms）   success 请求结果：Y 表示成功；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 21:31:50.435\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerDataSource\u0026amp;quot;,\u0026amp;quot;database.name\u0026amp;quot;:\u0026amp;quot;test\u0026amp;quot;,\u0026amp;quot;sql\u0026amp;quot;:\u0026amp;quot;DROP TABLE IF EXISTS TEST; CREATE TABLE TEST(ID INT PRIMARY KEY%2C NAME VARCHAR(255));\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:15,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-datasource/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"647de768aff8ececc8276d247c5afee1","permalink":"/projects/sofa-tracer/log-format-datasource/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-datasource/","summary":"SOFATracer 对标准的 JDBC 数据源进行埋点，输出 SQL 语句执行链路数据，默认日志输出为 JSON 数据格式。 DataSource 摘要日志（datasource-client-digest.","tags":null,"title":"DataSource 日志","type":"projects","url":"/projects/sofa-tracer/log-format-datasource/","wordcount":427},{"author":null,"categories":null,"content":" ﻿SOFABoot is based on Spring Boot. It means SOFABoot manages SOFA middleware dependencies and provides the Starter for Spring Boot, facilitating the use of SOFA middleware in Spring Boot.\nSOFABoot dependency management You must load SOFABoot\u0026amp;rsquo;s management dependencies before using SOFA middleware. In a way similar to use Spring Boot, add the configuration tag \u0026amp;lt;parent/\u0026amp;gt; in the project settings:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Where ${sofa.boot.version} represents the SOFABoot version (refer to release history).\nUse Middleware of SOFAStack For SOFABoot, use -sofa-boot-starter suffixes to name middleware components. If you want to use middleware, simply add its dependencies; To use SOFARPC, for example, simply add the following Maven dependencies:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Note that there is no version declaration in the above Maven dependencies as the version has already been declared in sofabook-dependencies. This allows for unified upgrade of all SOFA middleware versions, allaying concerns over dependency conflicts or incompatibility brought by upgrade of a single middleware version. The SOFABoot middleware under control is listed as follows:\n   Middleware starter     SOFARPC rpc-sofa-boot-starter   SOFATracer tracer-sofa-boot-starter   SOFALookout lookout-sofa-boot-starter    Introducing SOFABoot Extension Based on Spring Boot, SOFABoot provides extended capabilities such as health check, module isolation, and class isolation. In accordance with Spring Boot\u0026amp;rsquo;s the dependency-as-a-service principle, the extension capability will be ready immediately after relevant dependencies are added. Currently, there are several extension modules available:\n   Extension components starter     Health check healthcheck-sofa-boot-starter   Module isolation Isle-Sofa-boot-starter   Class isolation sofa-Ark-springboot-starter   Test extension test-Sofa-boot-starter    Introducing the SOFA middleware: the Ark plug-in SOFABoot provides a class isolation component—SOFAArk, which enables users to package third-party packages with dependency conflicts into an Ark plug-in. At run time, the Ark plug-in is loaded with a separate classloader; it is isolated from other Ark plug-ins and business dependencies to address class conflicts. SOFABoot provides SOFARPC and SOFATracer\u0026amp;rsquo;s Ark plug-ins; the Ark plug-in SOFARPC, for example, is loaded into the application to replace SOFARPC starter, to isolate the application from SOFARPC and its indirect dependencies. The controlled Ark plug-ins are listed as follows:\n   Ark plug-in plugin     SOFARPC rpc-sofa-boot-plugin   SOFATracer tracer-sofa-boot-plugin    Introducing SOFABoot namespace Before using SOFA middleware, we need to add …","date":-62135596800,"description":"","dir":"projects/sofa-boot/dependency-management/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dabdbd425f20dee4d7ab580d43574456","permalink":"/en/projects/sofa-boot/dependency-management/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/dependency-management/","summary":"﻿SOFABoot is based on Spring Boot. It means SOFABoot manages SOFA middleware dependencies and provides the Starter for Spring Boot, facilitating the use of SOFA middleware in Spring Boot.\nSOFABoot dependency management You must load SOFABoot\u0026rsquo;s management dependencies before using SOFA middleware. In a way similar to use Spring Boot, add the configuration tag \u0026lt;parent/\u0026gt; in the project settings:\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;sofaboot-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${sofa.boot.version}\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt;  Where ${sofa.boot.version} represents the SOFABoot version (refer to release history).","tags":null,"title":"Dependency management","type":"projects","url":"/en/projects/sofa-boot/dependency-management/","wordcount":391},{"author":null,"categories":null,"content":" Environment preparation To use SOFARegistry, you need to prepare the basic environment first. SOFARegistry depends on the following environment:\n Linux, UNIX, Mac, and Windows are supported. JDK8 Compile it with Apache Maven 3.2.5 or later versions.  Two deployment modes  Integrated deployment  Package and integrate the three roles of meta, data, and session into one jvm, which can be deployed on a standalone machine or a cluster. The deployment is simple.  Independent deployment  Deploy the meta, data, and session roles separately. You can deploy each of them on a standalone machine or a cluster. You can deploy different numbers of servers for each role as needed. We recommend that you use this deployment mode in the production environment.   Deployment steps 1. Download source code, and compile and package the code 1.1 Download the source code git clone https://github.com/sofastack/sofa-registry.git cd sofa-registry  1.2 Compile and package the code mvn clean package -DskipTests  2. Deploy SOFARegistry 2.1 Integrated deployment Package and integrate the three roles of meta, data, and session into one jvm, which can be deployed on a standalone machine or a cluster.\n2.1.1 Standalone deployment For more information about the standalone deployment mode of integrated deployment, see Quick start- Server deployment.\n2.1.2 Cluster deployment  Decompress registry-integration.tgz and modify the configuration file.  Cluster deployment: In this mode, you need to build a cluster of two or more servers. We recommend that you use at least three servers. Note: Currently, you cannot deploy more than one SOFARegistry instance on the same server, which means you must have at least three different servers. The method for deploying SOFARegistry on each server is basically the same as that in standalone deployment:\ncp server/distribution/integration/target/registry-integration.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-integration tar -zxvf registry-integration.tgz -C registry-integration  The difference is that, when you deploy each server in the cluster deployment mode, you need to modify the conf/application.properties configuration:\n# Enter the IP addresses or hostnames of the three servers in the following fields (the hostname will be resolved to the IP address within SOFARegistry) nodes.metaNode=DefaultDataCenter:\u0026amp;lt;hostname1\u0026amp;gt;,\u0026amp;lt;hostname2\u0026amp;gt;,\u0026amp;lt;hostname3\u0026amp;gt; nodes.localDataCenter=DefaultDataCenter nodes.localRegion=DefaultZone   Start registry-integration  After modifying the configuration file for each server, you can start registry-integration as specified in Server deployment.\n Linux/Unix/Mac: sh bin/startup.sh. Windows: Double click the startup.bat file under the bin directory. Check the running status: For each server, you can access the healthcheck API provided by these three roles, or view logs/registry-startup.log to check the running status.  # View the healthcheck API of the meta role (one leader and two followers): …","date":-62135596800,"description":"","dir":"projects/sofa-registry/deployment/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7e28583bc38be66af8d704d7fbcd9dd4","permalink":"/en/projects/sofa-registry/deployment/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-registry/deployment/","summary":"Environment preparation To use SOFARegistry, you need to prepare the basic environment first. SOFARegistry depends on the following environment:\n Linux, UNIX, Mac, and Windows are supported. JDK8 Compile it with Apache Maven 3.2.5 or later versions.  Two deployment modes  Integrated deployment  Package and integrate the three roles of meta, data, and session into one jvm, which can be deployed on a standalone machine or a cluster.","tags":null,"title":"Deployment","type":"projects","url":"/en/projects/sofa-registry/deployment/","wordcount":951},{"author":null,"categories":null,"content":" 1. How to compile  Install JDK7 or later versions, and Maven 3.2.5 or later versions. Directly download the code, and execute the following command in the code directory:\n mvn clean install  2. Version release Version number ACTS uses a three-digit version number in the form of major, minor, and patch, for example, 1.0.1.\nFor more information, see https://semver.org/.\n Major version number: All versions within a major version number must be compatible with each other. They are not necessarily compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, more features it has. Patch number: represents the BugFix version. Such versions are only used for bug fixing. The larger the version number, the more stable the application is.  Version maintenance At most two versions can be maintained simultaneously.\nFor example, if the current version of the master branch code is 1.3.0, the BugFix branch of version 1.2.x will be maintained, but bugs in branch 1.1.x will no longer be fixed. Therefore, a version upgrade for 1.1.x is recommended.\nRelease process  The develop branches use SNAPSHOT versions, for example, 1.3.0-SNAPSHOT. Upon formal release, SNAPSHOT is replaced with a formal version number, for example 1.3.0. After the formal release, the next version is pulled, for example, 1.3.1-SNAPSHOT.  3. Testing Unit test Add the unit test case to the model that you have developed. The package name of the test class is identical to that of the tested class.\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/developer-guide/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fcacc7e89b979f3aec8dc3333a7a3c37","permalink":"/en/projects/sofa-acts/developer-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-acts/developer-guide/","summary":"1. How to compile  Install JDK7 or later versions, and Maven 3.2.5 or later versions. Directly download the code, and execute the following command in the code directory:\n mvn clean install  2. Version release Version number ACTS uses a three-digit version number in the form of major, minor, and patch, for example, 1.0.1.\nFor more information, see https://semver.org/.\n Major version number: All versions within a major version number must be compatible with each other.","tags":null,"title":"Developer guide","type":"projects","url":"/en/projects/sofa-acts/developer-guide/","wordcount":249},{"author":null,"categories":null,"content":" Develope guide of code contribution First refer to the basic Notes for code contribution  Note the test case coverage; Note the code format;  Verify samples  Import the sample Maven project separately; Modify the dependency version in the corresponding Pom file; Verify that samples can work correctly as well.  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/development-use-guide/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"423a54ec3f5fbfc9c0e150eb853738ae","permalink":"/en/projects/sofa-lookout/development-use-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-lookout/development-use-guide/","summary":" Develope guide of code contribution First refer to the basic Notes for code contribution  Note the test case coverage; Note the code format;  Verify samples  Import the sample Maven project separately; Modify the dependency version in the corresponding Pom file; Verify that samples can work correctly as well.  ","tags":null,"title":"Development guide","type":"projects","url":"/en/projects/sofa-lookout/development-use-guide/","wordcount":48},{"author":null,"categories":null,"content":"SOFARPC supports scenarios where a specified address is called.\nThe use of direct call in Java API is as follows, only set the direct connection address:\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumer = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig) .setDirectUrl(\u0026amp;quot;bolt://127.0.0.1:12201\u0026amp;quot;);  The use of direct call in XML is as follows:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sample.HelloService\u0026amp;quot; id=\u0026amp;quot;helloService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  The use of direct call in Annotation is as follows:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, directUrl = \u0026amp;quot;127.0.0.1:12220\u0026amp;quot;)) private SampleService sampleService;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/peer-to-peer/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b1815c322f5dc9528f6429d1d5e38369","permalink":"/en/projects/sofa-rpc/peer-to-peer/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/peer-to-peer/","summary":"SOFARPC supports scenarios where a specified address is called.\nThe use of direct call in Java API is as follows, only set the direct connection address:\nConsumerConfig\u0026lt;HelloService\u0026gt; consumer = new ConsumerConfig\u0026lt;HelloService\u0026gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig) .setDirectUrl(\u0026quot;bolt://127.0.0.1:12201\u0026quot;);  The use of direct call in XML is as follows:\n\u0026lt;sofa:reference interface=\u0026quot;com.alipay.sample.HelloService\u0026quot; id=\u0026quot;helloService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt\u0026gt; \u0026lt;sofa:route target-url=\u0026quot;127.0.0.1:12200\u0026quot;/\u0026gt; \u0026lt;/sofa:binding.bolt\u0026gt; \u0026lt;/sofa:reference\u0026gt;  The use of direct call in Annotation is as follows:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026quot;bolt\u0026quot;, directUrl = \u0026quot;127.","tags":null,"title":"Direct call","type":"projects","url":"/en/projects/sofa-rpc/peer-to-peer/","wordcount":73},{"author":null,"categories":null,"content":" Fault Recover Including Fault-Hystrix and Fault-Tolerance features.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e567dc5e291867e92c8dd1c4f953b768","permalink":"/en/projects/sofa-rpc/fault/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/fault/","summary":"Fault Recover Including Fault-Hystrix and Fault-Tolerance features.","tags":null,"title":"Disaster recovery","type":"projects","url":"/en/projects/sofa-rpc/fault/","wordcount":7},{"author":null,"categories":null,"content":" Distributed consensus algorithm Understand distributed consensus  Multiple participants reach a complete consensus on one thing: one conclusion for one thing. The conclusion cannot be overthrown.  Typical distributed consensus algorithms  Paxos: It is considered as the foundation of distributed consensus algorithms. Other algorithms are its variants. However, the Paxos paper only provides the process of a single proposal, without describing the details of multi-paxos that is required for state machine replication. Paxos implementation involves high engineering complexity, for example, multiple-point writes and log hole tolerance. Zab: It is applied in ZooKeeper and widely used in the industry. However, it is not available as a universal library. Raft: It is known for being easy to understand. There are many renowned Raft implementations in the industry, such as etcd, Braft, and TiKV.  Introduction to Raft Raft is in nature a Paxos-based distributed consensus algorithm that is much easier to understand than Paxos. Unlike Paxos, Raft divides the protocols into independent modules, and uses a streamlined design, making the Raft protocol easier to implement.\nSpecifically, Raft divides consensus protocols into almost completely decoupled modules, such as leader election, membership change, log replication, and snapshot.\nRaft adopts a more streamlined design by preventing reordering commits, simplifying roles (it has only three roles: leader, follower, and candidate), allowing only the leader to write, and using randomized timeout values to design leader election.\nFeature: strong leader  The system can have only one leader at the same time, and only the leader can accept requests sent by clients. The leader is responsible for communication with all followers, sending proposals to all followers, and receiving responses from the majority of followers. The leader also needs to send heartbeats to all followers to maintain its leadership.  To summarize, a strong leader tells its followers: \u0026amp;ldquo;Do not say anything. Do what I said and let me know when you finish!\u0026amp;rdquo; In addition, a leader must always remain active by sending heartbeats to followers. Otherwise, a follower will take its place.\nReplicated state machine Assume we have an infinitely incrementing sequence (system) a[1, 2, 3…]. If for any integer i, the value of a[i] meets the distributed consensus requirement, the system meets the requirement of a consensus state machine. Basically, all real life systems are subject to continuous operations, and reaching consensus on a single value is definitely not enough. To make sure all replicas of a real life system are consistent, we usually convert the operations into entries of a write-ahead-log(WAL). Then, we make sure all replicas of the system reach a consensus on the WAL entries, so that each replica performs operations of the WAL entries in order. As a result, the replicas are in consistent states.\n A client sent a write (operation) request to …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/consistency-raft-jraft/","fuzzywordcount":4900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a0e98df1bec305cca7db6fc34fc97771","permalink":"/en/projects/sofa-jraft/consistency-raft-jraft/","publishdate":"0001-01-01T00:00:00Z","readingtime":23,"relpermalink":"/en/projects/sofa-jraft/consistency-raft-jraft/","summary":"Distributed consensus algorithm Understand distributed consensus  Multiple participants reach a complete consensus on one thing: one conclusion for one thing. The conclusion cannot be overthrown.  Typical distributed consensus algorithms  Paxos: It is considered as the foundation of distributed consensus algorithms. Other algorithms are its variants. However, the Paxos paper only provides the process of a single proposal, without describing the details of multi-paxos that is required for state machine replication.","tags":null,"title":"Distributed consensus - Raft and JRaft","type":"projects","url":"/en/projects/sofa-jraft/consistency-raft-jraft/","wordcount":4871},{"author":null,"categories":null,"content":"SOFARPC provides support for the Dubbo protocol, making it convenient for you to interface with existing Dubbo service. * Basic usage\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/dubbo/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"87d78ed0c2d06fe7e1dbf9cb9d6c1c9d","permalink":"/en/projects/sofa-rpc/dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/dubbo/","summary":"SOFARPC provides support for the Dubbo protocol, making it convenient for you to interface with existing Dubbo service. * Basic usage","tags":null,"title":"Dubbo","type":"projects","url":"/en/projects/sofa-rpc/dubbo/","wordcount":21},{"author":null,"categories":null,"content":" Dubbo Integration In this document will demonstrate how to use SOFATracer to track of Dubbo, this example address.\nPrepare Environment The versions of the framework components used in this case are as follows:\n SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 2.4.0/3.0.4 JDK 8  This case includes three submodules:\n tracer-sample-with-dubbo-consumer service provider tracer-sample-with-dubbo-provider service consumer tracer-sample-with-dubbo-facade service interface define  New SOFABoot project as parent project After creating a Spring Boot project, you need to introduce the SOFABoot\u0026amp;rsquo;s dependency. First, you need to unzip the generated zip package of Spring Boot project and modify the Maven project configuration file pom.xml.\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Replace the above with the followings:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  The ${sofa.boot.version} specifies the latest version of SOFABoot. For more information about SOFABoot versions, refer to Release notes.\nNew tracer-sample-with-dubbo-facade Module provider a service interface\npublic interface HelloService { String SayHello(String name); }  New tracer-sample-with-dubbo-provider Module  provider SOFATracer dependency\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   SOFATracer versions are controlled by SOFABoot versions. If the SOFABoot versions used do not match, you need to manually specify a tracer version that is higher than 2.4.0.\n  application.properties Configuration  # Spring boot application spring.application.name=dubbo-provider # Base packages to scan Dubbo Component: @org.apache.dubbo.config.annotation.Service dubbo.scan.base-packages=com.alipay.sofa.tracer.examples.dubbo.impl ## Filter dubbo.provider.filter=dubboSofaTracerFilter # Dubbo Protocol dubbo.protocol.name=dubbo ## Dubbo Registry dubbo.registry.address=zookeeper://localhost:2181 logging.path=./logs  Publish the Dubbo service using annotations\n@Service public class HelloServiceImpl implements HelloService { @Override public String SayHello(String name) { return \u0026amp;quot;Hello , \u0026amp;quot;+name; } }  New tracer-sample-with-dubbo-consumer Module  provider SOFATracer dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  application.properties Configuration\nspring.application.name=dubbo-consumer dubbo.registry.address=zookeeper://localhost:2181 dubbo.consumer.filter=dubboSofaTracerFilter logging.path=./logs   Service reference  @Reference(async = false) public HelloService …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-dubbo/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"03f845606b454a7224333238aeecd9ab","permalink":"/en/projects/sofa-tracer/usage-of-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/usage-of-dubbo/","summary":"Dubbo Integration In this document will demonstrate how to use SOFATracer to track of Dubbo, this example address.\nPrepare Environment The versions of the framework components used in this case are as follows:\n SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 2.4.0/3.0.4 JDK 8  This case includes three submodules:\n tracer-sample-with-dubbo-consumer service provider tracer-sample-with-dubbo-provider service consumer tracer-sample-with-dubbo-facade service interface define  New SOFABoot project as parent project After creating a Spring Boot project, you need to introduce the SOFABoot\u0026rsquo;s dependency.","tags":null,"title":"Dubbo Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-dubbo/","wordcount":297},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA) and Locate on hmily-demo-dubbo Module Configuring（hmily-demo-dubbo-account module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Configure with your zookeeper address(es)(can run one locally)  \u0026amp;lt;dubbo:registry protocol=\u0026amp;quot;zookeeper\u0026amp;quot; address=\u0026amp;quot;localhost:2181\u0026amp;quot;/\u0026amp;gt;   run DubboHmilyAccountApplication.java  Run hmily-demo-dubbo-inventory(refer to simillar instructions above). Run hmily-demo-dubbo-order(refer to simillar instructions above). Access on http://127.0.0.1:8087/swagger-ui.html for more. ","date":-62135596800,"description":"Dubbo Quick Start","dir":"projects/hmily/quick-start-dubbo/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"559586953294644ad030b2d360281bf5","permalink":"/en/projects/hmily/quick-start-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/quick-start-dubbo/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA) and Locate on hmily-demo-dubbo Module Configuring（hmily-demo-dubbo-account module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026lt;db_host_ip\u0026gt;:\u0026lt;db_host_port\u0026gt;/hmily_account?useUnicode=true\u0026amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.","tags":null,"title":"Dubbo Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-dubbo/","wordcount":153},{"author":null,"categories":null,"content":" Dubbo Log Format SOFATracer integrates Dubbo and outputs the requested link log data format. The default is JSON data format.\nDubbo service consumer digest log（dubbo-client-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   traceId TraceId   spanId SpanId   span.kind span Type   local.app Current application name   protocol protocol   service service interface   method service method   invoke.type invoke type   remote.host remote host   remote.port remote port   local.host local host   client.serialize.time request serialize time   client.deserialize.time response deserialize time   req.size.bytes Request Body Size   resp.size.bytes Response Body Size   result.code result code   current.thread.name Current thread name   time.cost.milliseconds Request time (ms)   baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-04-03 11:36:01.909\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8451554262561656100126684\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;dubbo-consumer\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;dubbo\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.dubbo.facade.HelloService\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;SayHello\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;remote.host\u0026amp;quot;:\u0026amp;quot;10.15.232.69\u0026amp;quot;,\u0026amp;quot;remote.port\u0026amp;quot;:\u0026amp;quot;20880\u0026amp;quot;,\u0026amp;quot;local.host\u0026amp;quot;:\u0026amp;quot;10.15.232.69\u0026amp;quot;,\u0026amp;quot;client.serialize.time\u0026amp;quot;:35,\u0026amp;quot;client.deserialize.time\u0026amp;quot;:0,\u0026amp;quot;req.size.bytes\u0026amp;quot;:323,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:323,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:252,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Dubbo service provider digest log（dubbo-server-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   traceId TraceId   spanId SpanId   span.kind span Type   local.app current application name   service service inteface   method service method   local.host local host   local.host local port   protocol protocol   server.serialize.time response serialize time   server.deserialize.time request deserialize time   result.code result code   current.thread.name current thread name   time.cost.milliseconds Request time (ms)   baggage Transparently transmitted baggage data    Example\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-04-03 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-dubbo/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9bac8256ee1a74546b74799f9f1c0de9","permalink":"/en/projects/sofa-tracer/log-format-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/log-format-dubbo/","summary":"Dubbo Log Format SOFATracer integrates Dubbo and outputs the requested link log data format. The default is JSON data format.\nDubbo service consumer digest log（dubbo-client-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   traceId TraceId   spanId SpanId   span.kind span Type   local.app Current application name   protocol protocol   service service interface   method service method   invoke.","tags":null,"title":"Dubbo log","type":"projects","url":"/en/projects/sofa-tracer/log-format-dubbo/","wordcount":275},{"author":null,"categories":null,"content":" The Dubbo Interface Sectioon  Introduce the jar packages into your interface project.  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.\npublic interface HelloService { @Hmily void say(String hello); }  The project with Dubbo implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration\n Step 3 ： Add the specific annotation to the implementation method. you need to complete the development of confirm and cancel method, if in TCC mode.\n  Introduce The Maven dependency Spring-Namespace  for Alibaba-Dubbo Users  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  for Aapche-Dubbo Users\n  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-apache-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   the configuration should be made in the xml file like below  \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot  for Alibaba-Dubbo Users  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   for Aapche-Dubbo Users  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-apache-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  Add annotations on the implementation interface We have completed the integration described above,and the next we will talk about the specific implementation.\nTCC Mode  Add @HmilyTCC (confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation to the concrete implementation of the interface method identified by \u0026amp;lsquo;@Hmily\u0026amp;rsquo;.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be …","date":-62135596800,"description":"Dubbo user guide","dir":"projects/hmily/user-dubbo/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c28d182a0aec22568b1dbf4e64014041","permalink":"/en/projects/hmily/user-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/hmily/user-dubbo/","summary":"The Dubbo Interface Sectioon  Introduce the jar packages into your interface project.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.\npublic interface HelloService { @Hmily void say(String hello); }  The project with Dubbo implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration","tags":null,"title":"Dubbo user guide","type":"projects","url":"/en/projects/hmily/user-dubbo/","wordcount":630},{"author":null,"categories":null,"content":"SOFARPC 提供了 Dubbo 协议的支持，可以让用户非常方便地和现有的 Dubbo 的系统做对接。 * 基本使用\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/dubbo/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"87d78ed0c2d06fe7e1dbf9cb9d6c1c9d","permalink":"/projects/sofa-rpc/dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/dubbo/","summary":"SOFARPC 提供了 Dubbo 协议的支持，可以让用户非常方便地和现有的 Dubbo 的系统做对接。 * 基本使用","tags":null,"title":"Dubbo 协议","type":"projects","url":"/projects/sofa-rpc/dubbo/","wordcount":38},{"author":null,"categories":null,"content":" 在 SOFARPC 中，使用不同的通信协议只要设置使用不同的 Binding 即可，如果需要使用 Dubbo 协议，只要将 Binding 设置为 Dubbo 即可。下面使用以注解的方式来例举，其他的使用方式可以参考 Bolt 协议基本使用，这里不再重复说明。：\n发布服务 发布一个 Dubbo 的服务，只需要将 @SofaServiceBinding 的 bindingType 设置为 dubbo 即可：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  引用服务 引用一个 Dubbo 的服务，只需要将 @SofaReferenceBinding 的 bindingType 设置为 dubbo 即可：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;), jvmFirst = false) private SampleService sampleService;  设置 Dubbo 服务的 Group 在 SOFARPC 的模型中，没有直接的一个字段叫做 Group，但是 SOFARPC 有一个 uniqueId 的模型，可以直接映射到 Dubbo 的模型中的 Group，比如下面的代码，就是发布了一个 Group 为 groupDemo 的服务：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;)}, uniqueId = \u0026amp;quot;groupDemo\u0026amp;quot;) public class SampleServiceImpl implements SampleService { }  如下的代码，就是引用了一个 Group 为 groupDemo 的服务：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;dubbo\u0026amp;quot;), uniqueId = \u0026amp;quot;groupDemo\u0026amp;quot;, jvmFirst = false) private SampleService sampleService;   注意，目前 Dubbo 协议只支持 Zookeeper 作为服务注册中心。\n ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/dubbo-usage/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1bf72c194a20a5dccea70423690191f4","permalink":"/projects/sofa-rpc/dubbo-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/dubbo-usage/","summary":"在 SOFARPC 中，使用不同的通信协议只要设置使用不同的 Binding 即可，如果需要使用 Dubbo 协议，只要将 Binding 设置为 Dubbo 即可。下面使用以注解的方式来例举，其他的使用方式可以","tags":null,"title":"Dubbo 协议基本使用","type":"projects","url":"/projects/sofa-rpc/dubbo-usage/","wordcount":322},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 Dubbo 进行埋点，本示例工程地址。\n基础环境 本案例使用的各框架组件的版本如下：\n SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 2.4.0/3.0.4 JDK 8  本案例包括三个子模块：\n tracer-sample-with-dubbo-consumer 服务调用方 tracer-sample-with-dubbo-provider 服务提供方 tracer-sample-with-dubbo-facade 接口  原理 SOFATracer 对象 Dubbo 的埋点实现依赖于 Dubbo 的 SPI 机制来实现，Tracer 中基于 调用拦截扩展 自定义了 DubboSofaTracerFilter 用于实现对 Dubbo 的调用埋点。由于 DubboSofaTracerFilter 并没有成为 Dubbo 的官方扩展，因此在使用 SOFATracer 时需要安装 调用拦截扩展 中 所提供的方式进行引用，即：\n\u0026amp;lt;!-- 消费方调用过程拦截 --\u0026amp;gt; \u0026amp;lt;dubbo:reference filter=\u0026amp;quot;dubboSofaTracerFilter\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;!-- 消费方调用过程缺省拦截器，将拦截所有reference --\u0026amp;gt; \u0026amp;lt;dubbo:consumer filter=\u0026amp;quot;dubboSofaTracerFilter\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- 提供方调用过程拦截 --\u0026amp;gt; \u0026amp;lt;dubbo:service filter=\u0026amp;quot;dubboSofaTracerFilter\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;!-- 提供方调用过程缺省拦截器，将拦截所有service --\u0026amp;gt; \u0026amp;lt;dubbo:provider filter=\u0026amp;quot;dubboSofaTracerFilter\u0026amp;quot;/\u0026amp;gt;  新建 SOFABoot 工程作为父工程 在创建好一个 Spring Boot 的工程之后，接下来就需要引入 SOFABoot 的依赖，首先，需要将上文中生成的 Spring Boot 工程的 zip 包解压后，修改 Maven 项目的配置文件 pom.xml，将\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  替换为：\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  这里的 ${sofa.boot.version} 指定具体的 SOFABoot 版本，参考发布历史。\n新建 tracer-sample-with-dubbo-facade 提供一个接口\npublic interface HelloService { String SayHello(String name); }  新建 tracer-sample-with-dubbo-provider  在工程模块的 pom 文件中添加 SOFATracer 依赖\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   SOFATracer 版本受 SOFABoot 版本管控，如果使用的 SOFABoot 版本不匹配，则需要手动指定 tracer 版本，且版本需高于 2.4.0.\n  在工程的 application.properties 文件下添加相关参数，  # Spring boot application spring.application.name=dubbo-provider # Base packages to scan Dubbo Component: @org.apache.dubbo.config.annotation.Service dubbo.scan.base-packages=com.alipay.sofa.tracer.examples.dubbo.impl ## Filter 必须配置 dubbo.provider.filter=dubboSofaTracerFilter # Dubbo Protocol dubbo.protocol.name=dubbo ## Dubbo Registry dubbo.registry.address=zookeeper://localhost:2181 logging.path=./logs  使用注解方式发布 Dubbo 服务\n@Service public class HelloServiceImpl implements HelloService { @Override public String SayHello(String name) { return \u0026amp;quot;Hello , \u0026amp;quot;+name; } }  新建 tracer-sample-with-dubbo-consumer  在工程模块的 pom 文件中添加 SOFATracer 依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  在工程的 application.properties 文件下添加相关参数\nspring.application.name=dubbo-consumer dubbo.registry.address=zookeeper://localhost:2181 # Filter 必须配置 dubbo.consumer.filter=dubboSofaTracerFilter logging.path=./logs   服务引用  @Reference(async = false) public HelloService helloService; @Bean …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-dubbo/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"03f845606b454a7224333238aeecd9ab","permalink":"/projects/sofa-tracer/usage-of-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-dubbo/","summary":"在本文档将演示如何使用 SOFATracer 对 Dubbo 进行埋点，本示例工程地址。 基础环境 本案例使用的各框架组件的版本如下： SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 2.4.0/3.0.4 JDK 8 本案例包括三个子模块： tracer-sample-with-dubbo-consumer 服务调","tags":null,"title":"Dubbo 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-dubbo/","wordcount":760},{"author":null,"categories":null,"content":" SOFATracer 集成 Dubbo 后输出请求的链路数据格式，默认为 JSON 数据格式。\nDubbo 服务消费方摘要日志（dubbo-client-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   protocol 协议   service 服务接口   method 调用方法   invoke.type 调用类型   remote.host 目标主机   remote.port 目标端口   local.host 本地主机   client.serialize.time 请求序列化时间   client.deserialize.time 响应反序列化时间   req.size.bytes Request Body 大小   resp.size.bytes Response Body 大小   error 错误信息   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 23:36:08.250\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;dubbo-consumer\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;1e27a79c156743856804410019644\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-2\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;205ms\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;dubbo\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.glmapper.bridge.boot.service.HelloService\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;SayHello\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;remote.host\u0026amp;quot;:\u0026amp;quot;192.168.2.103\u0026amp;quot;,\u0026amp;quot;remote.port\u0026amp;quot;:\u0026amp;quot;20880\u0026amp;quot;,\u0026amp;quot;local.host\u0026amp;quot;:\u0026amp;quot;192.168.2.103\u0026amp;quot;,\u0026amp;quot;client.serialize.time\u0026amp;quot;:35,\u0026amp;quot;client.deserialize.time\u0026amp;quot;:5,\u0026amp;quot;req.size.bytes\u0026amp;quot;:336,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:48,\u0026amp;quot;error\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Dubbo 服务提供方摘要日志（dubbo-server-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   protocol 协议   service 服务接口   method 调用方法   invoke.type 调用类型   local.host 本地主机   local.port 本地端口   server.serialize.time 响应序列化时间   server.deserialize.time 请求反序列化时间   req.size.bytes Request Body 大小   resp.size.bytes Response Body 大小   error 错误信息   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-dubbo/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9bac8256ee1a74546b74799f9f1c0de9","permalink":"/projects/sofa-tracer/log-format-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/log-format-dubbo/","summary":"SOFATracer 集成 Dubbo 后输出请求的链路数据格式，默认为 JSON 数据格式。 Dubbo 服务消费方摘要日志（dubbo-client-digest.log） 以 JSON 格式输出的数据","tags":null,"title":"Dubbo 日志","type":"projects","url":"/projects/sofa-tracer/log-format-dubbo/","wordcount":550},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git Zookeeper  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n使用你的工具 idea 打开项目，找到hmily-demo-dubbo项目。 修改项目配置（hmily-demo-dubbo-account为列子）  修改业务数据库(account项目为列子)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: #改成你的用户名 password: #改成你的密码   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   在spring-dubbo中修改你的zookeeper地址（可以在自己电脑本地启动一个zookeeper服务）  \u0026amp;lt;dubbo:registry protocol=\u0026amp;quot;zookeeper\u0026amp;quot; address=\u0026amp;quot;localhost:2181\u0026amp;quot;/\u0026amp;gt;   run DubboHmilyAccountApplication.java  启动hmily-demo-dubbo-inventory 参考上述。 启动hmily-demo-dubbo-order 参考上述。 访问：http://127.0.0.1:8087/swagger-ui.html。 ","date":-62135596800,"description":"Dubbo快速体验","dir":"projects/hmily/quick-start-dubbo/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"559586953294644ad030b2d360281bf5","permalink":"/projects/hmily/quick-start-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/quick-start-dubbo/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git Zookeeper 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 使用你的工具 idea 打开项目，找到hmily-dem","tags":null,"title":"Dubbo快速体验","type":"projects","url":"/projects/hmily/quick-start-dubbo/","wordcount":519},{"author":null,"categories":null,"content":" The engine architecture is shown in the following diagram. Node A node in a Raft cluster connects and encapsulates all underlayer service modules, and main service interfaces that are visible to users. Specifically, the leader node of a raft group calls apply(task) to commit new tasks to the state machine replication cluster made up by the Raft group, which will then apply the task to the business state machine.\nStorage  It stores Raft configuration changes and log entries converted from requests submitted by users, and replicates log entries from the leader\u0026amp;rsquo;s log to followers\u0026amp;rsquo; logs. LogStorage stores logs, while LogManager is responsible for calling the underlayer storage, caching and batch submitting storage calls, and conducting necessary checks and optimization. MetaStorage stores the metadata and records the internal states of the Raft implementation, for example, the current term of the node and the node to vote for. Optional. Snapshot storage is used to store users\u0026amp;rsquo; state-machine snapshots and meta information. SnapshotStorage stores snapshots, while SnapshotExecutor manages the actual storage, remote installation, and replication of snapshots.  State machine  StateMachine is an implementation of users\u0026amp;rsquo; core logic. It calls the onApply(Iterator) method to apply log entries that are submitted with Node#apply(task) to the business state machine. FSMCaller encapsulates state transition calls that are sent to the User StateMachine, writes log entries, implements a finite-state machine (FSM), conducts necessary checks, and merges requests for batch submission and concurrent processing.  Replication  Replicator is used by the leader to replicate log entries to followers. It does the same thing as an AppendEntries RPC of Raft. Without log entries, it is sent by the leader as heartbeats. ReplicatorGroup is used by a Raft group to manage all replicators, and to perform necessary permission checks and dispatches.  RPC The RPC module is used for network communication between nodes.\n The RPC server is built in a node to receive requests from other nodes or clients, and to redirect such requests to the corresponding service modules. The RPC client is used to issue requests to other nodes, such as requests for votes, log replication requests, and heartbeats.  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/engine-architecture/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d2cc9de133aed20695229d0cde5b6ff9","permalink":"/en/projects/sofa-jraft/engine-architecture/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-jraft/engine-architecture/","summary":"The engine architecture is shown in the following diagram. Node A node in a Raft cluster connects and encapsulates all underlayer service modules, and main service interfaces that are visible to users. Specifically, the leader node of a raft group calls apply(task) to commit new tasks to the state machine replication cluster made up by the Raft group, which will then apply the task to the business state machine.","tags":null,"title":"Engine architecture","type":"projects","url":"/en/projects/sofa-jraft/engine-architecture/","wordcount":347},{"author":null,"categories":null,"content":" ExtensionLoader To ensure that all steps of SOFARPC have sufficient scalability, SOFARPC defines a very flexible extension mechanism in which all extension implementations are equal.\nThis mechanism is very useful for both SOFARPC developers and users. SOFARPC abstracts itself into multiple modules which have no explicit dependencies on each other and interact via SPI.\nThis extension mechanism abstracts the interaction method of SPI. If you have read the documents about Filter and Router, you may have such experience.\nThe following sections introduce how to extend through the SPI interaction method.\nSOFARPC provides the capabilities of ExtensionLoader.\nDesign extension points SOFARPC defines an annotation @Extensible, which is on the interface or abstract class to identify that the class is an extension point. Namely, it informs SOFARPC that the class is extensible and SOFARPC needs to find the implementation of the extension point. In addition, the annotation defines the file name of the implementation class and whether the class is a singleton.\n@Documented @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.TYPE }) public @interface Extensible { /** * Specify the name of the custom extension file, which is the full class name by default * * @return Custom extension file name */ String file() default \u0026amp;quot;\u0026amp;quot;; /** * Whether the extension class uses a singleton, yes by default * * @return Whether to use a singleton */ boolean singleton() default true; /** * Whether the extension class needs to be encoded, not by default * * @return Whether to encode */ boolean coded() default false; }  SOFARPC also defines the @Extension annotation, which indicates an extension implementation class. It also defines the name that the extension point uses when looking for an extension implementation in the file.\n@Documented @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.TYPE }) public @interface Extension { /** * Extension point name * * @return Extension point name */ String value(); /** * Extension point coding, not required by default; it is required when the interface needs to be encoded * * @return Extension point encoding * @see Extensible#coded() */ byte code() default -1; /** * Priority sorting, not required by default, the larger number, the higher priority * * @return Sort */ int order() default 0; /** * Whether to override other extensions with low {@link #order()} * * @return Whether to override other low-order extensions * @since 5.2.0 */ boolean override() default false; /** * Exclude other extensions, namely exclude other extensions with low {@link #order()} * * @return Excludes other extensions * @since 5.2.0 */ String[] rejection() default {}; }  Add extension point  Define extension points.  @Extensible public interface Person { void getName(); }   Define the extension implementation.  @Extension(\u0026amp;quot;A\u0026amp;quot;) public class PersonA implements Person{ @Override public void getName() { System.out.println(\u0026amp;quot;li wei\u0026amp;quot;); } } …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/extension-loader/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"acc5628da3a7ea2df5eb68bd8ec17159","permalink":"/en/projects/sofa-rpc/extension-loader/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/extension-loader/","summary":"ExtensionLoader To ensure that all steps of SOFARPC have sufficient scalability, SOFARPC defines a very flexible extension mechanism in which all extension implementations are equal.\nThis mechanism is very useful for both SOFARPC developers and users. SOFARPC abstracts itself into multiple modules which have no explicit dependencies on each other and interact via SPI.\nThis extension mechanism abstracts the interaction method of SPI. If you have read the documents about Filter and Router, you may have such experience.","tags":null,"title":"Extension mechanism","type":"projects","url":"/en/projects/sofa-rpc/extension-loader/","wordcount":603},{"author":null,"categories":null,"content":" Customize different engine stages You can rewrite APIs provided by ActsTestBase in the test script or in the base class.\n Rewrite the prepare, execute, check, and clear actions. For example, you can add some actions before or after super.prepare(). Rewrite the process method. You can add some actions before or after super.process() to reorchestrate the entire script. For example, you can add some personalized steps in the existing clear \u0026amp;gt; prepare \u0026amp;gt; execute \u0026amp;gt; check process. Rewrite beforeActsTest and afterActsTest to add some personalized actions before or after the running of each test case, such as preparing the context and refreshing the cache.  Parameterization In response expectation and database expectation data, you can use $Variable name to define a value as a variable. You can set values of the variable in the test script. Supported scope: request parameters, responses, and database table fields. Supported types: Currently, only string parameterization is supported.\nUsage:\n(1) Add $ before a value to define it as a variable in the interface.\n(2) Assign values to the variables in the test script.\n@Override public void beforeActsTest(ActsRuntimeContext actsRuntimeContext) { actsRuntimeContext.paramMap.put(\u0026amp;quot;roleId\u0026amp;quot;, \u0026amp;quot;123\u0026amp;quot;); actsRuntimeContext.refreshDataParam(); }  When you write the database expectation, you can use the equal sign = to assign a value to the variable. This indicates that the value is a query result, and subsequent tables can use this variable as the value.\nAssume that the interface will insert data into two tables.\n   id_A value_A     123 abc       id_B value_B     abc efg    When you query these two tables, first query for value_A based on id_A in Table A that is returned by the interface. Then use value_A as a condition for the query in Table B. You can set the values as follows in the plug-in.\n   Field Flag Value     id_A C $param1   value_A Y =param2       Field Flag Value     id_B C $param2   value_B Y efg    Operation description:\n =param2 and $param2 indicate that the ACTS framework will first query for value_A in Table A, and then select from B where id_B = value_A to obtain the property value of id_B in Table B. $param1 indicates that you can assign a value to id_A in the code, for example:  actsRuntimeContext.paramMap.put(\u0026amp;quot;param1\u0026amp;quot;,\u0026amp;quot;123\u0026amp;quot;);  This snippet assigns the value 123 to the variable param1. You can write this snippet to beforeActsTest in the test script to make the ACTS framework query table A before assigning the value 123 to id_A.\nComponentization Currently, only string componentization is supported.\nIf a property is a dynamically generated string, for example, some IDs. You can use the at sign @ to call a component to generate this property. The component must be placed in the component package at the same level as the test module, namely: com.corpname.appname.acts.component (appname is the name of the system, and corpname is the name of the company, for …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-api/","fuzzywordcount":1200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ce7e264713a6f7a3f0672e2432489f59","permalink":"/en/projects/sofa-acts/usage-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/sofa-acts/usage-api/","summary":"Customize different engine stages You can rewrite APIs provided by ActsTestBase in the test script or in the base class.\n Rewrite the prepare, execute, check, and clear actions. For example, you can add some actions before or after super.prepare(). Rewrite the process method. You can add some actions before or after super.process() to reorchestrate the entire script. For example, you can add some personalized steps in the existing clear \u0026gt; prepare \u0026gt; execute \u0026gt; check process.","tags":null,"title":"Extensions","type":"projects","url":"/en/projects/sofa-acts/usage-api/","wordcount":1102},{"author":null,"categories":null,"content":" Q: What should I do if NoSuchMethodError is returned? Generally, this error is returned in the case of dependency conflicts. Commonly known dependency conflicts are listed as follows. Exclude the corresponding dependencies when you encounter relevant conflicts.\nLog conflict commons-logging conflict \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-logging\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt;  logback-classic conflict Rule out logback-classic by the location of the conflict. For example, application dependencies spring-boot-starter-logging and spring-test conflict with each other.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;ch.qos.logback\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;logback-classic\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-test\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.3.4.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;ch.qos.logback\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;logback-classic\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  snakeyaml conflict java.lang.NoSuchMethodError: org.yaml.snakeyaml.Yaml.\u0026amp;lt;init\u0026amp;gt;(Lorg/yaml/snakeyaml/constructor/BaseConstructor;)V  org.yaml referenced in spring-boot-starter-test conflicts with org.yaml referenced in org.testing. In the following sample code, a conflict of org.yaml in spring-boot-starter-test is ruled out (it can also be ruled out at other conflict locations such as org.testing):\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-test\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;scope\u0026amp;gt;test\u0026amp;lt;/scope\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.yaml\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;snakeyaml\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Q: What should I do if NoClassDefFoundError is returned? Generally, this error is returned in the case of missing dependencies or dependency conflicts.\nMockito returns a no class found error While using Mockito with SOFABoot, you do not have to import Mockito if the spring-boot-starter-test dependency already exists.\nQ: What should I do if \u0026amp;ldquo;No bean dataAccessConfigManager available\u0026amp;rdquo; is returned? This error is returned because the application starter class specified by the test script does not have the acts-core.xml file. You can add the acts-core.xml file according to the following figure.\nQ: What should I do if \u0026amp;ldquo;No runnable methods\u0026amp;rdquo; is returned? Generally, this error is caused when you run your Junit test with the ACTS test script. You can use the TestNG framework to run the ACTS test script.\nQ: What should I do in the case of a model …","date":-62135596800,"description":"","dir":"projects/sofa-acts/faq/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5f89d1f5695cbe6b669a8738741529bd","permalink":"/en/projects/sofa-acts/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-acts/faq/","summary":"Q: What should I do if NoSuchMethodError is returned? Generally, this error is returned in the case of dependency conflicts. Commonly known dependency conflicts are listed as follows. Exclude the corresponding dependencies when you encounter relevant conflicts.\nLog conflict commons-logging conflict \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;commons-logging\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;commons-logging\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt;  logback-classic conflict Rule out logback-classic by the location of the conflict. For example, application dependencies spring-boot-starter-logging and spring-test conflict with each other.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.","tags":null,"title":"FAQ","type":"projects","url":"/en/projects/sofa-acts/faq/","wordcount":633},{"author":null,"categories":null,"content":" Common issues Q: Is SOFARPC the version used inside Ant Financial? Yes, SOFARPC has excellent extension interfaces, and the version for internal use just has some additional extension implementations based on the open source version. For example, the cloud-based commercial version integrates the Ant Financial Technology\u0026amp;rsquo;s shared registry center, Distributed System Tracing (DST) and other products. The version for internal use integrates Ant Financial\u0026amp;rsquo;s internal registry center, LDC router and other individual extensions.\nQ: Does SOFARPC use ZooKeeper as the registry center internally? Can it integrate other registry centers such as etcd? Ant Financial uses its self-developed registry products internally. SOFARPC\u0026amp;rsquo;s registry modules are extensible. All the registry modules use the same set of core interfaces for both internal and external use. Currently, the open-source version has integrated with ZooKeeper, and other registry implementation communities are being integrated.\nQ: What is the difference between SOFARPC and Dubbo? Dubbo, developed by Alibaba Group, is an excellent open-source RPC framework featuring high performance and good scalability. Dubbo is a comparatively mature open source framework, with a large number of users and rich open source ecology. Now, it has joined the Apache Foundation for incubation. Dubbo was first widely used in the Alibaba B2B department.\nOriginated from HSF in Alibaba Group, SOFARPC now has grown to an independent product. SOFARPC has carried out a lot of reconstruction and optimization on the aspects of protocol, network, routing, and scalability to meet the large-scale financial business scenarios of Ant Financial. In the Ant Financial\u0026amp;rsquo;s middleware (SOFAStack) ecosystem, SOFARPC is supported by a comprehensive microservices technology stack, including Microservices R\u0026amp;amp;D framework, RPC framework, service registry center, distributed scheduling task, throttling framework, Dynamic Configuration, DST, Metrics and others. By Dec. 11, 2017, SOFARPC has been used by thousands of systems in Ant Financial, and the production environment has released more than tens of thousands of interfaces.\nHowever, in the open source field, SOFARPC is still at the initial stage, and its open source ecosystem is still under construction. With the advancement of the open source plan, various related components will be available in the subsequent versions to improve the microservices stack. You are welcome to build SOFAStack together with us.\nIn terms of performance, the technical points of both products involved in protocols are similar. As for scalability, both products have good scalability. As for other functional differences, here are some features that have been opened source or will be open sourced in the near future for reference:\n SOFARPC supports HTTP/2 and GRPC protocols and provides such capabilities as service warm-up weight, automatic degradation upon fault, negotiation mechanism, and CRC data …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/faq/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a6ec77ce5a423c5345394f42c64a416b","permalink":"/en/projects/sofa-rpc/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-rpc/faq/","summary":"Common issues Q: Is SOFARPC the version used inside Ant Financial? Yes, SOFARPC has excellent extension interfaces, and the version for internal use just has some additional extension implementations based on the open source version. For example, the cloud-based commercial version integrates the Ant Financial Technology\u0026rsquo;s shared registry center, Distributed System Tracing (DST) and other products. The version for internal use integrates Ant Financial\u0026rsquo;s internal registry center, LDC router and other individual extensions.","tags":null,"title":"FAQ","type":"projects","url":"/en/projects/sofa-rpc/faq/","wordcount":742},{"author":null,"categories":null,"content":"Usually, a service have multiple service providers in a cluster. Some of the service providers may have persistent connections still survived due to network, configuration, long-term fullgc, full thread pool, hardware failure and others, but the program cannot respond properly. The stand-alone fault tolerance function can degrade the exceptional service providers so that the client requests can be pointed to the healthy node. When the exceptional nodes become normal, the standalone fault tolerance function will restore the nodes, so that the client requests can gradually distribute traffic to the nodes. The standalone fault tolerance function solves the problem that service failures continue to affect the business, avoids the avalanche effect, reduces the long response time required for manual intervention and increases system availability.\nRunning mechanism:\n Standalone fault tolerance counts the number of calls and the number of exceptions in a time range, and calculates the abnormal rate of IP for each service and the average abnormal rate of the service. When the IP abnormal rate is greater than the service average abnormal rate to a certain ratio, the dimension of the service + ip is degraded. If the weight of the service + ip dimension is not degraded to 0, then when the call of the service + ip dimension is normal, the weight will be restored. The entire calculation and control process proceeds asynchronously, thus not blocking the call.  The standalone fault tolerance is used as follows:\nFaultToleranceConfig faultToleranceConfig = new FaultToleranceConfig(); faultToleranceConfig.setRegulationEffective(true); faultToleranceConfig.setDegradeEffective(true); faultToleranceConfig.setTimeWindow(20); faultToleranceConfig.setWeightDegradeRate(0.5); FaultToleranceConfigManager.putAppConfig(\u0026amp;quot;appName\u0026amp;quot;, faultToleranceConfig);  As above, after the standalone fault tolerance switch is turned on, the application will calculate the exceptions every 20s time window. If a service + IP calling dimension is determined to be a faulty node, the weight of the service + IP will be degraded to 0.5 times.\nFor more detailed parameters, please refer to Standalone troubleshooting\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault-tolerance/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7501b0fac1d1d89c61de0d591e29e1d0","permalink":"/en/projects/sofa-rpc/fault-tolerance/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/fault-tolerance/","summary":"Usually, a service have multiple service providers in a cluster. Some of the service providers may have persistent connections still survived due to network, configuration, long-term fullgc, full thread pool, hardware failure and others, but the program cannot respond properly. The stand-alone fault tolerance function can degrade the exceptional service providers so that the client requests can be pointed to the healthy node. When the exceptional nodes become normal, the standalone fault tolerance function will restore the nodes, so that the client requests can gradually distribute traffic to the nodes.","tags":null,"title":"Fault tolerance","type":"projects","url":"/en/projects/sofa-rpc/fault-tolerance/","wordcount":306},{"author":null,"categories":null,"content":"Fault tolerance automatically monitors the RPC calls, degrades the weight of the failed node, and recovers the weight when the node restored to normal. The bolt protocol is currently supported.\nIn SOFABoot, you only need to configure fault tolerance parameters to application.properties. You can select not to configure all parameters but only configure the parameters that you care about. Then, the remaining parameters will take the default values. Note that rpc.aft.regulation.effective is a global switch for this function. If it is off, the function will not work and other parameters will not take effect.\n   Attribute Description Default value     timeWindow Time window size: the period in which statistics are calculated. 10s   leastWindowCount Minimum number of calls in the time window: Only data that has reached this minimum value in the time window will be added in calculation and control. 10 times   leastWindowExceptionRateMultiple Degradation ratio of the exception rate in the time window to the average exception rate of the service: When calculating the statistical information, the average exception rate of all valid call IPs of the service is calculated. If the exception rate of an IP is greater than or equal to the lowest ratio, the IP will be degraded. 6 times   weightDegradeRate Degradation ratio: The rate of degradation of an address when it is degraded. 1\u0026amp;frasl;20   weightRecoverRate Recovery ratio: The recovery ratio of the address when it is weighted. 2 times   degradeEffective Degradation switch: If the application turns on this switch, it will degrade the address that matches the degradation criteria; otherwise, only the log will be printed. false (off)   degradeLeastWeight Degradation minimum weight: If the address weight is degraded to the weight less than this minimum weight, the minimum weight will be used. 1   degradeMaxIpCount Maximum number of IPs for degradation: The number of IPs in the same service that have been degraded cannot exceed this value. 2   regulationEffective Global switch: If the switch is turned on by the application, the entire standalone fault tolerance function will be turned on; otherwise, this function will not be used at all. false (off)     Example\nCom.alipay.sofa.rpc.aft.time.window=20 Com.alipay.sofa.rpc.aft.least.window.count=30 Com.alipay.sofa.rpc.aft.least.window.exception.rate.multiple=6 Com.alipay.sofa.rpc.aft.weight.degrade.rate=0.5 Com.alipay.sofa.rpc.aft.weight.recover.rate=1.2 Com.alipay.sofa.rpc.aft.degrade.effective=ture Com.alipay.sofa.rpc.aft.degrade.least.weight=1 Com.alipay.sofa.rpc.aft.degrade.max.ip.count=2 Com.alipay.sofa.rpc.aft.regulation.effective=true  As configured above, the fault tolerance function and degradation switch are enabled. When a node fails too many times, its weight is degraded, and during recovery, the weight will be restored. The node healthy is measured every 20s, and the nodes called for more than 30 times in 20s are recognized as calculation data. If the …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration-fault-tolerance/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a132b54b2398534d1773489e2b0db166","permalink":"/en/projects/sofa-rpc/configuration-fault-tolerance/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/configuration-fault-tolerance/","summary":"Fault tolerance automatically monitors the RPC calls, degrades the weight of the failed node, and recovers the weight when the node restored to normal. The bolt protocol is currently supported.\nIn SOFABoot, you only need to configure fault tolerance parameters to application.properties. You can select not to configure all parameters but only configure the parameters that you care about. Then, the remaining parameters will take the default values. Note that rpc.","tags":null,"title":"Fault tolerance configuration","type":"projects","url":"/en/projects/sofa-rpc/configuration-fault-tolerance/","wordcount":487},{"author":null,"categories":null,"content":" Feature architecture SOFABolt provides the following basic features:  Basic communication functions (remoting-core)  Netty-based, highly-effective network I/O and thread model practice Connection management (lock-free connection establishment, timed disconnection, automatic reconnection) Basic communication models (oneway, sync, future, callback) Timeout control Batch unpacking and batch submission processor Heartbeat and IDLE event processing  Protocol framework (protocol-skeleton)  Commands and command processor Coding and decoding processor Heartbeat trigger  Custom private protocol implementation - RPC communication protocol (protocol-implementation)  RPC communication protocol design Flexible deserialization timing control Request processing timeout FailFast mechanism User request processor (UserProcessor) Duplex communication   Usage 1 Use SOFABolt as a remote communication framework. You do not need to consider the details of how to implement a private protocol, just use our built-in RPC communication protocol. You can simply enable the client side and the server side, and simultaneously register a user request processor, thereby completing the remote calling. In addition, basic features such as connection management and heartbeat are available by default. Currently supported call types are shown in the figure below:\n For a sample demonstration, refer to our user guide.  Usage 2 Use SOFABolt as a protocol framework. You can reuse the basic functions of the basic communication model, the interface definitions included in the protocols, etc. Then, according to the private protocol you designed, you can customize the command types, command processors, decoding processors, etc. The RPC and message command definition structure is as shown in the figure below:\n","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-functions/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fde29139cbd8b786326a6479e52814dd","permalink":"/en/projects/sofa-bolt/sofa-bolt-functions/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-bolt/sofa-bolt-functions/","summary":"Feature architecture SOFABolt provides the following basic features:  Basic communication functions (remoting-core)  Netty-based, highly-effective network I/O and thread model practice Connection management (lock-free connection establishment, timed disconnection, automatic reconnection) Basic communication models (oneway, sync, future, callback) Timeout control Batch unpacking and batch submission processor Heartbeat and IDLE event processing  Protocol framework (protocol-skeleton)  Commands and command processor Coding and decoding processor Heartbeat trigger  Custom private protocol implementation - RPC communication protocol (protocol-implementation)  RPC communication protocol design Flexible deserialization timing control Request processing timeout FailFast mechanism User request processor (UserProcessor) Duplex communication   Usage 1 Use SOFABolt as a remote communication framework.","tags":null,"title":"Features","type":"projects","url":"/en/projects/sofa-bolt/sofa-bolt-functions/","wordcount":237},{"author":null,"categories":null,"content":" Features  Service publishing and reference Communication Protocol  Bolt protocol  Basic usage Calling type Timeout control Generic call Serialization protocol Custom thread pool  RESTful protocol  Basic usage Custom filter Integrated Swagger  Dubbo  Basic usage  H2C  Basic usage   Registry center Direct call Load balancing Custom filter Custom router addressing Call retry Tracing  SOFATracer Skywalking  Custom thread pool Link data transparent transmission Warm-up weight Fault tolerance Node cross-language call Graceful shutdown  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/features/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0059809aed46360e8787e945ff098610","permalink":"/en/projects/sofa-rpc/features/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/features/","summary":" Features  Service publishing and reference Communication Protocol  Bolt protocol  Basic usage Calling type Timeout control Generic call Serialization protocol Custom thread pool  RESTful protocol  Basic usage Custom filter Integrated Swagger  Dubbo  Basic usage  H2C  Basic usage   Registry center Direct call Load balancing Custom filter Custom router addressing Call retry Tracing  SOFATracer Skywalking  Custom thread pool Link data transparent transmission Warm-up weight Fault tolerance Node cross-language call Graceful shutdown  ","tags":null,"title":"Features","type":"projects","url":"/en/projects/sofa-rpc/features/","wordcount":68},{"author":null,"categories":null,"content":" 本文描述的是 MOSN 的 FilterChain 配置。\nFilterChain 是 MOSN Listener 配置中核心逻辑配置，不同的 FilterChain 配置描述了 Listener 会如何处理请求。\n目前 MOSN 一个 Listener 只支持一个 FilterChain。\nFilterChain 的配置结构如下所示。\n{ \u0026amp;quot;tls_context\u0026amp;quot;: {}, \u0026amp;quot;tls_context_set\u0026amp;quot;: [], \u0026amp;quot;filters\u0026amp;quot;: [] }  tls_context_set  一组 tls_context 配置，MOSN 默认使用 tls_context_set 来描述 listener 的 TLS 的证书信息。 一个 listener 可同时支持配置多张 TLS 证书。  tls_context  单独配置 tls_context 而不是使用 tls_context_set 是兼容 MOSN 历史配置（只支持一张证书配置时）的场景，这种配置方式后面会逐步废弃。 tls_context 的详细配置说明，参考 tls_context。  filters 一组 network filter 配置。\nnetwork filter network filter 描述了 MOSN 在连接建立以后如何在 4 层处理连接数据。\n{ \u0026amp;quot;type\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;config\u0026amp;quot;: {} }   type 是一个字符串，描述了 filter 的类型。 config 可以是任意 json 配置，描述不同 filter 的配置。 network filter 可自定义扩展实现，默认支持的 type 包括 proxy、tcp proxy、connection_manager。  connection_manager 是一个特殊的 network filter，它需要和 proxy 一起使用，用于描述 proxy 中路由相关的配置，是一个兼容性质的配置，后续可能有修改。   ","date":-62135596800,"description":"","dir":"projects/mosn/configuration/listener/filter-chain/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d79979b85edad41aa6f0b3d8fe3295ee","permalink":"/projects/mosn/configuration/listener/filter-chain/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/configuration/listener/filter-chain/","summary":"本文描述的是 MOSN 的 FilterChain 配置。 FilterChain 是 MOSN Listener 配置中核心逻辑配置，不同的 FilterChain 配置描述了 Listener 会如何处理请求。 目前 MOSN 一个 Listener 只支持一个 FilterChain。 FilterChain 的配","tags":null,"title":"FilterChain 配置","type":"projects","url":"/projects/mosn/configuration/listener/filter-chain/","wordcount":389},{"author":null,"categories":null,"content":" Framework preparation Before reading, you can download and install ACTS IDE and import the ACTS framework by refering to Quick start.\nThis topic mainly describes the encoding, datasource configuration, and quick configuration to help you use the ACTS framework.\nEncoding Ensure that the encoding of ACTS and that of the system code are consistent, specifically, ensure that the encoding for script generation and the encoding of the IDEA workspace are consistent with the encoding of your application code. Otherwise, the code may get corrupted.\nThe encoding selected for test script generation is shown as follows.\nEncoding of the IDEA workspace:\nDatasource configuration The purpose of configuring data sources in ACTS is to ensure that you can use the system\u0026amp;rsquo;s data sources to properly perform database addition, deletion, and query operations during the preparation, clearance, and check stages.\nDatasource configuration Configure the mapping relationship between the ModuleName, datasource, and tables at the DAL layer in src/test/resource/config/acts-config.properties. The name of datasources starts with ds_ as follows:\ndatasource_bundle_name =com.alipay.testapp.common.dal ds_bean1=table1,table2 ds_bean2=table3,table4 #Configuration format #ds_datasource bean=logical table 1,logical table 2  Bean 1 and bean 2 are the names of the datasource beans at the DAL layer of the application code. Multiple datasources are supported. The table name supports regular expressions and sharding suffixes are not required. In the case of multiple datasources, a table must belong to only one datasource. See the following figure.\nDirect JDBC connection to the database The direct JDBC connection to the database is used to generate the DB data model. The configuration in devdb.conf or testdb.conf under src/test/resource/config/dbConf/ is as follows:\nxxx_url = jdbc:oracle:thin:@localhost:1521:cifdb xxx_username = myname xxx_password = mypswd  Quick configuration description The quick test framework configuration mainly generates the basic Java classes and the necessary configuration files:\nJava classes  AppNameActsBaseUtils.java  The utility class that is commonly used in the test script writing process to get various data from the ACTS framework. The initial version provides only common methods. You can add desired methods by yourself.\n AppNameActsTestBase.java  The encapsulated application test base class. If you have special business system requirements, you can encapsulate additional methods based on this class. If not, ignore this file.\nConfiguration files ","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-ready/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c3a89cbf42d55c98206a08e94d05ffde","permalink":"/en/projects/sofa-acts/usage-ready/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-acts/usage-ready/","summary":"Framework preparation Before reading, you can download and install ACTS IDE and import the ACTS framework by refering to Quick start.\nThis topic mainly describes the encoding, datasource configuration, and quick configuration to help you use the ACTS framework.\nEncoding Ensure that the encoding of ACTS and that of the system code are consistent, specifically, ensure that the encoding for script generation and the encoding of the IDEA workspace are consistent with the encoding of your application code.","tags":null,"title":"Framework preparation","type":"projects","url":"/en/projects/sofa-acts/usage-ready/","wordcount":358},{"author":null,"categories":null,"content":" Generic calls provide the ability for clients to initiate calls without having to rely on the server`s interface. Currently, the generic call of SOFARPC only supports using Hessian2 as the serialization protocol under the Bolt communication protocol.\nSOFABoot environment Publish Service There is nothing special about publishing a service. Just publish the service normally, for example:\n\u0026amp;lt;!-- generic --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;sampleGenericServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleGenericServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Reference Service \u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;sampleGenericServiceReference\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.api.GenericService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs generic-interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericService\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  The jvm-first can be left empty according to the actual situation. The interface should be the general interface of generic call. As for the generic-interface, you can just write in the name of the interface to be called.\nInitiate a call GenericService sampleGenericServiceReference = (GenericService) applicationContext .getBean(\u0026amp;quot;sampleGenericServiceReference\u0026amp;quot;); GenericObject genericResult = (GenericObject) sampleGenericServiceReference.$genericInvoke(\u0026amp;quot;sayGeneric\u0026amp;quot;, new String[] { \u0026amp;quot;com.alipay.sofa.rpc.samples.generic.SampleGenericParamModel\u0026amp;quot; }, new Object[] { genericObject });  RPC API ConsumerConfig\u0026amp;lt;GenericService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;GenericService\u0026amp;gt;() .setInterfaceId(\u0026amp;quot;com.alipay.sofa.rpc.quickstart.HelloService\u0026amp;quot;) .setGeneric(true); GenericService testService = consumerConfig.refer(); String result = (String) testService.$invoke(\u0026amp;quot;sayHello\u0026amp;quot;, new String[] { \u0026amp;quot;java.lang.String\u0026amp;quot; },new Object[] { \u0026amp;quot;1111\u0026amp;quot; });  You can set the service as a generic service and set the interface name of the server by setGeneric as above. GenericService is used as a generic service, and GenericService can initiate generic calls. You need to pass in the method name, method type, and method parameters when invoking a call.\nIf the parameter or return result is also required to be generalized on the client side, you can achieve this with GenericObject.\nGenericObject genericObject = new GenericObject(\u0026amp;quot;com.alipay.sofa.rpc.invoke.generic.TestObj\u0026amp;quot;); genericObject.putField(\u0026amp;quot;str\u0026amp;quot;, \u0026amp;quot;xxxx\u0026amp;quot;); genericObject.putField(\u0026amp;quot;num\u0026amp;quot;, 222); GenericObject result = (GenericObject) testService.$genericInvoke(\u0026amp;quot;echoObj\u0026amp;quot;, new String[] { \u0026amp;quot;com.alipay.sofa.rpc.invoke.generic.TestObj\u0026amp;quot; }, new Object[] { genericObject }); String str = result.getField(\u0026amp;quot;str\u0026amp;quot;); String …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/generic-invoke/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"84ac624dc99a42a8f89489aa10304ef7","permalink":"/en/projects/sofa-rpc/generic-invoke/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/generic-invoke/","summary":"Generic calls provide the ability for clients to initiate calls without having to rely on the server`s interface. Currently, the generic call of SOFARPC only supports using Hessian2 as the serialization protocol under the Bolt communication protocol.\nSOFABoot environment Publish Service There is nothing special about publishing a service. Just publish the service normally, for example:\n\u0026lt;!-- generic --\u0026gt; \u0026lt;bean id=\u0026quot;sampleGenericServiceImpl\u0026quot; class=\u0026quot;com.alipay.sofa.rpc.samples.generic.SampleGenericServiceImpl\u0026quot;/\u0026gt; \u0026lt;sofa:service ref=\u0026quot;sampleGenericServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.rpc.samples.generic.SampleGenericService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;/sofa:service\u0026gt;  Reference Service \u0026lt;sofa:reference jvm-first=\u0026quot;false\u0026quot; id=\u0026quot;sampleGenericServiceReference\u0026quot; interface=\u0026quot;com.","tags":null,"title":"Generic call","type":"projects","url":"/en/projects/sofa-rpc/generic-invoke/","wordcount":501},{"author":null,"categories":null,"content":" This document introduces how to use SOFARPC for service publishing and reference in SOFABoot.\nYou can get the code sample of this document by clicking here. Note that the code sample requires a local installation of the zookeeper environment. If not, you need to remove the com.alipay.sofa.rpc.registry.address configuration in application.properties to use the local file as a registry center.\nCreate a project  Prepare environment: SOFABoot requires JDK7 or JDK8 and needs to be compiled with Apache Maven 2.2.5 or above. Build SOFABoot project: SOFABoot is based on Spring Boot. So you can use Spring Boot\u0026amp;rsquo;s project generation tool to generate a standard Spring Boot project. Add SOFABoot dependency: The generated standard Spring Boot project directly uses Spring parent dependency, which should be changed to the parent dependency provided by SOFABoot. The parent dependency provides and manages a variety of starters provided by SOFABoot.  \u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Replace the above with the followings:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;   Configure application.properties: application.properties is the configuration file in SOFABoot project. Here you need to configure the application name.\nspring.application.name=AppName  Introduce RPC starter:\n  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Declare the xsd file of SOFABoot:  In the XML configuration file to be used, configure the declaration of the header xsd file to the followings. This enables development using the XML elements defined by SOFABoot.\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www .w3.org/2001/XMLSchema-instance\u0026amp;quot; xmlns:sofa=\u0026amp;quot;http://sofastack.io/schema/sofaboot\u0026amp;quot; xmlns:context=\u0026amp;quot;http://www.springframework.org/schema/context\u0026amp;quot; xsi:schemaLocation =\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://sofastack.io/schema/sofaboot http://sofastack .io/schema/sofaboot.xsd\u0026amp;quot;  Define service interface and implementation public interface HelloSyncService { String saySync(String string); }  public class HelloSyncServiceImpl implements HelloSyncService { @Override public String saySync(String string) { return string; } }  Publish service on server Configure the followings in the xml file. When the Spring context is refreshed, SOFABoot registers the service implementation on the server, communicates with the client by bolt protocol, and publishes metadata …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/getting-started-with-sofa-boot/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0dd5e0e5116473aee630cba38679d493","permalink":"/en/projects/sofa-rpc/getting-started-with-sofa-boot/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/getting-started-with-sofa-boot/","summary":"This document introduces how to use SOFARPC for service publishing and reference in SOFABoot.\nYou can get the code sample of this document by clicking here. Note that the code sample requires a local installation of the zookeeper environment. If not, you need to remove the com.alipay.sofa.rpc.registry.address configuration in application.properties to use the local file as a registry center.\nCreate a project  Prepare environment: SOFABoot requires JDK7 or JDK8 and needs to be compiled with Apache Maven 2.","tags":null,"title":"Get started with SOFABoot","type":"projects","url":"/en/projects/sofa-rpc/getting-started-with-sofa-boot/","wordcount":437},{"author":null,"categories":null,"content":" This document introduces how to apply SOFARPC for service publishing and reference. This example will simulate a server locally to listen to a port and publish a service, and the client will reference the service for direct call.\nYou can get the code sample of this document by clicking here.\nCreate a project You need to install JDK 6 or above and Maven 3 or above.\nCreate a new Maven project and introduce SOFARPC dependency.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-rpc-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;latest version\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Note: The latest version can be found at https://github.com/sofastack/sofa-rpc/releases.\nWrite a server implementation Step 1: Create interface\n/** * Quick Start demo interface */ public interface HelloService { String sayHello(String string); }  Step 2: Create interface implementation\n/** * Quick Start demo implement */ public class HelloServiceImpl implements HelloService { @Override public String sayHello(String string { System.out.println(\u0026amp;quot;Server receive: \u0026amp;quot; + string); return \u0026amp;quot;hello \u0026amp;quot; + string + \u0026amp;quot; !\u0026amp;quot;; } }  Step 3: Write the server code\n/** * Quick Start Server */ public class QuickStartServer { public static void main(String[] args) { ServerConfig serverConfig = new ServerConfig() .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) // Set a protocol, which is bolt by default .setPort(12200) // set a port, which is 12200 by default .setDaemon(false); // non-daemon thread ProviderConfig\u0026amp;lt;HelloService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) // Specify the interface .setRef(new HelloServiceImpl()) // Specify the implementation .setServer(serverConfig); // Specify the server providerConfig.export (); // Publish service } }  Write a client implementation Step 1: Get the server interface\nIn general, the server provides the interface class to the client in the form of jar. In this example, this step is skipped since the server and client are in the same project.\nStep 2: Write the client code\n/** * Quick Start client */ public class QuickStartClient { public static void main(String[] args) { ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) // Specify the interface .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) // Specify the protocol.setDirectUrl .setDirectUrl(\u0026amp;quot;bolt://127.0.0.1:12200\u0026amp;quot;); // Specify the direct connection address // Generate the proxy class HelloService helloService = consumerConfig.refer(); while (true) { System.out.println(helloService.sayHello(\u0026amp;quot;world\u0026amp;quot;)); try { Thread.sleep(2000); } catch (Exception e) { } } } }  Run Start the server and client separately.\nThe server outputs:\n Server receive: The world\n The client outputs:\n hello world !\n More For more examples, please refer to: example\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/getting-started-with-rpc/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"192d252b0b36266622284b68d10e9fe4","permalink":"/en/projects/sofa-rpc/getting-started-with-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/getting-started-with-rpc/","summary":"This document introduces how to apply SOFARPC for service publishing and reference. This example will simulate a server locally to listen to a port and publish a service, and the client will reference the service for direct call.\nYou can get the code sample of this document by clicking here.\nCreate a project You need to install JDK 6 or above and Maven 3 or above.\nCreate a new Maven project and introduce SOFARPC dependency.","tags":null,"title":"Get started with SOFARPC","type":"projects","url":"/en/projects/sofa-rpc/getting-started-with-rpc/","wordcount":371},{"author":null,"categories":null,"content":" Graceful shutdown includes two parts. One is the RPC framework as client, and the other is the RPC framework as server.\nAs server As the server, the RPC framework should not be violently shutdown.\ncom.alipay.sofa.rpc.context.RpcRuntimeContext  Added a ShutdownHook to the static initialization snippet:\n// Add jvm shutdown event if (RpcConfigs.getOrDefaultValue(RpcOptions.JVM_SHUTDOWN_HOOK, true)) { Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { @Override public void run() { if (LOGGER.isWarnEnabled()) { LOGGER.warn(\u0026amp;quot;SOFA RPC Framework catch JVM shutdown event, Run shutdown hook now.\u0026amp;quot;); } destroy(false); } }, \u0026amp;quot;SOFA-RPC-ShutdownHook\u0026amp;quot;)); }  The logic in ShutdownHook is executed first when the publishing platform or users run the following method kill pid. In the destroy operation, the RPC framework first performs actions such as canceling service registration to the registry center and closing the service port.\nprivate static void destroy(boolean active) { RpcRunningState.setShuttingDown (true); for (Destroyable.DestroyHook destroyHook : DESTROY_HOOKS) { destroyHook.preDestroy(); } List\u0026amp;lt;ProviderConfig\u0026amp;gt; providerConfigs = new ArrayList\u0026amp;lt;ProviderConfig\u0026amp;gt;(); for (ProviderBootstrap bootstrap : EXPORTED_PROVIDER_CONFIGS) { providerConfigs.add(bootstrap.getProviderConfig()); } // First, unregister the server List\u0026amp;lt;Registry\u0026amp;gt; registries = RegistryFactory.getRegistries(); if (CommonUtils.isNotEmpty(registries) \u0026amp;amp;\u0026amp;amp; CommonUtils.isNotEmpty(providerConfigs)) { for (Registry registry : registries) { registry.batchUnRegister(providerConfigs); } } / / Shut down the port that has been started ServerFactory.destroyAll(); // Close the published service for (ProviderBootstrap bootstrap : EXPORTED_PROVIDER_CONFIGS) { bootstrap.unExport(); } // Close the called service for (ConsumerBootstrap bootstrap : REFERRED_CONSUMER_CONFIGS) { ConsumerConfig config = bootstrap.getConsumerConfig(); If (!CommonUtils.isFalse(config.getParameter(RpcConstants.HIDDEN_KEY_DESTROY))) { // Unless you do not let the active unrefer bootstrap.unRefer(); } } // Shut down the registry center RegistryFactory.destroyAll(); / / Close some public resources of the client ClientTransportFactory.closeAll(); // Uninstall the module if (!RpcRunningState.isUnitTestMode()) { ModuleFactory.uninstallModules(); } // Uninstall the hook for (Destroyable.DestroyHook destroyHook : DESTROY_HOOKS) { destroyHook.postDestroy(); } // Clean up the cache RpcCacheManager.clearAll(); RpcRunningState.setShuttingDown (false); if (LOGGER.isWarnEnabled()) { LOGGER.warn(\u0026amp;quot;SOFA RPC Framework has been release all resources {}...\u0026amp;quot;, active ? \u0026amp;quot;actively \u0026amp;quot; : \u0026amp;quot;\u0026amp;quot;); } }  Taking bolt as an example, closing the port is not an immediate action.\n@Override public void destroy() { if (!started) { return; } int stopTimeout = serverConfig.getStopTimeout(); If (stopTimeout \u0026amp;gt; 0) { // need to wait for the end time AtomicInteger count = …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/graceful-shutdown/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"53af179e23ba184b01eb8234c055b15d","permalink":"/en/projects/sofa-rpc/graceful-shutdown/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-rpc/graceful-shutdown/","summary":"Graceful shutdown includes two parts. One is the RPC framework as client, and the other is the RPC framework as server.\nAs server As the server, the RPC framework should not be violently shutdown.\ncom.alipay.sofa.rpc.context.RpcRuntimeContext  Added a ShutdownHook to the static initialization snippet:\n// Add jvm shutdown event if (RpcConfigs.getOrDefaultValue(RpcOptions.JVM_SHUTDOWN_HOOK, true)) { Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { @Override public void run() { if (LOGGER.isWarnEnabled()) { LOGGER.warn(\u0026quot;SOFA RPC Framework catch JVM shutdown event, Run shutdown hook now.","tags":null,"title":"Graceful shutdown","type":"projects","url":"/en/projects/sofa-rpc/graceful-shutdown/","wordcount":669},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA) and Locate on hmily-demo-grpc Module Configuring（hmily-demo-grpc-accoun module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   run GrpcHmilyAccountApplication.java  Run hmily-demo-tars-springboot-inventory(refer to simillar instructions above). Run hmily-demo-tars-springboot-order(refer to simillar instructions above). Access on http://127.0.0.1:28087/swagger-ui.html for more. ","date":-62135596800,"description":"Grpc Quick Start","dir":"projects/hmily/quick-start-grpc/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a7e77647d234ed18a0a917bc12e9b229","permalink":"/en/projects/hmily/quick-start-grpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/quick-start-grpc/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA) and Locate on hmily-demo-grpc Module Configuring（hmily-demo-grpc-accoun module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026lt;db_host_ip\u0026gt;:\u0026lt;db_host_port\u0026gt;/hmily_account?useUnicode=true\u0026amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.","tags":null,"title":"Grpc Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-grpc/","wordcount":142},{"author":null,"categories":null,"content":" Grpc User Guide  Unary synchronous calls to GRPC are supported only at present.\n Introduce the jar packages\n Introduce the Hmily configuration\n Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n  Introduce The Maven dependency Spring-Namespace\n Introduce the hmily-grpc dependency   \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-grpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   make the configuration in the XML configuration file as below:\n\u0026amp;lt;!--Configure the base packages that the Hmily framework need to scan --\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Configure the bean parameters for Hmily startup --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot\n Introduce the hmily-spring-boot-starter-grpc dependency\nxml \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-grpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;    Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  Grpc Filter configuration  Add filter GrpcHmilyTransactionFilter to GRPC client. Add filter GrpcHmilyServerFilter to GRPC server.  Grpc client call  Use the GrpcHmilyClient to make the remote call instead of the original call.  AccountResponse response = GrpcHmilyClient.syncInvoke(accountServiceBlockingStub, \u0026amp;quot;payment\u0026amp;quot;, request, AccountResponse.class);  The input parameters are the called AbstratcSub, the method name, the specific parameter, and the return value type,respectively.\nTCC Mode  Add the @HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation on the concrete implementation of the transaction method on the server side.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be consistent with the identification method.\n The TCC mode should ensure the idempotence of the confirm and cancel methods,Users need to develop these two methods by themselves,The confirmation and rollback behavior of all transactions are completely up tp users.The Hmily framework is just responsible for making calls.\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) …","date":-62135596800,"description":"Grpc User Guide","dir":"projects/hmily/user-grpc/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"175826163ccd507408bea8b4c9a51017","permalink":"/en/projects/hmily/user-grpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/user-grpc/","summary":"Grpc User Guide  Unary synchronous calls to GRPC are supported only at present.\n Introduce the jar packages\n Introduce the Hmily configuration\n Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n  Introduce The Maven dependency Spring-Namespace\n Introduce the hmily-grpc dependency   \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-grpc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   make the configuration in the XML configuration file as below:\n\u0026lt;!--Configure the base packages that the Hmily framework need to scan --\u0026gt; \u0026lt;context:component-scan base-package=\u0026quot;org.","tags":null,"title":"Grpc User Guide","type":"projects","url":"/en/projects/hmily/user-grpc/","wordcount":367},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git Zookeeper  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n使用你的工具 idea 打开项目，找到hmily-demo-grpc项目。 修改项目配置（hmily-demo-grpc-account为列子）  修改业务数据库(account项目为列子)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: #改成你的用户名 password: #改成你的密码   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   run GrpcHmilyAccountApplication.java  启动hmily-demo-tars-springboot-inventory 参考上述。 启动hmily-demo-tars-springboot-order 参考上述。 访问：http://127.0.0.1:28087/swagger-ui.html。 ","date":-62135596800,"description":"Grpc快速体验","dir":"projects/hmily/quick-start-grpc/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a7e77647d234ed18a0a917bc12e9b229","permalink":"/projects/hmily/quick-start-grpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/quick-start-grpc/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git Zookeeper 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 使用你的工具 idea 打开项目，找到hmily-dem","tags":null,"title":"Grpc快速体验","type":"projects","url":"/projects/hmily/quick-start-grpc/","wordcount":480},{"author":null,"categories":null,"content":" Grpc用户指南  目前只支持grpc的一元同步调用\n 引入jar包\n 引入hmily配置\n 在具体的实现方法上（服务提供端），加上@HmilyTCC or HmilyTAC 注解\n  引入依赖 Spring-Namespace\n 引入依赖\n  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-grpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在xml中进行如下配置\n\u0026amp;lt;!--配置扫码hmily框架的包--\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--配置Hmily启动的bean参数--\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot\n 引入依赖\nxml \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-grpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;    引入Hmily配置  在项目的 resource 新建文件名为: hmily.ym 配置文件。\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  Grpc拦截器配置  在 grpc客户端中添加拦截器 GrpcHmilyTransactionFilter。 在 grpc服务端中添加拦截器 GrpcHmilyServerFilter。  Grpc客户端调用  使用GrpcHmilyClient代替原来的调用方式来进行远程调用。  AccountResponse response = GrpcHmilyClient.syncInvoke(accountServiceBlockingStub, \u0026amp;quot;payment\u0026amp;quot;, request, AccountResponse.class);  入参分别为被调用的AbstratcSub,方法名,具体的参数以及返回值类型\nTCC模式  对服务端事务方法的具体实现,加上@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  重要注意事项 异常  try, confirm, cancel 方法的所有异常不要自行catch 任何异常都应该抛出给 Hmily框架处理。  ","date":-62135596800,"description":"Grpc用户指南","dir":"projects/hmily/user-grpc/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"175826163ccd507408bea8b4c9a51017","permalink":"/projects/hmily/user-grpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/user-grpc/","summary":"Grpc用户指南 目前只支持grpc的一元同步调用 引入jar包 引入hmily配置 在具体的实现方法上（服务提供端），加上@HmilyTCC or HmilyTAC 注","tags":null,"title":"Grpc用户指南","type":"projects","url":"/projects/hmily/user-grpc/","wordcount":728},{"author":null,"categories":null,"content":" H2C protocol SOFARPC provides support for the H2C protocol, which can be used to publish and reference services.\n Basic usage  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/h2c/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"45b6d6ba0ca1ae7f6d415d79c184f766","permalink":"/en/projects/sofa-rpc/h2c/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/h2c/","summary":" H2C protocol SOFARPC provides support for the H2C protocol, which can be used to publish and reference services.\n Basic usage  ","tags":null,"title":"H2C","type":"projects","url":"/en/projects/sofa-rpc/h2c/","wordcount":20},{"author":null,"categories":null,"content":"SOFARPC 提供了 H2C 协议的支持，可以可以采用 H2C 协议来进行服务的发布和引用 * 基本使用\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/h2c/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"45b6d6ba0ca1ae7f6d415d79c184f766","permalink":"/projects/sofa-rpc/h2c/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/h2c/","summary":"SOFARPC 提供了 H2C 协议的支持，可以可以采用 H2C 协议来进行服务的发布和引用 * 基本使用","tags":null,"title":"H2C","type":"projects","url":"/projects/sofa-rpc/h2c/","wordcount":36},{"author":null,"categories":null,"content":" 在 SOFARPC 中，使用不同的通信协议只要设置使用不同的 Binding 即可，如果需要使用 H2C 协议，只要将 Binding 设置为 H2C 即可。下面使用以注解的方式来例举，其他的使用方式可以参考 Bolt 协议基本使用，这里不再重复说明。：\n发布服务 发布一个 H2C 的服务，只需要将 @SofaServiceBinding 的 bindingType 设置为 h2c 即可：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;h2c\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  引用服务 引用一个 H2C 的服务，只需要将 @SofaReferenceBinding 的 bindingType 设置为 h2c 即可：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;h2c\u0026amp;quot;), jvmFirst = false) private SampleService sampleService;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/h2c-usage/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"fa75eff1e99b3acad5087160a1b44a09","permalink":"/projects/sofa-rpc/h2c-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/h2c-usage/","summary":"在 SOFARPC 中，使用不同的通信协议只要设置使用不同的 Binding 即可，如果需要使用 H2C 协议，只要将 Binding 设置为 H2C 即可。下面使用以注解的方式来例举，其他的使用方式可以","tags":null,"title":"H2C 协议基本使用","type":"projects","url":"/projects/sofa-rpc/h2c-usage/","wordcount":168},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"","dir":"projects/sofa-rpc/http/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0e92b5faec8584280cc296255f3a4541","permalink":"/en/projects/sofa-rpc/http/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/en/projects/sofa-rpc/http/","summary":"","tags":null,"title":"HTTP","type":"projects","url":"/en/projects/sofa-rpc/http/","wordcount":0},{"author":null,"categories":null,"content":" SOFABoot provides Readiness Check to enhance Spring Boot\u0026amp;rsquo;s Health Check. If you need to use the SOFA middleware, you are advised to use the Health Check extension of SOFABoot to launch application examples in a more elegant way.\nEnable Health Check To enable the Health Check feature in SOFABoot, you only need to import the following starter:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;healthcheck-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Without the Health Check extension, users still can perform Liveness Check with native Spring Boot Actuator directly relying on the HealthIndicator interface.\nSecurity alert From SOFABoot 2.3.0 on, the Health Check depends on the Actuator component in SpringBoot 1.4.x, and the component opens a lot of EndPoint such as \u0026amp;lsquo;/dump \u0026amp;rsquo; and \u0026amp;lsquo;/trace\u0026amp;rsquo;. So there may be a security risk. Refer to the Security Recommendations in the official document for settings.\nSpringBoot 1.5.x and SpringBoot 2.x. have fixed some security issues. SOFABoot will be supported by upgrading the SpringBoot kernel.\nView Health Check results After adding the Health Check extension, you can directly browser http://localhost:8080/health/readiness to view the Readiness Check results. To view the Liveness Check results, access the URL of the Spring Boot Health Check results. http://localhost:8080/health。\nIn SOFABoot, you can also view Health Check results by checking the specific logs in the health-check directory. Generally, such logs contain the following content:\n2018-04-06 23:29:50,240 INFO main - Readiness check result: success  At present, the SOFA middleware has controlled upstream traffic access through the Readiness Check offered by SOFABoot. But, apart from the middleware, traffic of an application may come from other sources such as the load balancer. To control such traffic, users are advised to view the Readiness Check results by PAAS and determine whether to launch corresponding nodes in the load balancer based on the results.\n** Note: Versions after SOFABoot 2.x no longer indirectly introduce spring-boot-starter-web dependencies. To view Health Check results in the browser, you need to introduce Web container dependencies in the project. **\n** Note: In SOFABoot 3.x, the endpoint path has been changed from health/readiness to actuator/readiness**\nReadiness Check extension SOFABoot allows extension in every phase of the Readiness Check. Applications can be extended according to their needs. In version 2.x, the extendable points are as follows:\n   Callback interface Description     org.springframework.context.ApplicationListener If you want to do something before the Readiness Check, you can monitor the SofaBootBeforeHealthCheckEvent event of this listener.   org.springframework.boot.actuate.health.HealthIndicator If you want to add a check item to the Readiness Check in SOFABoot, you can directly extend this interface of Spring Boot. …","date":-62135596800,"description":"","dir":"projects/sofa-boot/health-check/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a366b25125fa4aedb08a9cef572db1c8","permalink":"/en/projects/sofa-boot/health-check/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/health-check/","summary":"SOFABoot provides Readiness Check to enhance Spring Boot\u0026rsquo;s Health Check. If you need to use the SOFA middleware, you are advised to use the Health Check extension of SOFABoot to launch application examples in a more elegant way.\nEnable Health Check To enable the Health Check feature in SOFABoot, you only need to import the following starter:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;healthcheck-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Without the Health Check extension, users still can perform Liveness Check with native Spring Boot Actuator directly relying on the HealthIndicator interface.","tags":null,"title":"Health check","type":"projects","url":"/en/projects/sofa-boot/health-check/","wordcount":707},{"author":null,"categories":null,"content":" @Hmily /** * The annotation Hmily. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Hmily { }   This annotation is the interface identification of Hmily Distributed Transaction,it indicated that the interface participates in Hmily Distributed Transaction.  @HmilyTCC @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTCC { /** * Confirm method string. * * @return the string */ String confirmMethod() default \u0026amp;quot;\u0026amp;quot;; /** * Cancel method string. * * @return the string */ String cancelMethod() default \u0026amp;quot;\u0026amp;quot;; /** * Pattern pattern enum. * * @return the pattern enum */ TransTypeEnum pattern() default TransTypeEnum.TCC; }   This annotation is the AOP point of TCC Mode of Hmily Distributed Transaction,it can add on your local concrete implementation method.\n confirmMethod : to annotate the identification method,it is the method name for confirm,the method parameter list and return type should be consistent with the identification method.\n cancelMethod : to annotate the identification method,it is the method name for rollback,the method parameter list and return type should be consistent with the identification method.\n  @HmilyTAC /** * The annotation HmilyTAC. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTAC { }   This annotation is the AOP point of TAC Mode of Hmily Distributed Transaction,it can add on your local concrete implementation method.  ","date":-62135596800,"description":"Hmily Annotation","dir":"projects/hmily/annotation/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"03ef60b752c3f007c96118cfd9cdc1b2","permalink":"/en/projects/hmily/annotation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/annotation/","summary":"@Hmily /** * The annotation Hmily. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Hmily { }   This annotation is the interface identification of Hmily Distributed Transaction,it indicated that the interface participates in Hmily Distributed Transaction.  @HmilyTCC @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTCC { /** * Confirm method string. * * @return the string */ String confirmMethod() default \u0026quot;\u0026quot;; /** * Cancel method string. * * @return the string */ String cancelMethod() default \u0026quot;\u0026quot;; /** * Pattern pattern enum.","tags":null,"title":"Hmily Annotation","type":"projects","url":"/en/projects/hmily/annotation/","wordcount":203},{"author":null,"categories":null,"content":" Hmily Metrics At present,Prometheus is used to collect metrics in hmily\u0026amp;rsquo;s metrics module, and the pull mode is used to expose metrics information interface.\nThe metrics collected fall into two fundamental categories:\n JVM information to application: Memory, CPU, Thread Usage, etc.\n Transaction information: including the transactions total, the transaction latency, the transaction status, the transaction role.\n  Hmily Metrics in detail How to show  You can pull the metrics information from the metrics configuration of application via Grafana.  ","date":-62135596800,"description":"Hmily Metrics","dir":"projects/hmily/metrics/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ac25a8a8d3dbee4303f272ae2fa5eeed","permalink":"/en/projects/hmily/metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/metrics/","summary":"Hmily Metrics At present,Prometheus is used to collect metrics in hmily\u0026rsquo;s metrics module, and the pull mode is used to expose metrics information interface.\nThe metrics collected fall into two fundamental categories:\n JVM information to application: Memory, CPU, Thread Usage, etc.\n Transaction information: including the transactions total, the transaction latency, the transaction status, the transaction role.\n  Hmily Metrics in detail How to show  You can pull the metrics information from the metrics configuration of application via Grafana.","tags":null,"title":"Hmily Metrics","type":"projects","url":"/en/projects/hmily/metrics/","wordcount":77},{"author":null,"categories":null,"content":" Project members (the names not listed in order)    Name github Role Company     Xiao Yu yu199195 VP JD   Zhang Yong Lun tuohai666 committer JD   Zhao Jun cherrylzhao committer China Unicom   Chen Bin prFor committer A startup company   Jiang Xiao Feng SteNicholas committer Alibaba Cloud   Li Lang cysy-lli committer Ctrip   Tang Yi Dong tydhot committer perfma    ","date":-62135596800,"description":"Hmily Team","dir":"projects/hmily/team/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7d27bab44ea88b422afcda3ff9b66b36","permalink":"/en/projects/hmily/team/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/team/","summary":" Project members (the names not listed in order)    Name github Role Company     Xiao Yu yu199195 VP JD   Zhang Yong Lun tuohai666 committer JD   Zhao Jun cherrylzhao committer China Unicom   Chen Bin prFor committer A startup company   Jiang Xiao Feng SteNicholas committer Alibaba Cloud   Li Lang cysy-lli committer Ctrip   Tang Yi Dong tydhot committer perfma    ","tags":null,"title":"Hmily Team","type":"projects","url":"/en/projects/hmily/team/","wordcount":54},{"author":null,"categories":null,"content":" Term  Initiator: The initiator of a global transaction, the first place where transactions need to be performed on distributed resources in a request link resource method. In the Hmily framework, it can be expressed as: a request first encounters @HmilyTCC or @HmilyTAC annotated method, the method which application belongs to is called the initiator.\n Participants: Distributed services or resources that need to participate in a distributed transaction scenario together with other services. In the Hmily framework, it\u0026amp;rsquo;s showed as an interface that appears as an RPC framework is annotated with @Hmily.\n Coordinator: The role used to coordinate distributed transactions is commit or rollback. It can be remote, local, centralized, or decentralized. The coordinator in the Hmily framework is a local decentralized role.\n TCC ：Abbreviation for the three stages of Try, Confirm, and Cancel.\n TAC ：Short for Try Auto Cancel. Hmily framework will automatically generate the reverse operation resource behavior after the resources are reserved in the Try stage.\n  ","date":-62135596800,"description":"Hmily Term","dir":"projects/hmily/term/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8a22c263ed5f16b93fa1175383094d9d","permalink":"/en/projects/hmily/term/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/term/","summary":"Term  Initiator: The initiator of a global transaction, the first place where transactions need to be performed on distributed resources in a request link resource method. In the Hmily framework, it can be expressed as: a request first encounters @HmilyTCC or @HmilyTAC annotated method, the method which application belongs to is called the initiator.\n Participants: Distributed services or resources that need to participate in a distributed transaction scenario together with other services.","tags":null,"title":"Hmily Term","type":"projects","url":"/en/projects/hmily/term/","wordcount":158},{"author":null,"categories":null,"content":" Hmily Transaction Context @Data public class HmilyTransactionContext { /** * transId. */ private Long transId; /** * participant id. */ private Long participantId; /** * participant ref id. */ private Long participantRefId; /** * this hmily action. */ private int action; /** * Transaction Participant Role. */ private int role; /** * transType. */ private String transType; }  HmilyTransactionContext is the core class used by the Hmily distributed transaction framework to pass the transaction context when making RPC calls. it was stored in \u0026amp;lsquo;ThreadLocal\u0026amp;rsquo; by default and then do RPC parameter passing. You can also configure it to the scenarios that use thread context switching. the contextTransmittalMode = transmittable should be specified in the configuration at this point, then the Alibaba opensource library will be used.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;transmittable-thread-local\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.11.5\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Passing Transaction Context with Dubbo The concrete implementation class was in the org.dromara.hmily.dubbo.filter.DubboHmilyTransactionFilter class.， Please do RPC parameter passing through the RpcContext.getContext().setAttachment(String key, String value).\nPassing Transaction Context with Motan The concrete implementation class was in the org.dromara.hmily.motan.filter.MotanHmilyTransactionFilter class.， Please do RPC parameter passing through the Request.setAttachment(String key, String value).\nPassing Transaction Context with Spring Cloud The concrete implementation class was in the org.dromara.hmily.springcloud.feign.HmilyFeignInterceptor class.， Please do RPC parameter passing through the RequestTemplate.header(String name, String... values).\n","date":-62135596800,"description":"Hmily Transaction Context","dir":"projects/hmily/context/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"29b5b0d8bf36fca4d23e1dadbc22824e","permalink":"/en/projects/hmily/context/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/context/","summary":"Hmily Transaction Context @Data public class HmilyTransactionContext { /** * transId. */ private Long transId; /** * participant id. */ private Long participantId; /** * participant ref id. */ private Long participantRefId; /** * this hmily action. */ private int action; /** * Transaction Participant Role. */ private int role; /** * transType. */ private String transType; }  HmilyTransactionContext is the core class used by the Hmily distributed transaction framework to pass the transaction context when making RPC calls.","tags":null,"title":"Hmily Transaction Context","type":"projects","url":"/en/projects/hmily/context/","wordcount":207},{"author":null,"categories":null,"content":" Hmily是什么？ Hmily是一款高性能，零侵入，金融级分布式事务解决方案，目前主要提供柔性事务的支持，包含 TCC, TAC(自动生成回滚SQL) 方案，未来还会支持 XA 等方案。\n功能  高可靠性 ：支持分布式场景下，事务异常回滚，超时异常恢复，防止事务悬挂。\n 易用性 ：提供零侵入性式的 Spring-Boot, Spring-Namespace 快速与业务系统集成。\n 高性能 ：去中心化设计，与业务系统完全融合，天然支持集群部署。\n 可观测性 ：Metrics多项指标性能监控，以及admin管理后台UI展示。\n 多种RPC ： 支持 Dubbo, SpringCloud,Motan, brpc, tars 等知名RPC框架。\n 日志存储 ： 支持 mysql, oracle, mongodb, redis, zookeeper 等方式。\n 复杂场景 ： 支持RPC嵌套调用事务。\n  必要前提  必须使用 JDK8+\n TCC模式下，用户必须要使用一款 RPC 框架, 比如 : Dubbo, SpringCloud,Motan\n TAC模式下，用户必须使用关系型数据库, 比如：mysql, oracle, sqlsever\n  TCC模式 当使用TCC模式的时候,用户根据自身业务需求提供 try, confirm, cancel 等三个方法， 并且 confirm, cancel 方法由自身完成实现，框架只是负责来调用，来达到事务的一致性。\nTAC模式 当用户使用TAC模式的时候，用户必须使用关系型数据库来进行业务操作，框架会自动生成回滚SQL, 当业务异常的时候，会执行回滚SQL来达到事务的一致性\n关于Hmily Hmily是柔性分布式事务解决方案，提供了TCC 与 TAC 模式。\n它以零侵入以及快速集成方式能够方便的被业务进行整合。\n在性能上，日志存储异步（可选）以及使用异步执行的方式，不损耗业务方法方法。\n之前是由我个人开发，目前在京东数科重启，未来会成为京东数科的分布式事务解决方案。\n技术支持  如有任何问题欢迎加入QQ群进行讨论  微信公众号   ","date":-62135596800,"description":"","dir":"projects/hmily/overview/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"640bae80ac20018dd13658f8b7021ab1","permalink":"/projects/hmily/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/overview/","summary":"Hmily是什么？ Hmily是一款高性能，零侵入，金融级分布式事务解决方案，目前主要提供柔性事务的支持，包含 TCC, TAC(自动生成回滚SQL) 方","tags":null,"title":"Hmily 介绍","type":"projects","url":"/projects/hmily/overview/","wordcount":645},{"author":null,"categories":null,"content":" Hmily-Admin startup tutorial (not completed）:  Admin is the background management system for viewing transaction logs in Hmily. It has many features, Such as viewing abnormal logs, modifying the number of retries and so on.\n First, make sure that your project is using Hmily and is running properly.\n Second, the JDK used by the user must be 1.8+. Git and Maven are installed locally, then execute the following command\n  Step 2：Modify index.html under the static folder of your project \u0026amp;lt;!--href use your ip and port--\u0026amp;gt; \u0026amp;lt;a id=\u0026amp;quot;serverIpAddress\u0026amp;quot; style=\u0026amp;quot;display: none\u0026amp;quot; href=\u0026amp;quot;http://192.168.1.132:8888/admin\u0026amp;quot;\u0026amp;gt;  Step 3: Run the main method in AdminApplication Step 4: Visit http://ip:port/tcc-admin/index.html in the browser, then enter the user name and password to login. If you have any questions, please join the QQ group: 162614487 for discussion ","date":-62135596800,"description":"Hmily-Admin","dir":"projects/hmily/admin/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7f471e3463f5cd785a11145907248bad","permalink":"/en/projects/hmily/admin/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/admin/","summary":"Hmily-Admin startup tutorial (not completed）:  Admin is the background management system for viewing transaction logs in Hmily. It has many features, Such as viewing abnormal logs, modifying the number of retries and so on.\n First, make sure that your project is using Hmily and is running properly.\n Second, the JDK used by the user must be 1.8+. Git and Maven are installed locally, then execute the following command","tags":null,"title":"Hmily-Admin","type":"projects","url":"/en/projects/hmily/admin/","wordcount":126},{"author":null,"categories":null,"content":" Hmily-Admin 启动教程（未完成）:  admin 是Hmily中查看事务日志的后台管理系统。 可以查看异常的日志，修改重试次数等功能.\n 首先确保你的项目使用了Hmily并且正常运行.\n 首先用户使用的JDK必须是1.8+ 本地安装了git ,maven ，执行以下命令\n  步骤二：修改本项目static 文件夹下的 index.html \u0026amp;lt;!--href 修改成你的ip 端口--\u0026amp;gt; \u0026amp;lt;a id=\u0026amp;quot;serverIpAddress\u0026amp;quot; style=\u0026amp;quot;display: none\u0026amp;quot; href=\u0026amp;quot;http://192.168.1.132:8888/admin\u0026amp;quot;\u0026amp;gt;  步骤三: 运行 AdminApplication 中的main方法。 步骤四:在浏览器访问 http://ip:port/tcc-admin/index.html ,输入用户名，密码登录。 如有任何问题欢迎加入QQ群：162614487 进行讨论 ","date":-62135596800,"description":"Hmily-Admin","dir":"projects/hmily/admin/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7f471e3463f5cd785a11145907248bad","permalink":"/projects/hmily/admin/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/admin/","summary":"Hmily-Admin 启动教程（未完成）: admin 是Hmily中查看事务日志的后台管理系统。 可以查看异常的日志，修改重试次数等功能. 首先确保你的项目使用了Hmily并","tags":null,"title":"Hmily-Admin","type":"projects","url":"/projects/hmily/admin/","wordcount":217},{"author":null,"categories":null,"content":" Configuration Detail：  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n  hmily: server: configMode: local appName: xiaoyu # The following configuration will be read when server.configMode equals local config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:  Hmily.server Configuration |Name | Type | Default | Required | Description | |:\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash; |:\u0026amp;mdash;\u0026amp;ndash; |:\u0026amp;mdash;\u0026amp;mdash;-: |:\u0026amp;mdash;\u0026amp;mdash;-:|:\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;\u0026amp;mdash;-| |configMode |String | local | yes |Configuration mode supports local,zookeeper,nacos, and apollo now. If configuration is local, it will read the configuration from yml file;If the configuration is other modes, it will get the configuration form configuration centre. |appName |String | null | yes |Application name. AppName will be overwritten if also configured in hmilyConfig.\nHmily.config Configuration  This is the core configuration of the whole framework     Name Type Default Required Description     appName String null yes It filled in your Microservices application\u0026amp;rsquo;s name generally, but don\u0026amp;rsquo;t repeat it   serializer String kryo no This is mode of serializing the specified transaction log，and currently supports filling in kryo, hessian, jdk, jdk, protostuff   contextTransmittalMode String threadLocal no This is the mode of transaction context transfer, and currently supports filling in threadLocal, transmittable (Cross-thread mode)   scheduledThreadMax int CPU * 2 no Maximum number of scheduled threads   scheduledRecoveryDelay int(unit:sec) 60 no Scheduling cycle of auto recovering transaction log …","date":-62135596800,"description":"Hmily Configuration Detail","dir":"projects/hmily/config/","fuzzywordcount":1300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4a00b98dda1404f77a49b1ec28ba179f","permalink":"/en/projects/hmily/config/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/hmily/config/","summary":"Configuration Detail：  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n  hmily: server: configMode: local appName: xiaoyu # The following configuration will be read when server.configMode equals local config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.","tags":null,"title":"Hmily-Config","type":"projects","url":"/en/projects/hmily/config/","wordcount":1263},{"author":null,"categories":null,"content":" 配置详解：  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n  hmily: server: configMode: local appName: xiaoyu # 如果server.configMode eq local 的时候才会读取到这里的配置信息. config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:  hmily.server配置    名称 类型 默认值 是否必填 说明     configMode String local 必填 配置模式，现在支持local,zookeeper,nacos,apollo,配置为local，则会读取yml文件里的配置，其他模式，则会读取配置中心的   appName String 无 必填 应用的名称，如果hmilyConfig中也配置了appName则会覆盖此配置    hmily.config配置  这是整个框架的核心配置     名称 类型 默认值 是否必填 说明     appName String 无 必填 一般填你微服务的应用名称，请不要重复   serializer String kryo 非必填 这是指定事务日志的序列化方式，目前支持填写 kryo, hessian, jdk, jdk, protostuff   contextTransmittalMode String threadLocal 非必填 这是事务上下文传递的模式，目前支持填写 threadLocal, transmittable (跨线程模式)   scheduledThreadMax int CPU * 2 非必填 调度线程数最大线程数量   scheduledRecoveryDelay int(单位:秒) 60 非必填 事务日志自动恢复调度周期   scheduledCleanDelay int(单位:秒) 60 非必填 事务日志清理调度周期   scheduledPhyDeletedDelay int(单位:秒) 60 非必填 事务日志物理删除调度周期   scheduledInitDelay int(单位:秒) 30 非必填 调度任务启动延迟时间   recoverDelayTime int(单位:秒) 60 非必填 事务日志恢复迟延时间   cleanDelayTime int(单位:秒) 60 非必填 事务日志清理迟延时间   limit int 100 非必填 获取事务日志行数大小   retryMax int 10 非必填 最大重试次数   bufferSize int 4096 * 2 * 2 非必填 disruptor的bufferSize大小   consumerThreads int CPU * 2 非必填 disruptor消费者线程数量   asyncRepository boolean true 非必填 是否异步存储事务日志，设置为false则为同步   autoSql boolean true 非必填 是否自动执行框架自动建库建表SQL语句（如果已经创建可以设置为false）   phyDeleted boolean true 非必填 在运行过程中，是否物理删除日志。设置为false，则只会更改日志状态   storeDays int(单位:天) 3 非必填 如果 phyDeleted 设置为false的时候，日志存储天数   repository String mysql 必填 这是指定事务日志的存储方式，目前支持填写 mysql, oracle, postgresql, sqlserver, mongo, redis, file    repository配置 repository是Hmily用来存储事务日志的配置，目前支持:database(mysql, oracle, postgresql, sqlserver), file(本地模式，测试，开发环境用), mongodb, zookeeper, redis。\n database配置(默认使用hikari连接池): …","date":-62135596800,"description":"Hmily配置详解","dir":"projects/hmily/config/","fuzzywordcount":2600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4a00b98dda1404f77a49b1ec28ba179f","permalink":"/projects/hmily/config/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/projects/hmily/config/","summary":"配置详解： 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf","tags":null,"title":"Hmily-Config","type":"projects","url":"/projects/hmily/config/","wordcount":2582},{"author":null,"categories":null,"content":" Local Configuration  File Name : hmily.yml.\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = apollo\n The framework will pull the configuration according to your configured apollo.\n  hmily: server: configMode: apollo appName: # The following configuration will be read when server.configMode equals apollo. remote: apollo: configService: 127.0.0.1:8080 # apollo service address namespace: test # namespace appId: test # group secret: xxxx # the secret key env: dev # environment fileExtension: yml # format of configuration file of apollo (properties or yml）   You can add configurations in apollo, and the format is as follows(yml): ```yaml hmily: config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql  repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000\nmetrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig: ```\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  ","date":-62135596800,"description":"apollo configuration centre mode","dir":"projects/hmily/config-apollo/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"880948a69885ed46516aa4e1bf9ef40b","permalink":"/en/projects/hmily/config-apollo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/config-apollo/","summary":"Local Configuration  File Name : hmily.yml.\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = apollo\n The framework will pull the configuration according to your configured apollo.\n  hmily: server: configMode: apollo appName: # The following configuration will be read when server.","tags":null,"title":"Hmily-Config-Apollo","type":"projects","url":"/en/projects/hmily/config-apollo/","wordcount":292},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = apollo\n 框架的或首先根据你的 apollo 配置，然后从 apollo 获取配置\n  hmily: server: configMode: apollo appName: # 如果server.configMode eq local 的时候才会读取到这里的配置信息. remote: apollo: configService: 127.0.0.1:8080 # apollo服务地址 namespace: test # namespace appId: test # group secret: xxxx #秘钥 env: dev #环境 fileExtension: yml #apollo上配置文件的格式（properties或者yml）二选一   然后，你可以在apollo上添加配置，配置格式如果下（yml）：  hmily: config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  ","date":-62135596800,"description":"apollo配置中心模式","dir":"projects/hmily/config-apollo/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"880948a69885ed46516aa4e1bf9ef40b","permalink":"/projects/hmily/config-apollo/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/config-apollo/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Apollo","type":"projects","url":"/projects/hmily/config-apollo/","wordcount":454},{"author":null,"categories":null,"content":" Local Configuration  File Name: hmily.yml。\n Path: The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = consul\n The framework will pull the configuration according to your configured consul.\n  hmily: server: configMode: consul appName: xxxxx remote: consul: hostAndPort: 127.0.0.1:8500 # consul service address hostAndPorts: key: test # the key of consul blacklistTimeInMillis: 6000 fileExtension: yml # the format of the configuration of consul (properties or yml) passive: true # enable automatic update   Pay attention to consul service address configuration, if it is cluster, please configure hostAndPorts Nodes, configure hostAndPort to standalone mode. You have to set one of them, if the two is empty, it will adopt cluster configuration.\n And you need to add hmily configuration in the above configured key, the configuration is as follows:\n  hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  ","date":-62135596800,"description":"Consul configuration centre mode","dir":"projects/hmily/config-consul/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8482ea0a85b3bad66bd4e82413bb050f","permalink":"/en/projects/hmily/config-consul/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/config-consul/","summary":"Local Configuration  File Name: hmily.yml。\n Path: The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = consul\n The framework will pull the configuration according to your configured consul.\n  hmily: server: configMode: consul appName: xxxxx remote: consul: hostAndPort: 127.","tags":null,"title":"Hmily-Config-Consul","type":"projects","url":"/en/projects/hmily/config-consul/","wordcount":319},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = consul\n 框架的或首先根据你的 consul 配置，然后从 consul 获取配置\n  hmily: server: configMode: consul appName: xxxxx remote: consul: hostAndPort: 127.0.0.1:8500 # consul服务地址 hostAndPorts: key: test # consul 上的key blacklistTimeInMillis: 6000 fileExtension: yml # consul上配置文件的格式（properties或者yml）二选一 passive: true # 开启自动更新   注意consul服务地址配置，如果是集群，那么请配置hostAndPorts节点，单击配置hostAndPort，两者必须有一个不为空。两者都不为空，采用的是集群配置\n 然后，你需要在上述配置的key上写入hmily的配置，配置文件如下：\n  hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  ","date":-62135596800,"description":"Consul配置中心模式","dir":"projects/hmily/config-consul/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8482ea0a85b3bad66bd4e82413bb050f","permalink":"/projects/hmily/config-consul/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/config-consul/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Consul","type":"projects","url":"/projects/hmily/config-consul/","wordcount":504},{"author":null,"categories":null,"content":" Local Configuration  File Name: hmily.yml.\n Path: The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = etcd\n The framework will pull the configuration according to your configured etcd.\n  hmily: server: configMode: etcd appName: xxxxx # The following configuration will be read when server.configMode equals etcd. remote: etcd: server: http://127.0.0.1:2379 # etcd service address key: test # key of etcd timeoutMs: 6000 fileExtension: yml # format of configuration of etcd (properties or yml) update : # Default is false，whether need to write local configuration file to zookeeper updateFileName: # when update is true, name of configuration file, which located in the yaml file of resource directory of the project位于项目的 resource文件夹下的yaml格式   And you need to add hmily configuration in the above configured key, the context is as follows: ```yaml hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql  repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000\nmetrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig: ```\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  ","date":-62135596800,"description":"etcd configuration centre mode","dir":"projects/hmily/config-etcd/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0c5d688311c683877bb86f99c9caf1b2","permalink":"/en/projects/hmily/config-etcd/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/config-etcd/","summary":"Local Configuration File Name: hmily.yml. Path: The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource The specific contents are as follows : Notice setting hmily.server.configMode = etcd The framework will pull the configuration according to your configured etcd. hmily: server: configMode: etcd appName: xxxxx # The following","tags":null,"title":"Hmily-Config-Etcd","type":"projects","url":"/en/projects/hmily/config-etcd/","wordcount":375},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = etcd\n 框架的或首先根据你的 etcd 配置，然后从 etcd 获取配置\n  hmily: server: configMode: etcd appName: xxxxx # 如果server.configMode eq local 的时候才会读取到这里的配置信息. remote: etcd: server: http://127.0.0.1:2379 # etcd服务地址 key: test # etcd 上的key， timeoutMs: 6000 fileExtension: yml #etcd上配置文件的格式（properties或者yml）二选一 update : #默认是false ，是否需要将本地的配置文件写到zookeeper updateFileName: #update属性为true时候 ，配置文件名称，位于项目的 resource文件夹下的yaml格式   然后，你需要在上述配置的key上写入hmily的配置，配置文件如下：  hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  ","date":-62135596800,"description":"etcd配置中心模式","dir":"projects/hmily/config-etcd/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0c5d688311c683877bb86f99c9caf1b2","permalink":"/projects/hmily/config-etcd/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/config-etcd/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Etcd","type":"projects","url":"/projects/hmily/config-etcd/","wordcount":530},{"author":null,"categories":null,"content":" Local Configuration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = local\n All configurations of the framework base on your local configuration files.\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  hmily: server: configMode: local appName: # The following configuration will be read when server.configMode equals local config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:  ","date":-62135596800,"description":"Local configuration mode","dir":"projects/hmily/config-local/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d4ae9d925ca0faf01e01d542657f5ecc","permalink":"/en/projects/hmily/config-local/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/config-local/","summary":"Local Configuration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = local\n All configurations of the framework base on your local configuration files.\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026rsquo;t have to be configured all.","tags":null,"title":"Hmily-Config-Local","type":"projects","url":"/en/projects/hmily/config-local/","wordcount":238},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = local\n 框架的所有的配置就是基于你本地文件里面的配置\n 注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  hmily: server: configMode: local appName: # 如果server.configMode eq local 的时候才会读取到这里的配置信息. config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:  ","date":-62135596800,"description":"本地配置模式","dir":"projects/hmily/config-local/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d4ae9d925ca0faf01e01d542657f5ecc","permalink":"/projects/hmily/config-local/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/config-local/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Local","type":"projects","url":"/projects/hmily/config-local/","wordcount":351},{"author":null,"categories":null,"content":" Local Conguration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = naocs\n The framework will pull the configuration from your configured nacos.\n  hmily: server: configMode: nacos appName: # The following configuration will be read when server.configMode equals nacos remote: nacos: server: 127.0.0.1:2181 # nacos service address dataId: test # dataId group: test # group timeoutMs: 6000 # Timeout(ms) fileExtension: yml # the format of the configuration of nacos (properties or yml)   And you can add configuration in nacos, the format is as follows(yml): ```yaml hmily: config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql  repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000\nmetrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig: ```\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  ","date":-62135596800,"description":"nacos configuration centre mode","dir":"projects/hmily/config-nacos/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f246c71dfc1be4b6893491257f9c1a98","permalink":"/en/projects/hmily/config-nacos/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/config-nacos/","summary":"Local Conguration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = naocs\n The framework will pull the configuration from your configured nacos.\n  hmily: server: configMode: nacos appName: # The following configuration will be read when server.","tags":null,"title":"Hmily-Config-Nacos","type":"projects","url":"/en/projects/hmily/config-nacos/","wordcount":285},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = naocs\n 框架的或首先根据你的 nacos 配置，然后从 nacos 获取配置\n  hmily: server: configMode: nacos appName: # 如果server.configMode eq local 的时候才会读取到这里的配置信息. remote: nacos: server: 127.0.0.1:2181 # nacos服务地址 dataId: test # dataId group: test # group timeoutMs: 6000 #超时时间（ms） fileExtension: yml #nacos上配置文件的格式（properties或者yml）二选一   然后，你可以在nacos上添加配置，配置格式如果下（yml）：  hmily: config: appName: xiaoyu serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  ","date":-62135596800,"description":"nacos配置中心模式","dir":"projects/hmily/config-nacos/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f246c71dfc1be4b6893491257f9c1a98","permalink":"/projects/hmily/config-nacos/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/config-nacos/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Nacos","type":"projects","url":"/projects/hmily/config-nacos/","wordcount":452},{"author":null,"categories":null,"content":" Local Configuration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = zookeeper\n The framework will pull the configuration from your configured zookeeper.\n  hmily: server: configMode: zookeeper appName: # The following configuration will be read when server.configMode equals zookeeper remote: zookeeper: serverList: 127.0.0.1:2181 # your zookeeper service address, multiple addresses are separated by \u0026#39;,\u0026#39; fileExtension: yml # the format of the configuration of zookeeper(properties or yml) path: /hmily/xiaoyu # file path to zookeeper configuration update : # the deaflut is false, it means whether write the local configuration to zookeeper updateFileName: # it is the name of configuration when the property of \u0026#39;update\u0026#39; is true, and it is under the resource directory and the format is yaml   And you can add the needed configuration of hmily under the path, the configuration format is as follows(yml): ```yaml hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql  repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000\nmetrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig: ```\n Notice that the configurations of repository are extensions of SPI, you can select one from those modes, which don\u0026amp;rsquo;t have to be configured all.\n metrics is optional; If it is not configured，it means you don\u0026amp;rsquo;t enable metrics.\n  ","date":-62135596800,"description":"zookeeper configuration centre mode","dir":"projects/hmily/config-zookeeper/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"63f96bd367e08ba65bd1f8e32679af10","permalink":"/en/projects/hmily/config-zookeeper/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/config-zookeeper/","summary":"Local Configuration  File Name: hmily.yml。\n Path： The default path is the resource directory of the project, which can be specified by -Dhmily.conf, and you can also put the configuration in user.dir directory. Priority: -Dhmily.conf \u0026gt; user.dir \u0026gt; resource\n The specific contents are as follows : Notice setting hmily.server.configMode = zookeeper\n The framework will pull the configuration from your configured zookeeper.\n  hmily: server: configMode: zookeeper appName: # The following configuration will be read when server.","tags":null,"title":"Hmily-Config-Zookeeper","type":"projects","url":"/en/projects/hmily/config-zookeeper/","wordcount":335},{"author":null,"categories":null,"content":" 本地配置  文件名为 : hmily.yml。\n 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026amp;gt; user.dir \u0026amp;gt; resource\n 具体的全内容如下 : 注意设置 hmily.server.configMode = zookeeper\n 框架的或首先根据你的 zookeeper 配置，然后从 zookeeper 获取配置\n  hmily: server: configMode: zookeeper appName: # 如果server.configMode eq local 的时候才会读取到这里的配置信息. remote: zookeeper: serverList: 127.0.0.1:2181 #你的zookeeper服务地址，多个使用逗号分隔 fileExtension: yml #zookeeper上配置文件的格式（properties或者yml）二选一 path: /hmily/xiaoyu #zookeeper上配置文件的路径 update : #默认是false ，是否需要将本地的配置文件写到zookeeper updateFileName: #update属性为true时候 ，配置文件名称，位于项目的 resource文件夹下的yaml格式   然后，你可以在上述的 path 配置路径下，去写入hmily框架所需要的配置，配置格式如果下（yml）：  hmily: config: appName: serializer: kryo contextTransmittalMode: threadLocal scheduledThreadMax: 16 scheduledRecoveryDelay: 60 scheduledCleanDelay: 60 scheduledPhyDeletedDelay: 600 scheduledInitDelay: 30 recoverDelayTime: 60 cleanDelayTime: 180 limit: 200 retryMax: 10 bufferSize: 8192 consumerThreads: 16 asyncRepository: true autoSql: true phyDeleted: true storeDays: 3 repository: mysql repository: database: driverClassName: com.mysql.jdbc.Driver url : username: password: maxActive: 20 minIdle: 10 connectionTimeout: 30000 idleTimeout: 600000 maxLifetime: 1800000 file: path: prefix: /hmily mongo: databaseName: url: userName: password: zookeeper: host: localhost:2181 sessionTimeOut: 1000 rootPath: /hmily redis: cluster: false sentinel: false clusterUrl: sentinelUrl: masterName: hostName: port: password: maxTotal: 8 maxIdle: 8 minIdle: 2 maxWaitMillis: -1 minEvictableIdleTimeMillis: 1800000 softMinEvictableIdleTimeMillis: 1800000 numTestsPerEvictionRun: 3 testOnCreate: false testOnBorrow: false testOnReturn: false testWhileIdle: false timeBetweenEvictionRunsMillis: -1 blockWhenExhausted: true timeOut: 1000 metrics: metricsName: prometheus host: port: 9091 async: true threadCount : 16 jmxConfig:   注意 repository的配置是SPI的扩展方式，几种方式由你去选择一种，并不需要全部配置。\n metrics 配置可有可无，如果不配置，则代表不开启metrics\n  ","date":-62135596800,"description":"zookeeper配置中心模式","dir":"projects/hmily/config-zookeeper/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"63f96bd367e08ba65bd1f8e32679af10","permalink":"/projects/hmily/config-zookeeper/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/config-zookeeper/","summary":"本地配置 文件名为 : hmily.yml。 路径： 默认路径为项目的 resource目录下，也可以使用 -Dhmily.conf 指定，也可以把配置放在 user.dir 目录下。 优先级别 -Dhmily.conf \u0026gt;","tags":null,"title":"Hmily-Config-Zookeeper","type":"projects","url":"/projects/hmily/config-zookeeper/","wordcount":571},{"author":null,"categories":null,"content":" HmilyTransactionContext事务上下文 @Data public class HmilyTransactionContext { /** * transId. */ private Long transId; /** * participant id. */ private Long participantId; /** * participant ref id. */ private Long participantRefId; /** * this hmily action. */ private int action; /** * 事务参与的角色. */ private int role; /** * transType. */ private String transType; }  HmilyTransactionContext 是Hmily分布式事务框架进行RPC调用时用于传递事务上下文的核心类, 默认会将其存储在ThreadLocal中，然后进行RPC的参数传递，也可以配置使用线程上下文切换的的场景， 这个时候需要在配置中指定 contextTransmittalMode = transmittable,将会使用alibaba开源类库。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;transmittable-thread-local\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.11.5\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Dubbo框架传递事务上下文 具体实现类在org.dromara.hmily.dubbo.filter.DubboHmilyTransactionFilter中， 通过 RpcContext.getContext().setAttachment(String key, String value) 进行RPC传参数。\nMotan框架传递事务上下文 具体实现类在org.dromara.hmily.motan.filter.MotanHmilyTransactionFilter中， 通过 Request.setAttachment(String key, String value) 进行RPC传参数。\nSpringCloud框架传递事务上下文 具体实现类在org.dromara.hmily.springcloud.feign.HmilyFeignInterceptor中， 通过 RequestTemplate.header(String name, String... values) 进行RPC传参数。\n","date":-62135596800,"description":"Hmily-Context事务上下文","dir":"projects/hmily/context/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"29b5b0d8bf36fca4d23e1dadbc22824e","permalink":"/projects/hmily/context/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/context/","summary":"HmilyTransactionContext事务上下文 @Data public class HmilyTransactionContext { /** * transId. */ private Long transId; /** * participant id. */ private Long participantId; /** * participant ref id. */ private Long participantRefId; /** * this hmily action. */ private int action; /** * 事务","tags":null,"title":"Hmily-Context","type":"projects","url":"/projects/hmily/context/","wordcount":516},{"author":null,"categories":null,"content":" Metrics 目前hmily的metrics模块，采用 prometheus来进行采集，使用pull模式对外暴露metrics信息接口。\n收集的metrics主要分为二个大类。\n 应用的JVM信息：内存，cpu，线程使用等等\n 事务信息：包括事务的总数，事务的迟延，事务的状态，事务的角色\n  指标详解 如何展示  用户可以使用 Grafana 从应用里面的metrics配置拉取的metrics信息  ","date":-62135596800,"description":"Metrics","dir":"projects/hmily/metrics/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ac25a8a8d3dbee4303f272ae2fa5eeed","permalink":"/projects/hmily/metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/metrics/","summary":"Metrics 目前hmily的metrics模块，采用 prometheus来进行采集，使用pull模式对外暴露metrics信息接口。 收集的metric","tags":null,"title":"Hmily-Metrics","type":"projects","url":"/projects/hmily/metrics/","wordcount":174},{"author":null,"categories":null,"content":" TAC The TAC mode is actually a variant of the TCC mode. Just as the name implies, the TAC mode is called automatic rollback. As compared with the TCC mode, the user doesn\u0026amp;rsquo;t have to concern about how to write the rollback method at all. and then it can reduces user development volume and is entirely transparent to users.\n TAC Mode is only suitable for Relational Database.\n TAC Mode will intercept the user\u0026amp;rsquo;s SQL statement to generate reverse rollback SQL, and the compatibility of SQL will also be a ordeal.\n  ","date":-62135596800,"description":"Hmily-TAC","dir":"projects/hmily/tac/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"67cddb93648c369d6a4e559fb337022c","permalink":"/en/projects/hmily/tac/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/tac/","summary":"TAC The TAC mode is actually a variant of the TCC mode. Just as the name implies, the TAC mode is called automatic rollback. As compared with the TCC mode, the user doesn\u0026rsquo;t have to concern about how to write the rollback method at all. and then it can reduces user development volume and is entirely transparent to users.\n TAC Mode is only suitable for Relational Database.\n TAC Mode will intercept the user\u0026rsquo;s SQL statement to generate reverse rollback SQL, and the compatibility of SQL will also be a ordeal.","tags":null,"title":"Hmily-TAC","type":"projects","url":"/en/projects/hmily/tac/","wordcount":90},{"author":null,"categories":null,"content":" TCC TCC模式是经典的柔性事务解决方案，需要使用者提供 try, confirm, cancel 三个方法， 真正的情况下会执行 try, confirm, 异常情况下会执行try, cancel。 confirm 方法并不是 必须的，完全依赖于用户的try 方法如何去写。 confirm, cancel 2个方法也需要用户去保证幂等性, 这会附加一定的工作量，由于在try方法完成之后，数据已经提交了，因此它并不保证数据的隔离性。但是这样，它的 性能相对较高，一个好的系统设计，是非常适用适用TCC模式。下面是Hmily 框架的 TCC 流程图  在极端异常情况下，比如服务突然宕机，超时异常等，依赖与自身的调用任务，来进行日志的事务恢复。\n 在confirm, cancel 阶段，如果有任何异常会继续执行相应的阶段，如果超过最大重试次数还未成功，将不再进行重试，需要人工介入。\n 在服务集群的情况下，confirm, cancel 2个方法用户去尽量保证其幂等性。\n  ","date":-62135596800,"description":"Hmily-TCC","dir":"projects/hmily/tcc/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"bc572e24c3dc77e36b27bc24b0ec01fb","permalink":"/projects/hmily/tcc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/tcc/","summary":"TCC TCC模式是经典的柔性事务解决方案，需要使用者提供 try, confirm, cancel 三个方法， 真正的情况下会执行 try, confirm, 异常情况下会执行try, cancel。 confirm 方法并不是","tags":null,"title":"Hmily-TCC","type":"projects","url":"/projects/hmily/tcc/","wordcount":349},{"author":null,"categories":null,"content":" @Hmily /** * The annotation Hmily. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Hmily { }   该注解为hmily分布式事务接口标识，表示该接口参与hmily分布式事务  @HmilyTCC @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTCC { /** * Confirm method string. * * @return the string */ String confirmMethod() default \u0026amp;quot;\u0026amp;quot;; /** * Cancel method string. * * @return the string */ String cancelMethod() default \u0026amp;quot;\u0026amp;quot;; /** * Pattern pattern enum. * * @return the pattern enum */ TransTypeEnum pattern() default TransTypeEnum.TCC; }   该注解为Hmily分布式事务TCC模式的切面（AOP point），可以标识在你本地具体实现方法上。\n confirmMethod : 注解标识方法的，确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 注解标识方法的，回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n  @HmilyTAC /** * The annotation HmilyTAC. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTAC { }   该注解为Hmily分布式事务TAC模式的切面（AOP point），可以标识在你的本地方法具体实现上。  ","date":-62135596800,"description":"annotation","dir":"projects/hmily/annotation/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"03ef60b752c3f007c96118cfd9cdc1b2","permalink":"/projects/hmily/annotation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/annotation/","summary":"@Hmily /** * The annotation Hmily. * * @author xiaoyu */ @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface Hmily { } 该注解为hmily分布式事务接口标识，表示该接口参与hmily分布式事务 @HmilyTCC @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.METHOD) public @interface HmilyTCC { /** * Confirm method string. * * @return the","tags":null,"title":"Hmily-annotation","type":"projects","url":"/projects/hmily/annotation/","wordcount":301},{"author":null,"categories":null,"content":" Development Guidelines  Intentions Write codes with heart. Pursue clean, simplified and extremely elegant codes. Readable The code is unambiguous, and the intention of the code is revealed through reading rather than debugging. Tidy Agree with concepts in and . Consistent Be familiar with codes already had, to keep consistent with the style and use. Simplified Express meaning with the least code. Highly reusable, no duplicated codes or configurations. Delete codes out of use in time. Abstract The levels are clearly divided and the concepts are reasonably refined. Keep methods, classes, packages and modules at the same abstract level.  Contributor Covenant Submitting of Conduct  Make sure all the test cases are passed, Make sure ./mvnw clean install can be compiled and tested successfully. Make sure the test coverage rate is not lower than the master branch. Make sure to check codes with Checkstyle. codes that violate check rules should have special reasons. Find checkstyle template from https://github.com/dromara/hmily/blob/master/script/hmily_checkstyle.xml, please use checkstyle 8.8 to run the rules. Careful consideration for each pull request; Small and frequent pull request with complete unit function is welcomed. Conform to Contributor Covenant Code of Conduct below.  Contributor Covenant Code of Conduct  Use linux line separators. Keep indents (including blank lines) consistent with the previous one. Keep one blank line after class definition. No meaningless blank lines. Please extract private methods to instead of blank lines if too long method body or different logic code fragments. Use meaningful class, method and variable names, avoid to use abbreviate. Return values are named with result; Variables in the loop structure are named with each; Replace each with entry in map. Name property files with Spinal Case(a variant of Snake Case which uses hyphens - to separate words). Split codes that need to add notes with it into small methods, which are explained with method names. Have constants on the left and variable on the right in == and equals conditional expressions; Have variable on the left and constants on the right in greater than and less than conditional expressions. Beside using same names as input parameters and global fields in assign statement, avoid using this modifier. Design class as final class except abstract class for extend. Make nested loop structures a new method. The order of definition of member variables and the order of parameter passing are kept consistent in each class and method. Order of members definition and parameters should be consistent during classes and methods. Use guard clauses in priority. Minimize the access permission for classes and methods. Private method should be just next to the method in which it is used; writing private methods should be in the same as the appearance order of private methods. No null parameters or return values. Replace if else return and assign statement with ternary …","date":-62135596800,"description":"hmily development guidelines","dir":"projects/hmily/code-conduct/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ff3eeb9c16eee349c0062fed70effaa9","permalink":"/en/projects/hmily/code-conduct/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/hmily/code-conduct/","summary":"Development Guidelines  Intentions Write codes with heart. Pursue clean, simplified and extremely elegant codes. Readable The code is unambiguous, and the intention of the code is revealed through reading rather than debugging. Tidy Agree with concepts in and . Consistent Be familiar with codes already had, to keep consistent with the style and use. Simplified Express meaning with the least code. Highly reusable, no duplicated codes or configurations. Delete codes out of use in time.","tags":null,"title":"Hmily-code-conduct","type":"projects","url":"/en/projects/hmily/code-conduct/","wordcount":530},{"author":null,"categories":null,"content":" 开发理念  用心 保持责任心和敬畏心，以工匠精神持续雕琢。 可读 代码无歧义，通过阅读而非调试手段浮现代码意图。 整洁 认同《重构》和《代码整洁之道》的理念，追求整洁优雅代码。 一致 代码风格、命名以及使用方式保持完全一致。 精简 极简代码，以最少的代码表达最正确的意思。高度复用，无重复代码和配置。及时删除无用代码。 抽象 层次划分清晰，概念提炼合理。保持方法、类、包以及模块处于同一抽象层级。  代码提交行为规范  确保通过全部测试用例，确保执行./mvnw clean install可以编译和测试通过。 确保覆盖率不低于master分支。 确保使用Checkstyle检查代码，违反验证规则的需要有特殊理由。模板位置在https://github.com/dromara/hmily/blob/master/script/hmily_checkstyle.xml，请使用checkstyle 8.8运行规则。 应尽量将设计精细化拆分；做到小幅度修改，多次数提交，但应保证提交的完整性。 确保遵守编码规范。  编码规范  使用linux换行符。 缩进（包含空行）和上一行保持一致。 类声明后与下面的变量或方法之间需要空一行。 不应有无意义的空行。请提炼私有方法，代替方法体过长或代码段逻辑闭环而采用的空行间隔。 类、方法和变量的命名要做到顾名思义，避免使用缩写。 返回值变量使用result命名；循环中使用each命名循环变量；map中使用entry代替each。 配置文件使用Spinal Case命名（一种使用-分割单词的特殊Snake Case）。 需要注释解释的代码尽量提成小方法，用方法名称解释。 equals和==条件表达式中，常量在左，变量在右；大于小于等条件表达式中，变量在左，常量在右。 除了构造器入参与全局变量名称相同的赋值语句外，避免使用this修饰符。 除了用于继承的抽象类之外，尽量将类设计为final。 嵌套循环尽量提成方法。 成员变量定义顺序以及参数传递顺序在各个类和方法中保持一致。 优先使用卫语句。 类和方法的访问权限控制为最小。 方法所用到的私有方法应紧跟该方法，如果有多个私有方法，书写私有方法应与私有方法在原方法的出现顺序相同。 方法入参和返回值不允许为null。 优先使用三目运算符代替if else的返回和赋值语句。 优先考虑使用LinkedList，只有在需要通过下标获取集合中元素值时再使用ArrayList。 ArrayList，HashMap等可能产生扩容的集合类型必须指定集合初始大小，避免扩容。 日志与注释一律使用英文。 注释只能包含javadoc，todo和fixme。 公开的类和方法必须有javadoc，其他类和方法以及覆盖自父类的方法无需javadoc。  ","date":-62135596800,"description":"hmily编码指南","dir":"projects/hmily/code-conduct/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ff3eeb9c16eee349c0062fed70effaa9","permalink":"/projects/hmily/code-conduct/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/hmily/code-conduct/","summary":"开发理念 用心 保持责任心和敬畏心，以工匠精神持续雕琢。 可读 代码无歧义，通过阅读而非调试手段浮现代码意图。 整洁 认同《重构》和《代码整洁之道》的理","tags":null,"title":"Hmily-code-conduct","type":"projects","url":"/projects/hmily/code-conduct/","wordcount":1105},{"author":null,"categories":null,"content":" Committer Promotion After you have made a lot of contributions, the community will invite you join Committers\nBecome a committer you will have\n Hmily repository write permissions\n Idea free license\n  Committer Responsibilities  Develop new features; Refactor codes; Review pull requests reliably and in time; Consider and accept feature requests; Answer questions; Update documentation and example; Improve processes and tools; Guide new contributors join community.  Committer Routine  Committer needs to check the list of pull requests and issues to be processed in the community on a daily basis and assign them to the appropriate committer, that is, assignee.\n After a committer is assigned with an issue, the following work is required:\n Estimate whether it is a long-term issue. If it is, please label it as pending. Add issue labels, such as bug, enhancement, discussion, etc. Add milestone.   Notice\nRegardless of whether it is a community issue, there must be an assignee until the issue is resolved.\n","date":-62135596800,"description":"Hmily committer guidelines","dir":"projects/hmily/committer/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"587564943f36f7111229c0388d4c5ba0","permalink":"/en/projects/hmily/committer/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/committer/","summary":"Committer Promotion After you have made a lot of contributions, the community will invite you join Committers\nBecome a committer you will have\n Hmily repository write permissions\n Idea free license\n  Committer Responsibilities  Develop new features; Refactor codes; Review pull requests reliably and in time; Consider and accept feature requests; Answer questions; Update documentation and example; Improve processes and tools; Guide new contributors join community.  Committer Routine  Committer needs to check the list of pull requests and issues to be processed in the community on a daily basis and assign them to the appropriate committer, that is, assignee.","tags":null,"title":"Hmily-committer","type":"projects","url":"/en/projects/hmily/committer/","wordcount":155},{"author":null,"categories":null,"content":" 提交者提名 当你做了很多贡献以后，社区会进行提名。 成为committer你会拥有\n hmily仓库写的权限\n idea 正版使用\n  提交者责任  开发新功能； 代码重构； 及时和可靠的评审Pull Request； 思考和接纳新特性请求； 解答问题； 维护文档和代码示例； 改进流程和工具； 引导新的参与者融入社区。  日常工作  committer需要每天查看社区待处理的Pull Request和issue列表，指定给合适的committer，即assignee。\n assignee在被分配issue后，需要进行如下判断：\n 判断是否是长期issue，如是，则标记为pending。 判断issue类型，如：bug，enhancement，discussion等。 判断Milestone，并标记。   注意\n无论是否是社区issue，都必须有assignee，直到issue完成。\n","date":-62135596800,"description":"Hmily-committer提交者指南","dir":"projects/hmily/committer/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"587564943f36f7111229c0388d4c5ba0","permalink":"/projects/hmily/committer/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/committer/","summary":"提交者提名 当你做了很多贡献以后，社区会进行提名。 成为committer你会拥有 hmily仓库写的权限 idea 正版使用 提交者责任 开发新功能； 代码重构","tags":null,"title":"Hmily-committer","type":"projects","url":"/projects/hmily/committer/","wordcount":358},{"author":null,"categories":null,"content":" You can report a bug, submit a new function enhancement suggestion, or submit a pull request directly.\nSubmit an Issue  Before submitting an issue, please go through a comprehensive search to make sure the problem cannot be solved just by searching. Check the Issue List to make sure the problem is not repeated. Create a new issue and choose the type of issue. Define the issue with a clear and descriptive title.. Fill in necessary information according to the template. Choose a label after issue created, for example: bug，enhancement，discussion. Please pay attention for your issue, you may need provide more information during discussion.  Developer Flow Fork Hmily repo  Fork a Hmily repo to your own repo to work, then setting upstream.  git remote add upstream https://github.com/dromara/hmily.git  Choose Issue  Please choose the issue to be edited. If it is a new issue discovered or a new function enhancement to offer, please create an issue and set the right label for it. After choosing the relevant issue, please reply with a deadline to indicate that you are working on it.  Create Branch  Switch to forked master branch, pull codes from upstream, then create a new branch.  git checkout master git pull upstream master git checkout -b issueNo  Notice ：We will merge PR using squash, commit log will be different form upstream if you use old branch\nCoding  Please obey the Code of Conduct during the process of development and finish the check before submitting the pull request. push code to your fork repo.  git add modified-file-names git commit -m \u0026#39;commit log\u0026#39; git push origin issueNo  Submit Pull Request  Send a pull request to the master branch. The mentor will do code review before discussing some details (including the design, the implementation and the performance) with you. The request will be merged into the branch of current development version after the edit is well enough. At last, congratulate to be an official contributor of Hmily  Delete Branch  You can delete the remote branch (origin/issueNo) and the local branch (issueNo) associated with the remote branch (origin/issueNo) after the mentor merged the pull request into the master branch of Hmily.  git checkout master git branch -d issueNo git push origin --delete issueNo  Notice Please note that in order to show your id in the contributor list, don’t forget the configurations below:\ngit config --global user.name \u0026amp;quot;username\u0026amp;quot; git config --global user.email \u0026amp;quot;username@mail.com\u0026amp;quot;  ","date":-62135596800,"description":"Hmily-contributor-guide","dir":"projects/hmily/contributor/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e87656f289592fdd9b0d791c58640162","permalink":"/en/projects/hmily/contributor/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/contributor/","summary":"You can report a bug, submit a new function enhancement suggestion, or submit a pull request directly.\nSubmit an Issue  Before submitting an issue, please go through a comprehensive search to make sure the problem cannot be solved just by searching. Check the Issue List to make sure the problem is not repeated. Create a new issue and choose the type of issue. Define the issue with a clear and descriptive title.","tags":null,"title":"Hmily-contributor","type":"projects","url":"/en/projects/hmily/contributor/","wordcount":396},{"author":null,"categories":null,"content":" 您可以报告bug，提交一个新的功能增强建议或者直接对以上内容提交改进补丁。\n提交issue  在提交issue之前，请经过充分的搜索，确定该issue不是通过简单的检索即可以解决的问题。 查看issue列表，确定该issue不是一个重复的问题。 新建一个issue并选择您的issue类型。 使用一个清晰并有描述性的标题来定义issue。 根据模板填写必要信息。 在提交issue之后，对该issue分配合适的标签。如：bug，enhancement，discussion等。 请对自己提交的issue保持关注，在讨论中进一步提供必要信息。  开发流程 Fork分支到本地，设置upstream  从hmily的repo上fork一个分支到您自己的repo来开始工作，并设置upstream为hmily的repo。  git remote add upstream https://github.com/dromara/hmily.git  选择issue  请在选择您要修改的issue。如果是您新发现的问题或想提供issue中没有的功能增强，请先新建一个issue并设置正确的标签。 在选中相关的issue之后，请回复以表明您当前正在这个issue上工作。并在回复的时候为自己设置一个deadline，添加至回复内容中。  创建分支  切换到fork的master分支，拉取最新代码，创建本次的分支。  git checkout master git pull upstream master git checkout -b issueNo  注意 ：PR会按照squash的方式进行merge，如果不创建新分支，本地和远程的提交记录将不能保持同步。\n编码  请您在开发过程中遵循hmily的 开发规范。并在准备提交pull request之前完成相应的检查。 将修改的代码push到fork库的分支上。  git add 修改代码 git commit -m \u0026#39;commit log\u0026#39; git push origin issueNo  提交PR  发送一个pull request到hmily的master分支。 接着导师做CodeReview，然后他会与您讨论一些细节（包括设计，实现，性能等）。当导师对本次修改满意后，会将提交合并到当前开发版本的分支中。 最后，恭喜您已经成为了hmily的贡献者！  删除分支  在导师将pull request合并到hmily的master分支中之后，您就可以将远程的分支（origin/issueNo）及与远程分支（origin/issueNo）关联的本地分支（issueNo）删除。  git checkout master git branch -d issueNo git push origin --delete issueNo  注意 为了让您的id显示在contributor列表中，别忘了以下设置：\ngit config --global user.name \u0026amp;quot;username\u0026amp;quot; git config --global user.email \u0026amp;quot;username@mail.com\u0026amp;quot;  ","date":-62135596800,"description":"Hmily-contributor贡献者指南","dir":"projects/hmily/contributor/","fuzzywordcount":1000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e87656f289592fdd9b0d791c58640162","permalink":"/projects/hmily/contributor/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/contributor/","summary":"您可以报告bug，提交一个新的功能增强建议或者直接对以上内容提交改进补丁。 提交issue 在提交issue之前，请经过充分的搜索，确定该iss","tags":null,"title":"Hmily-contributor","type":"projects","url":"/projects/hmily/contributor/","wordcount":963},{"author":null,"categories":null,"content":" TAC TAC模式其实是TCC模式的变种,顾名思义 TAC 模式被称为自动回滚,相比于 TCC模式，用户完全不用关心 回滚方法如何去写，减少了用户的开发量，对用户完全透明。\n TAC 模式只适合于关系型数据库。\n TAC 模式会拦截用户的SQL语句生成反向回滚SQL，SQL的兼容度也会是一大考验。\n  ","date":-62135596800,"description":"tac","dir":"projects/hmily/tac/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"67cddb93648c369d6a4e559fb337022c","permalink":"/projects/hmily/tac/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/tac/","summary":"TAC TAC模式其实是TCC模式的变种,顾名思义 TAC 模式被称为自动回滚,相比于 TCC模式，用户完全不用关心 回滚方法如何去写，减少了用户的开发量，对","tags":null,"title":"Hmily-tac","type":"projects","url":"/projects/hmily/tac/","wordcount":130},{"author":null,"categories":null,"content":" TCC The TCC Mode is a classic flexible transaction solution that needs the users to provided try, confirm, cancel methods. The try, confirm methods will be invoked under normal circumstances,and the try, cancel methods will be invoked as an exception occurs. the confirm method is not required,it entirely depends on the users how to implement the try method. the both confirm and cancel method also need the users to guarantee the idempotency, but it will bring addtional workload to the users. Because after the try method finished, the data had been committed. But with this,the performances will be even better. A good system design is very applicable to the TCC Mode. This is the flow diagram of TCC in Hmily framework as below:  In extreme cases, such as sudden service crash, timeout exceptions, and much more, the transaction recovery of the log depends on its own calling task.\n At the both confirm and cancel stage,if there are any exception occur, the corresponding stage will continue to be executed. If the maximum number of retries is exceeded, the transaction has not succeeded, It will not retry any more, then manual intervention is required at this time.\n In the case of a service cluster, the users need to do the best to ensure the idempotence of these two methods confirm, cancel.\n  ","date":-62135596800,"description":"tcc","dir":"projects/hmily/tcc/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"bc572e24c3dc77e36b27bc24b0ec01fb","permalink":"/en/projects/hmily/tcc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/tcc/","summary":"TCC The TCC Mode is a classic flexible transaction solution that needs the users to provided try, confirm, cancel methods. The try, confirm methods will be invoked under normal circumstances,and the try, cancel methods will be invoked as an exception occurs. the confirm method is not required,it entirely depends on the users how to implement the try method. the both confirm and cancel method also need the users to guarantee the idempotency, but it will bring addtional workload to the users.","tags":null,"title":"Hmily-tcc","type":"projects","url":"/en/projects/hmily/tcc/","wordcount":219},{"author":null,"categories":null,"content":" Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to the article Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing an ACTS bug or adding an ACTS feature, submit an issue on ACTS GitHub to describe the bug you are going to fix or the feature you intend to add before you submit the code. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project to result in repetitive work. The ACTS maintenance personnel will discuss the issue or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start code development and submit the code after an agreement is reached. This reduces the cost of communication between both parties and the number of rejected pull requests.  Get the source code To modify or add a feature after submitting an issue, click the fork button in the upper left corner to copy the master branch code to your code repository.\nPull a branch All ACTS modifications are performed on branches. After the modification, submit a pull request. The modifications will then be merged into the master branch by the project maintenance personnel after the code review. Therefore, after getting familiar with how to get the source code, you need to:\n Download the code locally. You may select the git/https mode in this step.\nGit clone https://github.com/your account name/acts.git  Pull a branch to prepare for code modification.\ngit branch add_xxx_feature   After the preceding command is executed, your code repository will switch to the corresponding branch. To view the current branch, execute the following command:\n git branch -a  If you want to switch back to the master branch, execute the following command:\n git checkout -b master  If you want to switch back to the branch, execute the following command:\n git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally After a branch is pulled, you can modify the code.\n After modifying the code, execute the following command to submit all modifications to your local repository:\ngit commit -am \u0026#39;Add xx feature\u0026#39;   When modifying the code, note the following:  Keep the code style consistent. ACTS uses the Maven plug-in to keep the code style consistent. Before submitting the code, be sure to execute the following command locally.\nmvn clean compile  Supplement unit test code.\n New modifications should have passed existing unit tests.\n Provide a new unit test to prove that the previous code has bugs and the bugs have been fixed in the new code. Execute the following command to run all tests:\nmvn clean test   You can also use the IDE to help run a test.\nOther do\u0026amp;rsquo;s and …","date":-62135596800,"description":"","dir":"projects/sofa-acts/contributing/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cd68baede6258921f83665ef0a446f1f","permalink":"/en/projects/sofa-acts/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-acts/contributing/","summary":"Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to the article Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing an ACTS bug or adding an ACTS feature, submit an issue on ACTS GitHub to describe the bug you are going to fix or the feature you intend to add before you submit the code.","tags":null,"title":"How to contribute","type":"projects","url":"/en/projects/sofa-acts/contributing/","wordcount":808},{"author":null,"categories":null,"content":"  We recommend that you go to the Roadmap topic to learn about the development tasks and plans first.\n Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.  GitHub Code Contribution Process Submitting an issue Regardless of whether you are fixing a SOFADashboard bug or adding a SOFADashboard feature, submit an issue on SOFADashboard GitHub to describe the bug you are going to fix or the feature you intend to add before you submit the code.\nThere are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project. This avoids repetitive work. The SOFADashboard maintenance personnel will discuss the issue or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start code development and submit the code after an agreement is reached. This reduces the cost of communication between both parties and the number of rejected pull requests.  Get the source code To modify or add a feature after submitting an issue, click the fork button in the upper left corner to copy the master branch code to your code repository.\nPull a branch All SOFADashboard modifications are performed on branches. After the modification, submit a pull request. The modifications will then be merged into the master branch by the project maintenance personnel after the code review. Therefore, after getting familiar with how to get the source code, you need to:\n Download the code locally. You may select the git/https mode in this step.  git clone https://github.com/your account name/sofa-dashboard.git   Pull a branch to prepare for code modification.  git branch add_xxx_feature   After the preceding command is executed, your code repository is switched to the corresponding branch. To view the current branch, execute the following command:  git branch -a   If you want to switch back to the master branch, execute the following command:  git checkout -b master   If you want to switch back to the branch, execute the following command:  git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent.  SOFADashboard uses the Maven plug-in to keep the code style consistent. Before submitting the code, be sure to execute the following command locally.\nmvn clean compile   Add the unit test code.\n Modifications should have passed existing unit tests.\n You should provide a new unit test to prove that the previous code has bugs and the bugs\nhave been fixed in the new code. You can execute the following code to run all tests:\n  mvn clean test   You can also use the IDE to help run a test. …","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/contribution/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"584584be9c13f2d36c85890dd192368a","permalink":"/en/projects/sofa-dashboard/contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-dashboard/contribution/","summary":"We recommend that you go to the Roadmap topic to learn about the development tasks and plans first.\n Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.","tags":null,"title":"How to contribute","type":"projects","url":"/en/projects/sofa-dashboard/contribution/","wordcount":837},{"author":null,"categories":null,"content":"  We recommend that you go to the Roadmap topic to learn about the development tasks and plans first.\n Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of git tools, refer to official books on gitand get familiarized by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing a SOFARegistry bug or adding a SOFARegistry feature, submit an issue on the SOFARegistry GitHub address to describe the bug you are going to fix or the feature you intend to add before you submit the code. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project. This avoids repetitive work. The SOFARegistry maintenance personnel will discuss the issue or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start code development and submit the code after an agreement is reached. This reduces the cost of communication between both parties and the number of rejected pull requests.  Get the source code To modify or add a feature after submitting an issue, click the fork button in the upper left corner to copy the master branch code to your code repository.\nPull a branch All SOFARegistry modifications are performed on branches. After the modification, submit a pull request. The modifications will then be merged into the master branch by the project maintenance personnel after the code review.\nTherefore, after getting familiar with how to get the source code, you need to:\n Download the code locally. You may select the git/https mode in this step.  git clone https://github.com/your account name/sofa-registry.git   Pull a branch to prepare for code modification.  git branch add_xxx_feature  After the preceding command is executed, your code repository is switched to the corresponding branch. To view the current branch, execute the following command:\ngit branch -a  If you want to switch back to the master branch, execute the following command:\ngit checkout -b master  If you want to switch back to the branch, execute the following command:\ngit checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent.  SOFARegistry uses the Maven plug-in to keep the code style consistent. Before submitting the code, be sure to execute the following command locally.\nmvn clean compile   Add the unit test code. Modifications should have passed existing unit tests. You should provide a new unit test to prove that the previous code has bugs and the bugs have been fixed in the new code. You can execute the following code to run all tests:  mvn clean test  You can also use the IDE to help run a test.\nOther do\u0026amp;rsquo;s …","date":-62135596800,"description":"","dir":"projects/sofa-registry/contributing/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c08b5945719137833634c111c43a8d9e","permalink":"/en/projects/sofa-registry/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-registry/contributing/","summary":"We recommend that you go to the Roadmap topic to learn about the development tasks and plans first.\n Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of git tools, refer to official books on gitand get familiarized by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing a SOFARegistry bug or adding a SOFARegistry feature, submit an issue on the SOFARegistry GitHub address to describe the bug you are going to fix or the feature you intend to add before you submit the code.","tags":null,"title":"How to contribute","type":"projects","url":"/en/projects/sofa-registry/contributing/","wordcount":839},{"author":null,"categories":null,"content":" How to contribute SOFABolt\u0026amp;rsquo;s code is open source. You can submit your contributions to the code after signing the required agreement.\nContributor License Agreement Alterations and modifications made to SOFABolt\u0026amp;rsquo;s code must comply with the Contributor License Agreement.\nPrerequisites Before contributing any code, you need to know how to use the Git tool and the GitHub website.\nFor the use of Git tools, refer to the official Pro Git book and get familiar with the tools by reading the first few chapters.\nFor the Git collaboration process, refer to Git Workflows.\nGitHub Code Contribution Process Submit an issue Regardless of whether you are fixing a Bolt bug or adding a Bolt feature, submit an issue on the Bolt GitHub address to describe the bug you are going to fix or the feature you intend to add before you submit the code. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project. This avoids repetitive work. The Bolt maintenance personnel will discuss the issue or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start developing and submitting code after agreement to reduce the cost of communication between both parties as well as the number of rejected pull requests.  Get the source code To modify or add a feature after submitting an issue, click the fork button in the upper left corner to copy the Bolt\u0026amp;rsquo;s master branch code to your code repository.\nPull a branch All Bolt modifications are performed on branches. After the modification, submit a pull request. The modifications will then be merged into the master branch by the project maintenance personnel after the code review. Therefore, after getting familiar with the getting source code step, you need to:\n Download the code locally. You may select the git/https mode in this step.  git clone https://github.com/sofastack/sofa-bolt.git   Pull a branch to prepare for code modification.  git branch add_xxx_feature   After the preceding command is executed, your code repository is switched to the corresponding branch. To view the current branch, execute the following command:  git branch -a   If you want to switch back to the master branch, execute the following command:  git checkout -b master   If you want to switch back to your branch, execute the following command:  git checkout -b \u0026amp;quot;branchName\u0026amp;quot;   If you want to directly pull a branch from GitHub, execute the following command:  git clone -b branchname https://xxx.git  Modify the code and submit it locally After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent. Bolt uses the Maven plug-in to keep the code style consistent. Before submitting the code, be sure to execute the following command locally.  mvn clean package   Add the unit test code. Modifications should have passed existing unit …","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-contribution/","fuzzywordcount":1200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c044ad534cf99e4d6d400113b490f816","permalink":"/en/projects/sofa-bolt/sofa-bolt-contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/sofa-bolt/sofa-bolt-contribution/","summary":"How to contribute SOFABolt\u0026rsquo;s code is open source. You can submit your contributions to the code after signing the required agreement.\nContributor License Agreement Alterations and modifications made to SOFABolt\u0026rsquo;s code must comply with the Contributor License Agreement.\nPrerequisites Before contributing any code, you need to know how to use the Git tool and the GitHub website.\nFor the use of Git tools, refer to the official Pro Git book and get familiar with the tools by reading the first few chapters.","tags":null,"title":"How to contribute to SOFABolt","type":"projects","url":"/en/projects/sofa-bolt/sofa-bolt-contribution/","wordcount":1140},{"author":null,"categories":null,"content":" Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing a SOFAJRaft bug or adding a SOFAJRaft feature, submit an issue on the SOFAJRaft GitHub to describe the bug you are going to fix or the feature you intend to add before you submit the code. There are several advantages of doing so:\n There will not be any conflict with other developers or their plans for this project. This avoids repetitive work. The SOFAJRaft maintenance personnel will discuss the issue or new feature you submitted to determine whether the modification is necessary, or if there is any room for improvement or a better solution. Start code development and submit the code after an agreement is reached. This reduces the cost of communication between both parties and the number of rejected pull requests.  Get the source code To modify or add a feature after submitting an issue, click the fork button in the upper left corner to copy the master branch code to your code repository.\nPull a branch We recommend that you first read the SOFAJRaft Branch management policy.\nAll SOFAJRaft modifications are performed on branches. After the modification, submit a pull request. The modifications will then be merged into the master branch by the project maintenance personnel after the code review. Therefore, after getting familiar with how to get the source code, you need to:\n Download the code locally. You may select the git/https mode in this step.  git clone https://github.com/your account name/sofa-jraft   Pull a branch to prepare for code modification.  git branch add_xxx_feature   After the preceding command is executed, your code repository is switched to the corresponding branch. To view the current branch, execute the following command:  git branch -a   If you want to switch back to the master branch, execute the following command:  git checkout -b master   If you want to switch back to the branch, execute the following command:  git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  Modify the code and submit it locally After a branch is pulled, you can modify the code.\nWhen modifying the code, note the following:  Keep the code style consistent. SOFAJRaft uses the Maven plug-in to keep the code style consistent. Before submitting the code, be sure to execute the following command locally.  mvn clean compile   Add the unit test code.\n New modifications should have passed existing unit tests.\n Provide a new unit test to prove that the previous code has bugs and the bugs have been fixed in the new code. Execute the following command to run all tests:\n  mvn clean test  You can also use the IDE to help execute a command.\nOther do\u0026amp;rsquo;s and don\u0026amp;rsquo;ts  Retain the original style of the …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"99034715298f73cd835672b872141609","permalink":"/en/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","summary":"Prerequisites Before contributing any code, you need to know how to use the Git tools and the GitHub website.\n For the use of Git tools, refer to the official Pro Git book and get familiar with it by reading the first few chapters. For the Git collaboration process, refer to Git Workflows.  GitHub Code Contribution Process Submit an issue Regardless of whether you are fixing a SOFAJRaft bug or adding a SOFAJRaft feature, submit an issue on the SOFAJRaft GitHub to describe the bug you are going to fix or the feature you intend to add before you submit the code.","tags":null,"title":"How to contribute to SOFAJRaft","type":"projects","url":"/en/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","wordcount":841},{"author":null,"categories":null,"content":" Http 协议基本使用 在 SOFARPC (非SOFABoot 环境)中，当使用Http作为服务端协议的时候，支持Json作为序列化方式，作为一些基础的测试方式使用。\nSOFARPC API 使用 发布服务 // 只有1个线程 执行 ServerConfig serverConfig = new ServerConfig() .setStopTimeout(60000) .setPort(12300) .setProtocol(RpcConstants.PROTOCOL_TYPE_HTTP) .setDaemon(true); // 发布一个服务，每个请求要执行1秒 ProviderConfig\u0026amp;lt;HttpService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HttpService\u0026amp;gt;() .setInterfaceId(HttpService.class.getName()) .setRef(new HttpServiceImpl()) .setApplication(new ApplicationConfig().setAppName(\u0026amp;quot;serverApp\u0026amp;quot;)) .setServer(serverConfig) .setUniqueId(\u0026amp;quot;uuu\u0026amp;quot;) .setRegister(false); providerConfig.export();  服务引用 因为是Http+Json，所以引用方可以直接通过HttpClient进行调用,以下为一段测试代码。\nprivate ObjectMapper mapper = new ObjectMapper(); HttpClient httpclient = HttpClientBuilder.create().build(); // POST 正常请求 String url = \u0026amp;quot;http://127.0.0.1:12300/com.alipay.sofa.rpc.server.http.HttpService:uuu/object\u0026amp;quot;; HttpPost httpPost = new HttpPost(url); httpPost.setHeader(RemotingConstants.HEAD_SERIALIZE_TYPE, \u0026amp;quot;json\u0026amp;quot;); ExampleObj obj = new ExampleObj(); obj.setId(1); obj.setName(\u0026amp;quot;xxx\u0026amp;quot;); byte[] bytes = mapper.writeValueAsBytes(obj); ByteArrayEntity entity = new ByteArrayEntity(bytes, ContentType.create(\u0026amp;quot;application/json\u0026amp;quot;)); httpPost.setEntity(entity); HttpResponse httpResponse = httpclient.execute(httpPost); Assert.assertEquals(200, httpResponse.getStatusLine().getStatusCode()); byte[] data = EntityUtils.toByteArray(httpResponse.getEntity()); ExampleObj result = mapper.readValue(data, ExampleObj.class); Assert.assertEquals(\u0026amp;quot;xxxxx\u0026amp;quot;, result.getName());  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/http-json/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"28abdf6369247346bad670c639a422b8","permalink":"/projects/sofa-rpc/http-json/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/http-json/","summary":"Http 协议基本使用 在 SOFARPC (非SOFABoot 环境)中，当使用Http作为服务端协议的时候，支持Json作为序列化方式，作为一些基础的测试方式使用。","tags":null,"title":"Http 协议基本使用","type":"projects","url":"/projects/sofa-rpc/http-json/","wordcount":242},{"author":null,"categories":null,"content":" HttpClient Integration In this document will demonstrate how to use SOFATracer to track of HttpClient, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026amp;lt;!-- SOFATracer dependency --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- HttpClient dependency --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.httpcomponents\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;httpclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!-- 4.5.X --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.5.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.httpcomponents\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;httpasyncclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!-- 4.X --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.1.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Project Configuration Then, add the parameters to be used by SOFATracer under the project\u0026amp;rsquo;s application.properties file, including spring.application.name that indicates the current application name and logging.path that specifies the log output directory.\n# Application Name spring.application.name=HttpClientDemo # logging path logging.path=./logs  Add a Controller that provides RESTful service @RestController public class SampleRestController { private final AtomicLong counter = new AtomicLong(0); /** * Request http://localhost:8080/httpclient?name= * @param name name * @return Map of Result */ @RequestMapping(\u0026amp;quot;/httpclient\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; greeting(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;httpclient\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); map.put(\u0026amp;quot;name\u0026amp;quot;, name); return map; } }  Construct HttpClient to initiate a call to the RESTful service above The code example is as follows:\n Construct an HttpClient synchronous call instance:  HttpClientBuilder httpClientBuilder = HttpClientBuilder.create(); // SOFATracer SofaTracerHttpClientBuilder.clientBuilder(httpClientBuilder); CloseableHttpClient httpClient = httpClientBuilder.setConnectionManager(connManager).disableAutomaticRetries() .build();   Construct an HttpClient asynchronous call instance:  RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(6000).setConnectTimeout(6000).setConnectionRequestTimeout(6000).build(); HttpAsyncClientBuilder httpAsyncClientBuilder = HttpAsyncClientBuilder.create(); //tracer SofaTracerHttpClientBuilder.asyncClientBuilder(httpAsyncClientBuilder); CloseableHttpAsyncClient asyncHttpclient = httpAsyncClientBuilder.setDefaultRequestConfig(requestConfig).build();  When you construct the HttpClient via the SofaTracerHttpClientBuilder (clientBuilder method for synchronous call instance, and asyncClientBuilder method for asynchronous call instance) to initiate a call to the RESTful service above, the link data of …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-httpclient/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3efb3d0d5bd884665537aa974ec21359","permalink":"/en/projects/sofa-tracer/usage-of-httpclient/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-tracer/usage-of-httpclient/","summary":"HttpClient Integration In this document will demonstrate how to use SOFATracer to track of HttpClient, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026lt;!-- SOFATracer dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- HttpClient dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;!-- 4.5.X --\u0026gt; \u0026lt;version\u0026gt;4.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.httpcomponents\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;httpasyncclient\u0026lt;/artifactId\u0026gt; \u0026lt;!-- 4.X --\u0026gt; \u0026lt;version\u0026gt;4.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Project Configuration Then, add the parameters to be used by SOFATracer under the project\u0026rsquo;s application.","tags":null,"title":"HttpClient Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-httpclient/","wordcount":498},{"author":null,"categories":null,"content":" HttpClient Log Format After integrating tracer-httpclient-plugin, SOFATracer outputs the link data requested by HttpClient in JSON data by default.\nHttpClient digest log (httpclient-digest.log) The data is output in JSON format. Each key meaning is as follows:\n   Key Meaning     Time log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Request.url Request URL   Method Request HTTP method   Result.code HTTP call returns status code   req.size.bytes Request body size   resp.size.bytes Response body size   Time.cost.milliseconds Request time (ms)   Current.thread.name Thread name   Remote.app Name of the called application   Baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-09-27 21:58:43.067\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;HttpClientDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8801538056723034100235072\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/httpclient\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:33,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;I/O dispatcher 1\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Note: The application name can be passed in as a parameter when constructing an HttpClient instance via SofaTracerHttpClientBuilder.\nHttpClient statistical Log (httpclient-stat.log) stat.key is the collection of statistical keywords in this period, which uniquely determines a set of statistical data, including local.app, request.url, and method field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   request.url Request URL    method  Request HTTP method   count Number of requests in this period   total.cost.milliseconds Total duration (ms) for requests in this period   success Request result: Y means success (the result code starting with 1 and 2 indicates success, and 302 indicates that the redirection is successful, and others indicate failure); N indicates failure   load.test Pressure test mark: T indicates pressure test; F indicates non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-09-27 21:59:42.233\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/httpclient\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;HttpClientDemo\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:2,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:562,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-httpclient/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7df3d68ba21f1b2a43c0265fdc4eae3e","permalink":"/en/projects/sofa-tracer/log-format-httpclient/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/log-format-httpclient/","summary":"HttpClient Log Format After integrating tracer-httpclient-plugin, SOFATracer outputs the link data requested by HttpClient in JSON data by default.\nHttpClient digest log (httpclient-digest.log) The data is output in JSON format. Each key meaning is as follows:\n   Key Meaning     Time log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Request.url Request URL   Method Request HTTP method   Result.","tags":null,"title":"HttpClient log","type":"projects","url":"/en/projects/sofa-tracer/log-format-httpclient/","wordcount":220},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 HttpClient 进行埋点，本示例工程地址。\n假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n依赖引入 \u0026amp;lt;!-- SOFATracer 依赖 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- HttpClient 依赖 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.httpcomponents\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;httpclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!-- 版本 4.5.X 系列 --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.5.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.apache.httpcomponents\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;httpasyncclient\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!-- 版本 4.X 系列 --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.1.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  工程配置 在工程的 application.properties 文件下添加 SOFATracer 要使用的参数，包括spring.application.name 用于标示当前应用的名称；logging.path 用于指定日志的输出目录。\n# Application Name spring.application.name=HttpClientDemo # logging path logging.path=./logs  添加一个提供 RESTful 服务的 Controller 在工程代码中，添加一个简单的 Controller，例如：\n@RestController public class SampleRestController { private final AtomicLong counter = new AtomicLong(0); /** * Request http://localhost:8080/httpclient?name= * @param name name * @return Map of Result */ @RequestMapping(\u0026amp;quot;/httpclient\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; greeting(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;httpclient\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); map.put(\u0026amp;quot;name\u0026amp;quot;, name); return map; } }  构造 HttpClient 发起一次对上文的 RESTful 服务的调用 代码示例如下：\n 构造 HttpClient 同步调用实例：  HttpClientBuilder httpClientBuilder = HttpClientBuilder.create(); //SOFATracer SofaTracerHttpClientBuilder.clientBuilder(httpClientBuilder); CloseableHttpClient httpClient = httpClientBuilder.setConnectionManager(connManager).disableAutomaticRetries() .build();   构造 HttpClient 异步调用实例：  RequestConfig requestConfig = RequestConfig.custom().setSocketTimeout(6000).setConnectTimeout(6000).setConnectionRequestTimeout(6000).build(); HttpAsyncClientBuilder httpAsyncClientBuilder = HttpAsyncClientBuilder.create(); //tracer SofaTracerHttpClientBuilder.asyncClientBuilder(httpAsyncClientBuilder); CloseableHttpAsyncClient asyncHttpclient = httpAsyncClientBuilder.setDefaultRequestConfig(requestConfig).build();  通过 SofaTracerHttpClientBuilder(clientBuilder 方法构造同步，asyncClientBuilder 方法构造异步) 构造的 HttpClient 在发起对上文的 RESTful 服务调用的时候，就会埋点 SOFATracer 的链路的数据。\n运行 启动 SOFABoot 应用，在控制台中看到启动打印的日志如下：\n2018-09-27 20:31:21.465 INFO 33277 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2018-09-27 20:31:21.599 INFO 33277 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2018-09-27 20:31:21.608 INFO 33277 --- [ main] c.a.s.t.e.h.HttpClientDemoApplication : Started HttpClientDemoApplication in 5.949 seconds (JVM running for 6.573)  当有类似如下的日志时，说明 HttpClient 的调用成功：\n2018-09-27 20:31:22.336 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-httpclient/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3efb3d0d5bd884665537aa974ec21359","permalink":"/projects/sofa-tracer/usage-of-httpclient/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-httpclient/","summary":"在本文档将演示如何使用 SOFATracer 对 HttpClient 进行埋点，本示例工程地址。 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作： 依赖引入 \u0026lt;!-- SOFATracer 依","tags":null,"title":"HttpClient 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-httpclient/","wordcount":748},{"author":null,"categories":null,"content":" SOFATracer 集成 sofa-tracer-httpclient-plugin 插件后输出 HttpClient 请求的链路数据，默认为 JSON 数据格式。\nHttpClient 摘要日志（httpclient-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   request.url 请求地址   method http method   req.size.bytes 请求大小   resp.size.bytes 响应大小   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 23:43:13.191\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;HttpClientDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;1e27a79c1567438993170100210107\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;I/O dispatcher 1\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;21ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/httpclient\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  备注：应用名称可以通过 SofaTracerHttpClientBuilder 构造 HttpClient 实例时以入参的形式传入。\nHttpClient 统计日志（httpclient-stat.log） stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、request.url、和 method 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   request.url 请求 URL    method  请求 HTTP 方法   count 本段时间内请求次数   total.cost.milliseconds 本段时间内的请求总耗时（ms）   success 请求结果：Y 表示成功(1 开头和 2 开头的结果码算是成功的，302表示的重定向算成功，其他算是失败的)；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-02 23:44:11.785\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;HttpClientDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/httpclient\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:2,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:229,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-httpclient/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7df3d68ba21f1b2a43c0265fdc4eae3e","permalink":"/projects/sofa-tracer/log-format-httpclient/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-httpclient/","summary":"SOFATracer 集成 sofa-tracer-httpclient-plugin 插件后输出 HttpClient 请求的链路数据，默认为 JSON 数据格式。 HttpClient 摘要日志（httpclient-digest.log） 以 JSON 格式输出的数据，相应 key 的含","tags":null,"title":"HttpClient 日志","type":"projects","url":"/projects/sofa-tracer/log-format-httpclient/","wordcount":407},{"author":null,"categories":null,"content":" SOFARPC is integrated Hystrix provides fuse capability and is currently available in the first preview version. More information about Hystrix can be found in Hystrix Official Documentation, Hystrix integration capabilities are provided primarily by ScienJus, thanks for contribution.\nNext, let\u0026amp;rsquo;s talk about how to experience the fuse capability of Hystrix. The following example uses the SOFARPC 5.5.0 version. More Hystrix configuration and SOFABoot integration usage will be provided in subsequent releases, so stay tuned.\nWork preparation The Hystrix module is not loaded directly as an optional module by default. If you need to use it, you need to actively add the Hystrix maven dependency:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.netflix.hystrix\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hystrix-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.5.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  By explicitly opening Hystrix by configuration, HystrixFilter will be loaded automatically:\n// Open globally RpcConfigs.putValue(HystrixConstants.SOFA_HYSTRIX_ENABLED, true); // Open for a specific Consumer ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setParameter(HystrixConstants.SOFA_HYSTRIX_ENABLED, String.valueOf(true));  FallbackFactory The FallbackFactory interface mainly provides the injection capability of the Fallback implementation, which is used to automatically perform the degraded logic when Hystrix executes an exception (throws an exception, timeout, thread pool rejection, and blown).\nDefine the interface Fallback implementation:\npublic class HelloServiceFallback implements HelloService { @Override public String sayHello(String name, int age) { return \u0026amp;quot;fallback \u0026amp;quot; + name + \u0026amp;quot; from server! age: \u0026amp;quot; + age; } }  Inject Fallback implementation:\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setParameter(HystrixConstants.SOFA_HYSTRIX_ENABLED, String.valueOf(true)); // You can directly inject Fallback implementation directly using the default FallbackFactory SofaHystrixConfig.registerFallback(consumerConfig, new HelloServiceFallback()); // You can also customize FallbackFactory to directly inject FallbackFactory SofaHystrixConfig.registerFallbackFactory(consumerConfig, new HelloServiceFallbackFactory());  When the server responds with a failure, the client automatically triggers the Fallback logic execution.\nSetterFactory SetterFactory provides Hystrix fine-grained configuration capabilities. SOFARPC has provided the default DefaultSetterFactory to generate the Setter for each caller. If there is a more customized description, it can also be provided for each ConsumerConfig. Customize SetterFactory.\nSofaHystrixConfig.registerSetterFactory(consumerConfig, new CustomSetterFactory());  In the implementation provided by default, GroupKey is InterfaceId, and CommandKey is the …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault-hystrix/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7482dc341bf16dd5671634ffa689604a","permalink":"/en/projects/sofa-rpc/fault-hystrix/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/fault-hystrix/","summary":"SOFARPC is integrated Hystrix provides fuse capability and is currently available in the first preview version. More information about Hystrix can be found in Hystrix Official Documentation, Hystrix integration capabilities are provided primarily by ScienJus, thanks for contribution.\nNext, let\u0026rsquo;s talk about how to experience the fuse capability of Hystrix. The following example uses the SOFARPC 5.5.0 version. More Hystrix configuration and SOFABoot integration usage will be provided in subsequent releases, so stay tuned.","tags":null,"title":"Hystrix fault tolerance","type":"projects","url":"/en/projects/sofa-rpc/fault-hystrix/","wordcount":335},{"author":null,"categories":null,"content":" SOFARPC 已集成 Hystrix 提供熔断能力，当前提供第一个预览版。关于 Hystrix 的更多介绍可以参考 Hystrix 官方文档，Hystrix 集成能力主要由 ScienJus 提供，感谢贡献。\n接下来介绍一下如何体验 Hystrix 带来的熔断能力，以下示例使用 SOFARPC 5.5.0 版本，更多 Hystrix 的配置及 SOFABoot 集成使用方式将在后续版本提供，敬请关注。\n准备工作 Hystrix 模块作为可选模块默认不会直接加载，如需要使用，需要先主动加入 Hystrix maven 依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.netflix.hystrix\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hystrix-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.5.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  通过配置显式开启 Hystrix，将会自动加载 HystrixFilter：\n// 全局开启 RpcConfigs.putValue(HystrixConstants.SOFA_HYSTRIX_ENABLED, true); // 对特定 Consumer 开启 ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setParameter(HystrixConstants.SOFA_HYSTRIX_ENABLED, String.valueOf(true));  FallbackFactory FallbackFactory 接口主要提供 Fallback 实现的注入能力，用于在 Hystrix 执行出现异常（抛出异常、超时、线程池拒绝和熔断等）时自动执行降级逻辑。\n定义接口 Fallback 实现：\npublic class HelloServiceFallback implements HelloService { @Override public String sayHello(String name, int age) { return \u0026amp;quot;fallback \u0026amp;quot; + name + \u0026amp;quot; from server! age: \u0026amp;quot; + age; } }  注入 Fallback 实现：\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setParameter(HystrixConstants.SOFA_HYSTRIX_ENABLED, String.valueOf(true)); // 可以直接使用默认的 FallbackFactory 直接注入 Fallback 实现 SofaHystrixConfig.registerFallback(consumerConfig, new HelloServiceFallback()); // 也可以自定义 FallbackFactory 直接注入 FallbackFactory SofaHystrixConfig.registerFallbackFactory(consumerConfig, new HelloServiceFallbackFactory());  当服务端响应失败时，客户端会自动触发 Fallback 逻辑执行。\nSetterFactory SetterFactory 提供 Hystrix 细粒度配置能力，SOFARPC 已提供默认的 DefaultSetterFactory 来生成每个调用方对应的 Setter，如有更定制化的述求，也可以针对每个 ConsumerConfig 提供自定义 SetterFactory。\nSofaHystrixConfig.registerSetterFactory(consumerConfig, new CustomSetterFactory());  默认提供的实现中 GroupKey 为 InterfaceId，CommandKey 为方法的名称。\n支持 Hystrix 的版本信息 SOFARPC: 5.5.0, SOFABoot: 2.5.3。\nSOAF RPC 集成验证 Hystrix 版本：1.5.12。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault-hystrix/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7482dc341bf16dd5671634ffa689604a","permalink":"/projects/sofa-rpc/fault-hystrix/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/fault-hystrix/","summary":"SOFARPC 已集成 Hystrix 提供熔断能力，当前提供第一个预览版。关于 Hystrix 的更多介绍可以参考 Hystrix 官方文档，Hystrix 集成能力主要由 ScienJus 提供，感谢贡献。 接下来介绍一","tags":null,"title":"Hystrix 客户端熔断","type":"projects","url":"/projects/sofa-rpc/fault-hystrix/","wordcount":552},{"author":null,"categories":null,"content":" Installation guide To use Istio in a non-Kubernetes environment, you must complete the following critical tasks first:\n Configure the Istio API server for the Istio control plane. You can also use MemStore to launch Pilot for demonstration purpose. Manually add SOFAMosn to all microservice instances and start in SideCar mode. Make sure that all requests are routed through SOFAMosn.  Set control plane The Istio control plane consists of four main services: Pilot, Mixter, Citadel, and API server.\nAPI server Istio\u0026amp;rsquo;s API server, which is based on Kubernetes API server, provides configuration management and role-based access control. The API server requires an etcd cluster as the underlying persistent storage.\nInstall locally Use the following Docker compose file to install an API server for POC:\nversion: \u0026#39;2\u0026#39; services: etcd: image: quay.io/coreos/etcd:latest networks: default: aliases: - etcd ports: - \u0026amp;quot;4001:4001\u0026amp;quot; - \u0026amp;quot;2380:2380\u0026amp;quot; - \u0026amp;quot;2379:2379\u0026amp;quot; environment: - SERVICE_IGNORE=1 command: [ \u0026amp;quot;/usr/local/bin/etcd\u0026amp;quot;, \u0026amp;quot;-advertise-client-urls=http://0.0.0.0:2379\u0026amp;quot;, \u0026amp;quot;-listen-client-urls=http://0.0.0.0:2379\u0026amp;quot; ] istio-apiserver: image: gcr.io/google_containers/kube-apiserver-amd64:v1.7.3 networks: default: aliases: - apiserver ports: - \u0026amp;quot;8080:8080\u0026amp;quot; privileged: true environment: - SERVICE_IGNORE=1 command: [ \u0026amp;quot;kube-apiserver\u0026amp;quot;, \u0026amp;quot;--etcd-servers\u0026amp;quot;, \u0026amp;quot;http://etcd:2379\u0026amp;quot;, \u0026amp;quot;--service-cluster-ip-range\u0026amp;quot;, \u0026amp;quot;10.99.0.0/16\u0026amp;quot;, \u0026amp;quot;--insecure-port\u0026amp;quot;, \u0026amp;quot;8080\u0026amp;quot;, \u0026amp;quot;-v\u0026amp;quot;, \u0026amp;quot;2\u0026amp;quot;, \u0026amp;quot;--insecure-bind-address\u0026amp;quot;, \u0026amp;quot;0.0.0.0\u0026amp;quot; ]  Other control plane components Currently, SOFAMosn hasn\u0026amp;rsquo;t integrated with the components other than Pilot, so you don\u0026amp;rsquo;t need to install Mixer, Citadel and other components.\nAdd SOFAMosn Sidecar to microservice instances Every microservice application instance must have an associated SOFAMosn instance.\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-setup-zookeeper-installation/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4c0bd56673dc8aebef9011a22496392d","permalink":"/en/projects/sofa-mesh/pilot-setup-zookeeper-installation/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-mesh/pilot-setup-zookeeper-installation/","summary":"Installation guide To use Istio in a non-Kubernetes environment, you must complete the following critical tasks first:\n Configure the Istio API server for the Istio control plane. You can also use MemStore to launch Pilot for demonstration purpose. Manually add SOFAMosn to all microservice instances and start in SideCar mode. Make sure that all requests are routed through SOFAMosn.  Set control plane The Istio control plane consists of four main services: Pilot, Mixter, Citadel, and API server.","tags":null,"title":"Installation guide","type":"projects","url":"/en/projects/sofa-mesh/pilot-setup-zookeeper-installation/","wordcount":221},{"author":null,"categories":null,"content":"  Project address\n Introduction SOFABoot extends the Health Check of Spring Boot. For detailed information, see SOFABoot Documentation. This sample project is intended to demonstrate how to integrate the Health Check component of SOFABoot during merged deployment. Differences between the Health Check in merged deployment and that of a single SOFABoot application are as follows: + During static merged deployment, all Biz packages must pass the Health Check before the Ark package can be started normally. + When deploying the Biz packages dynamically in Jarslink2.0, all packages must pass the Health Check before successful deployment. + In merged deployment, a new check item named multiApplicationHealthChecker will be added when you access Spring Boot\u0026amp;rsquo;s default /health. The item is used to check the health of all Biz packages. Only after all Biz packages pass the Health Check can the merged package pass the Health Check.\nDependency To integrate the Health Check capability of SOFABoot in merged deployment, you need to add the following dependencies:\n\u0026amp;lt;!--health check--\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;healthcheck-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-web\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Note that spring-boot-starter-web is excluded to avoid starting multiple web applications when you introduce the healthcheck-sofa-boot-starter dependency.\nDemo  cd biz-health-check-sample/app-one \u0026amp;amp;\u0026amp;amp; mvn clean package Execute the mvn clean package command in the app-one root directory and package the application into an Ark or Biz package. The file will be exported to the biz-health-check-sample/app-one/target directory.\n cd biz-health-check-sample/app-two \u0026amp;amp;\u0026amp;amp; mvn clean package Execute the mvn clean package command in the app-two root directory and package the application into an Ark or Biz package. The file will be exported to the biz-health-check-sample/app-two/target directory.\n Use java -jar to start the Ark package for app-one.\n After the Ark package has started, visit http://localhost:8080/health in the browser. This is Spring Boot\u0026amp;rsquo;s default Health Check endpoint. A new check item named multiApplicationHealthChecker is added in the results and there is now only one Biz package. The page is displayed as follows: …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-health-demo/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"99832d969ec54b925c3dca1205b95165","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-health-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-health-demo/","summary":"Project address\n Introduction SOFABoot extends the Health Check of Spring Boot. For detailed information, see SOFABoot Documentation. This sample project is intended to demonstrate how to integrate the Health Check component of SOFABoot during merged deployment. Differences between the Health Check in merged deployment and that of a single SOFABoot application are as follows: + During static merged deployment, all Biz packages must pass the Health Check before the Ark package can be started normally.","tags":null,"title":"Integrate SOFABoot health check","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-health-demo/","wordcount":389},{"author":null,"categories":null,"content":" Since rpc-sofa-boot-starter version 6.0.1, SOFARPC provide the ability to integrate RESTful service with Swagger easily.\nIf you are using rpc-sofa-boot-starter in SOFABoot or Spring Boot environment and you want to enable Swagger support, first, you need add Swagger dependencies in your pom.xml:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.swagger.core.v3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;swagger-jaxrs2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.guava\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;guava\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;20.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Then you need add a configuration com.alipay.sofa.rpc.restSwagger=true in application.properties.\nFinally, visit http://localhost:8341/swagger/openapi and you can get all the Swagger OpenAPI information about SOFARPC\u0026amp;rsquo;s RESTful services.\nIf you are not using rpc-sofa-boot-starter or the version of rpc-sofa-boot-starter you depends is smaller then 6.0.1, you can integration SOFARPC RESTful service with Swagger by using the following tutorial.\nCurrently, SOFARPC does not provide the ability to integrate RESTful service with Swagger via one click. The ability will be provided in future versions, but you can refer to this document to integrate RESTful service with Swagger in the existing versions of SOFARPC.\nFirst, you need to add Swagger related dependencies into your application. Since SOFARPC\u0026amp;rsquo;s RESTful protocol is based on the JAXRS, so you just need to add Swagger\u0026amp;rsquo;s JAXRS dependency:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.swagger.core.v3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;swagger-jaxrs2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.guava\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;guava\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;20.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  The 20.0 version of Guava was added to resolve Guava version conflict.\nAdd a Swagger RESTful service To enable Swagger to expose SOFARPC\u0026amp;rsquo;s RESTful services through Swagger OpenAPI, we can provide Swagger\u0026amp;rsquo;s OpenAPI services through SOFARPC\u0026amp;rsquo;s RESTful services. First, you need to create a new interface:\n@Path(\u0026amp;quot;swagger\u0026amp;quot;) public interface OpenApiService { @GET @Path(\u0026amp;quot;openapi\u0026amp;quot;) @Produces(\u0026amp;quot;application/json\u0026amp;quot;) String openApi(); }  Then provide an implementation class and publish it as a RESTful service of SOFARPC:\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)}, interfaceType = OpenApiService.class) public class OpenApiServiceImpl implements OpenApiService, InitializingBean { private OpenAPI openAPI; @Override public String openApi() { return Json.pretty(openAPI); } @Override public void afterPropertiesSet() { List\u0026amp;lt;Package\u0026amp;gt; resources = new ArrayList\u0026amp;lt;\u0026amp;gt;(); Resources.add(this.getClass().getPackage()); // Scan the package of the current class, or scan the packages of other SOFARPC RESTful service …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-swagger/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d068767fe0dd2922eecef69736684be8","permalink":"/en/projects/sofa-rpc/restful-swagger/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/restful-swagger/","summary":"Since rpc-sofa-boot-starter version 6.0.1, SOFARPC provide the ability to integrate RESTful service with Swagger easily.\nIf you are using rpc-sofa-boot-starter in SOFABoot or Spring Boot environment and you want to enable Swagger support, first, you need add Swagger dependencies in your pom.xml:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.swagger.core.v3\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;swagger-jaxrs2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;20.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Then you need add a configuration com.alipay.sofa.rpc.restSwagger=true in application.properties.\nFinally, visit http://localhost:8341/swagger/openapi and you can get all the Swagger OpenAPI information about SOFARPC\u0026rsquo;s RESTful services.","tags":null,"title":"Integrate with Swagger","type":"projects","url":"/en/projects/sofa-rpc/restful-swagger/","wordcount":462},{"author":null,"categories":null,"content":"Jarslink2.0 supports receiving dynamic commands at runtime to manage the Biz package lifecycle. Before starting an Ark package that has introduced the Jarslink2.0 plugin, you can send commands through the telnet connection protocol with port 1234. For example, execute telnet ip 1234 to enter the Jarslink2.0 command interface and type \u0026amp;ldquo;help\u0026amp;rdquo; in the interface to obtain all relevant command manuals. Next we will introduce the syntax of each Jarslink2.0 command.\n Install the Biz package: The installation command is used to dynamically deploy of applications. Its syntax is install -b $bizFile. You can replace -b with -biz. All Jarslink2.0 commands must contain either a –b or –biz. The \u0026amp;ldquo;install\u0026amp;rdquo; command has only one parameter, the Biz package URI, which can either be the path of a local file or the link to a remote file, for example, install -b file:///Users/qilong.zql/sample-ark-biz.jar.\n Uninstall the Biz package: The uninstall command is used to close the application. The services released by the application and the resources that it occupied will be destroyed. Command syntax: uninstall -b -n $bizName -v $bizVersion. The command must specify the name and version number of the Biz package by -n and -v, which can be replaced with -name and -version. The name and version number of a Biz package are determined at the time of packaging. For detailed information, see Application Packaging.\n Switch the Biz package: The switch command is used for the Biz package hot update to ensure service continuity. Jarslink2.0 allows loading different versions of Biz packages with the same name at runtime. However, only one Biz package can deliver services at one time. To upgrade the loaded Biz package that is delivering services at runtime, execute the installation command to install a later version of the Biz package. After installation, the newer version is inactive because the older version is providing services. Execute the switch command to switch to the newer version without suspending the services that the application is delivering. This is called a hot update. The command syntax is switch -b -n $bizName -v $bizVersion. Parameters are the same as the above.\n Query the Biz package: The query command is used to query the Biz packages installed in JVM and their status. The command syntax is check -b -n $bizName -v $bizVersion, where the Biz package\u0026amp;rsquo;s name and version number are optional parameters. If you do not specify the name and the version number, information for all Biz packages will be returned. If you only specify the name, information for all versions with the specified name will be returned.\n  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-instruction/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c7e69fe8035b59c0e191538c8ef3da18","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-instruction/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-instruction/","summary":"Jarslink2.0 supports receiving dynamic commands at runtime to manage the Biz package lifecycle. Before starting an Ark package that has introduced the Jarslink2.0 plugin, you can send commands through the telnet connection protocol with port 1234. For example, execute telnet ip 1234 to enter the Jarslink2.0 command interface and type \u0026ldquo;help\u0026rdquo; in the interface to obtain all relevant command manuals. Next we will introduce the syntax of each Jarslink2.0 command.","tags":null,"title":"Interactive instruction","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-instruction/","wordcount":424},{"author":null,"categories":null,"content":" Introduction Jarslink2.0 is a functional SOFABoot plugin developed based on SOFAArk. It manages the merged deployment of multiple applications on top of the SOFAArk container, with the following features: + It supports runtime dynamic installation and uninstallation of applications. + It supports runtime application hot replacement capability to ensure service continuity. + For cross-application communication, it supports the JVM services publish and reference. Cross-application communication can base on RPC framework or internal JVM services. + It supports application Health Check.\nBackground At Ant Financial, it is common to deploy multiple applications on top of the same JVM. Main advantages of this approach are as follows:\n Merged deployment of unrelated applications: Some applications have no service dependencies on each other when they are deployed independently and their volume of business is small, so it would be a waste of resources to start the Java Virtual Machine just for them. Merged deployment of these applications can save resources.\n Merged deployment of related applications: Some applications have service dependencies between them. When deployed independently, RPC calls are used between applications. Despite the high stability of the distributed architecture, there are still delays caused by network jitter. By merged deployment of these applications, JVM internal calls will replace RPC calls, which reduces the call overhead.\n  Not only is there merged deployment between applications, but the near-client package has the same appeal.\nThe near-client package is a three-party component that provides a series of public services, normally introduced by the application as a dependency. This development mode is likely to cause two problems:\n The three-party dependency that is introduced by the near-client package conflicts with the dependency of the application itself, so an isolated deployment is expected.\n Since the near-client package is introduced by the application as a dependency, any upgrade of the near-client package will require upgrade of the application as well. However, as a common functional component, many business applications rely on the near-client package as a dependency, which entails a huge amount of transformation. Consequently, a dynamic upgrade of the near-client package is expected.\n  In addition to merged deployment, many Ant Financial business scenarios require hot deployment of modules, that is, when the application is running, a specific module needs to be dynamically replaced without affecting the normal running of other modules.\nJarslink2.0 is designed to solve such problems. It is an Ark Plugin developed based on SOFAArk and used to manage the merged deployment of multiple applications. Before getting to know Jarslink2.0, you need to understand the SOFAArk framework. For detailed information of SOFAArk, visit the link.\nPrinciple Jarslink2.0 is an Ark Plugin developed based on SOFAArk. Assuming that you …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-readme/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"48a4bc23f10f1ecca3960aecfd0a77d5","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-readme/","summary":"Introduction Jarslink2.0 is a functional SOFABoot plugin developed based on SOFAArk. It manages the merged deployment of multiple applications on top of the SOFAArk container, with the following features: + It supports runtime dynamic installation and uninstallation of applications. + It supports runtime application hot replacement capability to ensure service continuity. + For cross-application communication, it supports the JVM services publish and reference. Cross-application communication can base on RPC framework or internal JVM services.","tags":null,"title":"Introduction to Jarslink","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-readme/","wordcount":651},{"author":null,"categories":null,"content":" Pilot introduction The SOFAMesh project forked the Istio project to enhance Pilot\u0026amp;rsquo;s capabilities. Currently, the ongoing enhancements are focused on the following three areas:\n Support ZooKeeper as a registry center, and support SOFA, DUBBO and other microservice frameworks using ZooKeeper as a registry center. Support the common protocol framework. Use a common protocol, and support multiple protocols simultaneously based on Kubernetes DNS. Add register agent to support the container models of SOFA, Dubbo and HSF. Namely, a single application can register multiple service instances.  ","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-readme/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7b098e394986596d8fb01e1fe2120829","permalink":"/en/projects/sofa-mesh/pilot-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-mesh/pilot-readme/","summary":"Pilot introduction The SOFAMesh project forked the Istio project to enhance Pilot\u0026rsquo;s capabilities. Currently, the ongoing enhancements are focused on the following three areas:\n Support ZooKeeper as a registry center, and support SOFA, DUBBO and other microservice frameworks using ZooKeeper as a registry center. Support the common protocol framework. Use a common protocol, and support multiple protocols simultaneously based on Kubernetes DNS. Add register agent to support the container models of SOFA, Dubbo and HSF.","tags":null,"title":"Introduction to Pilot","type":"projects","url":"/en/projects/sofa-mesh/pilot-readme/","wordcount":84},{"author":null,"categories":null,"content":" ﻿## Product description SOFAArk is a light-weight，java based classloader isolation framework open sourced by Ant Financial. Based on Fat Jar technology, the container can pack simple single-module Java applications or Spring Boot applications into a self-contained executable Fat Jar, known as an Ark package. When the java -jar command is used to start an Ark package embedded with the SOFAArk class isolation container, the SOFAArk container will start, and it then starts each Ark plugin and application.\nBackground In Java world, dependency is always a problem, and can cause various errors, such as LinkageError, NoSuchMethodError etc. There are many ways to solve the dependency problems, the Spring Boot\u0026amp;rsquo;s way is using a dependency management to manage all the dependencies, make sure that all the dependencies in the dependency management will not conflict and can work pretty well. This is quite a simple and efficient way, it can cover most scenario, but there is some exceptions.\nFor example, there is a project that need protobuf version 2 and protobuf version 3, and because protobuf version 3 is not compatible with version 2, so the project can not simply upgrade the protobuf to version 3 to solve the problem. There is same problem for hessian version 3 and version 4.\nTo cover those exceptions, we need to introduce a classloader isolation way, make different version of a framework loaded by different classloader. There are many framework that can do classloader isolation, perhaps the most famous one is OSGi, but OSGi classloader schema is too complex, beside classloader isolation, it also has ability to do hot deploy and a lot of other functionalities that we actually don\u0026amp;rsquo;t want.\nSo this is the origin of SOFAArk, it\u0026amp;rsquo;s goal is to use a light-weight classloader isolation mechanism to solve the problem that Spring Boot did not solve. And just a remind that SOFAArk is not bind to Spring Boot, actually it is a more general classloader isolation framework that can be used with any other frameworks too.\nHow SOFAArk Works There are three concepts in SOFAArk: Ark Container, Ark-Plugin and Ark-Biz; they are organized as what the following graph shows:\nFirst of all, we explain what roles these concepts play;\n Ark Container: It\u0026amp;rsquo;s the runtime manager of total framework; it will startup in the first place, then it resolves Ark Plugin and Ark Biz in classpath and deploys them.\n Ark Plugin: A fat jar packaged by sofa-ark-plugin-maven-plugin, generally it would bring with a class-index configuration which describes what class would be exported and imported. Ark Plugin can resolve classes from each other.\n Ark Biz: A fat jar packaged by sofa-ark-maven-plugin, it mainly contains all staff what a project need in runtime. Ark Biz can resolve classes form Ark Plugin, but not inverse.\n  In runtime, Ark Container would automatically recognize Ark-Plugin and Ark-Biz in classpath, and load them with the independent classloader. According to …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-readme/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cdb6729fc7a63954b7559c8ea319f550","permalink":"/en/projects/sofa-boot/sofa-ark-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-ark-readme/","summary":"﻿## Product description SOFAArk is a light-weight，java based classloader isolation framework open sourced by Ant Financial. Based on Fat Jar technology, the container can pack simple single-module Java applications or Spring Boot applications into a self-contained executable Fat Jar, known as an Ark package. When the java -jar command is used to start an Ark package embedded with the SOFAArk class isolation container, the SOFAArk container will start, and it then starts each Ark plugin and application.","tags":null,"title":"Introduction to SOFAArk","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-readme/","wordcount":642},{"author":null,"categories":null,"content":"    MOSN, the short name of Modular Observable Smart Network, is a powerful proxy acting as Service Mesh\u0026amp;rsquo;s data plane like Envoy but written in Go. MOSN supports Envoy and Istio\u0026amp;rsquo;s APIs and can be integrated with Istio, so we use MOSN instead of Envoy in SOFAMesh. The initial version of MOSN was jointly contributed by Ant Financial and UC Business Unit of Alibaba, and we look forward to the community to participate in the follow-up development and build an open source excellent project together.\nCore competence  Integrated with Istio  Integrated with Istio 1.0 and V4 APIs to run based on full dynamic resource configuration  Core forwarding  Self-contained Web server Support TCP proxy Support TProxy mode  Multi-protocol  Support HTTP/1.1 and HTTP/2 Support SOFARPC Support Dubbo protocol (under development)  Core routing  Support Virtual Host routing Support Headers/URL/Prefix routing Support Host Metadata-based Subset routing Support retry  Backend Management and load balancing  Support connection pool Support throttling Support active backend health check Support load balancing strategies, such as Random and RR Support Host Metadata-based Subset load balancing strategy  Observability  Observe network data Observing protocol data  TLS  Support HTTP/1.1 on TLS Support HTTP/2.0 on TLS Support SOFARPC on TLS  Process management + Support smooth reload + Support smooth upgrade Extension capability + Support custom private protocols + Support adding custom extensions in protocol at the TCP IO layer  ","date":-62135596800,"description":"","dir":"projects/sofa-mesh/sofa-mosn-readme/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"11219bb3b9689ec5f328b8281bd62a95","permalink":"/en/projects/sofa-mesh/sofa-mosn-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-mesh/sofa-mosn-readme/","summary":"MOSN, the short name of Modular Observable Smart Network, is a powerful proxy acting as Service Mesh\u0026rsquo;s data plane like Envoy but written in Go. MOSN supports Envoy and Istio\u0026rsquo;s APIs and can be integrated with Istio, so we use MOSN instead of Envoy in SOFAMesh. The initial version of MOSN was jointly contributed by Ant Financial and UC Business Unit of Alibaba, and we look forward to the community to participate in the follow-up development and build an open source excellent project together.","tags":null,"title":"Introduction to SOFAMosn","type":"projects","url":"/en/projects/sofa-mesh/sofa-mosn-readme/","wordcount":223},{"author":null,"categories":null,"content":" Introduction to RheaKV RheaKV is a lightweight, distributed, and embedded KV storage library, which is included in the JRaft project as a submodule.\nFeatures\n Embedded: RheaKV is embedded in applications in the form of Jar files. Strong consistency: RheaKV ensures data reliability and consistency based on the multi-raft distributed consensus protocol. Self-driven (not fully implemented at present): RheaKV supports automatic diagnosis, optimization, decision making, and recovery. Monitorable: RheaKV automatically reports meta information and state information by node to the PD. Basic APIs: get, put, and delete; cross-region APIs: scan, batch put, and distributed lock.  Architecture design Terms and definitions  PD: The global central master node that is responsible for scheduling the entire cluster. A PD server can manage multiple clusters, with each of them isolated by clusterId. The PD server requires separate deployment. Actually, many scenarios do not need automatic cluster management, and RheaKV does not support PD. Store: A physical storage node within a cluster. A store may contain one or more regions. Region: The minimal KV data unit. Each region can be understood as a database partition or database shard, and has a left closed and right open interval [startKey, endKey).  Storage design  The storage layer adopts a pluggable design and supports both MemoryDB and RocksDB currently:  MemoryDB is implemented based on ConcurrentSkipListMap and provides better performance. However, its independent storage capacity is restricted by the memory. RocksDB is suitable for scenarios with large data volumes, because its storage capacity is only restricted by the disk.  Strong data consistency is ensured. RheaKV synchronizes data to other replicas with the help of JRaft, and each data change is recorded as a Raft log entry. The log replication feature of Raft ensures all data is securely and reliably synchronized to all nodes within the same Raft group.  Scenarios  Lightweight state/meta information storage and cluster synchronization Distributed lock service  API description Generally, RheaKV APIs are divided into two types: synchronous APIs and asynchronous APIs. Methods whose names start with letter b (block) are synchronous blocking APIs, and the rest are asynchronous APIs. All asynchronous APIs return the same CompletableFuture parameter. The read method may contain another important parameter, that is readOnlySafe. When this parameter is set to true, linearizable read is supported. Read methods that do not contain this parameter provide linearizable read by default.\nget CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final byte[] key); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final String key); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final byte[] key, final boolean readOnlySafe); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final String key, final boolean readOnlySafe); byte[] bGet(final byte[] key); byte[] bGet(final String key); byte[] bGet(final byte[] key, final boolean …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jraft-rheakv-user-guide/","fuzzywordcount":4100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e6fa7125455982961214dfe82245be4d","permalink":"/en/projects/sofa-jraft/jraft-rheakv-user-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":20,"relpermalink":"/en/projects/sofa-jraft/jraft-rheakv-user-guide/","summary":"Introduction to RheaKV RheaKV is a lightweight, distributed, and embedded KV storage library, which is included in the JRaft project as a submodule.\nFeatures\n Embedded: RheaKV is embedded in applications in the form of Jar files. Strong consistency: RheaKV ensures data reliability and consistency based on the multi-raft distributed consensus protocol. Self-driven (not fully implemented at present): RheaKV supports automatic diagnosis, optimization, decision making, and recovery. Monitorable: RheaKV automatically reports meta information and state information by node to the PD.","tags":null,"title":"JRaft RheaKV user guide","type":"projects","url":"/en/projects/sofa-jraft/jraft-rheakv-user-guide/","wordcount":4086},{"author":null,"categories":null,"content":" RheaKV 是一个轻量级的分布式的嵌入式的 KV 存储 lib， rheaKV 包含在 jraft 项目中，是 jraft 的一个子模块。\n定位与特性\n 嵌入式: jar 包方式嵌入到应用中 强一致性: 基于 multi-raft 分布式一致性协议保证数据可靠性和一致性 自驱动 （目前未完全实现）: 自诊断, 自优化, 自决策, 自恢复 可监控: 基于节点自动上报到PD的元信息和状态信息 基本API: get/put/delete 和跨分区 scan/batch put, distributed lock 等等  架构设计 功能名词  PD: 全局的中心总控节点，负责整个集群的调度，一个 PD server 可以管理多个集群，集群之间基于 clusterId 隔离；PD server 需要单独部署，当然，很多场景其实并不需要自管理，rheaKV 也支持不启用 PD Store: 集群中的一个物理存储节点，一个 store 包含一个或多个 region Region: 最小的 KV 数据单元，可理解为一个数据分区或者分片，每个 region 都有一个左闭右开的区间 [startKey, endKey)  存储设计  存储层为可插拔设计， 目前支持 MemoryDB 和 RocksDB 两种实现：  MemoryDB 基于 ConcurrentSkipListMap 实现，有更好的性能，但是单机存储容量受内存限制 RocksDB 在存储容量上只受磁盘限制，适合更大数据量的场景  数据强一致性， 依靠 jraft 来同步数据到其他副本, 每个数据变更都会落地为一条 raft 日志, 通过 raft 的日志复制功能, 将数据安全可靠地同步到同 group 的全部节点中  使用场景  轻量级的状态/元信息存储以及集群同步 分布式锁服务  API 说明 整体上 rheaKV apis 分为异步和同步两类， 其中以 b （block）开头的方法均为同步阻塞方法， 其他为异步方法，异步方法均返回一个 CompletableFuture，对于 read method， 还有一个重要参数 readOnlySafe，为 true 时表示提供线性一致读， 不包含该参数的 read method 均为默认提供线性一致读\nget CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final byte[] key); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final String key); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final byte[] key, final boolean readOnlySafe); CompletableFuture\u0026amp;lt;byte[]\u0026amp;gt; get(final String key, final boolean readOnlySafe); byte[] bGet(final byte[] key); byte[] bGet(final String key); byte[] bGet(final byte[] key, final boolean readOnlySafe); byte[] bGet(final String key, final boolean readOnlySafe);   String 类型入参，rheaKV 内部提供了更高效的 Utf8String encoder/decoder， 业务 key 为 String 时， 推荐的做法是直接使用 String 参数的接口 不需要线性一致读语义的场景可以将 readOnlySafe 设置为 false， 负载均衡器会优先选择本地调用，本地不能提供服务则轮询选择一台远程机器发起读请求  multiGet CompletableFuture\u0026amp;lt;Map\u0026amp;lt;ByteArray, byte[]\u0026amp;gt;\u0026amp;gt; multiGet(final List\u0026amp;lt;byte[]\u0026amp;gt; keys); CompletableFuture\u0026amp;lt;Map\u0026amp;lt;ByteArray, byte[]\u0026amp;gt;\u0026amp;gt; multiGet(final List\u0026amp;lt;byte[]\u0026amp;gt; keys, final boolean readOnlySafe); Map\u0026amp;lt;ByteArray, byte[]\u0026amp;gt; bMultiGet(final List\u0026amp;lt;byte[]\u0026amp;gt; keys); Map\u0026amp;lt;ByteArray, byte[]\u0026amp;gt; bMultiGet(final List\u0026amp;lt;byte[]\u0026amp;gt; keys, final boolean readOnlySafe);   multiGet 支持跨分区查询，rheaKV 内部会自动计算每个 key 的所属分区（region）并行发起调用， 最后合并查询结果 为了可以将 byte[] 放进 HashMap，这里曲线救国，返回值中 Map 的 key 为 ByteArray 对象，是对 byte[] 的一层包装，实现了 byte[] 的 hashCode  scan \u0026amp;amp; iterator CompletableFuture\u0026amp;lt;List\u0026amp;lt;KVEntry\u0026amp;gt;\u0026amp;gt; scan(final byte[] startKey, final byte[] endKey); CompletableFuture\u0026amp;lt;List\u0026amp;lt;KVEntry\u0026amp;gt;\u0026amp;gt; scan(final String startKey, final String endKey); CompletableFuture\u0026amp;lt;List\u0026amp;lt;KVEntry\u0026amp;gt;\u0026amp;gt; scan(final byte[] startKey, final byte[] endKey, final boolean readOnlySafe); CompletableFuture\u0026amp;lt;List\u0026amp;lt;KVEntry\u0026amp;gt;\u0026amp;gt; scan(final String startKey, final String endKey, final boolean readOnlySafe); List\u0026amp;lt;KVEntry\u0026amp;gt; bScan(final byte[] startKey, final byte[] endKey); List\u0026amp;lt;KVEntry\u0026amp;gt; bScan(final String startKey, final String endKey); List\u0026amp;lt;KVEntry\u0026amp;gt; bScan(final byte[] startKey, final byte[] endKey, final boolean readOnlySafe); List\u0026amp;lt;KVEntry\u0026amp;gt; bScan(final String startKey, final String endKey, final boolean readOnlySafe); RheaIterator\u0026amp;lt;KVEntry\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jraft-rheakv-user-guide/","fuzzywordcount":6500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e6fa7125455982961214dfe82245be4d","permalink":"/projects/sofa-jraft/jraft-rheakv-user-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":13,"relpermalink":"/projects/sofa-jraft/jraft-rheakv-user-guide/","summary":"RheaKV 是一个轻量级的分布式的嵌入式的 KV 存储 lib， rheaKV 包含在 jraft 项目中，是 jraft 的一个子模块。 定位与特性 嵌入式: jar 包方式嵌入到应用中 强一致性: 基于 multi-raft 分布","tags":null,"title":"JRaft RheaKV 用户指南","type":"projects","url":"/projects/sofa-jraft/jraft-rheakv-user-guide/","wordcount":6406},{"author":null,"categories":null,"content":" 0. Basic concepts  Every request submitted by log index to a Raft group is serialized into a log entry. Each log entry has an ID, which monotonically increases within the Raft group, and the log entries are replicated to every Raft node in the group. Term is a long-type number that monotonically increases within the Raft group. You can simply take it as the number of votes. The term of an elected leader is called the leader term. Before this leader steps down, log entries submitted during this period have the same term.  1. Configuration and supporting classes This topic mainly describes the configuration and utility methods and classes. The core objects are:\n Endpoint, which refers to a service address PeerId, which refers to ID of a Raft node Configuration, which refers to the configuration of a Raft group, or a node list in other words.  1.1 Endpoint Endpoint refers to a service address, including the IP address and the port number. Raft nodes must not be started on the IPv4 address 0.0.0.0. The startup IP address must be clearly specified Create a address, and bind it to port 8080 of the local host, as shown in the following example:\nEndpoint addr = new Endpoint(\u0026amp;quot;localhost\u0026amp;quot;, 8080); String s = addr.toString(); // The result is localhost:8080 PeerId peer = new PeerId(); boolean success = peer.parse(s); // Specifies whether parsing the endpoint from a string is supported. The result is true.  1.2 PeerId A PeerId indicates a participant (leader, follower, or candidate) of the Raft protocol. It comprises three elements in the format of ip:port:index, where ip is the IP address of the node, port is the port number, and index is the serial number of the same port. Currently, the index is not used, and is always set to 0. This field is reserved to allow starting different Raft nodes from the same port and to differentiate them by index.\nCreate a PeerId and set the index to 0, the IP to localhost, and the port to 8080:\nPeerId peer = new PeerId(\u0026amp;quot;localhost\u0026amp;quot;, 8080); EndPoint addr = peer.getEndpoint(); // Gets the endpoint of a node int index = peer.getIdx(); // Gets the index of a node, which is always set to 0 currently String s = peer.toString(); // The result is localhost:8080 boolean success = peer.parse(s); // Specifies whether PeerId parsing from a string is supported. The result is true.  1.3 Configuration It refers to the configuration of a Raft group, or a participant list in other words.\nPeerId peer1 = ... PeerId peer2 = ... PeerId peer3 = ... // A Raft group that consists of three nodes Configuration conf = new Configuration(); conf.addPeer(peer1); conf.addPeer(peer2); conf.addPeer(peer3);  1.4 JRaftUtils utility class To enable users conveniently create objects such as Endpoint, PeerId, and Configuration, Jraft provides the JRaftUtils class to help users quickly create the required objects from strings.\nEndpoint addr = JRaftUtils.getEndpoint(\u0026amp;quot;localhost:8080\u0026amp;quot;); PeerId peer = …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jraft-user-guide/","fuzzywordcount":7100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"105dfa34c3b20df1f2c23c112730507d","permalink":"/en/projects/sofa-jraft/jraft-user-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":34,"relpermalink":"/en/projects/sofa-jraft/jraft-user-guide/","summary":"0. Basic concepts  Every request submitted by log index to a Raft group is serialized into a log entry. Each log entry has an ID, which monotonically increases within the Raft group, and the log entries are replicated to every Raft node in the group. Term is a long-type number that monotonically increases within the Raft group. You can simply take it as the number of votes. The term of an elected leader is called the leader term.","tags":null,"title":"JRaft user guide","type":"projects","url":"/en/projects/sofa-jraft/jraft-user-guide/","wordcount":7075},{"author":null,"categories":null,"content":" 1. 基本概念说明  log index 提交到 raft group 中的任务都将序列化为一条日志存储下来，每条日志一个编号，在整个 raft group 内单调递增并复制到每个 raft 节点。 term 在整个 raft group 中单调递增的一个 long 数字，可以简单地认为表示一轮投票的编号，成功选举出来的 leader 对应的 term 称为 leader term，在这个 leader 没有发生变更的阶段内提交的日志都将拥有相同的 term 编号。  2. 配置和辅助类 本节主要介绍 jraft 的配置和辅助工具相关接口和类。核心包括：\n Endpoint 表示一个服务地址。 PeerId 表示一个 raft 参与节点。 Configuration 表示一个 raft group 配置，也就是节点列表。  2.1 地址 Endpoint Endpoint 表示一个服务地址，包括 IP 和端口， raft 节点不允许启动在 0.0.0.0 所有的 IPv4 上，需要明确指定启动的 IP 创建一个地址，绑定在 localhost 的 8080 端口上，如下例：\nEndpoint addr = new Endpoint(\u0026amp;quot;localhost\u0026amp;quot;, 8080); String s = addr.toString(); // 结果为 localhost:8080 PeerId peer = new PeerId(); boolean success = peer.parse(s); // 可以从字符串解析出地址，结果为 true  2.2 节点 PeerId PeerId 表示一个 raft 协议的参与者（leader/follower/candidate etc.)， 它由三元素组成： ip:port:index， IP 就是节点的 IP， port 就是端口， index 表示同一个端口的序列号，目前没有用到，总被认为是 0。预留此字段是为了支持同一个端口启动不同的 raft 节点，通过 index 区分。\n创建一个 PeerId, index 指定为 0， ip 和端口分别是 localhost 和 8080:\nPeerId peer = new PeerId(\u0026amp;quot;localhost\u0026amp;quot;, 8080); EndPoint addr = peer.getEndpoint(); // 获取节点地址 int index = peer.getIdx(); // 获取节点序号，目前一直为 0 String s = peer.toString(); // 结果为 localhost:8080 boolean success = peer.parse(s); // 可以从字符串解析出 PeerId，结果为 true  2.3 配置 Configuration Configuration 表示一个 raft group 的配置，也就是参与者列表：\nPeerId peer1 = ... PeerId peer2 = ... PeerId peer3 = ... // 由 3 个节点组成的 raft group Configuration conf = new Configuration(); conf.addPeer(peer1); conf.addPeer(peer2); conf.addPeer(peer3);  2.4 工具类 JRaftUtils 为了方便创建 Endpoint/PeerId/Configuration 等对象， jraft 提供了 JRaftUtils 来快捷地从字符串创建出所需要的对象：\nEndpoint addr = JRaftUtils.getEndpoint(\u0026amp;quot;localhost:8080\u0026amp;quot;); PeerId peer = JRaftUtils.getPeerId(\u0026amp;quot;localhost:8080\u0026amp;quot;); // 三个节点组成的 raft group 配置，注意节点之间用逗号隔开 Configuration conf = JRaftUtils.getConfiguration(\u0026amp;quot;localhost:8081,localhost:8082,localhost:8083\u0026amp;quot;);  2.5 回调 Closure 和状态 Status Closure 就是一个简单的 callback 接口， jraft 提供的大部分方法都是异步的回调模式，结果通过此接口通知：\npublic interface Closure { /** * Called when task is done. * * @param status the task status. */ void run(Status status); }  结果通过 Status 告知，Status#isOk() 告诉你成功还是失败，错误码和错误信息可以通过另外两个方法获取：\nboolean success= status.isOk(); RaftError error = status.getRaftError(); // 错误码，RaftError 是一个枚举类 String errMsg = status.getErrorMsg(); // 获取错误详情  Status 提供了一些方法来方便地创建：\n// 创建一个成功的状态 Status ok = Status.OK(); // 创建一个失败的错误，错误信息支持字符串模板 String filePath = \u0026amp;quot;/tmp/test\u0026amp;quot;; Status status = new Status(RaftError.EIO, \u0026amp;quot;Fail to read file from %s\u0026amp;quot;, filePath);  2.6 任务 Task Task 是用户使用 jraft 最核心的类之一，用于向一个 raft 复制分组提交一个任务，这个任务提交到 leader，并复制到其他 follower 节点， Task 包括：\n ByteBuffer data 任务的数据，用户应当将要复制的业务数据通过一定序列化方式（比如 java/hessian2) 序列化成一个 ByteBuffer，放到 task 里。 long expectedTerm = -1 任务提交时预期的 leader term，如果不提供(也就是默认值 -1 )，在任务应用到状态机之前不会检查 leader 是否发生了变更，如果提供了（从状态机回调中获取，参见下文），那么在将任务应用到状态机之前，会检查 term 是否匹配，如果不匹配将拒绝该任务。 Closure done 任务的回调，在任务完成的时候通知此对象，无论成功还是失败。这个 closure 将在 StateMachine#onApply(iterator) 方法应用到状态机的时候，可以拿到并调用，一般用于客户端应答的返回。  创建一个简单 Task 实例：\nClosure done = ...; Task task = new …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jraft-user-guide/","fuzzywordcount":11800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"105dfa34c3b20df1f2c23c112730507d","permalink":"/projects/sofa-jraft/jraft-user-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":24,"relpermalink":"/projects/sofa-jraft/jraft-user-guide/","summary":"1. 基本概念说明 log index 提交到 raft group 中的任务都将序列化为一条日志存储下来，每条日志一个编号，在整个 raft group 内单调递增并复制到每个 raft 节点。 term 在整个 raft group 中单","tags":null,"title":"JRaft 用户指南","type":"projects","url":"/projects/sofa-jraft/jraft-user-guide/","wordcount":11778},{"author":null,"categories":null,"content":" SOFABoot 提供三种方式给开发人员发布和引用 JVM 服务\n XML 方式 Annotation 方式 编程 API 方式  XML 方式 服务发布 首先需要定义一个 Bean：\n\u0026amp;lt;bean id=\u0026amp;quot;sampleService\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleServiceImpl\u0026amp;quot;\u0026amp;gt;  然后通过 SOFA 提供的 Spring 扩展标签来将上面的 Bean 发布成一个 SOFA JVM 服务。\n\u0026amp;lt;sofa:service interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; ref=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  上面的配置中的 interface 指的是需要发布成服务的接口，ref 指向的是需要发布成 JVM 服务的 Bean，至此，我们就已经完成了一个 JVM 服务的发布。\n服务引用 使用 SOFA 提供的 Spring 扩展标签引用服务:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleServiceRef\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  上面的配置中的 interface 是服务的接口，需要和发布服务时配置的 interface 一致。id 属性的含义同 Spring BeanId。上面的配置会生成一个 id 为 sampleServiceRef 的 Spring Bean，你可以将 sampleServiceRef 这个 Bean 注入到当前 SOFABoot 模块 Spring 上下文的任意地方。\n service/reference 标签还支持 RPC 服务发布，相关文档: RPC 服务发布与引用\n Annotation 方式  警告\n如果一个服务已经被加上了 @SofaService 的注解，它就不能再用 XML 的方式去发布服务了，选择一种方式发布服务，而不是两种混用。\n 除了通过 XML 方式发布 JVM 服务和引用之外，SOFABoot 还提供了 Annotation 的方式来发布和引用 JVM 服务。通过 Annotation 方式发布 JVM 服务，只需要在实现类上加一个 @SofaService 注解即可，如下：\n@SofaService public class SampleImpl implements SampleInterface { public void test() { } }   提示\n@SofaService 的作用是将一个 Bean 发布成一个 JVM 服务，这意味着虽然你可以不用再写 \u0026amp;lt;sofa:service/\u0026amp;gt; 的配置，但是还是需要事先将 @SofaService 所注解的类配置成一个 Spring Bean。\n 在使用 XML 配置 \u0026amp;lt;sofa:service/\u0026amp;gt; 的时候，我们配置了一个 interface 属性，但是在使用 @SofaService 注解的时候，却没有看到有配置服务接口的地方。这是因为当被 @SofaService 注解的类只有一个接口的时候，框架会直接采用这个接口作为服务的接口。当被 @SofaService 注解的类实现了多个接口时，可以设置 @SofaService 的 interfaceType 字段来指定服务接口，比如下面这样：\n@SofaService(interfaceType=SampleInterface.class) public class SampleImpl implements SampleInterface, Serializable { public void test() { } }  和 @SofaService 对应，Sofa 提供了 @SofaReference 来引用一个 JVM 服务。假设我们需要在一个 Spring Bean 中使用 SampleJvmService 这个 JVM 服务，那么只需要在字段上加上一个 @SofaReference 的注解即可：\npublic class SampleServiceRef { @SofaReference private SampleService sampleService; }  和 @SofaService 类似，我们也没有在 @SofaReference 上指定服务接口，这是因为 @SofaReference 在不指定服务接口的时候，会采用被注解字段的类型作为服务接口，你也可以通过设定 @SofaReference 的 interfaceType 属性来指定：\npublic class SampleServiceRef { @SofaReference(interfaceType=SampleService.class) private SampleService sampleService; }  使用 @SofaService 注解发布服务时，需要在实现类上打上 @SofaService 注解；在 Spring Boot 使用 Bean Method 创建 Bean 时，会导致 @Bean 和 @SofaService 分散在两处，而且无法对同一个实现类使用不同的 unique id。因此自 SOFABoot v2.6.0 及 v3.1.0 版本起，支持 @SofaService 作用在 Bean Method 之上，例如：\n@Configuration public class SampleSofaServiceConfiguration { @Bean(\u0026amp;quot;sampleSofaService\u0026amp;quot;) @SofaService(uniqueId = \u0026amp;quot;service1\u0026amp;quot;) SampleService service() { return new SampleServiceImpl(\u0026amp;quot;\u0026amp;quot;); } }  同样为了方便在 Spring Boot Bean Method 使用注解 @SofaReference 引用服务，自 SOFABoot v2.6.0 及 v3.1.0 版本起，支持在 Bean Method 参数上使用 @SofaReference 注解引用 JVM 服务，例如：\n@Configuration public class MultiSofaReferenceConfiguration { @Bean(\u0026amp;quot;sampleReference\u0026amp;quot;) TestService …","date":-62135596800,"description":"","dir":"projects/sofa-boot/module-service/","fuzzywordcount":2000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"527472fbe57ce450e4e2b41d878704cb","permalink":"/projects/sofa-boot/module-service/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/module-service/","summary":"SOFABoot 提供三种方式给开发人员发布和引用 JVM 服务 XML 方式 Annotation 方式 编程 API 方式 XML 方式 服务发布 首先需要定义一个 Bean： \u0026lt;bean id=\u0026quot;sampleService\u0026quot; class=\u0026quot;com.alipay.sofa.runtime.test.service.SampleServiceImpl\u0026quot;\u0026gt; 然后通过 SOFA 提供的 Spring 扩展标签来将上","tags":null,"title":"JVM 服务发布与引用","type":"projects","url":"/projects/sofa-boot/module-service/","wordcount":1926},{"author":null,"categories":null,"content":" 1. Create a Maven project and import the dependency \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${registry.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2. Create the SOFARegistry client instance The key code for creating the SOFARegistry client instance is as follows:\nRegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init();  Properties related to SOFARegistry are specified by the DefaultRegistryClientConfigBuilder class, which provides the following key properties:\npublic class DefaultRegistryClientConfigBuilder { private String instanceId; private String zone = DEFAULT_ZONE; private String registryEndpoint; private int registryEndpointPort = 9603; private String dataCenter = DEFAULT_DATA_CENTER; private String appName; private int connectTimeout = 3000; private int socketTimeout = 3000; private int invokeTimeout = 1000; private int recheckInterval = 500; private int observerThreadCoreSize = 5; private int observerThreadMaxSize = 10; private int observerThreadQueueLength = 1000; private int syncConfigRetryInterval = 30000; }     Property Type Description     instanceId String The ID of the instance. Default value: DEFAULT_INSTANCE_ID. The same instance ID must be used for data publishing and subscription. The unique data identifier consists of dataId+group+instanceId.   zone String The zone where the instance is located. Default value: DEFAULT_ZONE.   registryEndpoint String The endpoint of any session node of the servers.   registryEndpointPort Integer The session.server.httpServerPort configured for a session node. Default value: 9603.   dataCenter String The data center of SOFARegistry. Default value: DefaultDataCenter.   appName String The name of the app that accesses SOFARegistry.   connectTimeout Integer Specifies the timeout for establishing a connection with a server. Default value: 3000 ms.   socketTimeout Integer Specifies the timeout for accessing the servers\u0026amp;rsquo; REST API. Default value: 3000 ms.   invokeTimeout Integer Specifies the timeout for calling services on the servers. Default value: 1000 ms.   recheckInterval Integer Specifies the interval for checking the task queue. Default value: 500 ms.   observerThreadCoreSize Integer Specifies the number of core threads in the thread pool that process data pushed from the servers. Default value: 5.   observerThreadMaxSize Integer Specifies the maximum number of threads in the thread pool that process data pushed from the servers. Default value: 10.   observerThreadQueueLength Integer Specifies the maximum thread queue length of the thread pool that processes data pushed from the servers. Default value: 1000.   syncConfigRetryInterval Integer Specifies the retry interval to synchronize the …","date":-62135596800,"description":"","dir":"projects/sofa-registry/java-sdk/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"32cff5cc5d89ffa85b12c207a1c0c6f3","permalink":"/en/projects/sofa-registry/java-sdk/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-registry/java-sdk/","summary":"1. Create a Maven project and import the dependency \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;registry-client-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${registry.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  2. Create the SOFARegistry client instance The key code for creating the SOFARegistry client instance is as follows:\nRegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026quot;127.0.0.1\u0026quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init();  Properties related to SOFARegistry are specified by the DefaultRegistryClientConfigBuilder class, which provides the following key properties:\npublic class DefaultRegistryClientConfigBuilder { private String instanceId; private String zone = DEFAULT_ZONE; private String registryEndpoint; private int registryEndpointPort = 9603; private String dataCenter = DEFAULT_DATA_CENTER; private String appName; private int connectTimeout = 3000; private int socketTimeout = 3000; private int invokeTimeout = 1000; private int recheckInterval = 500; private int observerThreadCoreSize = 5; private int observerThreadMaxSize = 10; private int observerThreadQueueLength = 1000; private int syncConfigRetryInterval = 30000; }     Property Type Description     instanceId String The ID of the instance.","tags":null,"title":"Java SDK","type":"projects","url":"/en/projects/sofa-registry/java-sdk/","wordcount":890},{"author":null,"categories":null,"content":" 1. Maven 坐标 \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${registry.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2. 创建 SOFARegistry 客户端实例 构建 SOFARegistry 客户端实例的关键代码如下：\nRegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init();  其中注册中心相关的属性通过 DefaultRegistryClientConfigBuilder 构建指定，该类包含以下关键属性：\npublic class DefaultRegistryClientConfigBuilder { private String instanceId; private String zone = DEFAULT_ZONE; private String registryEndpoint; private int registryEndpointPort = 9603; private String dataCenter = DEFAULT_DATA_CENTER; private String appName; private int connectTimeout = 3000; private int socketTimeout = 3000; private int invokeTimeout = 1000; private int recheckInterval = 500; private int observerThreadCoreSize = 5; private int observerThreadMaxSize = 10; private int observerThreadQueueLength = 1000; private int syncConfigRetryInterval = 30000; }     属性名 属性类型 描述     instanceId String 实例ID，发布订阅时需要使用相同值，数据唯一标识由dataId+group+instanceId组成，默认值 DEFAULT_INSTANCE_ID。   zone String 单元化所属 zone，默认值 DEFAULT_ZONE。   registryEndpoint String 服务端任一 Session 节点地址。   registryEndpointPort int Session 节点配置的 session.server.httpServerPort 端口值，默认值 9603。   dataCenter String 数据中心，默认值 DefaultDataCenter。   appName String 应用名。   connectTimeout int 与服务端建立连接超时时间，默认值 3000ms。   socketTimeout int 访问服务端 REST 接口超时时间，默认值 3000ms。   invokeTimeout int 调用服务端服务超时时间，默认值 1000ms。   recheckInterval int 检测任务队列间隔时间，默认值 500ms。   observerThreadCoreSize int 处理服务端推送数据线程池核心线程大小，默认值 5。   observerThreadMaxSize int 处理服务端推送数据线程池最大线程大小，默认值 10。   observerThreadQueueLength int 处理服务端推送数据线程池队列大小，默认值 1000。   syncConfigRetryInterval int 同步服务端列表间隔时间，默认值 30000ms。    3. 发布数据 发布数据的关键代码如下：\n// 构造发布者注册表 PublisherRegistration registration = new PublisherRegistration(\u0026amp;quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026amp;quot;); registration.setGroup(\u0026amp;quot;TEST_GROUP\u0026amp;quot;); registration.setAppName(\u0026amp;quot;TEST_APP\u0026amp;quot;); // 将注册表注册进客户端并发布数据 Publisher publisher = registryClient.register(registration, \u0026amp;quot;10.10.1.1:12200?xx=yy\u0026amp;quot;); // 如需覆盖上次发布的数据可以使用发布者模型重新发布数据 publisher.republish(\u0026amp;quot;10.10.1.1:12200?xx=zz\u0026amp;quot;);  发布数据的关键是构造 PublisherRegistration，该类包含三个属性：\n   属性名 属性类型 描述     dataId String 数据ID，发布订阅时需要使用相同值，数据唯一标识由 dataId + group + instanceId 组成。   group String 数据分组，发布订阅时需要使用相同值，数据唯一标识由 dataId + group + instanceId 组成，默认值 DEFAULT_GROUP。   appName String 应用 appName。    4. 订阅数据 订阅数据的关键代码如下：\n// 创建 SubscriberDataObserver SubscriberDataObserver subscriberDataObserver = new SubscriberDataObserver() { @Override public void handleData(String dataId, UserData userData) { System.out.println(\u0026amp;quot;receive data success, dataId: \u0026amp;quot; + dataId + \u0026amp;quot;, data: \u0026amp;quot; + userData); } }; // 构造订阅者注册表，设置订阅维度，ScopeEnum 共有三种级别 zone, dataCenter, …","date":-62135596800,"description":"","dir":"projects/sofa-registry/java-sdk/","fuzzywordcount":1400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"32cff5cc5d89ffa85b12c207a1c0c6f3","permalink":"/projects/sofa-registry/java-sdk/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-registry/java-sdk/","summary":"1. Maven 坐标 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;registry-client-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${registry.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. 创建 SOFARegistry 客户端实例 构建 SOFARegistry 客户端实例的关键代码如下： RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026quot;127.0.0.1\u0026quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); 其中注册中心相关的属性通过 DefaultRegistryClientConfigBuilder 构建指定，该类包含以下关","tags":null,"title":"Java SDK","type":"projects","url":"/projects/sofa-registry/java-sdk/","wordcount":1368},{"author":null,"categories":null,"content":"In addition to hundreds of unit tests and some chaos tests, SOFAJRaft also uses a distributed verification and fault injection testing framework Jepsen to simulate many cases, and has passed all these tests:\n Randomized partitioning with two partitions: a big one and a small one Randomly adding and removing nodes Randomly stopping and starting nodes Randomly kill -9 and starting nodes Randomly dividing a cluster into two groups, with one node connection the two to simulate network partitioning Randomly dividing a cluster into different majority groups  sofa-jraft-jepsen project address\n","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jepson-test/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c60bc5083fdf888f6eef5b344b1ad157","permalink":"/en/projects/sofa-jraft/jepson-test/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-jraft/jepson-test/","summary":"In addition to hundreds of unit tests and some chaos tests, SOFAJRaft also uses a distributed verification and fault injection testing framework Jepsen to simulate many cases, and has passed all these tests:\n Randomized partitioning with two partitions: a big one and a small one Randomly adding and removing nodes Randomly stopping and starting nodes Randomly kill -9 and starting nodes Randomly dividing a cluster into two groups, with one node connection the two to simulate network partitioning Randomly dividing a cluster into different majority groups  sofa-jraft-jepsen project address","tags":null,"title":"Jepsen tests","type":"projects","url":"/en/projects/sofa-jraft/jepson-test/","wordcount":89},{"author":null,"categories":null,"content":"除了几百个单元测试以及部分 chaos 测试之外, SOFAJRaft 还使用 jepsen 这个分布式验证和故障注入测试框架模拟了很多种情况，都已验证通过：\n 随机分区，一大一小两个网络分区 随机增加和移除节点 随机停止和启动节点 随机 kill -9 和启动节点 随机划分为两组，互通一个中间节点，模拟分区情况 随机划分为不同的 majority 分组  sofa-jraft-jepsen 项目地址\n","date":-62135596800,"description":"","dir":"projects/sofa-jraft/jepson-test/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c60bc5083fdf888f6eef5b344b1ad157","permalink":"/projects/sofa-jraft/jepson-test/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-jraft/jepson-test/","summary":"除了几百个单元测试以及部分 chaos 测试之外, SOFAJRaft 还使用 jepsen 这个分布式验证和故障注入测试框架模拟了很多种情况，都已验证通过： 随机分区，一大一小两个网络分","tags":null,"title":"Jepsen 验证","type":"projects","url":"/projects/sofa-jraft/jepson-test/","wordcount":137},{"author":null,"categories":null,"content":"In the Interactive Instructions section, we have described the set of instructions that Jarslink2.0 supports. In this section, we will focus on all the possible state transitions behind these instructions in the following diagram of a Biz package being loaded from a static file to the runtime and to being uninstalled.\nThe diagram above basically shows the complete life cycle of a Biz package. Now we will explain the direction of each state transition in the diagram:\n Label 1: Execute the install instruction, and Jarslink2.0 will resolve the file format. If the format is correct, it is the Biz package file, and the Biz package will be registered and installed.\n Label 2: When the Biz package is successfully installed, the Biz package\u0026amp;rsquo;s main function is executed, the Spring context is loaded successfully, and passes the health check. If Biz packages with the same name but different versions are detected to be activated, the Biz package state will be set to an inactive state. The JVM services that are published by an inactive Biz package will not be called.\n Label 3: When the Biz package is successfully installed, the Biz package\u0026amp;rsquo;s main function is executed, the Spring context is loaded successfully, and passes the health check. If Biz packages with the same name but different versions are detected to be activated, the Biz package state will be set as active and can provide services.\n Label 4: If there are any exceptions or a health check failure, the Biz package state will be set to broken. During the installation, the resources that the Biz package occupies will be quickly released and unregistered, at which point the Biz state will be set to unresolved.\n Label 5: When running, Jarslink2.0 can load Biz packages with the same name but different versions, but only one Biz package is in the active state and can provide services. Execute the switch instruction, and the two Biz packages\u0026amp;rsquo; states will be interchanged.\n Label 6: Execute the uninstallation instruction, the Biz package will be uninstalled, and its occupied resources and published services will be unregistered.\n  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-lifecycle/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0f166dd5388f3dc7d968bce31d0f6e4f","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-lifecycle/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-lifecycle/","summary":"In the Interactive Instructions section, we have described the set of instructions that Jarslink2.0 supports. In this section, we will focus on all the possible state transitions behind these instructions in the following diagram of a Biz package being loaded from a static file to the runtime and to being uninstalled.\nThe diagram above basically shows the complete life cycle of a Biz package. Now we will explain the direction of each state transition in the diagram:","tags":null,"title":"Lifecycle","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-lifecycle/","wordcount":346},{"author":null,"categories":null,"content":" The link data transparent transmission function allows the applications to store data in the calling context, and then any applications in the entire link can operate the data. This feature is used as follows. Data can be put into the request and response of the link for transparent transmission, and then the applications can get the corresponding data from the link.\nRpcInvokeContext.getContext().putRequestBaggage(\u0026amp;quot;key_request\u0026amp;quot;,\u0026amp;quot;value_request\u0026amp;quot;); RpcInvokeContext.getContext().putResponseBaggage(\u0026amp;quot;key_response\u0026amp;quot;,\u0026amp;quot;value_response\u0026amp;quot;); String requestValue=RpcInvokeContext.getContext().getRequestBaggage(\u0026amp;quot;key_request\u0026amp;quot;); String responseValue=RpcInvokeContext.getContext().getResponseBaggage(\u0026amp;quot;key_response\u0026amp;quot;);  Example For example, in the scenario of A -\u0026amp;gt; B -\u0026amp;gt; C, the request arguments set by A are transmitted to B and C. On return, response arguments of C and B are transmitted to A.\nRequester A is set as follows:\n// Set the value of the request transparently before calling RpcInvokeContext context = RpcInvokeContext.getContext(); context.putRequestBaggage(\u0026amp;quot;reqBaggageB\u0026amp;quot;, \u0026amp;quot;a2bbb\u0026amp;quot;); // Call service String result = service.hello(); // Get the result value context.getResponseBaggage(\u0026amp;quot;respBaggageB\u0026amp;quot;);  Business code for B is as follows:\npublic String hello() { / / Get the value of the request transparent transmission RpcInvokeContext context = RpcInvokeContext.getContext(); String reqBaggage = context.getRequestBaggage(\u0026amp;quot;reqBaggageB\u0026amp;quot;); // doSomthing(); // result passes a value transparently context.putResponseBaggage(\u0026amp;quot;respBaggageB\u0026amp;quot;, \u0026amp;quot;b2aaa\u0026amp;quot;); return result; }  If you start the child thread halfway, you need to set the context of the child thread:\nCountDownLatch latch = new CountDownLatch(1); final RpcInvokeContext parentContext = RpcInvokeContext.peekContext(); Thread thread = new Thread(new Runnable(){ public void run(){ Try { RpcInvokeContext.setContext(parentContext); / / Call a remote service xxxService.sayHello(); latch.countDown(); } finally { RpcInvokeContext.removeContext(); } } }, \u0026amp;quot;new-thread\u0026amp;quot;); thread.start(); // If failed to get the transparently transmitted data of the return value. latch.await(); //wait // Return ends, and you can get the value returned by transparent transmission.  Compare with SOFATracer SOFATracer is an open-source distributed link tracing system of Ant Finanicial. RPC has been integrated with Tracer and is enabled by default.\nThe differences between data transparent transmission and data transfer by Tracer are as follows:\n RPC data transparent transmission is business-oriented. And it can implement two-way data transmission in the full link. The caller can transmit data to the service provider, and the service provider can also transmit data to the caller. SOFATracer is middleware-oriented and is more suitable for the data transfer without service itself perceving. It can only implement one-way data …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/invoke-chain-pass-data/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"96cfb41f07a6a2ad979b53093ff5eee9","permalink":"/en/projects/sofa-rpc/invoke-chain-pass-data/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/invoke-chain-pass-data/","summary":"The link data transparent transmission function allows the applications to store data in the calling context, and then any applications in the entire link can operate the data. This feature is used as follows. Data can be put into the request and response of the link for transparent transmission, and then the applications can get the corresponding data from the link.\nRpcInvokeContext.getContext().putRequestBaggage(\u0026quot;key_request\u0026quot;,\u0026quot;value_request\u0026quot;); RpcInvokeContext.getContext().putResponseBaggage(\u0026quot;key_response\u0026quot;,\u0026quot;value_response\u0026quot;); String requestValue=RpcInvokeContext.getContext().getRequestBaggage(\u0026quot;key_request\u0026quot;); String responseValue=RpcInvokeContext.getContext().getResponseBaggage(\u0026quot;key_response\u0026quot;);  Example For example, in the scenario of A -\u0026gt; B -\u0026gt; C, the request arguments set by A are transmitted to B and C.","tags":null,"title":"Link data transparent transmission","type":"projects","url":"/en/projects/sofa-rpc/invoke-chain-pass-data/","wordcount":427},{"author":null,"categories":null,"content":" 本文描述的是 MOSN listener 配置。\n Listener 配置详细描述了 MOSN 启动时监听的端口，以及对应的端口对应不同逻辑的配置。 Listener 的配置可以通过Listener动态接口进行添加和修改。  { \u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;type\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;address\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;bind_port\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;use_original_dst\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;access_logs\u0026amp;quot;:[], \u0026amp;quot;filter_chains\u0026amp;quot;:[], \u0026amp;quot;stream_filters\u0026amp;quot;:[], \u0026amp;quot;inspector\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;connection_idle_timeout\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot; }  name 用于唯一区分 Listener，如果配置为空，会默认生成一个 UUID 作为 name。在对 Listener 进行动态更新时，使用 name 作为索引，如果 name 不存在，则是新增一个 listener，如果 name 存在则是对 listener 进行更新。\ntype 标记 Listener 的类型，目前支持 ingress 和 egress 两种类型。不同 type 的 Listener 输出的 tracelog 不同。\naddress IP:Port 形式的字符串，Listener 监听的地址，唯一。\nbind_port bool 类型，表示 Listener 是否会占用 address 配置的地址，通常情况下都需要配置为true。\nuse_original_dst bool 类型，用于透明代理。\naccess_logs 一组 access_log 配置。\nfilter_chains 一组 FilterChain 配置，但是目前 MOSN 仅支持一个 filter_chain。\nstream_filters 一组 stream_filter 配置，目前只在 filter_chain 中配置了 filter 包含 proxy 时生效。\ninspector bool 类型，当此值为 true 时，表示即便 listener 在 filter_chain 中配置开启了 TLS 监听，listener 依然可以处理非 TLS 的请求。\nconnection_idle_timeout Duration String，空闲连接超时配置。当 listener 上建立的连接空闲超过配置的超时时间以后，MOSN 会将此连接关闭。\n","date":-62135596800,"description":"","dir":"projects/mosn/configuration/listener/overview/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"028e9053b21853890114c38d55d15390","permalink":"/projects/mosn/configuration/listener/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/configuration/listener/overview/","summary":"本文描述的是 MOSN listener 配置。 Listener 配置详细描述了 MOSN 启动时监听的端口，以及对应的端口对应不同逻辑的配置。 Listener 的配置可以通过Listener动态接口进行添加","tags":null,"title":"Listener 配置","type":"projects","url":"/projects/mosn/configuration/listener/overview/","wordcount":447},{"author":null,"categories":null,"content":" SOFARPC provides a variety of load balancing algorithms and currently supports the following five types:\n   Type Name Description     random Random algorithm The default load balancing algorithm.   localPref Local preference algorithm Firstly detect whether the service is published locally, if not, random algorithm is used.   roundRobin Round Robin algorithm Method-level polling, the polling is carried out separately to each method, without affecting each other.   consistentHash Consistent hash algorithm The same method-level request is routed to the same node.   weightRoundRobin Weighted Round Robin algorithm Poll nodes by weight. Not recommended due to poor performance.    To use a specific load balancing algorithm, you can configure as follows:\nIn XML If you reference the service using XML, you can configure it by setting the loadBalancer property of the sofa:global-attrs tag:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs loadBalancer=\u0026amp;quot;roundRobin\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  In Annotation It is currently not supported to configure a load balancing algorithm for a reference in Annotation. The function will be provided in subsequent releases.\nIn API under Spring environment If you use the API in a Spring or Spring Boot environment, you can configure it by calling the setLoadBalancer method of BoltBindingParam:\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setLoadBalancer(\u0026amp;quot;roundRobin\u0026amp;quot;);  In API under non-Spring environment If you directly use the bare API provided by SOFARPC in a non-Spring environment, you can configure it by calling the setLoadBalancer method of ConsumerConfig:\nConsumerConfig consumerConfig = new ConsumerConfig(); consumerConfig.setLoadbalancer(\u0026amp;quot;random\u0026amp;quot;);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/load-balance/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"739984ca9a414429304f85010fd73ad0","permalink":"/en/projects/sofa-rpc/load-balance/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/load-balance/","summary":"SOFARPC provides a variety of load balancing algorithms and currently supports the following five types:\n   Type Name Description     random Random algorithm The default load balancing algorithm.   localPref Local preference algorithm Firstly detect whether the service is published locally, if not, random algorithm is used.   roundRobin Round Robin algorithm Method-level polling, the polling is carried out separately to each method, without affecting each other.","tags":null,"title":"Load balance","type":"projects","url":"/en/projects/sofa-rpc/load-balance/","wordcount":230},{"author":null,"categories":null,"content":"To use local file as service registry center, you can configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=local:///home/admin/registry/localRegistry.reg  The /home/admin/registry/localRegistry.reg is the directory of the local files to be used.\nOn windows OS, the above path indicates the following directory:\ncom.alipay.sofa.rpc.registry.address=local://c://users/localRegistry.reg  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-local/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"33bc89393392e21b3917f090313c0df5","permalink":"/en/projects/sofa-rpc/registry-local/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-local/","summary":"To use local file as service registry center, you can configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=local:///home/admin/registry/localRegistry.reg  The /home/admin/registry/localRegistry.reg is the directory of the local files to be used.\nOn windows OS, the above path indicates the following directory:\ncom.alipay.sofa.rpc.registry.address=local://c://users/localRegistry.reg  ","tags":null,"title":"Local","type":"projects","url":"/en/projects/sofa-rpc/registry-local/","wordcount":40},{"author":null,"categories":null,"content":" Instructions on performance report The following performance report presents the performance comparison data of MOSN 0.1.0 with envoy in terms of pure TCP forwarding for Bolt and HTTP1.x protocols, mainly including QPS, RTT, failure rate, success rate and other indicators.\nIt is significant to note the following optimizations in v0.1.0 which are intended to improve the forwarding performance of MOSN.\n For thread model, MOSN uses the worker goroutine pool to handle stream events, and uses two independent goroutines to handle read and write IO separately. For single-core forwarding, in the case of specifying P=1, MOSN binds CPU with cores to improve the call execution efficiency of the system and the locality affinity of cache. For memory, in the case of binding single core, MOSN uses SLAB-style recycling mechanism to improve reuse and reduce memory copy. For IO, MOSN mainly implements optimization by controlling the read/write buffer size, read/write timing, read/write frequency and other parameters.  The performance test data is as follows:\nTCP proxy performance data For the same deployment mode, this report compares MOSN 0.1.0 and envoy for the upper-layer protocol Bolt (SOFARPC related protocol) and HTTP1.1 respectively.\nDeployment mode The pressure test is deployed in a pure proxy mode. The client process accesses the server process through the MOSN process and serves as a forwarding proxy. The client process, MOSN process, and server process run on the machines which belong to different network segments. The network delay of the direct access from the client to server is about 2.5ms.\nClient Bolt protocol (send 1K string) The client that sends the Bolt protocol data uses the online pressure generators developed by Ant Financial and deploys the SOFARPC client.\nOn the pressure generator performance page, you can see the QPS, success/failure counts, RT and other parameters.\nHTTP1.1 protocol (send 1K string) Use ApacheBench/2.3. The test instructions are:\nab -n $RPC -c $CPC -p 1k.txt -T \u0026amp;quot;text/plain\u0026amp;quot; -k http://11.166.161.136:12200/tcp_bench \u0026amp;gt; ab.log.$CPU_IDX \u0026amp;amp;  Mesh machine specifications The mesh runs in a container where the CPU is an exclusive logical core. The specifications are as follows:\n   Category Information     OS 3.10.0-327.ali2008.alios7.x86_64   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5-2650 v2 @ 2.60GHz X 1    Upstream machine specifications    Category Information     OS 2.6.32-431.17.1.el6.FASTSOCKET   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5620 @ 2.40GHz X 16    Bolt protocol test result Performance data    Indicators MOSN Envoy     QPS 103500 104000   RT 16.23ms 15.88ms   MEM 31m 18m   CPU 100% 100%    Conclusion For single-core TCP forwarding, there is little difference between MOSN 0.1.0 and Envoy 1.7 in terms of performance in the condition with full load, such as QPS, RTT and success/failure counts. We will continue to optimize in the subsequent versions.\nHTTP/1.1 test result Since the HTTP/1.1 request response model is …","date":-62135596800,"description":"","dir":"projects/mosn/reference-performance-report010/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3cb22950b4be5a25b90f8aa1376786e9","permalink":"/en/projects/mosn/reference-performance-report010/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/mosn/reference-performance-report010/","summary":"Instructions on performance report The following performance report presents the performance comparison data of MOSN 0.1.0 with envoy in terms of pure TCP forwarding for Bolt and HTTP1.x protocols, mainly including QPS, RTT, failure rate, success rate and other indicators.\nIt is significant to note the following optimizations in v0.1.0 which are intended to improve the forwarding performance of MOSN.\n For thread model, MOSN uses the worker goroutine pool to handle stream events, and uses two independent goroutines to handle read and write IO separately.","tags":null,"title":"MOSN 0.1.0 performance report","type":"projects","url":"/en/projects/mosn/reference-performance-report010/","wordcount":730},{"author":null,"categories":null,"content":" Instructions on performance report The following performance report presents the performance comparison data of MOSN 0.1.0 with envoy in terms of pure TCP forwarding for Bolt and HTTP1.x protocols, mainly including QPS, RTT, failure rate, success rate and other indicators.\nIt is significant to note the following optimizations in v0.1.0 which are intended to improve the forwarding performance of MOSN.\n For thread model, MOSN uses the worker goroutine pool to handle stream events, and uses two independent goroutines to handle read and write IO separately. For single-core forwarding, in the case of specifying P=1, MOSN binds CPU with cores to improve the call execution efficiency of the system and the locality affinity of cache. For memory, in the case of binding single core, MOSN uses SLAB-style recycling mechanism to improve reuse and reduce memory copy. For IO, MOSN mainly implements optimization by controlling the read/write buffer size, read/write timing, read/write frequency and other parameters.  The performance test data is as follows:\nTCP proxy performance data For the same deployment mode, this report compares MOSN 0.1.0 and envoy for the upper-layer protocol Bolt (SOFARPC related protocol) and HTTP1.1 respectively.\nDeployment mode The pressure test is deployed in a pure proxy mode. The client process accesses the server process through the MOSN process and serves as a forwarding proxy. The client process, MOSN process, and server process run on the machines which belong to different network segments. The network delay of the direct access from the client to server is about 2.5ms.\nClient Bolt protocol (send 1K string) The client that sends the Bolt protocol data uses the online pressure generators developed by Ant Financial and deploys the SOFARPC client.\nOn the pressure generator performance page, you can see the QPS, success/failure counts, RT and other parameters.\nHTTP1.1 protocol (send 1K string) Use ApacheBench/2.3. The test instructions are:\nab -n $RPC -c $CPC -p 1k.txt -T \u0026amp;quot;text/plain\u0026amp;quot; -k http://11.166.161.136:12200/tcp_bench \u0026amp;gt; ab.log.$CPU_IDX \u0026amp;amp;  Mesh machine specifications The mesh runs in a container where the CPU is an exclusive logical core. The specifications are as follows:\n   Category Information     OS 3.10.0-327.ali2008.alios7.x86_64   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5-2650 v2 @ 2.60GHz X 1    Upstream machine specifications    Category Information     OS 2.6.32-431.17.1.el6.FASTSOCKET   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5620 @ 2.40GHz X 16    Bolt protocol test result Performance data    Indicators MOSN Envoy     QPS 103500 104000   RT 16.23ms 15.88ms   MEM 31m 18m   CPU 100% 100%    Conclusion For single-core TCP forwarding, there is little difference between MOSN 0.1.0 and Envoy 1.7 in terms of performance in the condition with full load, such as QPS, RTT and success/failure counts. We will continue to optimize in the subsequent versions.\nHTTP/1.1 test result Since the HTTP/1.1 request response model is …","date":-62135596800,"description":"","dir":"projects/occlum/reference-performance-report010/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"035dfbc7e310acf31561343432aea680","permalink":"/en/projects/occlum/reference-performance-report010/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/occlum/reference-performance-report010/","summary":"Instructions on performance report The following performance report presents the performance comparison data of MOSN 0.1.0 with envoy in terms of pure TCP forwarding for Bolt and HTTP1.x protocols, mainly including QPS, RTT, failure rate, success rate and other indicators.\nIt is significant to note the following optimizations in v0.1.0 which are intended to improve the forwarding performance of MOSN.\n For thread model, MOSN uses the worker goroutine pool to handle stream events, and uses two independent goroutines to handle read and write IO separately.","tags":null,"title":"MOSN 0.1.0 performance report","type":"projects","url":"/en/projects/occlum/reference-performance-report010/","wordcount":730},{"author":null,"categories":null,"content":" 以下的的性能报告为 MOSN 0.1.0 在做 Bolt 与 HTTP1.x 协议的纯 TCP 转发上与 envoy 的一些性能对比数据，主要表现在 QPS、RTT、失败率/成功率等。\n这里需要强调的是，为了提高 MOSN 的转发性能，在 0.1.0 版本中，我们做了如下的一些优化手段：\n 在线程模型优化上，使用 worker 协程池处理 stream 事件，使用两个独立的协程分别处理读写 IO 在单核转发优化上，在指定 P=1 的情况下，我们通过使用 CPU 绑核的形式来提高系统调用的执行效率以及 cache 的 locality affinity 在内存优化上，同样是在单核绑核的情况下，我们通过使用 SLAB-style 的回收机制来提高复用，减少内存 copy 在 IO 优化上，主要是通过读写 buffer 大小以及读写时机和频率等参数的控制上进行调优  以下为具体的性能测试数据。\nTCP 代理性能数据 这里，针对相同的部署模式，我们分别针对上层协议为 \u0026amp;quot;Bolt(SofaRpc相关协议)\u0026amp;quot; 与 \u0026amp;quot;HTTP1.1\u0026amp;quot; 来进行对比。\n部署模式 压测采用纯代理模式部署，client 进程通过 MOSN 进程作为转发代理访问server进程。其中，client 进程，MOSN 进程，server 进程分别运行在属于不同网段的机器中。client 直连访问 server 网络延时为 2.5ms 左右。\n客户端 Bolt 协议（发送 1K 字符串） 发送 Bolt 协议数据的客户端使用 \u0026amp;ldquo;蚂蚁金服\u0026amp;rdquo;内部开发的线上压力机，并部署 sofa rpc client。 通过压力机的性能页面，可反映压测过程中的QPS、成功/失败次数，以及RT等参数。\nHTTP1.1 协议（发送 1K 字符串） 使用 ApacheBench/2.3, 测试指令:\nab -n $RPC -c $CPC -p 1k.txt -T \u0026amp;quot;text/plain\u0026amp;quot; -k http://11.166.161.136:12200/tcp_bench \u0026amp;gt; ab.log.$CPU_IDX \u0026amp;amp;  Service mesh 运行机器规格 Service mesh 运行在容器中，其中 CPU 为独占的一个逻辑核，具体规格如下：\n   类别 信息     OS 3.10.0-327.ali2008.alios7.x86_64   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5-2650 v2 @ 2.60GHz X 1    Upstream 运行机器规格    类别 信息     OS 2.6.32-431.17.1.el6.FASTSOCKET   CPU Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5620 @ 2.40GHz X 16    Bolt 协议测试结果 性能数据    指标 MOSN Envoy     QPS 103500 104000   RT 16.23ms 15.88ms   MEM 31m 18m   CPU 100% 100%    结论 可以看到，在单核 TCP 转发场景下，MOSN 0.1.0 版本和 Envoy 1.7版本，在满负载情况下的 QPS、RTT、成功数/失败数等性能数据上相差不大，后续版本我们会继续优化。\nHTTP/1.1 测试结果 由于 HTTP/1.1 的请求响应模型为 PING-PONG，因此 QPS 与并发数会呈现正相关。下面分别进行不同并发数的测试。\n并发20    指标 MOSN Envoy     QPS 5600 5600   RT(mean) 3.549ms 3.545ms   RT(P99) 4ms 4ms   RT(P98) 4ms 4ms   RT(P95) 4ms 4ms   MEM 24m 23m   CPU 40% 20%    并发40    指标 MOSN Envoy     QPS 11150 11200   RT(mean) 3.583ms 3.565ms   RT(P99) 4ms 4ms   RT(P98) 4ms 4ms   RT(P95) 4ms 4ms   MEM 34m 24m   CPU 70% 40%    并发200    指标 MOSN Envoy     QPS 29670 38800   RT(mean) 5.715ms 5.068ms   RT(P99) 16ms 7ms   RT(P98) 13ms 7ms   RT(P95) 11ms 6ms   MEM 96m 24m   CPU 100% 95%    并发220    指标 MOSN Envoy     QPS 30367 41070   RT(mean) 8.201ms 5.369ms   RT(P99) 20ms 9ms   RT(P98) 19ms 8ms   RT(P95) 16ms 8ms   MEM 100m 24m   CPU 100% 100%    结论 可以看到，在上层协议为 HTTP/1.X 时，MOSN 的性能和 Envoy 的性能存在一定差距，对于这种现象我们的初步结论为：在 PING-PONG 的发包模型下，MOSN 无法进行 read/write 系统调用合并，相比 SOFARPC 可以合并的场景，syscall 数量大幅上升，因此导致相比 SOFARPC 的场景，HTTP 性能上相比 Envoy 会存在差距。针对这个问题，在 0.2.0 版本中，我们会进行相应的优化。\n附录 Envoy 版本信息  version：1.7 tag：1ef23d481a4701ad4a414d1ef98036bd2ed322e7  Envoy TCP 测试配置 static_resources: listeners: - address: socket_address: address: 0.0.0.0 port_value: 12200 filter_chains: - filters: - name: envoy.tcp_proxy config: stat_prefix: ingress_tcp cluster: sofa_server clusters: - name: sofa_server connect_timeout: 0.25s type: static lb_policy: round_robin hosts: - socket_address: address: 10.210.168.5 port_value: 12222 - socket_address: address: 10.210.168.5 port_value: 12223 - socket_address: address: 10.210.168.5 port_value: 12224 - socket_address: address: 10.210.168.5 port_value: 12225 admin: access_log_path: …","date":-62135596800,"description":"","dir":"projects/mosn/reference-performance-report010/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3cb22950b4be5a25b90f8aa1376786e9","permalink":"/projects/mosn/reference-performance-report010/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/mosn/reference-performance-report010/","summary":"以下的的性能报告为 MOSN 0.1.0 在做 Bolt 与 HTTP1.x 协议的纯 TCP 转发上与 envoy 的一些性能对比数据，主要表现在 QPS、RTT、失败率/成功率等。 这里需要强调的是，为了提","tags":null,"title":"MOSN 0.1.0 性能报告","type":"projects","url":"/projects/mosn/reference-performance-report010/","wordcount":1229},{"author":null,"categories":null,"content":" 以下性能报告的基准版本为 MOSN 0.2.1。在 0.2.1 版本中，我们进行了如下一些优化手段： - 添加内存复用框架，涵盖 io/protocol/stream/proxy 层级，减少对象分配、内存使用和 GC 压力。 - 针对大量链接场景，新增 Raw Epoll 模式，该模式使用了事件回调机制 + IO 协程池，规避了海量协程带来的堆栈内存消耗以及调度开销。\n需要注意的是，由于目前 SOFARPC 和 H2 的压测工具还没有 pxx 指标的展示，我们在性能报告中选取的数据都为均值。后续需要我们自行进行相关压测环境工具的建设来完善相关指标（P99，P95……）\n总览 本次性能报告在0.1.0 性能报告的基础上，新增了若干场景的覆盖，总体包含以下几部分： - 单核性能（sidecar场景） - 7层代理 - Bolt（串联） - Http/1.1（串联） - Http/2（串联） - 多核性能（gateway场景） - 7层代理 - Bolt（直连） - Http/1.1（直连） - Http/2（直连） - 长连接网关 - Bolt（read/write loop with goroutine/raw epoll）\n单核性能（sidecar 场景） 测试环境 机器信息    机器 OS CPU     11.166.190.224 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel（R） Xeon（R） CPU E5-2640 v3 @ 2.60GHz   11.166.136.110 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel（R） Xeon（R） CPU E5-2430 0 @ 2.20GHz   bolt client client 为压力平台，有 5 台压力机，共计与client MOSN 之间会建立 500 条链接    http1 client（10.210.168.5） ApacheBench/2.3 -n 2000000 -c 500 -k   http2 client（10.210.168.5） nghttp.h2load -n1000000 -c5 -m100 -t4    部署结构    压测模式 部署结构     串联 client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.136.110） \u0026amp;ndash;\u0026amp;gt; server（11.166.136.110）    网络时延    节点 PING     client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） 1.356ms   MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.136.110） 0.097 ms    请求模式    请求内容     1K req/resp    7层代理    场景 QPS RT(ms) MEM(K) CPU(%)     Bolt 16000 15.8 77184 98   Http/1.1 4610 67 47336 90   Http/2 5219 81 31244 74    多核性能（gateway 场景） 测试环境 机器信息    机器 OS CPU     11.166.190.224 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel（R） Xeon（R） CPU E5-2640 v3 @ 2.60GHz   11.166.136.110 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel（R） Xeon（R） CPU E5-2430 0 @ 2.20GHz   bolt client client为压力平台，有5台压力机，共计与client MOSN之间会建立500条链接    http1 client（10.210.168.5） ApacheBench/2.3 -n 2000000 -c 500 -k   http2 client（10.210.168.5） nghttp.h2load -n1000000 -c5 -m100 -t4    部署结构    压测模式 部署结构     直连 client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; server（11.166.136.110）    网络时延    节点 PING     client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） 1.356ms   MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.136.110） 0.097 ms    请求模式    请求内容     1K req/resp    7层代理    场景 QPS RT(ms) MEM(K) CPU(%)     Bolt 45000 23.4 544732 380   Http/1.1 21584 23 42768 380   Http/2 8180 51.7 173180 300    长连接网关 测试环境 机器信息    机器 OS CPU     11.166.190.224 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5-2640 v3 @ 2.60GHz   11.166.136.110 3.10.0-327.ali2010.rc7.alios7.x86_64 Intel\u0026amp;reg; Xeon\u0026amp;reg; CPU E5-2430 0 @ 2.20GHz    部署结构    压测模式 部署结构     直连 client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; server（11.166.136.110）    网络时延    节点 PING     client \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.190.224） 1.356ms   MOSN（11.166.190.224） \u0026amp;ndash;\u0026amp;gt; MOSN（11.166.136.110） 0.097 ms    请求模式    链接数 请求内容     2 台压力机，每台 5w 链接 + 500 QPS，共计10W链接 + 1000 QPS 1K req/resp    长连接网关    场景 QPS MEM(g) CPU(%) goroutine     RWLoop + goroutine 1000 3.3 60 200028   Raw epoll 1000 2.5 18 28    总结 MOSN 0.2.1引入了内存复用框架，相比0.1.0，在 bolt 协议转发场景性能表现得到了大幅优化。在提升了20% 的 QPS 的同时，还优化了 30% 的内存占用。\n与此同时，我们对 HTTP/1.1 及 HTTP/2 的场景也进行 …","date":-62135596800,"description":"","dir":"projects/mosn/reference-performance-report021/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7b04a2e6cf1c4f9e732dc1dfdab74c57","permalink":"/projects/mosn/reference-performance-report021/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/mosn/reference-performance-report021/","summary":"以下性能报告的基准版本为 MOSN 0.2.1。在 0.2.1 版本中，我们进行了如下一些优化手段： - 添加内存复用框架，涵盖 io/protocol/stream/proxy 层级，减少对象分配、内存使用和 GC 压力","tags":null,"title":"MOSN 0.2.1 性能报告","type":"projects","url":"/projects/mosn/reference-performance-report021/","wordcount":1616},{"author":null,"categories":null,"content":" MOSN 的官方网站 mosn.io 正在建设中，文档临时托管在这里。\nMOSN 是一款使用 Go 语言开发的网络代理软件，作为云原生的网络数据平面，旨在为服务提供多协议，模块化，智能化，安全的代理能力。MOSN 是 Modular Open Smart Network-proxy 的简称。MOSN 可以与任何支持 xDS API 的 Service Mesh 集成，亦可以作为独立的四、七层负载均衡，API Gateway，云原生 Ingress 等使用。\n快速开始 请参考快速开始。\n核心能力  Istio集成  集成 Istio 1.0 版本与 V4 API，可基于全动态资源配置运行  核心转发  自包含的网络服务器 支持 TCP 代理 支持 TProxy 模式  多协议  支持 HTTP/1.1，HTTP/2 支持 SOFARPC 支持 Dubbo 协议（开发中）  核心路由  支持 Virtual Host 路由 支持 Headers/URL/Prefix 路由 支持基于 Host Metadata 的 Subset 路由 支持重试  后端管理\u0026amp;amp;负载均衡  支持连接池 支持熔断 支持后端主动健康检查 支持 Random/RR 等负载策略 支持基于 Host Metadata 的 Subset 负载策略  可观察性  观察网络数据 观察协议数据  TLS  支持 HTTP/1.1 on TLS 支持 HTTP/2.0 on TLS 支持 SOFARPC on TLS  进程管理  支持平滑 reload 支持平滑升级  扩展能力  支持自定义私有协议 支持在 TCP IO 层，协议层面加入自定义扩展   社区 MOSN 仍处在初级阶段，有很多能力需要补全，所以我们欢迎所有人参与进来与我们一起共建。\n如有任何疑问欢迎提交 Issue。\n","date":-62135596800,"description":"","dir":"projects/mosn/overview/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f25c59cbb758b4dae5de39e1f1c3a2f4","permalink":"/projects/mosn/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/overview/","summary":"MOSN 的官方网站 mosn.io 正在建设中，文档临时托管在这里。 MOSN 是一款使用 Go 语言开发的网络代理软件，作为云原生的网络数据平面，旨在为服务提供多协议，模块化，","tags":null,"title":"MOSN 介绍","type":"projects","url":"/projects/mosn/overview/","wordcount":470},{"author":null,"categories":null,"content":" Service Mesh 中 Sidecar 运维一直是一个比较棘手的问题，数据平面的 Sidecar 升级是常有的事情，如何在升级 Sidecar（MOSN）的时候而不影响业务，对于存量的长连接如何迁移，本文将为你介绍 MOSN 的解决之道。\n背景 本文介绍 MOSN 支持平滑升级的原因和解决方案，对于平滑升级的一些基础概念，大家可以通过 Nginx vs Enovy vs Mosn 平滑升级原理解析了解。\n先简单介绍一下为什么 Nginx 和 Envoy 不需要具备 MOSN 这样的连接无损迁移方案，主要还是跟业务场景相关，Nginx 和 Envoy 主要支持的是 HTTP1 和 HTTP2 协议，HTTP1使用 connection: Close，HTTP2 使用 Goaway Frame 都可以让 Client 端主动断链接，然后新建链接到新的 New process，但是针对 Dubbo、SOFA PRC 等常见的多路复用协议，它们是没有控制帧，Old process 的链接如果断了就会影响请求的。\n一般的升级做法就是切走应用的流量，比如自己UnPub掉服务，等待一段时间没有请求之后，升级MOSN，升级好之后再Pub服务，整个过程比较耗时，并且会有一段时间是不提供服务的，还要考虑应用的水位，在大规模场景下，就很难兼顾评估。MOSN 为了满足自身业务场景，开发了长连接迁移方案，把这条链接迁移到 New process 上，整个过程对 Client 透明，不需要重新建链接，达到请求无损的平滑升级。\n正常流程  Client 发送请求 Request 到 MOSN MOSN 转发请求 Request 到 Server Server 回复响应 Response 到 MOSN MOSN 回复响应 Response 到 Client  上图简单介绍了一个请求的正常流程，我们后面需要迁移的是 TCP1 链接，也就是 Client 到 MOSN 的连接，MOSN 到 Server 的链接 TCP2 不需要迁移，因为 MOSN 访问 Server 是根据 LoadBalance 选择，我们可以主动控制断链建链。\n平滑升级流程 触发条件 有两个方式可以触发平滑升级流程：\n MOSN 对 SIGHUP 做了监听，发送 SIGHUP 信号给 MOSN 进程，通过 ForkExec 生成一个新的 MOSN 进程。 直接重新启动一个新 MOSN 进程。  为什么提供两种方式？最开始我们支持的是方法1，也就是 nginx 和 Envoy 使用的方式，这个在虚拟机或者容器内替换 MOSN 二级制来升级是可行的，但是我们的场景需要满足容器间的升级，所以需要新拉起一个容器，就需要重新启动一个新的 MOSN 进程来做平滑升级，所以后续又支持了方法2。容器间升级还需要 operator 的支持，本文不展开叙述。\n交互流程 首先，老的 MOSN 在启动最后阶段会启动一个协程运行 ReconfigureHandler() 函数监听一个 Domain Socket（reconfig.sock）, 该接口的作用是让新的 MOSN 来感知是否存在老的 MOSN。\nfunc ReconfigureHandler() { l, err := net.Listen(\u0026amp;quot;unix\u0026amp;quot;, types.ReconfigureDomainSocket) for { uc, err := ul.AcceptUnix() _, err = uc.Write([]byte{0}) reconfigure(false) } }  触发平滑升级流程的两种方式最终都是启动一个新的 MOSN 进程，然后调用GetInheritListeners()，通过 isReconfigure() 函数来判断本机是否存在一个老的 MOSN（就是判断是否存在 reconfig.sock 监听），如果存在一个老的 MOSN，就进入迁移流程，反之就是正常的启动流程。\n// 保留了核心流程 func GetInheritListeners() ([]net.Listener, net.Conn, error) { if !isReconfigure() { return nil, nil, nil } l, err := net.Listen(\u0026amp;quot;unix\u0026amp;quot;, types.TransferListenDomainSocket) uc, err := ul.AcceptUnix() _, oobn, _, _, err := uc.ReadMsgUnix(buf, oob) file := os.NewFile(fd, \u0026amp;quot;\u0026amp;quot;) fileListener, err := net.FileListener(file) return listeners, uc, nil }  如果进入迁移流程，新的 MOSN 将监听一个新的 Domain Socket（listen.sock），用于老的 MOSN 传递 listen FD 到新的 MOSN。FD 的传递使用了sendMsg 和 recvMsg。在收到 listen FD 之后，调用 net.FileListener() 函数生产一个 Listener。此时，新老 MOSN 都同时拥有了相同的 Listen 套接字。\n// FileListener returns a copy of the network listener corresponding // to the open file f. // It is the caller\u0026#39;s responsibility to close ln when finished. // Closing ln does not affect f, and closing f does not affect ln. func FileListener(f *os.File) (ln Listener, err error) { ln, err = fileListener(f) if err != nil { err = \u0026amp;amp;OpError{Op: \u0026amp;quot;file\u0026amp;quot;, Net: \u0026amp;quot;file+net\u0026amp;quot;, Source: nil, Addr: fileAddr(f.Name()), Err: err} } return }  这里的迁移和 Nginx 还是有一些区别，Nginx 是 fork 的方式，子进程自动就继承了 listen FD，MOSN 是新启动的进程，不存在父子关系，所以需要通过 sendMsg 的方式来传递。\n在进入迁移流程和 Listen 的迁移过程中，一共使用了两个 Domain Socket：\n reconfig.sock 是 Old MOSN 监听，用于 New MOSN 来判断是否存在 listen.sock 是 New MOSN 监听，用于 Old MOSN 传递 listen FD  两个 sock 其实是可以复用的，也可以用 reconfig.sock …","date":-62135596800,"description":"","dir":"projects/mosn/concept/smooth-upgrade/","fuzzywordcount":4000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2c09c045287c33c760368abe02bb8986","permalink":"/projects/mosn/concept/smooth-upgrade/","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/projects/mosn/concept/smooth-upgrade/","summary":"Service Mesh 中 Sidecar 运维一直是一个比较棘手的问题，数据平面的 Sidecar 升级是常有的事情，如何在升级 Sidecar（MOSN）的时候而不影响业务，对于存量的长连接","tags":null,"title":"MOSN 平滑升级原理解析","type":"projects","url":"/projects/mosn/concept/smooth-upgrade/","wordcount":3957},{"author":null,"categories":null,"content":" 1. registry-meta 1.1 Push switch When publishing new SOFARegistry versions, to minimize the impact on services, and avoid large amounts of push messages caused by large-scale service endpoint changes during the server restart process, we will temporarily turn off the push service at the management layer. After publishing the new SOFARegistry version, we can turn on the push service and restore the normal working conditions. Data subscription and service publication information generated for the period when the push service is turned off will be subject to global push for compensation.\nTurn on the push service:\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/stopPushDataSwitch/close\u0026amp;quot;  Turn off the push service:\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/stopPushDataSwitch/open\u0026amp;quot;  1.2 Query the endpoint list View the endpoint list of the meta cluster:\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/META/node/query\u0026amp;quot;  View the endpoint list of the data cluster:\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/DATA/node/query\u0026amp;quot;  View the endpoint list of the session cluster:\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/SESSION/node/query\u0026amp;quot;  1.3 Scale up/down the meta cluster 1.3.1 Modify the cluster: changePeer You can call this operation to modify the Raft cluster list when you have scaled up/down the cluster. This allows you to correctly add nodes to or remove nodes from the cluster:\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;ip1\u0026amp;gt;,\u0026amp;lt;ip2\u0026amp;gt;,\u0026amp;lt;ip3\u0026amp;gt;\u0026amp;quot;  1.3.2 Reset the cluster: resetPeer When a cluster is unavailable, for example, two of three servers are not functional, the cluster can not carry out leader election. Here, you can call this operation to reset the cluster list. For example, you can reset the cluster to a one-server cluster (with the only functional server) to resume election and restore service.\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/manage/resetPeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;ip1\u0026amp;gt;,\u0026amp;lt;ip2\u0026amp;gt;,\u0026amp;lt;ip3\u0026amp;gt;\u0026amp;quot;  2. registry-data 2.1 Query data View the pub count:\ncurl \u0026amp;quot;http://\u0026amp;lt;data_ip\u0026amp;gt;:9622/digest/datum/count\u0026amp;quot;  You can call this operation to view data published by a client based on its IP address and port number.\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;data_ip\u0026amp;gt;:9622/digest/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;{\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;\u0026amp;quot;:\u0026amp;quot;\u0026amp;lt;client port\u0026amp;gt;\u0026amp;quot;}\u0026#39;  3. registry-session 3.1 Query data You can call this operation to view data published by a client based on its IP address and port number.\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;session_ip\u0026amp;gt;:9603/digest/pub/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;[\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;:\u0026amp;lt;client port\u0026amp;gt;\u0026amp;quot;]\u0026#39;  You can call this operation to view data subscribed to by a client based on its IP address and port number.\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;session_ip\u0026amp;gt;:9603/digest/sub/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: …","date":-62135596800,"description":"","dir":"projects/sofa-registry/management-api/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2cf59ac422c84c279d73c1f7f1cd0902","permalink":"/en/projects/sofa-registry/management-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-registry/management-api/","summary":"1. registry-meta 1.1 Push switch When publishing new SOFARegistry versions, to minimize the impact on services, and avoid large amounts of push messages caused by large-scale service endpoint changes during the server restart process, we will temporarily turn off the push service at the management layer. After publishing the new SOFARegistry version, we can turn on the push service and restore the normal working conditions. Data subscription and service publication information generated for the period when the push service is turned off will be subject to global push for compensation.","tags":null,"title":"Management commands","type":"projects","url":"/en/projects/sofa-registry/management-api/","wordcount":406},{"author":null,"categories":null,"content":" pom dependencies \u0026amp;lt;!-- jraft --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;jraft-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- jsr305 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.code.findbugs\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;jsr305\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.0.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- bolt --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;bolt\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.5.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hessian\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.3.6\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- log --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.slf4j\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;slf4j-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.7.21\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- disruptor --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.lmax\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;disruptor\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.3.7\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-io\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-io\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.4\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-lang\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-lang\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.6\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- protobuf --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.protobuf\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protobuf-java\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.5.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- protostuff --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.protostuff\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protostuff-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.6.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.protostuff\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protostuff-runtime\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.6.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- rocksdb --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.rocksdb\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rocksdbjni\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;5.14.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- java thread affinity --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;net.openhft\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;affinity\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.1.7\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- metrics --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.dropwizard.metrics\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;metrics-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.0.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/maven-dependency/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"abb08cef5ebf1a10a723597a65415313","permalink":"/en/projects/sofa-jraft/maven-dependency/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-jraft/maven-dependency/","summary":"pom dependencies \u0026lt;!-- jraft --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jraft-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- jsr305 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.code.findbugs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jsr305\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- bolt --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bolt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hessian\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- log --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.21\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- disruptor --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.lmax\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;disruptor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- protobuf --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- protostuff --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.","tags":null,"title":"Maven dependencies","type":"projects","url":"/en/projects/sofa-jraft/maven-dependency/","wordcount":104},{"author":null,"categories":null,"content":" pom依赖 \u0026amp;lt;!-- jraft --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;jraft-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- jsr305 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.code.findbugs\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;jsr305\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.0.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- bolt --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;bolt\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.5.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hessian\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.3.6\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- log --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.slf4j\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;slf4j-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.7.21\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- disruptor --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.lmax\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;disruptor\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.3.7\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-io\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-io\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.4\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-lang\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-lang\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.6\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- protobuf --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.protobuf\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protobuf-java\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.5.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- protostuff --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.protostuff\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protostuff-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.6.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.protostuff\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;protostuff-runtime\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.6.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- rocksdb --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.rocksdb\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rocksdbjni\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;5.14.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- java thread affinity --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;net.openhft\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;affinity\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.1.7\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- metrics --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.dropwizard.metrics\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;metrics-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.0.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/maven-dependency/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"abb08cef5ebf1a10a723597a65415313","permalink":"/projects/sofa-jraft/maven-dependency/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-jraft/maven-dependency/","summary":"pom依赖 \u0026lt;!-- jraft --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jraft-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- jsr305 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.code.findbugs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jsr305\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- bolt --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bolt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hessian\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- log --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.21\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- disruptor --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.lmax\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;disruptor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-io\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-io\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-lang\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-lang\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- protobuf --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.protobuf\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;protobuf-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- protostuff","tags":null,"title":"Maven 依赖说明","type":"projects","url":"/projects/sofa-jraft/maven-dependency/","wordcount":107},{"author":null,"categories":null,"content":" In Jarslink 2.0, merged deployment refers to loading and running multiple Biz packages in the same JVM. In the section Application Packaging, we have described the relationship between the Spring Boot/SOFABoot application and the Biz package. We may think that merged deployment here refers to loading and running multiple Spring Boot/SOFABoot applications in the same JVM.\nIt is mentioned at the end of Application Packaging that a Biz package can be released to a remote repository through the mvn deploy command, similar to releasing common Jar packages. It comes naturally to mind that the advantage of doing so is that the Biz package generated by other applications can be introduced in the form of dependencies, just like introducing common Jar package dependencies. Then, what is the purpose of introducing the Biz package generated by other applications into your application? Also, how do we dynamically install and uninstall Biz packages in Jarslink 2.0?\nTo answer the two questions above is to understand the concepts of static merged deployment and dynamic merged deployment.\nStatic merged deployment To answer the first question: What is the purpose of introducing the Biz package generated by other applications into your application?\nIn the section Application Packaging, we have described how to package an application into an Ark package and offered a rough equation: Ark package = Biz package + SOFAArk framework + Ark Plugin. When a Biz package generated by other applications is introduced in the application, what kind of packaged Ark package will it be? The conclusion is that the packaging plugin will treat special dependency packages like the Biz package differently. The plugin will package all the non-Biz package dependencies into the application\u0026amp;rsquo;s Biz package, but will consider the introduced Biz package as equal to those of the current application. The final Ark package will contain multiple Biz packages. For details, refer to Ark Package Directory Structure. At this point, when you use java -jar to start this Ark package, you will find that all the contained Biz packages will be started as well.\nTo sum up, the application introduces the Biz packages generated by other applications in the form of dependencies, and the Ark package packaged by this application will contain multiple Biz packages. By executing this Ark package, all the Biz packages will be started, known as static merged deployment.\nStatic merged deployment does not depend on Jarslink 2.0 but is available directly with the SOFAArk packaging plugin.\nNote that the startup order of multiple Biz packages is controllable. When each Biz package is generated, you can use the packaging plugin to configure its priority, whose value is 100 by default. The higher the priority, the lower the value is. The priority determines the startup order of the Biz package.\nDynamic merged deployment To answer the second question: how do you dynamically install and uninstall Biz packages in Jarslink …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-deploy/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ce787203d9d834b7f796b3dbb40bf55d","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy/","summary":"In Jarslink 2.0, merged deployment refers to loading and running multiple Biz packages in the same JVM. In the section Application Packaging, we have described the relationship between the Spring Boot/SOFABoot application and the Biz package. We may think that merged deployment here refers to loading and running multiple Spring Boot/SOFABoot applications in the same JVM.\nIt is mentioned at the end of Application Packaging that a Biz package can be released to a remote repository through the mvn deploy command, similar to releasing common Jar packages.","tags":null,"title":"Merged deployment","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy/","wordcount":749},{"author":null,"categories":null,"content":" Quickly understand of ACTS models When you write a test case, you need to prepare some database tables, request parameter data of methods, or data for validating database tables and responses. You can save such data in models, and import it to preparation data or validation data when you edit the test case. This allows you to conveniently reuse data. Currently, ACTS models can be divided into database models and class models.\nIn conventional test case compilation, data preparation of models, such as database models, request parameter models, and response models, is based on the test code. The complexity of models increases with the business complexity, especially in financial-level business applications, where a class or data table may have dozens of properties or fields, and where class nesting is common. In this case, constructing complex objects is extremely difficult and prone to omissions. Some of the most frequently occurring problems are listed as follows:\n Omissions may occur and troubleshooting takes a lot of time for a large number of tables. Field names of tables are difficult to remember, and spelling errors frequently occur. The large number and complex types of interface request parameters are frustrating. There are so many class properties that important properties are prone to omission. Object construction with nested structures requires continuous effort in creating and setting values. Important properties are easily omitted when the inheritance and implementation relationships are complex.  ACTS models can effectively address the above problems by formatting classes and tables in CSV, which makes the structure of classes easier to understand. Class models and data table models can help you quickly create objects, and serialize them into the YAML file. ACTS models allow you to conveniently manage test case data.\nStorage location of models You can view existing models under the resource/model directory of the test module.\nFigure 4\nGenerate data table model Sample data table model Figure 5\n1. Validation flag description\nY: indicates that the data is to be inserted. N: indicates that the data is not to be inserted. C: indicates that ACTS will clean the inserted data by taking this value as the where condition. F: indicates that the value of this column is a database function. L: indicates that a large field data record requires line wrap. The preparation method for this data record is: A=B;C=D.  2. Quickly import data from models during test case editing\nWhen editing database table data (including preparing table data and expectation data) in ACTS IDE, you can right click to add a model of the specified table, to import all fields and values of the specified table directly from the CSV file of the table model for quick editing. For more information about the use of DB models, see Prepare database data.\nGenerate table model Figure 6\nFigure 7\nFigure 8\nClick OK to generate the model as shown in Figure 9.\nFigure 9\nACTS also supports …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-model/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"65aaf62462b3b0ea142ca75a5b61eb0d","permalink":"/en/projects/sofa-acts/usage-model/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-acts/usage-model/","summary":"Quickly understand of ACTS models When you write a test case, you need to prepare some database tables, request parameter data of methods, or data for validating database tables and responses. You can save such data in models, and import it to preparation data or validation data when you edit the test case. This allows you to conveniently reuse data. Currently, ACTS models can be divided into database models and class models.","tags":null,"title":"Models","type":"projects","url":"/en/projects/sofa-acts/usage-model/","wordcount":752},{"author":null,"categories":null,"content":" Since version 2.4.0, SOFABoot has started to support modular development capability based on Spring context isolation. To better understand the concept of modular development of SOFABoot, let\u0026amp;rsquo;s distinguish several common forms of modularization:\n Modularization based on code organization: This is the most common form. Codes with different functions are placed under different Java projects at development time and into different jar packages at compile time. At runtime, all Java classes are under the same classpath without any isolation; Modularization based on Spring context isolation: Use the Spring context to perform isolation of different function modules. At development and compile time, the codes and configurations are also placed under different Java projects. At runtime, however, if Spring beans are in different Spring contexts, they are invisible to each other, so dependency injection occurs within the same context. But, all the Java classes are still under the same ClassLoader; Modularization based on ClassLoader isolation: Borrow the ClassLoader to perform isolation. Each module has an independent ClassLoader, and the classpath between modules differs. SOFAArk is the practice of such modularization.  SOFABoot Modular Development belongs to the second modularization form\u0026amp;ndash;modularization based on Spring context isolation. Each SOFABoot module uses an independent Spring context to avoid BeanId conflicts between different SOFABoot modules and effectively reduces the cost of communication between teams during enterprise-level multi-module development.\nMore details about SOFABoot module is introduced in the article.\nFeature Description Import Dependency  To use SOFABoot module, you should import the following dependency:  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;isle-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  SOFABoot Module SOFABoot framework has defined the concept of SOFABoot module: A SOFABoot module is a common Jar package including Java code, Spring configuration files, and SOFABoot module identifiers. A SOFABoot application can be comprised of multiple SOFABoot modules, each of which has independent Spring context.\nThe modular development with SOFABoot provides developers with the following features:\n At runtime, the Spring context of each SOFABoot module is isolated, so the defined Beans between modules will not affect each other; Each SOFABoot module is full-featured and self-contained, allowing for easy migration and reuse in different SOFABoot applications. Developers only need to copy the whole SOFABoot module to the application and adjust the Maven dependence before running it.  For the format definition of SOFABoot module, see: Module Configuration.\nInvocation between SOFABoot Modules After isolation of context, the Bean between modules cannot be directly injected, so the SOFA service is required for invocation between the modules. Currently, SOFABoot offers …","date":-62135596800,"description":"","dir":"projects/sofa-boot/modular-development/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"95bc080787c3614bfa485d2f3cd0de4c","permalink":"/en/projects/sofa-boot/modular-development/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/modular-development/","summary":"Since version 2.4.0, SOFABoot has started to support modular development capability based on Spring context isolation. To better understand the concept of modular development of SOFABoot, let\u0026rsquo;s distinguish several common forms of modularization:\n Modularization based on code organization: This is the most common form. Codes with different functions are placed under different Java projects at development time and into different jar packages at compile time. At runtime, all Java classes are under the same classpath without any isolation; Modularization based on Spring context isolation: Use the Spring context to perform isolation of different function modules.","tags":null,"title":"Modular development","type":"projects","url":"/en/projects/sofa-boot/modular-development/","wordcount":578},{"author":null,"categories":null,"content":" The SOFABoot module combines a regular JAR with some SOFABoot-specific configurations, which enables a JAR to be identified by SOFABoot and modularized.\nThere are two differences between a complete SOFABoot module and a regular JAR:\n A SOFABoot module contains a sofa-module.properties file, where the name and the dependencies of the module are defined. We can place one or more Spring configuration files in the SOFABoot module\u0026amp;rsquo;s META-INF/spring directory; and SOFABoot will automatically load them as Spring configurations for that module.  Inside the sofa-module.properties file Let\u0026amp;rsquo;s look at a complete sofa-module.properties file:\nModule-Name=com.alipay.test.biz.service.impl Spring-Parent=com.alipay.test.common.dal Require-Module=com.alipay.test.biz.shared Module-Profile=dev  Module-Name This is the name of a SOFABoot module, which is also the unique identifier of the module. In a SOFABoot application, the Module-Name of a SOFABoot module must be different from that of another SOFABoot module. Note that the SOFABoot modules of a SOFABoot application runtime do not only cover the modules of the current application but also the modules introduced by dependency from other applications. When determining whether a module is unique, you must take these SOFABoot modules into consideration.\nRequire-Module This defines the dependency order of different modules. The value contains a comma-separated list of SOFABoot module names. For example, in the preceding configuration, it indicates that the current module depends on the com.alipay.test.biz.shared module. The SOFABoot framework processes this dependency by starting the com.alipay.test.biz.shared module before the current module.\nIn most cases, you do not have to define the Require-Module for a module. It is required only when the startup of a module\u0026amp;rsquo;s Spring context depends on that of another module\u0026amp;rsquo;s. For example, you have published a JVM Service in module A. In the init method of a Bean in module B, you need to call the JVM Service with a SOFA Reference. Assume that module B is started before module A, the Bean of module B will fail because the JVM Service of module A is not published yet and the init method fails. In this case, you can use the Require-Module to force module A to start before module B.\nSpring-Parent In a SOFABoot application, each SOFABoot module has a separate Spring context, and these Spring contexts are isolated from each other. Although this modular approach has many benefits, it can still cause some inconveniences in certain scenarios. For these scenarios, you can use the Spring-Parent to connect the Spring contexts of two SOFABoot modules. The name of a module can be configured with the Spring-Parent property. For example, in the preceding configuration, the Spring context of com.alipay.test.common.dal is set to the parent of the current module\u0026amp;rsquo;s Spring context.\nDue to Spring\u0026amp;rsquo;s limitations, a module\u0026amp;rsquo;s Spring-Parent contains only one …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofaboot-module/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2dbb8a536237f21afbee1e3f320b8193","permalink":"/en/projects/sofa-boot/sofaboot-module/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/sofaboot-module/","summary":"The SOFABoot module combines a regular JAR with some SOFABoot-specific configurations, which enables a JAR to be identified by SOFABoot and modularized.\nThere are two differences between a complete SOFABoot module and a regular JAR:\n A SOFABoot module contains a sofa-module.properties file, where the name and the dependencies of the module are defined. We can place one or more Spring configuration files in the SOFABoot module\u0026rsquo;s META-INF/spring directory; and SOFABoot will automatically load them as Spring configurations for that module.","tags":null,"title":"Module configuration","type":"projects","url":"/en/projects/sofa-boot/sofaboot-module/","wordcount":565},{"author":null,"categories":null,"content":"SOFABoot will calculate the dependency tree based on the Require-Module. For example, the following dependency tree represents that Modules B and C depend on Module A, Module E depends on Module D, and Module F depends on Module E:\nThe dependency tree guarantees that Module A starts before Modules B and C, Module D before Module E, and Module E before Module F, but without defining the start orders between Modules B and C, or Modules B, C and Modules D, E and F, which can start either in serial or parallel.\nSOFABoot will start the modules in parallel by default. During use, if you want to disable parallel start, you can add the following parameter to application.properties:\ncom.alipay.sofa.boot.module-start-up-parallel=false  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/parallel-start/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a6ef51b78d2a4f9af0debbc25ea45e8a","permalink":"/en/projects/sofa-boot/parallel-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/parallel-start/","summary":"SOFABoot will calculate the dependency tree based on the Require-Module. For example, the following dependency tree represents that Modules B and C depend on Module A, Module E depends on Module D, and Module F depends on Module E:\nThe dependency tree guarantees that Module A starts before Modules B and C, Module D before Module E, and Module E before Module F, but without defining the start orders between Modules B and C, or Modules B, C and Modules D, E and F, which can start either in serial or parallel.","tags":null,"title":"Module parallel startup","type":"projects","url":"/en/projects/sofa-boot/parallel-start/","wordcount":119},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-dubbo Module and Run Build with Maven Configuring（hmily-demo-motan-account module for instance）  Configure with your business database in application.yml(account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Configure with motan registration address(es) in application.yml (can run with local zookeeper instance(s))  hmily: motan: # for registration address registry: address: 127.0.0.1:2181 # replace with registration address   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   run MotanHmilyAccountApplication.java  Run hmily-demo-motan-inventory(refer to simillar instructions above). Run hmily-demo-motan-order(refer to simillar instructions above). Access on http://127.0.0.1:8088/swagger-ui.html for more. ","date":-62135596800,"description":"motan Quick Start","dir":"projects/hmily/quick-start-motan/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9b85af7dc7e27a5b9be975c9649be34b","permalink":"/en/projects/hmily/quick-start-motan/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/quick-start-motan/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-dubbo Module and Run Build with Maven Configuring（hmily-demo-motan-account module for instance）  Configure with your business database in application.yml(account module for instance)  spring: datasource: driver-class-name: com.","tags":null,"title":"Motan Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-motan/","wordcount":174},{"author":null,"categories":null,"content":" Motan Interface Sectioon  Introduce the jar packages into your interface project.  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.  public interface HelloService { @Hmily void say(String hello); }  The project with Motan implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration\n Step 3 ： Add the specific annotation to the implementation method. you need to complete the development of confirm and cancel method, if in TCC mode.\n  Introduce The Maven dependency Spring-Namespace  Introduce the hmily-motan dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-motan\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   make the configuration in the XML configuration file as below:  \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot-starter  Introduce the hmily-spring-boot-starter-motan dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-motan\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  Add annotations on the implementation interface We have completed the integration and configuration described above, and the next let\u0026amp;rsquo;s explain how to use it in detail.\nTCC Mode  Add @HmilyTCC (confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation to the concrete implementation of the interface method identified by \u0026amp;lsquo;@Hmily\u0026amp;rsquo;.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be consistent with the identification method.\n The TCC mode should ensure the idempotence of the confirm and cancel methods,Users need to develop these two methods by themselves,The confirmation and rollback behavior of all transactions are completely up tp users.The Hmily framework is just responsible for making calls.\n  public class HelloServiceImpl implements …","date":-62135596800,"description":"Hmily-Motan Distributed Transaction User Guide","dir":"projects/hmily/user-motan/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f3e4806ac6fc849d233a1b110e3e424b","permalink":"/en/projects/hmily/user-motan/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/hmily/user-motan/","summary":"Motan Interface Sectioon  Introduce the jar packages into your interface project.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.  public interface HelloService { @Hmily void say(String hello); }  The project with Motan implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration","tags":null,"title":"Motan User Guide","type":"projects","url":"/en/projects/hmily/user-motan/","wordcount":620},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git Zookeeper  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n使用你的工具 idea 打开项目，找到hmily-demo-motan项目，进行maven构建 修改项目配置（hmily-demo-motan-account为列子）  application.yml 下修改业务数据库(account项目为列子)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: #改成你的用户名 password: #改成你的密码   application.yml 下修改motan的注册中心地址(可以在自己电脑本地启动一个zookeeper服务)  hmily : motan : #注册中心配置 registry : address : 127.0.0.1:2181 #注册中心地址   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   run MotanHmilyAccountApplication.java  启动hmily-demo-motan-inventory 参考上述。 启动hmily-demo-motan-order 参考上述。 访问：http://127.0.0.1:8088/swagger-ui.html。 ","date":-62135596800,"description":"motan快速体验","dir":"projects/hmily/quick-start-motan/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9b85af7dc7e27a5b9be975c9649be34b","permalink":"/projects/hmily/quick-start-motan/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/quick-start-motan/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git Zookeeper 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 使用你的工具 idea 打开项目，找到hmily-dem","tags":null,"title":"Motan快速体验","type":"projects","url":"/projects/hmily/quick-start-motan/","wordcount":537},{"author":null,"categories":null,"content":" Motan接口部分  在你的接口项目中引入jar包。  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。  public interface HelloService { @Hmily void say(String hello); }  Motan实现项目  步骤一 ： 引入依赖hmily的jar包\n 步骤二 ： 新增Hmily配置\n 步骤三 ： 在实现方法上添加注解。TCC模式，则需要完成 confirm，cancel方法的开发\n  引入依赖 Spring-Namespace  引入依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-motan\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在xml中进行如下配置  \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot-starter  用户引入  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-motan\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入 hmily配置  在项目的 resource 添加文件名为:hmily.yml配置文件\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  实现接口上添加注解 在上述中，我们已经完成了集成与配置，现在我们来详解说一下如何进行使用。\nTCC模式  在添加@Hmily 标识的接口方法的具体实现上 加上@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  motan注解用户 对于使用 @MotanReferer 注解来注入motan服务的用户，请注意：你可以需要做如下配置:\nspring-namespace 用户 在你的xml配置中，需要将 org.dromara.hmily.spring.annotation.RefererAnnotationBeanPostProcessor 注入成spring的bean\n\u0026amp;lt;bean id = \u0026amp;quot;refererAnnotationBeanPostProcessor\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.annotation.RefererAnnotationBeanPostProcessor\u0026amp;quot;/\u0026amp;gt;  spring-boot用户 需要在yml文件里面开启注解支持：\nhmily.support.rpc.annotation = true  或者在项目中显示注入：\n@Bean public BeanPostProcessor refererAnnotationBeanPostProcessor() { return new RefererAnnotationBeanPostProcessor(); }  TAC模式 (在开发，未发布)  对@Hmily 标识的接口方法的具体实现加上@HmilyTAC   重要注意事项 在调用任何RPC调用之前，当你需要聚合rpc调用成为一次分布式事务的时候，需要在聚合RPC调用的方法上，先行添加 @HmilyTCC 或者 @HmilyTAC 注解,表示开启全局事务。\n负载均衡  如果服务部署了几个节点， 负载均衡算法最好使用 hmily, 这样 try, confirm, cancel 调用会落在同一个节点 充分利用了缓存，提搞了效率。\n 支持一下几种 hmilyActiveWeight, hmilyConfigurableWeight, hmilyConsistent, hmilyLocalFirst, hmilyRandom, hmilyRoundRobin 几种方式均是继承Motan …","date":-62135596800,"description":"Hmily-Motan分布式事务用户指南","dir":"projects/hmily/user-motan/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f3e4806ac6fc849d233a1b110e3e424b","permalink":"/projects/hmily/user-motan/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/hmily/user-motan/","summary":"Motan接口部分 在你的接口项目中引入jar包。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。 public interface HelloService { @Hmily void say(String hello); } Motan","tags":null,"title":"Motan用户指南","type":"projects","url":"/projects/hmily/user-motan/","wordcount":1045},{"author":null,"categories":null,"content":" myth 采用消息队列解决分布式事务的开源框架, 基于java语言来开发（JDK1.8），支持dubbo，springcloud,motan等rpc框架进行分布式事务。 Features  天然无缝集成 spring-boot-starter 。 RPC框架支持 : dubbo,motan,springcloud。 消息中间件支持 : jms(activimq),amqp(rabbitmq),kafka,roceketmq。 本地事务存储支持 : redis,mogondb,zookeeper,file,mysql。 事务日志序列化支持 ：java，hessian，kryo，protostuff。 采用Aspect AOP 切面思想与Spring无缝集成，天然支持集群,高可用,高并发。 配置简单，集成简单，源码简洁，稳定性高，已在生产环境使用。 内置经典的分布式事务场景demo工程，并有swagger-ui可视化界面可以快速体验。  源码解析  ## https://juejin.im/post/5a5c63986fb9a01cb64ec517   视频详解  ## 环境搭建以及运行 : http://www.iqiyi.com/w_19rw5zuigl.html ## 原理讲解（1）：http://www.iqiyi.com/w_19rw5ztpkh.html ## 原理讲解（2）：http://www.iqiyi.com/w_19rw5zslm1.html   Prerequisite  JDK 1.8+ Maven 3.2.x Git RPC framework dubbo or motan or springcloud。 Message Oriented Middleware  Support  ### 如有任何问题欢迎加入QQ群进行讨论   ### 微信公众号   ","date":-62135596800,"description":"","dir":"projects/myth/overview/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cd23b3131fd6adec40b27a5ee9dc98e3","permalink":"/en/projects/myth/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/myth/overview/","summary":"myth 采用消息队列解决分布式事务的开源框架, 基于java语言来开发（JDK1.8），支持dubbo，springcloud,motan等rpc框","tags":null,"title":"Myth 介绍","type":"projects","url":"/en/projects/myth/overview/","wordcount":535},{"author":null,"categories":null,"content":" myth 采用消息队列解决分布式事务的开源框架, 基于java语言来开发（JDK1.8），支持dubbo，springcloud,motan等rpc框架进行分布式事务。 Features  天然无缝集成 spring-boot-starter 。 RPC框架支持 : dubbo,motan,springcloud。 消息中间件支持 : jms(activimq),amqp(rabbitmq),kafka,roceketmq。 本地事务存储支持 : redis,mogondb,zookeeper,file,mysql。 事务日志序列化支持 ：java，hessian，kryo，protostuff。 采用Aspect AOP 切面思想与Spring无缝集成，天然支持集群,高可用,高并发。 配置简单，集成简单，源码简洁，稳定性高，已在生产环境使用。 内置经典的分布式事务场景demo工程，并有swagger-ui可视化界面可以快速体验。  源码解析  ## https://juejin.im/post/5a5c63986fb9a01cb64ec517   视频详解  ## 环境搭建以及运行 : http://www.iqiyi.com/w_19rw5zuigl.html ## 原理讲解（1）：http://www.iqiyi.com/w_19rw5ztpkh.html ## 原理讲解（2）：http://www.iqiyi.com/w_19rw5zslm1.html   Prerequisite  JDK 1.8+ Maven 3.2.x Git RPC framework dubbo or motan or springcloud。 Message Oriented Middleware  Support  ### 如有任何问题欢迎加入QQ群进行讨论   ### 微信公众号   ","date":-62135596800,"description":"","dir":"projects/myth/overview/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cd23b3131fd6adec40b27a5ee9dc98e3","permalink":"/projects/myth/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/myth/overview/","summary":"myth 采用消息队列解决分布式事务的开源框架, 基于java语言来开发（JDK1.8），支持dubbo，springcloud,motan等rpc框","tags":null,"title":"Myth 介绍","type":"projects","url":"/projects/myth/overview/","wordcount":535},{"author":null,"categories":null,"content":"SOFARPC already supports using Nacos as a service registry. Suppose you have deployed Nacos Server locally according to Nacos\u0026amp;rsquo;s Quick Start, and the service discovery port is set to 8848 by default.\nTo use Nacos as a service registry in SOFARPC, you only need to add the following configuration to application.properties:\ncom.alipay.sofa.rpc.registry.address=nacos://127.0.0.1:8848  If you use SOFARPC directly, not SOFABoot, you need to add dependency of nacos, notice that version is what you want to use in your project.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba.nacos\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;nacos-client\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  The current version of Nacos is supported:\nSOFARPC: 5.5.0, SOFABoot: 2.5.3。\nSOFARPC integration verification Nacos server version:0.6.0。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-nacos/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cc161f22cd2145fe309e63087581adc1","permalink":"/en/projects/sofa-rpc/registry-nacos/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-nacos/","summary":"SOFARPC already supports using Nacos as a service registry. Suppose you have deployed Nacos Server locally according to Nacos\u0026rsquo;s Quick Start, and the service discovery port is set to 8848 by default.\nTo use Nacos as a service registry in SOFARPC, you only need to add the following configuration to application.properties:\ncom.alipay.sofa.rpc.registry.address=nacos://127.0.0.1:8848  If you use SOFARPC directly, not SOFABoot, you need to add dependency of nacos, notice that version is what you want to use in your project.","tags":null,"title":"Nacos","type":"projects","url":"/en/projects/sofa-rpc/registry-nacos/","wordcount":100},{"author":null,"categories":null,"content":" 前言 本文是对 Nginx、Envoy 及 MOSN 的平滑升级原理区别的分析，适合对 Nginx 实现原理比较感兴趣的同学阅读，需要具备一定的网络编程知识。\n平滑升级的本质就是 listener fd 的迁移，虽然 Nginx、Envoy、MOSN 都提供了平滑升级支持，但是鉴于它们进程模型的差异，反映在实现上还是有些区别的。这里来探讨下它们其中的区别，并着重介绍 Nginx 的实现。\nNginx 相信有很多人认为 Nginx 的 reload 操作就能完成平滑升级，其实这是个典型的理解错误。实际上 reload 操作仅仅是平滑重启，并没有真正的升级新的二进制文件，也就是说其运行的依然是老的二进制文件。\nNginx 自身也并没有提供平滑升级的命令选项，其只能靠手动触发信号来完成。具体正确的操作步骤可以参考这里：Upgrading Executable on the Fly，这里只分析下其实现原理。\nNginx 的平滑升级是通过 fork + execve 这种经典的处理方式来实现的。准备升级时，Old Master 进程收到信号然后 fork 出一个子进程，注意此时这个子进程运行的依然是老的镜像文件。紧接着这个子进程会通过 execve 调用执行新的二进制文件来替换掉自己，成为 New Master。\n那么问题来了：New Master 启动时按理说会执行 bind + listen 等操作来初始化监听，而这时候 Old Master 还没有退出，端口未释放，执行 execve 时理论上应该会报：Address already in use 错误，但是实际上这里却没有任何问题，这是为什么？\n因为 Nginx 在 execve 的时候压根就没有重新 bind + listen，而是直接把 listener fd 添加到 epoll 的事件表。因为这个 New Master 本来就是从 Old Master 继承而来，自然就继承了 Old Master 的 listener fd，但是这里依然有一个问题：该怎么通知 New Master 呢？\n环境变量。execve 在执行的时候可以传入环境变量。实际上 Old Master 在 fork 之前会将所有 listener fd 添加到 NGINX 环境变量：\nngx_pid_t ngx_exec_new_binary(ngx_cycle_t *cycle, char *const *argv) { ... ctx.path = argv[0]; ctx.name = \u0026amp;quot;new binary process\u0026amp;quot;; ctx.argv = argv; n = 2; env = ngx_set_environment(cycle, \u0026amp;amp;n); ... env[n++] = var; env[n] = NULL; ... ctx.envp = (char *const *) env; ccf = (ngx_core_conf_t *) ngx_get_conf(cycle-\u0026amp;gt;conf_ctx, ngx_core_module); if (ngx_rename_file(ccf-\u0026amp;gt;pid.data, ccf-\u0026amp;gt;oldpid.data) == NGX_FILE_ERROR) { ... return NGX_INVALID_PID; } pid = ngx_execute(cycle, \u0026amp;amp;ctx); return pid; }  Nginx 在启动的时候，会解析 NGINX 环境变量：\nstatic ngx_int_t ngx_add_inherited_sockets(ngx_cycle_t *cycle) { ... inherited = (u_char *) getenv(NGINX_VAR); if (inherited == NULL) { return NGX_OK; } if (ngx_array_init(\u0026amp;amp;cycle-\u0026amp;gt;listening, cycle-\u0026amp;gt;pool, 10, sizeof(ngx_listening_t)) != NGX_OK) { return NGX_ERROR; } for (p = inherited, v = p; *p; p++) { if (*p == \u0026#39;:\u0026#39; || *p == \u0026#39;;\u0026#39;) { s = ngx_atoi(v, p - v); ... v = p + 1; ls = ngx_array_push(\u0026amp;amp;cycle-\u0026amp;gt;listening); if (ls == NULL) { return NGX_ERROR; } ngx_memzero(ls, sizeof(ngx_listening_t)); ls-\u0026amp;gt;fd = (ngx_socket_t) s; } } ... ngx_inherited = 1; return ngx_set_inherited_sockets(cycle); }  一旦检测到是继承而来的 socket，那就说明已经打开了，不会再继续 bind + listen 了：\nngx_int_t ngx_open_listening_sockets(ngx_cycle_t *cycle) { ... /* TODO: configurable try number */ for (tries = 5; tries; tries--) { failed = 0; /* for each listening socket */ ls = cycle-\u0026amp;gt;listening.elts; for (i = 0; i \u0026amp;lt; cycle-\u0026amp;gt;listening.nelts; i++) { ... if (ls[i].inherited) { /* TODO: close on exit */ /* TODO: nonblocking */ /* TODO: deferred accept */ continue; } ... ngx_log_debug2(NGX_LOG_DEBUG_CORE, log, 0, \u0026amp;quot;bind() %V #%d \u0026amp;quot;, \u0026amp;amp;ls[i].addr_text, s); if (bind(s, ls[i].sockaddr, ls[i].socklen) == -1) { ... } ... } } if (failed) { ngx_log_error(NGX_LOG_EMERG, log, 0, \u0026amp;quot;still could not bind()\u0026amp;quot;); return NGX_ERROR; } return NGX_OK; }  Envoy Envoy 使用的是单进程多线程模型，其局限就是无法通过环境变量来传递 listener fd。因此 Envoy 采用的是 UDS（unix domain sockets）方案。当 New Envoy 启动完成后，会通过 UDS 向 Old Envoy 请求 listener fd 副本， …","date":-62135596800,"description":"","dir":"projects/mosn/concept/nginx-envoy-mosn-hot-upgrade/","fuzzywordcount":1600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"63ca389b7e6e0a585d0183ad71887f65","permalink":"/projects/mosn/concept/nginx-envoy-mosn-hot-upgrade/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/mosn/concept/nginx-envoy-mosn-hot-upgrade/","summary":"前言 本文是对 Nginx、Envoy 及 MOSN 的平滑升级原理区别的分析，适合对 Nginx 实现原理比较感兴趣的同学阅读，需要具备一定的网络编程知识。 平滑升级的","tags":null,"title":"Nginx vs Envoy vs MOSN 平滑升级原理解析","type":"projects","url":"/projects/mosn/concept/nginx-envoy-mosn-hot-upgrade/","wordcount":1525},{"author":null,"categories":null,"content":" If you need to call SOFARPC through NodeJs, you can start by following this document.\nInstall First install the SOFARPC Node.\nhttps://github.com/sofastack/sofa-rpc-node\nUse the following command:\n$ npm install sofa-rpc-node --save  Code sample Expose an RPC service and publish it to registry center \u0026#39;use strict\u0026#39;; const { RpcServer } = require(\u0026#39;sofa-rpc-node\u0026#39;).server; const { ZookeeperRegistry } = require(\u0026#39;sofa-rpc-node\u0026#39;).registry; const logger = console; // 1. Create a Zookeeper registry client const registry = new ZookeeperRegistry({ logger, Address: \u0026#39;127.0.0.1:2181\u0026#39;, // need to start a zkServer locally }); // 2. Create an RPC Server instance const server = new RpcServer({ logger, Registry, // incoming registry client port: 12200, }); // 3. Add service server.addService({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, }, { async plus(a, b) { return a + b; }, }); // 4. Start the server and publish the service server.start() .then(() =\u0026amp;gt; { server.publish(); });  Call RPC service (Get service list from registry center) \u0026#39;use strict\u0026#39;; const { RpcClient } = require(\u0026#39;sofa-rpc-node\u0026#39;).client; const { ZookeeperRegistry } = require(\u0026#39;sofa-rpc-node\u0026#39;).registry; const logger = console; // 1. Create a Zookeeper registry client const registry = new ZookeeperRegistry({ logger, address: \u0026#39;127.0.0.1:2181\u0026#39;, }); async function invoke() { // 2. Create an RPC Client instance const client = new RpcClient({ logger, registry, }); // 3. Create a service consumer const consumer = client.createConsumer({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, }); // 4. Wait for the consumer ready (subscribe to the service list from registry center...) await consumer.ready(); // 5. Execute generic call const result = await consumer.invoke(\u0026#39;plus\u0026#39;, [ 1, 2 ], { responseTimeout: 3000 }); console.log(\u0026#39;1 + 2 = \u0026#39; + result); } invoke().catch(console.error);  Call RPC service (direct call) \u0026#39;use strict\u0026#39;; const { RpcClient } = require(\u0026#39;sofa-rpc-node\u0026#39;).client; const logger = console; async function invoke() { // No need to pass in the registry instance const client = new RpcClient({ logger, }); const consumer = client.createConsumer({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, serverHost: \u0026#39;127.0.0.1:12200\u0026#39;, // directly specify the service address }); await consumer.ready(); const result = await consumer.invoke(\u0026#39;plus\u0026#39;, [ 1, 2 ], { responseTimeout: 3000 }); console.log(\u0026#39;1 + 2 = \u0026#39; + result); } invoke().catch(console.error);  Expose and call the protobuf interface Define interface Define the interface with *.proto\nsyntax = \u0026amp;quot;proto3\u0026amp;quot;; package com.alipay.sofa.rpc.test; // optional option java_multiple_files = false; service ProtoService { rpc echoObj (EchoRequest) returns (EchoResponse) {} } message EchoRequest { string name = 1; Group group = 2; } message EchoResponse { int32 code = 1; string message = 2; } enum Group { A = 0; B = 1; }  Server code \u0026#39;use strict\u0026#39;; const antpb = require(\u0026#39;antpb\u0026#39;); const protocol = require(\u0026#39;sofa-bolt-node\u0026#39;); const { RpcServer } = require(\u0026#39;sofa-rpc-node\u0026#39;).server; …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/node-and-java-communicate/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3329852e9991868a3cdc473b861ca750","permalink":"/en/projects/sofa-rpc/node-and-java-communicate/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-rpc/node-and-java-communicate/","summary":"If you need to call SOFARPC through NodeJs, you can start by following this document.\nInstall First install the SOFARPC Node.\nhttps://github.com/sofastack/sofa-rpc-node\nUse the following command:\n$ npm install sofa-rpc-node --save  Code sample Expose an RPC service and publish it to registry center 'use strict'; const { RpcServer } = require('sofa-rpc-node').server; const { ZookeeperRegistry } = require('sofa-rpc-node').registry; const logger = console; // 1. Create a Zookeeper registry client const registry = new ZookeeperRegistry({ logger, Address: '127.","tags":null,"title":"NodeJS support","type":"projects","url":"/en/projects/sofa-rpc/node-and-java-communicate/","wordcount":635},{"author":null,"categories":null,"content":" 快速上手 如果你有通过 NodeJs 调用 SOFARPC 的需求.可以按照如下的文档来开始.\n安装 首先按照文档安装\nhttps://github.com/sofastack/sofa-rpc-node\n使用命令.\n$ npm install sofa-rpc-node --save  代码示例 暴露一个 RPC 服务，并发布到注册中心 \u0026#39;use strict\u0026#39;; const { RpcServer } = require(\u0026#39;sofa-rpc-node\u0026#39;).server; const { ZookeeperRegistry } = require(\u0026#39;sofa-rpc-node\u0026#39;).registry; const logger = console; // 1. 创建 zk 注册中心客户端 const registry = new ZookeeperRegistry({ logger, address: \u0026#39;127.0.0.1:2181\u0026#39;, // 需要本地启动一个 zkServer }); // 2. 创建 RPC Server 实例 const server = new RpcServer({ logger, registry, // 传入注册中心客户端 port: 12200, }); // 3. 添加服务 server.addService({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, }, { async plus(a, b) { return a + b; }, }); // 4. 启动 Server 并发布服务 server.start() .then(() =\u0026amp;gt; { server.publish(); });  调用 RPC 服务（从注册中心获取服务列表） \u0026#39;use strict\u0026#39;; const { RpcClient } = require(\u0026#39;sofa-rpc-node\u0026#39;).client; const { ZookeeperRegistry } = require(\u0026#39;sofa-rpc-node\u0026#39;).registry; const logger = console; // 1. 创建 zk 注册中心客户端 const registry = new ZookeeperRegistry({ logger, address: \u0026#39;127.0.0.1:2181\u0026#39;, }); async function invoke() { // 2. 创建 RPC Client 实例 const client = new RpcClient({ logger, registry, }); // 3. 创建服务的 consumer const consumer = client.createConsumer({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, }); // 4. 等待 consumer ready（从注册中心订阅服务列表...） await consumer.ready(); // 5. 执行泛化调用 const result = await consumer.invoke(\u0026#39;plus\u0026#39;, [ 1, 2 ], { responseTimeout: 3000 }); console.log(\u0026#39;1 + 2 = \u0026#39; + result); } invoke().catch(console.error);  调用 RPC 服务（直连模式） \u0026#39;use strict\u0026#39;; const { RpcClient } = require(\u0026#39;sofa-rpc-node\u0026#39;).client; const logger = console; async function invoke() { // 不需要传入 registry 实例了 const client = new RpcClient({ logger, }); const consumer = client.createConsumer({ interfaceName: \u0026#39;com.nodejs.test.TestService\u0026#39;, serverHost: \u0026#39;127.0.0.1:12200\u0026#39;, // 直接指定服务地址 }); await consumer.ready(); const result = await consumer.invoke(\u0026#39;plus\u0026#39;, [ 1, 2 ], { responseTimeout: 3000 }); console.log(\u0026#39;1 + 2 = \u0026#39; + result); } invoke().catch(console.error);  暴露和调用 protobuf 接口 接口定义 通过 *.proto 来定义接口\nsyntax = \u0026amp;quot;proto3\u0026amp;quot;; package com.alipay.sofa.rpc.test; // 可选 option java_multiple_files = false; service ProtoService { rpc echoObj (EchoRequest) returns (EchoResponse) {} } message EchoRequest { string name = 1; Group group = 2; } message EchoResponse { int32 code = 1; string message = 2; } enum Group { A = 0; B = 1; }  服务端代码 \u0026#39;use strict\u0026#39;; const antpb = require(\u0026#39;antpb\u0026#39;); const protocol = require(\u0026#39;sofa-bolt-node\u0026#39;); const { RpcServer } = require(\u0026#39;sofa-rpc-node\u0026#39;).server; const { ZookeeperRegistry } = require(\u0026#39;sofa-rpc-node\u0026#39;).registry; const logger = console; // 传入 *.proto 文件存放的目录，加载接口定义 const proto = antpb.loadAll(\u0026#39;/path/proto\u0026#39;); // 将 proto 设置到协议中 protocol.setOptions({ proto }); const registry = new ZookeeperRegistry({ logger, address: \u0026#39;127.0.0.1:2181\u0026#39;, }); const server = new RpcServer({ logger, protocol, // 覆盖协议 registry, codecType: \u0026#39;protobuf\u0026#39;, // 设置默认的序列化方式为 protobuf port: 12200, }); server.addService({ interfaceName: …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/node-and-java-communicate/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3329852e9991868a3cdc473b861ca750","permalink":"/projects/sofa-rpc/node-and-java-communicate/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/node-and-java-communicate/","summary":"快速上手 如果你有通过 NodeJs 调用 SOFARPC 的需求.可以按照如下的文档来开始. 安装 首先按照文档安装 https://github.com/sofastack/sofa-rpc-node 使用命令. $ npm install sofa-rpc-node --save 代码示例 暴露一个 RPC 服务，并发布到注册","tags":null,"title":"Node跨语言调用","type":"projects","url":"/projects/sofa-rpc/node-and-java-communicate/","wordcount":757},{"author":null,"categories":null,"content":" OkHttp Integration In this document will demonstrate how to use SOFATracer to track of OkHttp, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nDependency introduction \u0026amp;lt;!-- SOFATracer dependency --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- okhttp dependency --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.squareup.okhttp3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;okhttp\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.12.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026amp;rsquo;s application.properties file, including spring.application.name that indicates the name of the current application and logging.path that specifies the log output directory.\n# Application Name spring.application.name=HttpClientDemo # logging path logging.path=./logs # port server.port=8081  Add a Controller that provides RESTful services In the project, provide a simple Controller, for example:\n@RestController public class SampleRestController { private final AtomicLong counter = new AtomicLong(0); /** * Request http://localhost:8081/okhttp?name=sofa * @param name name * @return Map of Result */ @RequestMapping(\u0026amp;quot;/okhttp\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; greeting(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;okhttp\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); map.put(\u0026amp;quot;name\u0026amp;quot;, name); return map; } }  Construct OkHttp to initiate a call to the RESTful service above The code example is as follows:\n Construct the OkHttp Client instance:  OkHttpClientInstance httpClient = new OkHttpClientInstance(); String httpGetUrl = \u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;; String responseStr = httpClient.executeGet(httpGetUrl);  Run Start the SOFABoot app and see the log in the console as follows:\n2019-04-12 13:38:09.896 INFO 51193 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2019-04-12 13:38:09.947 INFO 51193 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8081 (http) 2019-04-12 13:38:09.952 INFO 51193 --- [ main] c.a.s.t.e.okhttp.OkHttpDemoApplication : Started OkHttpDemoApplication in 3.314 seconds (JVM running for 4.157)  When there is a log similar to the following, the call to OkHttp is successful:\n2019-04-12 13:38:10.205 INFO 51193 --- [ main] c.a.s.t.e.okhttp.OkHttpDemoApplication : Response is {\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;sofa\u0026amp;quot;}  View log In the application.properties, the log printing directory we configured is ./logs, which is the root directory of the current application (we can configure it based on actual situation). In the root directory, you can see log files in the structure similar to …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-okhttp/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2b665b984dd33a4c04b5cd7b4de2410c","permalink":"/en/projects/sofa-tracer/usage-of-okhttp/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/usage-of-okhttp/","summary":"OkHttp Integration In this document will demonstrate how to use SOFATracer to track of OkHttp, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nDependency introduction \u0026lt;!-- SOFATracer dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- okhttp dependency --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.squareup.okhttp3\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;okhttp\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.12.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026rsquo;s application.","tags":null,"title":"OkHttp Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-okhttp/","wordcount":374},{"author":null,"categories":null,"content":" OkHttp Log Format SOFATracer integrates OkHttp and outputs the requested link log data format. The default is JSON data format.\nOkHttp digest log（okhttp-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.url Request URL   method Request HTTP method   result.code HTTP return status code   req.size.bytes Request Body Size   resp.size.bytes Response Body Size   time.cost.milliseconds Request time (ms)   current.thread.name Current thread name   remote.app remote app   baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-04-12 13:38:10.187\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;OkHttpDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe85a1555047489980100151193\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:207,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  OkHttp stat log（okhttp-stat.log） stat.key is the collection of statistical keywords in this period, which uniquely determines a set of statistical data, including local.app, request.url, and method field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   request.url Request URL    method  Request HTTP method   count Number of requests in this period   total.cost.milliseconds Total duration (ms) for requests in this period   success Request result: Y means success; N indicates failure   load.test Pressure test mark: T indicates pressure test; F indicates non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-04-12 13:39:09.720\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;OkHttpDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:207,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-okhttp/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dbc9555f43d0b2eda22f10ed45713fb9","permalink":"/en/projects/sofa-tracer/log-format-okhttp/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/log-format-okhttp/","summary":"OkHttp Log Format SOFATracer integrates OkHttp and outputs the requested link log data format. The default is JSON data format.\nOkHttp digest log（okhttp-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.url Request URL   method Request HTTP method   result.","tags":null,"title":"OkHttp log","type":"projects","url":"/en/projects/sofa-tracer/log-format-okhttp/","wordcount":174},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 OkHttp 进行埋点，本示例工程地址。\n假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n依赖引入 \u0026amp;lt;!-- SOFATracer 依赖 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- okhttp 依赖 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.squareup.okhttp3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;okhttp\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;3.12.1\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  工程配置 在工程的 application.properties 文件下添加 SOFATracer 要使用的参数，包括spring.application.name 用于标示当前应用的名称；logging.path 用于指定日志的输出目录。\n# Application Name spring.application.name=OkHttpClientDemo # logging path logging.path=./logs # port server.port=8081  添加一个提供 RESTful 服务的 Controller 在工程代码中，添加一个简单的 Controller，例如：\n@RestController public class SampleRestController { private final AtomicLong counter = new AtomicLong(0); /** * Request http://localhost:8081/okhttp?name=sofa * @param name name * @return Map of Result */ @RequestMapping(\u0026amp;quot;/okhttp\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; greeting(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;okhttp\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); map.put(\u0026amp;quot;name\u0026amp;quot;, name); return map; } }  构造 OkHttp 发起一次对上文的 RESTful 服务的调用 代码示例如下：\n 构造 OkHttp Client 调用实例：  OkHttpClientInstance httpClient = new OkHttpClientInstance(); String httpGetUrl = \u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;; String responseStr = httpClient.executeGet(httpGetUrl);  运行 启动 SOFABoot 应用，在控制台中看到启动打印的日志如下：\n2019-04-12 13:38:09.896 INFO 51193 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2019-04-12 13:38:09.947 INFO 51193 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8081 (http) 2019-04-12 13:38:09.952 INFO 51193 --- [ main] c.a.s.t.e.okhttp.OkHttpDemoApplication : Started OkHttpDemoApplication in 3.314 seconds (JVM running for 4.157)  当有类似如下的日志时，说明 OkHttp 的调用成功：\n2019-04-12 13:38:10.205 INFO 51193 --- [ main] c.a.s.t.e.okhttp.OkHttpDemoApplication : Response is {\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;sofa\u0026amp;quot;}  查看日志 在上面的 application.properties 里面，我们配置的日志打印目录是 ./logs 即当前应用的根目录（我们可以根据自己的实践需要进行配置），在当前工程的根目录下可以看到类似如下结构的日志文件：\n./logs ├── spring.log └── tracelog ├── okhttp-digest.log ├── okhttp-stat.log ├── spring-mvc-digest.log ├── spring-mvc-stat.log ├── static-info.log └── tracer-self.log  示例中通过构造 OkHttp 对象发起 RESTful 服务的调用，调用完成后可以在 okhttp-digest.log 中看到类似如下的日志:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-okhttp/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2b665b984dd33a4c04b5cd7b4de2410c","permalink":"/projects/sofa-tracer/usage-of-okhttp/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-okhttp/","summary":"在本文档将演示如何使用 SOFATracer 对 OkHttp 进行埋点，本示例工程地址。 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作： 依赖引入 \u0026lt;!-- SOFATracer 依","tags":null,"title":"OkHttp 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-okhttp/","wordcount":578},{"author":null,"categories":null,"content":" SOFATracer 集成 OkHttp 后输出请求的链路数据格式，默认为 JSON 数据格式。\nOkHttp 摘要日志（okhttp-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   request.url 请求 URL   method 请求 HTTP 方法   result.code HTTP 返回状态码   req.size.bytes Request Body 大小   resp.size.bytes Response Body 大小   time.cost.milliseconds 请求耗时（ms）   current.thread.name 当前线程名   remote.app 目标应用   baggage 透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 11:35:28.429\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;OkHttpDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9271567481728265100112783\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;164ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  OkHttp 统计日志（okhttp-stat.log） stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、request.url、和 method 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   request.url 请求 URL    method  请求 HTTP 方法   count 本段时间内请求次数   total.cost.milliseconds 本段时间内的请求总耗时（ms）   success 请求结果：Y 表示成功；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 11:43:06.975\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;OkHttpDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8081/okhttp?name=sofa\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:174,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-okhttp/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dbc9555f43d0b2eda22f10ed45713fb9","permalink":"/projects/sofa-tracer/log-format-okhttp/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-okhttp/","summary":"SOFATracer 集成 OkHttp 后输出请求的链路数据格式，默认为 JSON 数据格式。 OkHttp 摘要日志（okhttp-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下","tags":null,"title":"OkHttp 日志","type":"projects","url":"/projects/sofa-tracer/log-format-okhttp/","wordcount":330},{"author":null,"categories":null,"content":" OpenFeign Integration In this document will demonstrate how to use SOFATracer to track of OpenFeign.\nPrepare Environment The versions of the framework components used in this case are as follows:\n Spring Cloud Greenwich.RELEASE SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 3.0.4 JDK 8  This case includes two submodules:\n tracer-sample-with-openfeign-provider service provider tracer-sample-with-openfeign-consumer service consumer  New SOFABoot project as parent project After creating a Spring Boot project, you need to introduce the SOFABoot\u0026amp;rsquo;s dependency. First, you need to unzip the generated zip package of Spring Boot project and modify the Maven project configuration file pom.xml.\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Replace the above with the followings:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  The ${sofa.boot.version} specifies the latest version of SOFABoot. For more information about SOFABoot versions, refer to Release notes.\nNew tracer-sample-with-openfeign-provider Module  Introducing dependence\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-zookeeper-discovery\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-openfeign\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   SOFATracer versions are controlled by SOFABoot versions. If the SOFABoot versions used do not match, you need to manually specify a tracer version that is higher than 3.0.4.\n  application.properties Configuration  spring.application.name=tracer-provider server.port=8800 spring.cloud.zookeeper.connect-string=localhost:2181 spring.cloud.zookeeper.discovery.enabled=true spring.cloud.zookeeper.discovery.instance-id=tracer-provider  Simple resource class\n@RestController public class UserController { @RequestMapping(\u0026amp;quot;/feign\u0026amp;quot;) public String testFeign(HttpServletRequest request) { return \u0026amp;quot;hello tracer feign\u0026amp;quot;; } }  New tracer-sample-with-openfeign-consumer Module  Introducing dependence  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-zookeeper-discovery\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-openfeign/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"75d9940033ad3b4342eab9d5c7a191f1","permalink":"/en/projects/sofa-tracer/usage-of-openfeign/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/usage-of-openfeign/","summary":"OpenFeign Integration In this document will demonstrate how to use SOFATracer to track of OpenFeign.\nPrepare Environment The versions of the framework components used in this case are as follows:\n Spring Cloud Greenwich.RELEASE SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 3.0.4 JDK 8  This case includes two submodules:\n tracer-sample-with-openfeign-provider service provider tracer-sample-with-openfeign-consumer service consumer  New SOFABoot project as parent project After creating a Spring Boot project, you need to introduce the SOFABoot\u0026rsquo;s dependency.","tags":null,"title":"OpenFeign Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-openfeign/","wordcount":368},{"author":null,"categories":null,"content":" OpenFeign Log Format SOFATracer integrates Spring Cloud OpenFeign and outputs the requested link log data format. The default is JSON data format.\nSpring Cloud OpenFeign digest log（feign-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.url Request URL   method Request HTTP method   result.code HTTP return status code   error error massage   req.size.bytes Request Body Size   resp.size.bytes Response Body Size   time.cost.milliseconds Request time (ms)   current.thread.name Current thread name   remote.host remote host   remote.port remote port   component.client.impl component name   baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-03-28 18:08:06.800\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;tracer-consumer\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe88f1553767685981100124403\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://10.15.232.143:8800/feign\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;error\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:18,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:206,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8082-exec-1\u0026amp;quot;,\u0026amp;quot;remote.host\u0026amp;quot;:\u0026amp;quot;10.15.232.143\u0026amp;quot;,\u0026amp;quot;remote.port\u0026amp;quot;:\u0026amp;quot;8800\u0026amp;quot;,\u0026amp;quot;component.client.impl\u0026amp;quot;:\u0026amp;quot;open-feign\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Spring Cloud OpenFeign stat log（feign-stat.log） stat.key is the collection of statistical keywords in this period, which uniquely determines a set of statistical data, including local.app, request.url, and method field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   request.url Request URL    method  Request HTTP method   count Number of requests in this period   total.cost.milliseconds Total duration (ms) for requests in this period   success Request result: Y means success ; N indicates failure   load.test Pressure test mark: T indicates pressure test; F indicates non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-03-28 18:09:06.800\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://10.15.232.143:8800/feign\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;tracer-consumer\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:206,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;Y\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-openfeign/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"81864817e297a7bf019705c72f8ff0a8","permalink":"/en/projects/sofa-tracer/log-format-openfeign/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/log-format-openfeign/","summary":"OpenFeign Log Format SOFATracer integrates Spring Cloud OpenFeign and outputs the requested link log data format. The default is JSON data format.\nSpring Cloud OpenFeign digest log（feign-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.","tags":null,"title":"OpenFeign log","type":"projects","url":"/en/projects/sofa-tracer/log-format-openfeign/","wordcount":190},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 Spring Cloud OpenFeign 进行埋点。\n基础环境 本案例使用的各框架组件的版本如下：\n Spring Cloud Greenwich.RELEASE SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 3.0.4 JDK 8  本案例包括两个子工程：\n tracer-sample-with-openfeign-provider 服务提供方 tracer-sample-with-openfeign-consumer 服务调用方  新建 SOFABoot 工程作为父工程 在创建好一个 Spring Boot 的工程之后，接下来就需要引入 SOFABoot 的依赖，首先，需要将上文中生成的 Spring Boot 工程的 zip 包解压后，修改 Maven 项目的配置文件 pom.xml，将\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  替换为：\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  这里的 ${sofa.boot.version} 指定具体的 SOFABoot 版本，参考发布历史。\n新建 tracer-sample-with-openfeign-provider  在工程模块的 pom 文件中添加 SOFATracer 依赖\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-zookeeper-discovery\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-openfeign\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   SOFATracer 版本受 SOFABoot 版本管控，如果使用的 SOFABoot 版本不匹配，则需要手动指定 tracer 版本，且版本需高于 3.0.4.\n  在工程的 application.properties 文件下添加相关参数  spring.application.name=tracer-provider server.port=8800 spring.cloud.zookeeper.connect-string=localhost:2181 spring.cloud.zookeeper.discovery.enabled=true spring.cloud.zookeeper.discovery.instance-id=tracer-provider  简单的资源类\n@RestController public class UserController { @RequestMapping(\u0026amp;quot;/feign\u0026amp;quot;) public String testFeign(HttpServletRequest request) { return \u0026amp;quot;hello tracer feign\u0026amp;quot;; } }  新建 tracer-sample-with-openfeign-consumer  在工程模块的 pom 文件中添加 SOFATracer 依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-zookeeper-discovery\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.cloud\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-cloud-starter-openfeign\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  在工程的 application.properties 文件下添加相关参数\nspring.application.name=tracer-consumer server.port=8082 spring.cloud.zookeeper.connect-string=localhost:2181 spring.cloud.zookeeper.discovery.enabled=true spring.cloud.zookeeper.discovery.instance-id=tracer-consumer   定义 feign 资源  @FeignClient(value = \u0026amp;quot;tracer-provider\u0026amp;quot;,fallback = FeignServiceFallbackFactory.class) public interface FeignService { @RequestMapping(value = \u0026amp;quot;/feign\u0026amp;quot;, method = RequestMethod.GET) …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-openfeign/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"75d9940033ad3b4342eab9d5c7a191f1","permalink":"/projects/sofa-tracer/usage-of-openfeign/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-openfeign/","summary":"在本文档将演示如何使用 SOFATracer 对 Spring Cloud OpenFeign 进行埋点。 基础环境 本案例使用的各框架组件的版本如下： Spring Cloud Greenwich.RELEASE SOFABoot 3.1.1/SpringBoot 2.1.0.RELEASE SOFATracer 3.0.4 JDK 8 本案例包括两个子工程： tracer-sample-with-openfeign-provider 服务提供方 tracer-sample-with-openfeign-consumer","tags":null,"title":"OpenFeign 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-openfeign/","wordcount":602},{"author":null,"categories":null,"content":" SOFATracer 集成 Spring Cloud OpenFeign 后输出请求的链路数据格式，默认为 JSON 数据格式。\nSpring Cloud OpenFeign 摘要日志（feign-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   request.url 请求地址   method http method   error 错误信息   req.size.bytes 请求大小   resp.size.bytes 响应大小   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:28:52.363\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;tracer-consumer\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9271567477731347100110969\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8082-exec-1\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;219ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://10.15.233.39:8800/feign\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;error\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:18,\u0026amp;quot;remote.host\u0026amp;quot;:\u0026amp;quot;10.15.233.39\u0026amp;quot;,\u0026amp;quot;remote.port\u0026amp;quot;:\u0026amp;quot;8800\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Spring Cloud OpenFeign 统计日志（feign-stat.log）ls stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、request.url、和 method 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   request.url 请求 URL    method  请求 HTTP 方法   count 本段时间内请求次数   total.cost.milliseconds 本段时间内的请求总耗时（ms）   success 请求结果：Y 表示成功；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:29:34.528\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;tracer-consumer\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://10.15.233.39:8800/feign\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:2,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:378,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-openfeign/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"81864817e297a7bf019705c72f8ff0a8","permalink":"/projects/sofa-tracer/log-format-openfeign/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-openfeign/","summary":"SOFATracer 集成 Spring Cloud OpenFeign 后输出请求的链路数据格式，默认为 JSON 数据格式。 Spring Cloud OpenFeign 摘要日志（feign-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解","tags":null,"title":"OpenFeign 日志","type":"projects","url":"/projects/sofa-tracer/log-format-openfeign/","wordcount":341},{"author":null,"categories":null,"content":" MOSN\u0026amp;rsquo;s official website mosn.io is under construction. The documents are temporarily hosted here.\nMOSN is a network proxy written in Golang. It can be used as a cloud-native network data plane, providing services with the following proxy functions: multi-protocol, modular, intelligent, and secure. MOSN is the short name of Modular Open Smart Network-proxy. MOSN can be integrated with any Service Mesh wich support xDS API. It can also be used as an independent Layer 4 or Layer 7 load balancer, API Gateway, cloud-native Ingress, etc.\nCore competence  Integrated with Istio  Integrated with Istio 1.0 and V4 APIs to run based on full dynamic resource configuration  Core forwarding  Self-contained Web server Support TCP proxy Support TProxy mode  Multi-protocol  Support HTTP/1.1 and HTTP/2 Support SOFARPC Support Dubbo protocol (under development)  Core routing  Support Virtual Host routing Support Headers/URL/Prefix routing Support Host Metadata-based Subset routing Support retry  Backend Management and load balancing  Support connection pool Support throttling Support active backend health check Support load balancing strategies, such as Random and RR Support Host Metadata-based Subset load balancing strategy  Observability  Observe network data Observing protocol data  TLS  Support HTTP/1.1 on TLS Support HTTP/2.0 on TLS Support SOFARPC on TLS  Process management + Support smooth reload + Support smooth upgrade Extension capability + Support custom private protocols + Support adding custom extensions in protocol at the TCP IO layer  Acknowledgement MOSN builds on open source projects such as Envoy and Istio, thanks to the efforts of the open source community.\n","date":-62135596800,"description":"","dir":"projects/mosn/overview/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f25c59cbb758b4dae5de39e1f1c3a2f4","permalink":"/en/projects/mosn/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/mosn/overview/","summary":"MOSN\u0026rsquo;s official website mosn.io is under construction. The documents are temporarily hosted here.\nMOSN is a network proxy written in Golang. It can be used as a cloud-native network data plane, providing services with the following proxy functions: multi-protocol, modular, intelligent, and secure. MOSN is the short name of Modular Open Smart Network-proxy. MOSN can be integrated with any Service Mesh wich support xDS API. It can also be used as an independent Layer 4 or Layer 7 load balancer, API Gateway, cloud-native Ingress, etc.","tags":null,"title":"Overview","type":"projects","url":"/en/projects/mosn/overview/","wordcount":245},{"author":null,"categories":null,"content":" MOSN\u0026amp;rsquo;s official website mosn.io is under construction. The documents are temporarily hosted here.\nMOSN is a network proxy written in Golang. It can be used as a cloud-native network data plane, providing services with the following proxy functions: multi-protocol, modular, intelligent, and secure. MOSN is the short name of Modular Open Smart Network-proxy. MOSN can be integrated with any Service Mesh wich support xDS API. It can also be used as an independent Layer 4 or Layer 7 load balancer, API Gateway, cloud-native Ingress, etc.\nCore competence  Integrated with Istio  Integrated with Istio 1.0 and V4 APIs to run based on full dynamic resource configuration  Core forwarding  Self-contained Web server Support TCP proxy Support TProxy mode  Multi-protocol  Support HTTP/1.1 and HTTP/2 Support SOFARPC Support Dubbo protocol (under development)  Core routing  Support Virtual Host routing Support Headers/URL/Prefix routing Support Host Metadata-based Subset routing Support retry  Backend Management and load balancing  Support connection pool Support throttling Support active backend health check Support load balancing strategies, such as Random and RR Support Host Metadata-based Subset load balancing strategy  Observability  Observe network data Observing protocol data  TLS  Support HTTP/1.1 on TLS Support HTTP/2.0 on TLS Support SOFARPC on TLS  Process management + Support smooth reload + Support smooth upgrade Extension capability + Support custom private protocols + Support adding custom extensions in protocol at the TCP IO layer  Acknowledgement MOSN builds on open source projects such as Envoy and Istio, thanks to the efforts of the open source community.\n","date":-62135596800,"description":"","dir":"projects/occlum/overview/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1d8851c81ed6dacf04ebbe841d1b2835","permalink":"/en/projects/occlum/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/occlum/overview/","summary":"MOSN\u0026rsquo;s official website mosn.io is under construction. The documents are temporarily hosted here.\nMOSN is a network proxy written in Golang. It can be used as a cloud-native network data plane, providing services with the following proxy functions: multi-protocol, modular, intelligent, and secure. MOSN is the short name of Modular Open Smart Network-proxy. MOSN can be integrated with any Service Mesh wich support xDS API. It can also be used as an independent Layer 4 or Layer 7 load balancer, API Gateway, cloud-native Ingress, etc.","tags":null,"title":"Overview","type":"projects","url":"/en/projects/occlum/overview/","wordcount":245},{"author":null,"categories":null,"content":" Introduction SOFALookout is a lightweight and open source middleware service of Ant Financial that solves the metrics and monitoring issues of the system. The services it provides include: Event logging, collecting, processing, storing, and querying of Metrics. The open source project consists of two separate parts, the client and server side services.\nClient-side service SOFALookout Client is a Java SDK that helps developers log events of metrics in project code. It also allows you to view real-time status information for the Java application.\n +------------------+ Reg: API: | dimension meters +--------+ +------------------+ | flatmap +---------------------------+ +-----------\u0026amp;gt; | Default/DropwizardMetrics| | +---------------------------+ | | http +--------------+ +-----------\u0026amp;gt; |Lookout server| | +--------------+ +----------------------+ | add common tags dimension EXTS: | JVM,OS,GC... +----+ +----------------------+  Server-side services SOFALookout Server helps you solve system state metrics in a distributed environment. Its data sources include, but not limited to the projects that use the lookout-client package. The server will be available in the next release, so stay tuned.\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/overview/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8a8a8ef02ca95d4d11e3e4b195bbae70","permalink":"/en/projects/sofa-lookout/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-lookout/overview/","summary":"Introduction SOFALookout is a lightweight and open source middleware service of Ant Financial that solves the metrics and monitoring issues of the system. The services it provides include: Event logging, collecting, processing, storing, and querying of Metrics. The open source project consists of two separate parts, the client and server side services.\nClient-side service SOFALookout Client is a Java SDK that helps developers log events of metrics in project code.","tags":null,"title":"Overview","type":"projects","url":"/en/projects/sofa-lookout/overview/","wordcount":160},{"author":null,"categories":null,"content":" SOFARPC is a Java-based RPC service framework open sourced by Ant Financial, which provides remote service call between applications, high scalability and fault tolerance features. Currently, all RPC calls of Ant Financial businesses use SOFARPC. SOFARPC provides users with functions such as load balancing, traffic forwarding, link tracing, link data transparent transmission, and fault removal.\nIn addition, SOFARPC supports different protocols, currently including bolt, RESTful, dubbo, and H2C. Bolt is a network communication framework based on Netty developed by Ant Financial Services Group.\nImplementation principle  When an SOFARPC application is started, if the current application needs to publish RPC services, SOFARPC will register these services to the service registry center. As shown in the figure, the service points to the Registry. When the SOFARPC application that references this service is started, it subscribes to the metadata information of the corresponding service from the service registry. After receiving the subscription request, the service registry will push the publisher\u0026amp;rsquo;s metadata list to the service reference party in real time. As shown in the figure, Register points to Reference. When the service reference party gets the addresses, it can pick up the address and initiate the call. As shown in the figure, Reference points to Service.  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/overview/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"62f0806ad40fcaaeab6a82470b14a2e2","permalink":"/en/projects/sofa-rpc/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/overview/","summary":"SOFARPC is a Java-based RPC service framework open sourced by Ant Financial, which provides remote service call between applications, high scalability and fault tolerance features. Currently, all RPC calls of Ant Financial businesses use SOFARPC. SOFARPC provides users with functions such as load balancing, traffic forwarding, link tracing, link data transparent transmission, and fault removal.\nIn addition, SOFARPC supports different protocols, currently including bolt, RESTful, dubbo, and H2C. Bolt is a network communication framework based on Netty developed by Ant Financial Services Group.","tags":null,"title":"Overview","type":"projects","url":"/en/projects/sofa-rpc/overview/","wordcount":203},{"author":null,"categories":null,"content":" Introduction This sample project shows how to build an executable-ark-jar based on a springboot project with the tool of sofa-ark-maven-plugin.\nPreparation As this project depends on the ark-plugin generated by the project of sample-ark-plugin, please ensure the sample sample-ark-plugin installed in your local maven repository before run this project.\nTools The Maven plugin of sofa-ark-maven-plugin is provided to build a standard executable-ark-jar, and just needs some simple configurations. Its maven coordinates is:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;   Refer to the document of plugin use for details\n Step By Step Based on the sample project, we will describe step by step how to package a Spring Boot Web project to an executable Ark package\nCreating Spring Boot Web Project Download a standard Spring Boot Web project from the official website https://start.spring.io/\nIntroducing sample-ark-plugin Configure items as follows under the main pom.xml of the project, and add the Ark Plugin dependency generated from another sample project, reference documents\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sample-ark-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Configuring Packaging Plugin Configure the Maven plugin (sofa-Ark-maven-plugin) as follows under the main pom.xml of the project:\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--specify destination where executable-ark-jar will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  In this sample project, we have configured only a fraction of the items, but they are enough to generate an executable Ark package. The meaning of each configuration item is shown below: * outputDirectory: the directory for the output Ark package files after packaging of mvn package;\n arkClassifier: the value of classifier included in the Maven coordinates of the Ark specified for release, which is null by default;  Note that arkClassifier is null by default. If you do not specify a classifier, the Jar package uploaded to the repository is actually an executable Ark package. If you need to distinguish it from common packaging, you need to configure a value for this …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-demo/","fuzzywordcount":800,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2c97c409788f41051c79836d277997be","permalink":"/en/projects/sofa-boot/sofa-ark-ark-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-ark-ark-demo/","summary":"Introduction This sample project shows how to build an executable-ark-jar based on a springboot project with the tool of sofa-ark-maven-plugin.\nPreparation As this project depends on the ark-plugin generated by the project of sample-ark-plugin, please ensure the sample sample-ark-plugin installed in your local maven repository before run this project.\nTools The Maven plugin of sofa-ark-maven-plugin is provided to build a standard executable-ark-jar, and just needs some simple configurations. Its maven coordinates is:","tags":null,"title":"Package into Ark JAR","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-ark-demo/","wordcount":757},{"author":null,"categories":null,"content":" Introduction This sample project demonstrates how to use Maven plugins to package a general Java project into an Ark plugin that meets the standard specifications.\nBackground In actual development, dependency conflicts often occur. Suppose we have developed a class library named sample-lib, and it might conflict with the existing dependencies when the business application is imported. At this point, we hope the library can be isolated from other business dependencies, without negotiating with each other over dependency package versions. Ark Plugin is exactly the result of our best practice under this demand background. It runs on top of the Ark Container, which is loaded by a container. Any Ark Plugin is loaded by a separate ClassLoader to be isolated from each other. There are four concepts of Ark Plugin: * Import class: When the plugin starts up, a plugin used to export the class is first delegated to load the class. If that fails, it will attempt to load from inside this plugin;\n Export class: If the class has been imported by other plugins, it will be first loaded from this plugin;\n Import resource: When the plugin is searching for resources, a plugin used to export the class is first delegated to load the class. If that fails, it will attempt to load from inside this plugin.\n Export resource: If the resource has been imported by other plugins, it will be first loaded from this plugin;\n   Refer to the plugin specifications for more details\n Tools Upon simple configurations, the officially provided Maven plugin sofa-ark-plugin-maven-plugin can package common Java projects into a standard-format Ark Plugin. The coordinates of Maven plugin are:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;   For more information, see the plugin configuration document\n Getting started Based on this sample project, we will describe how to build an Ark plugin step by step.\nCreate a Standard Maven Project This is a standard Maven project consisting of two modules: * The common module: contains the plugin export class\n The plugin module: contains com.alipay.sofa.ark.spi.service.PluginActivator interface implementation class and a plugin service class. The plugin packaging tool sofa-ark-plugin-maven-plugin can be configured in the module\u0026amp;rsquo;s pom.xml file;  Configuring Packaging Plugin Package the plugin in the pom.xml file according to the following configurations:\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--can only configure no more than one activator--\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-plugin-demo/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d8125843ced13352dd228299f222c74d","permalink":"/en/projects/sofa-boot/sofa-ark-ark-plugin-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-boot/sofa-ark-ark-plugin-demo/","summary":"Introduction This sample project demonstrates how to use Maven plugins to package a general Java project into an Ark plugin that meets the standard specifications.\nBackground In actual development, dependency conflicts often occur. Suppose we have developed a class library named sample-lib, and it might conflict with the existing dependencies when the business application is imported. At this point, we hope the library can be isolated from other business dependencies, without negotiating with each other over dependency package versions.","tags":null,"title":"Package into Ark Plugin","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-ark-plugin-demo/","wordcount":664},{"author":null,"categories":null,"content":"SOFA Mesh 项目 fork 了 Istio 项目，对 Pilot 的能力进行增强，目前在进行中的增强主要集中在下面三个方面： - 支持 Zookeeper 作为注册中心，并在此基础上支持 SOFA、DUBBO 等使用 Zookeeper 作为注册中心的微服务框架。 - 支持通用协议框架，使用一个通用协议，在 Kubernetes DNS 的基础上同时支持多种协议。 - 新增 register agent，支持 SOFA、DUBBO 和 HSF 的容器模型，即支持单个应用注册多个服务实例。\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-readme/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7b098e394986596d8fb01e1fe2120829","permalink":"/projects/sofa-mesh/pilot-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-readme/","summary":"SOFA Mesh 项目 fork 了 Istio 项目，对 Pilot 的能力进行增强，目前在进行中的增强主要集中在下面三个方面： - 支持 Zookeeper 作为注册中心，并在此基础上支持 SOFA、DUBBO","tags":null,"title":"Pilot 介绍","type":"projects","url":"/projects/sofa-mesh/pilot-readme/","wordcount":168},{"author":null,"categories":null,"content":" Print traceId And spanId To Application Log SLF4J provides MDC (Mapped Diagnostic Contexts), which supports you to define and modify log output formats and content. This document introduces the SLF4J MDC feature integrated in SOFATracer, which allows you to output the current SOFATracer context TraceId and SpanId with simply modifying the log configuration file.\nPrerequisites In order to properly print the TraceId and SpanId parameters in the logs of the application, the log programming interface needs to be programmed for SLF4J. That is, the programming interface for printing log does not rely on specific log implementation.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.slf4j\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;slf4j-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce dependency For SOFABoot or Spring Boot application, you need to introduce the specific log implementation. It is recommended to introduce Logback and Log4j2 instead of Log4j. Also, it is suggested to use only one log implementation rather than multiple implementations.\n Logback implementation:  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Log4j2 implementation:  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-log4j2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!--SOFABoot does not control log4j2 version --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Configuration method The corresponding TraceId and SpanId are printed based on SLF4J MDC. First, the log programming interface in application should be oriented to SLF4J, as follows:\n/ / Introduce interface import org.slf4j.Logger; import org.slf4j.LoggerFactory; / / Construct log printing instance private static final Logger logger = LoggerFactory.getLogger(XXX.class);  Second, to correctly print the TraceId and SpanId parameters, we also need to configure the extra parameters of PatternLayout in the log configuration file. The two parameters are %X{SOFA-TraceId} and %X. {SOFA-SpanId}, whose values ​​can be obtained from MDC.\npattern parameter configured with Logback as an example:\n\u0026amp;lt;pattern\u0026amp;gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%X{SOFA-TraceId}, %X{SOFA-SpanId}] ---- %m%n\u0026amp;lt;/pattern\u0026amp;gt;   Key configuration items: As a part of the Logback pattern, [%X{SOFA-TraceId},%X{SOFA-SpanId}] replaces the placeholders in the pattern with the specific TraceId and SpanId in the current thread process when the corresponding appender is called. If there is no corresponding TraceId and SpanId in the current thread, the placeholder is replaced with \u0026amp;ldquo;null\u0026amp;rdquo;.  Log4j2 PatternLayout configuration sample:\n\u0026amp;lt;PatternLayout pattern=\u0026amp;quot;%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%X{SOFA-TraceId},%X{SOFA-SpanId}] ---- %m%n \u0026amp;quot; /\u0026amp;gt;  Log4j PatternLayout configuration sample:\n\u0026amp;lt;layout class=\u0026amp;quot;org.apache.log4j.PatternLayout\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;param …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/print-traceid-spanid/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0d8cc680f811d1db2cffddbba269571c","permalink":"/en/projects/sofa-tracer/print-traceid-spanid/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/print-traceid-spanid/","summary":"Print traceId And spanId To Application Log SLF4J provides MDC (Mapped Diagnostic Contexts), which supports you to define and modify log output formats and content. This document introduces the SLF4J MDC feature integrated in SOFATracer, which allows you to output the current SOFATracer context TraceId and SpanId with simply modifying the log configuration file.\nPrerequisites In order to properly print the TraceId and SpanId parameters in the logs of the application, the log programming interface needs to be programmed for SLF4J.","tags":null,"title":"Print traceId and spanId in application log","type":"projects","url":"/en/projects/sofa-tracer/print-traceid-spanid/","wordcount":360},{"author":null,"categories":null,"content":"Describe several methods to use SOFARPC in different environments. * Use API in non-Spring environment * Use XML in SOFABoot environment * Use Annotation in SOFABoot environment * Use dynamic API in SOFABoot environment\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programming/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9a947dae761c84aa4d95121c076ac552","permalink":"/en/projects/sofa-rpc/programming/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/programming/","summary":"Describe several methods to use SOFARPC in different environments. * Use API in non-Spring environment * Use XML in SOFABoot environment * Use Annotation in SOFABoot environment * Use dynamic API in SOFABoot environment","tags":null,"title":"Programming","type":"projects","url":"/en/projects/sofa-rpc/programming/","wordcount":34},{"author":null,"categories":null,"content":" Configure HTTP protocol Mesher  See the sample project that MOSN forwards HTTP http-sample.   Configure SOFARPC protocol Mesher  See the sample project that MOSN forwards SOFARPC sofarpc-sample.  Configure TCP protocol Mesher  See the sample project that MOSN serves as a TCP Proxy tcpproxy-sample.  ","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-run-samples/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"600c182fdee786a59e14899ba0fce8a1","permalink":"/en/projects/mosn/quick-start-run-samples/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/mosn/quick-start-run-samples/","summary":" Configure HTTP protocol Mesher  See the sample project that MOSN forwards HTTP http-sample.   Configure SOFARPC protocol Mesher  See the sample project that MOSN forwards SOFARPC sofarpc-sample.  Configure TCP protocol Mesher  See the sample project that MOSN serves as a TCP Proxy tcpproxy-sample.  ","tags":null,"title":"Project sample","type":"projects","url":"/en/projects/mosn/quick-start-run-samples/","wordcount":42},{"author":null,"categories":null,"content":" Configure HTTP protocol Mesher  See the sample project that MOSN forwards HTTP http-sample.   Configure SOFARPC protocol Mesher  See the sample project that MOSN forwards SOFARPC sofarpc-sample.  Configure TCP protocol Mesher  See the sample project that MOSN serves as a TCP Proxy tcpproxy-sample.  ","date":-62135596800,"description":"","dir":"projects/occlum/quick-start-run-samples/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"93784b2fca67986e00aa3bc5ea0dbb6b","permalink":"/en/projects/occlum/quick-start-run-samples/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/occlum/quick-start-run-samples/","summary":" Configure HTTP protocol Mesher  See the sample project that MOSN forwards HTTP http-sample.   Configure SOFARPC protocol Mesher  See the sample project that MOSN forwards SOFARPC sofarpc-sample.  Configure TCP protocol Mesher  See the sample project that MOSN serves as a TCP Proxy tcpproxy-sample.  ","tags":null,"title":"Project sample","type":"projects","url":"/en/projects/occlum/quick-start-run-samples/","wordcount":42},{"author":null,"categories":null,"content":" Some sample projects are provided in the source project to assist in the use of the project. The readme file of the sample project has additional instructions for use, and you need to import these sample projects separately into IDE.\nClient-side sample project  lookout-client-samples-java   This sample project demonstrates how to use and configure the client in code form in a normal Java project.\n lookout-client-samples-boot   This sample project demonstrates how to use and configure the client in a SpringBoot (or SofaBoot) project.\n lookout-client-samples-prometheus   The sample project demonstrates how to use and configure the client to use prometheus in a SpringBoot (or SofaBoot) project.\nServer-side sample project ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-samples/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a8a0fcd3f99ce2fb46e4d543e30797c9","permalink":"/en/projects/sofa-lookout/use-guide-samples/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-lookout/use-guide-samples/","summary":"Some sample projects are provided in the source project to assist in the use of the project. The readme file of the sample project has additional instructions for use, and you need to import these sample projects separately into IDE.\nClient-side sample project  lookout-client-samples-java   This sample project demonstrates how to use and configure the client in code form in a normal Java project.\n lookout-client-samples-boot   This sample project demonstrates how to use and configure the client in a SpringBoot (or SofaBoot) project.","tags":null,"title":"Project sample","type":"projects","url":"/en/projects/sofa-lookout/use-guide-samples/","wordcount":105},{"author":null,"categories":null,"content":" ﻿SOFABoot provides developers with three ways to publish and reference JVM services\n XML Annotation Programming API  XML Service Publish First, we need to define a Bean:\n\u0026amp;lt;bean id=\u0026amp;quot;sampleService\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleServiceImpl\u0026amp;quot;\u0026amp;gt;  Then, publish the Bean as a SOFA JVM service by using the Spring extension tag provided by SOFA.\n\u0026amp;lt;sofa:service interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; ref=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  In the preceding configuration, the interface parameter indicates the interface for releasing services, and the ref parameter indicates the Bean to be published as a JVM service.\nAt this point, we have published a JVM service success.\nService Reference We can also reference a JVM service by using the Spring extension tag provided by SOFA.\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sofa.runtime.test.service.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleServiceRef\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.jvm/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  In the preceding configuration, the interface parameter indicates the service interface, which must be consistent with that configured during the service publish. The meaning of the ID attribute is the same as Spring BeanId. A Spring Bean with the ID sampleServiceRef will be generated from the above configuration. We can inject it anywhere in the Spring context of the current SOFABoot module.\n service/reference tag also supports RPC service publish, with related document: RPC Service Publish and Reference\n Annotation  Warning\nIf a service has been annotated with @SofaService, it cannot be published in the mode of XML. Choose one mode to publish the service instead of a mixture of two modes.\n In addition to publishing JVM services and reference through XML, SOFABoot also provides Annotation for JVM services publish and reference. To publish JVM services through Annotation, we only need to add an annotation @SofaService to the implementation class, as follows:\n@SofaService public class SampleImpl implements SampleInterface { public void test() { } }   Prompt\n@SofaService is used to publish a Spring Bean as a JVM service, which means that although you may not write the configuration of \u0026amp;lt;sofa:service/\u0026amp;gt;, you still need to configure the class annotated with @SofaService as a Spring Bean.\n When configuring \u0026amp;lt;sofa:service/\u0026amp;gt; in the XML mode, you have configured an interface for the service. However, when using the @SofaService annotation, you haven\u0026amp;rsquo;t configured the service interface. This is because when the class annotated with @SofaService has implemented only one interface, the framework directly uses the interface as the service interface. What if the class annotated with @SofaService has implemented multiple interfaces? In this case, you can set the interfaceType field of @SofaService to specify the service interface, as shown below: …","date":-62135596800,"description":"","dir":"projects/sofa-boot/module-service/","fuzzywordcount":1100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"527472fbe57ce450e4e2b41d878704cb","permalink":"/en/projects/sofa-boot/module-service/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-boot/module-service/","summary":"﻿SOFABoot provides developers with three ways to publish and reference JVM services\n XML Annotation Programming API  XML Service Publish First, we need to define a Bean:\n\u0026lt;bean id=\u0026quot;sampleService\u0026quot; class=\u0026quot;com.alipay.sofa.runtime.test.service.SampleServiceImpl\u0026quot;\u0026gt;  Then, publish the Bean as a SOFA JVM service by using the Spring extension tag provided by SOFA.\n\u0026lt;sofa:service interface=\u0026quot;com.alipay.sofa.runtime.test.service.SampleService\u0026quot; ref=\u0026quot;sampleService\u0026quot;\u0026gt; \u0026lt;sofa:binding.jvm/\u0026gt; \u0026lt;/sofa:service\u0026gt;  In the preceding configuration, the interface parameter indicates the interface for releasing services, and the ref parameter indicates the Bean to be published as a JVM service.","tags":null,"title":"Publish and reference JVM services","type":"projects","url":"/en/projects/sofa-boot/module-service/","wordcount":1001},{"author":null,"categories":null,"content":" To run this demo, you should sign up an Ant Financial technology account. Please see Ant Finanical Official Site to see more details.\nDemo content Service Mesh applies the communication capabilities between services to the infrastructure, thus decoupling and lightweighting applications.\nHowever, Service Mesh itself is still complex. CloudMesh can easily implement Service Mesh technology by hosting Service Mesh on the cloud.\nWith our workshop, you can easily deploy applications developed in multiple programming languages ​​to CloudMesh, thereby experiencing the capabilities of Service Mesh. The capabilities include accessing services, monitoring traffic, experiencing service goverance, managing Sidecar, and gray release of new versions of services.\nThis demo focuses on the powerful traffic control capability of CloudMesh. In the process of gray release, you can precisely control the gray traffic ratio, and monitor the actual traffic trend in CloudMesh:\nThe general gray release function occupies twice capacity in the gray process.\nThe gray release function of CloudMesh does not need to occupy extra capacity in gray release process, and also allows pausing the release process to modify gray ratio multiple times.\nOperation guide For convenience, we have prepared a detailed operation guide for this demo.\nClick here to visit online version.\n","date":-62135596800,"description":"This guide introduces how to quickly deploy applications to CloudMesh, access services, monitor traffic, experience service governance, manage Sidecar, and perform gray release of new versions of services.","dir":"guides/kc-cloud-mesh-demo/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e389a65e6736e909718275cd76505525","permalink":"/en/guides/kc-cloud-mesh-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/guides/kc-cloud-mesh-demo/","summary":"To run this demo, you should sign up an Ant Financial technology account. Please see Ant Finanical Official Site to see more details.\nDemo content Service Mesh applies the communication capabilities between services to the infrastructure, thus decoupling and lightweighting applications.\nHowever, Service Mesh itself is still complex. CloudMesh can easily implement Service Mesh technology by hosting Service Mesh on the cloud.\nWith our workshop, you can easily deploy applications developed in multiple programming languages ​​to CloudMesh, thereby experiencing the capabilities of Service Mesh.","tags":null,"title":"Put Service Mesh into practice with CloudMesh","type":"guides","url":"/en/guides/kc-cloud-mesh-demo/","wordcount":199},{"author":null,"categories":null,"content":" I have imported the jar package to my project, but found that my project cannot be started, what should I do if any error reported?  Answer : When you meet this kind of error, it requires you to locate the problem yourself, you can check the items as bellow:  check whether the framework configuration is carried out according to the document whether your project runtime environment is correct whether there is any dependency conflict problem   If your problem is still present after above check items, you can provide an issue on github of this project, our team will provide technique support as soon as possible.\nWhat if the microservice act as abnormal, but the transaction was not rolled back?  Answer : First of all, you can check the transaction log records. If the transaction log records exist, the rollback will be performed after the scheduled time you configured.  What should I do if I compile the source code and found that the get and set methods are missing?  Answer : The source code uses lombok, you may need to install the corresponding plug-in in your development tool. (No set get method does not affect the operation).  ","date":-62135596800,"description":"Frequently asked questions","dir":"projects/hmily/faq/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"291ff6f051e39fe15edb0c08d62aef12","permalink":"/en/projects/hmily/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/faq/","summary":"I have imported the jar package to my project, but found that my project cannot be started, what should I do if any error reported?  Answer : When you meet this kind of error, it requires you to locate the problem yourself, you can check the items as bellow:  check whether the framework configuration is carried out according to the document whether your project runtime environment is correct whether there is any dependency conflict problem   If your problem is still present after above check items, you can provide an issue on github of this project, our team will provide technique support as soon as possible.","tags":null,"title":"Questions","type":"projects","url":"/en/projects/hmily/faq/","wordcount":196},{"author":null,"categories":null,"content":" This topic comprises four parts:\n Part 1: Install the ACTS IDE visual editor on Intellij IDEA. Part 2: Import the ACTS dependency to a multi-module project. Part 3: Establish the ACTS framework in the test module to manage ACTS test cases. Part 4: Generate the ACTS test script.  1. Install ACTS IDE We recommend that you use Intellij IDEA 2017. For the sake of your data security, please download the ACTS IDE installation package from the following source only: Click to download ACTS IDE.\nLocal installation: Choose Preferences \u0026amp;gt; Plugins. Install the plugin from disk and restart Intellij IDEA. 2. Import the ACTS dependency Before introducing the dependencies, make sure your application is a multi-module project (including the test module). After you import the dependency, ACTS places all test code under the test module for convenient ACTS test case management.\nYou can read the following information based on the actual situation of your application:\nIf your application is a complete multi-module project, you can refer to section 2.1 to import the ACTS dependency. If your application is a multi-module project without a test module, you can refer to section 2.2 to quickly create a test module. If your application is not a multi-module project, you can refer to section 2.3 to quickly create a multi-module project. If you have not created a project yet, you can use SOFABoot to quickly create an application.\n2.1 Multi-module application - with the test module You only need to import acts-bom to the pom.xml file of the test module.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.acts\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;acts-bom\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${acts.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;type\u0026amp;gt;pom\u0026amp;lt;/type\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.2 Multi-module application - without the test module Here, Intellij IDEA is used to create the submodule.\nRight click the parent project, choose New \u0026amp;gt; Module, and enter the name for the test module, which follows the pattern of appname-test. The step-by-step procedure is illustrated in the following figures.\nStep 1: Create a test module Step 2: Manage the test module Manage the test module that you have created in the pom.xml file under the parent project.\nStep 3: Import the ACTS dependency Find the test module that you just created and import acts-bom to its pom.xml file.\n\u0026amp;lt;! -- Import the pom file that contains SOFABootApplication --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.example\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;example-service\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;! -- Import the ACTS dependency --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.acts\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;acts-bom\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${acts.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;type\u0026amp;gt;pom\u0026amp;lt;/type\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.3 Single-module application If you already have a sound single-module SOFABoot application, you can quickly create a multi-module project based on the existing project in the following …","date":-62135596800,"description":"","dir":"projects/sofa-acts/getting-started/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dfc5fb9b394ea14c280568dcb881a8b0","permalink":"/en/projects/sofa-acts/getting-started/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-acts/getting-started/","summary":"This topic comprises four parts:\n Part 1: Install the ACTS IDE visual editor on Intellij IDEA. Part 2: Import the ACTS dependency to a multi-module project. Part 3: Establish the ACTS framework in the test module to manage ACTS test cases. Part 4: Generate the ACTS test script.  1. Install ACTS IDE We recommend that you use Intellij IDEA 2017. For the sake of your data security, please download the ACTS IDE installation package from the following source only: Click to download ACTS IDE.","tags":null,"title":"Quick start","type":"projects","url":"/en/projects/sofa-acts/getting-started/","wordcount":650},{"author":null,"categories":null,"content":" This topic helps you quickly download, install, and use SOFADashboard on your computer.\nPrepare the environment sofa-dashboard-backend needs to be run in a Java environment. Make sure that it can be used normally in the following runtime environments:\n JDK 1.8+: Download and Configure. Maven 3.2.5+: Download and Configure.  sofa-dashboard-frontend uses the Ant Design Pro scaffold. For more information about the frontend environment, see Ant Design.\nInitialize the database  MySQL version: 5.6+\n SOFAArk control uses MySQL for resource data storage. You can find the SofaDashboardDB.sql script under the project directory and run this script to initialize database tables.\nZooKeeper  ZooKeeper 3.4.x and ZooKeeper 3.5.x\n Service governance and SOFAArk control of SOFADashboard are dependent on ZooKeeper, therefore you need to start the ZooKeeper service locally. For more information, see ZooKeeper Document.\nRun the backend project \u0026amp;gt; git clone https://github.com/sofastack/sofa-dashboard.git \u0026amp;gt; cd sofa-dashboard \u0026amp;gt; mvn clean package -DskipTests \u0026amp;gt; cd sofa-dashboard-backend/sofa-dashboard-web/target/ \u0026amp;gt; java -jar sofa-dashboard-web-1.0.0-SNAPSHOT.jar  Run the frontend project sofa-dashboard-front is the frontend code-based project of SOFADashboard. It is developed based on the open-source frontend framework antd of Ant Financial.\n\u0026amp;gt; cd sofa-dashboard-front \u0026amp;gt; npm i \u0026amp;gt; npm start  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/quick-start/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fa4c5f48810727f71d675255f19617a3","permalink":"/en/projects/sofa-dashboard/quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/quick-start/","summary":"This topic helps you quickly download, install, and use SOFADashboard on your computer.\nPrepare the environment sofa-dashboard-backend needs to be run in a Java environment. Make sure that it can be used normally in the following runtime environments:\n JDK 1.8+: Download and Configure. Maven 3.2.5+: Download and Configure.  sofa-dashboard-frontend uses the Ant Design Pro scaffold. For more information about the frontend environment, see Ant Design.\nInitialize the database  MySQL version: 5.","tags":null,"title":"Quick start","type":"projects","url":"/en/projects/sofa-dashboard/quick-start/","wordcount":186},{"author":null,"categories":null,"content":" This article is intended to help developers who are new to the MOSN project to quickly build a development environment, and compile, test, package, and run sample code.\nNote: MOSN is developed based on Go 1.12.7 and uses dep for dependency management.\nPrepare running environment  If you use a container to run MOSN, you must install Docker first. If you use a local machine, you must use a Unix-like environment. Install Go\u0026amp;rsquo;s build environment. Install dep. See the official installation documentation.  Get codes The codes for the MOSN project are hosted in GitHub and can be obtained in the following way:\ngo get mosn.io/mosn  If an error occurs when run \u0026amp;ldquo;go get\u0026amp;rdquo;, just create the project manually.\n# Enter src dirctory under GOPATH cd $GOPATH/src # Create mosn.io dirctory mkdir -p mosn.io cd mosn.io # clone MOSN codes git clone git@github.com:mosn/mosn.git cd sofa-mosn  The final path of MOSN source codes is $GOPATH/src/mosn.io/mosn.\nImport by using IDE Use the Golang IDE to import the $GOPATH/src/mosn.io/mosn project. Goland is recommended.\nCompile codes In the project root directory, select the following command to compile the MOSN binary file according to your machine type and the environment where you want to execute binary:\nCompile with Docker image\nmake build // compile linux 64bit executable binary  non-docker, local compilation\nCompile local executable binary files.\nmake build-local  Non-Linux machine compiles Linux 64-bit executable binary files crosswise.\nmake build-linux64  Non-Linux machine compiles Linux 32-bit executable binary files crosswise.\nmake build-linux32  Once compiled, the compiled binary files can be found in the build/bundles/${version}/binary directory.\nCreate image Run the following command to create an image:\nmake image  Run test In the project root directory, run the unit test:\nmake unit-test  In the project root directory, run the integrate test(slow):\nmake integrate  Start MOSN from configuration file ./mosn start -c \u0026#39;$CONFIG_FILE\u0026#39;  Start MOSN forwarding sample program See the sample project in the examples directory.\nUse MOSN to build a Service Mesh platform See Integrate Istio.\n","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-setup/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d41615315adb522aa4b84762f113a574","permalink":"/en/projects/mosn/quick-start-setup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/mosn/quick-start-setup/","summary":"This article is intended to help developers who are new to the MOSN project to quickly build a development environment, and compile, test, package, and run sample code.\nNote: MOSN is developed based on Go 1.12.7 and uses dep for dependency management.\nPrepare running environment  If you use a container to run MOSN, you must install Docker first. If you use a local machine, you must use a Unix-like environment.","tags":null,"title":"Quick start guide","type":"projects","url":"/en/projects/mosn/quick-start-setup/","wordcount":325},{"author":null,"categories":null,"content":" This article is intended to help developers who are new to the MOSN project to quickly build a development environment, and compile, test, package, and run sample code.\nNote: MOSN is developed based on Go 1.12.7 and uses dep for dependency management.\nPrepare running environment  If you use a container to run MOSN, you must install Docker first. If you use a local machine, you must use a Unix-like environment. Install Go\u0026amp;rsquo;s build environment. Install dep. See the official installation documentation.  Get codes The codes for the MOSN project are hosted in GitHub and can be obtained in the following way:\ngo get mosn.io/mosn  If an error occurs when run \u0026amp;ldquo;go get\u0026amp;rdquo;, just create the project manually.\n# Enter src dirctory under GOPATH cd $GOPATH/src # Create mosn.io dirctory mkdir -p mosn.io cd mosn.io # clone MOSN codes git clone git@github.com:mosn/mosn.git cd sofa-mosn  The final path of MOSN source codes is $GOPATH/src/mosn.io/mosn.\nImport by using IDE Use the Golang IDE to import the $GOPATH/src/mosn.io/mosn project. Goland is recommended.\nCompile codes In the project root directory, select the following command to compile the MOSN binary file according to your machine type and the environment where you want to execute binary:\nCompile with Docker image\nmake build // compile linux 64bit executable binary  non-docker, local compilation\nCompile local executable binary files.\nmake build-local  Non-Linux machine compiles Linux 64-bit executable binary files crosswise.\nmake build-linux64  Non-Linux machine compiles Linux 32-bit executable binary files crosswise.\nmake build-linux32  Once compiled, the compiled binary files can be found in the build/bundles/${version}/binary directory.\nCreate image Run the following command to create an image:\nmake image  Run test In the project root directory, run the unit test:\nmake unit-test  In the project root directory, run the integrate test(slow):\nmake integrate  Start MOSN from configuration file ./mosn start -c \u0026#39;$CONFIG_FILE\u0026#39;  Start MOSN forwarding sample program See the sample project in the examples directory.\nUse MOSN to build a Service Mesh platform See Integrate Istio.\n","date":-62135596800,"description":"","dir":"projects/occlum/quick-start-setup/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cd757e2e2cca38a99b2de1c0be1f6807","permalink":"/en/projects/occlum/quick-start-setup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/occlum/quick-start-setup/","summary":"This article is intended to help developers who are new to the MOSN project to quickly build a development environment, and compile, test, package, and run sample code.\nNote: MOSN is developed based on Go 1.12.7 and uses dep for dependency management.\nPrepare running environment  If you use a container to run MOSN, you must install Docker first. If you use a local machine, you must use a Unix-like environment.","tags":null,"title":"Quick start guide","type":"projects","url":"/en/projects/occlum/quick-start-setup/","wordcount":325},{"author":null,"categories":null,"content":" ﻿In this document, we will create a Spring Boot project and introduce the basic dependencies of SOFABoot as well as its Health Check expansion capability, to demonstrate how to get started quickly with SOFABoot.\nEnvironment Preparation To use SOFABoot, we need to prepare the basic environment first. SOFABoot depends on the following environment: - JDK7 or JDK8 - Needs to be compiled with Apache Maven 3.2.5 or above\nCreate Project SOFABoot is directly built on Spring Boot, so it can be generated by Spring Boot Generators. In this document, we need to add a web dependency for final view of its effect in the browser.\nAdd SOFABoot dependencies When creating a Spring Boot project, we need to import SOFABoot dependencies. First, extract the \u0026amp;lsquo;zip\u0026amp;rsquo; package of the project generated above and modify the \u0026amp;lsquo;pom.xml\u0026amp;rsquo; file, or the maven project configuration file. Replace\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  as:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Here, ${sofa.boot.version} denotes the SOFABoot version (please refer to release note). Then, add a SOFABoot dependency of Health Check extension and Spring Boot Web Starter.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;healthcheck-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-web\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Finally, configure parameters commonly used in the SOFABoot project in the application.properties file. The spring.application.name parameter is required to name the current application; the logging path specifies the output directory for logging information.\n# Application Name spring.application.name=SOFABoot Demo # logging path logging.path=./logs  Advice to refer to the SOFABoot Module document before learn this demo.\nRun it We can import the project into IDE and run the \u0026amp;lsquo;main\u0026amp;rsquo; method in the generated project (generally in the XXXApplication class) to start the application, or we can execute the mvn spring-boot:run command under the project\u0026amp;rsquo;s root directory, which will print the startup logging in the console:\n2018-04-05 21:36:26.572 INFO ---- Initializing ProtocolHandler [\u0026amp;quot;http-nio-8080\u0026amp;quot;] 2018-04-05 21:36:26.587 INFO ---- Starting ProtocolHandler [http-nio-8080] 2018-04-05 21:36:26.608 INFO ---- Using a shared selector for servlet write/read 2018-04-05 21:36:26.659 INFO ---- Tomcat started on port(s): 8080 (http)  We can browse http://localhost:8080/sofaboot/versions to view the version summary generated by Maven plugin in SOFABoot. The result is …","date":-62135596800,"description":"","dir":"projects/sofa-boot/quick-start/","fuzzywordcount":1400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7f582b905fde4a56791c03d4dd6b5a57","permalink":"/en/projects/sofa-boot/quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/en/projects/sofa-boot/quick-start/","summary":"﻿In this document, we will create a Spring Boot project and introduce the basic dependencies of SOFABoot as well as its Health Check expansion capability, to demonstrate how to get started quickly with SOFABoot.\nEnvironment Preparation To use SOFABoot, we need to prepare the basic environment first. SOFABoot depends on the following environment: - JDK7 or JDK8 - Needs to be compiled with Apache Maven 3.2.5 or above\nCreate Project SOFABoot is directly built on Spring Boot, so it can be generated by Spring Boot Generators.","tags":null,"title":"Quick start guide","type":"projects","url":"/en/projects/sofa-boot/quick-start/","wordcount":1381},{"author":null,"categories":null,"content":" SOFATracer integration component list reference:Introduction To SOFATracer, Please pay attention to the SOFATracer version and JDK version of different components when using.\nPrepare Environment To use SOFABoot, you need to prepare the basic environment first. SOFABoot relies on the following environments: - JDK7 or JDK8 - Apache Maven 3.2.5+ required for compilation\nSamples List The following Samples projects are all SOFABoot projects (also supported in the SpringBoot project). For information on how to create SOFABoot projects, please refer to SOFABoot quick start.\n Component Integration  Spring MVC Integration HttpClient Integration DataSource Integration RestTemplate Integration OkHttp Integration SOFARPC Integration Dubbo Integration Spring Cloud OpenFeign Integration  Sampling Report Data To Zipkin  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/componentaccess/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"42fbb0f6b6d459b7b04d45cad143d4ff","permalink":"/en/projects/sofa-tracer/componentaccess/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/componentaccess/","summary":"SOFATracer integration component list reference:Introduction To SOFATracer, Please pay attention to the SOFATracer version and JDK version of different components when using.\nPrepare Environment To use SOFABoot, you need to prepare the basic environment first. SOFABoot relies on the following environments: - JDK7 or JDK8 - Apache Maven 3.2.5+ required for compilation\nSamples List The following Samples projects are all SOFABoot projects (also supported in the SpringBoot project). For information on how to create SOFABoot projects, please refer to SOFABoot quick start.","tags":null,"title":"Quick start guide","type":"projects","url":"/en/projects/sofa-tracer/componentaccess/","wordcount":108},{"author":null,"categories":null,"content":" This project demonstrates how to use SOFALookout in SOFABoot and connect to the Actuator of Spring Boot. If you want to connect to Prometheus or other Registry, see the Registry section.\nCreate a SpringBoot (or SofaBoot) project Create a new Spring Boot application (In case of SOFABoot project, import to SOFABoot as described in SOFABoot Documentation - Dependency Management.\nIntroduce Lookout\u0026amp;rsquo;s Starter dependency Introduce the following dependency in pom.xml:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  In case of Spring Boot project, it is required to specify a version.\nCreate a Metrics indicator After completing the introduction of dependencies, you can add the following methods to the startup class in Spring Boot:\n@Autowired private Registry registry; @PostConstruct public void init() { Counter counter = registry.counter(registry.createId(\u0026amp;quot;http_requests_total\u0026amp;quot;).withTag(\u0026amp;quot;instant\u0026amp;quot;, NetworkUtil.getLocalAddress().getHostName())); counter.inc(); }  The above code directly injects a Registry field through @Autowired. Through the Registry field, you can create the corresponding Counter, and then modify the Counter data to generate the Metrics of the SOFALookout.\nAdd configuration item In SOFABoot project, you need to add a configuration item for the application name: spring.application.name=xxx.\nConnect to Spring Boot Actuator After adding a new indicator, you can choose to connect to the Spring Boot Actuator. Then the following dependency is required:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-actuator\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  After adding the above dependency, you can launch the application locally, visit http://localhost:8080/metrics, and you can see the metrics added earlier, as follows:\n\u0026amp;quot;http_requests_total.instant-MacBook-Pro-4.local\u0026amp;quot;: 1,  The above codes are at lookout-client-samples-boot, you can Download them as a reference.\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/quick-start-client-boot/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"27e057f8a8a4ac97f42ea66ca6a17fdd","permalink":"/en/projects/sofa-lookout/quick-start-client-boot/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-lookout/quick-start-client-boot/","summary":"This project demonstrates how to use SOFALookout in SOFABoot and connect to the Actuator of Spring Boot. If you want to connect to Prometheus or other Registry, see the Registry section.\nCreate a SpringBoot (or SofaBoot) project Create a new Spring Boot application (In case of SOFABoot project, import to SOFABoot as described in SOFABoot Documentation - Dependency Management.\nIntroduce Lookout\u0026rsquo;s Starter dependency Introduce the following dependency in pom.xml:","tags":null,"title":"Quick start guide for SOFABoot project","type":"projects","url":"/en/projects/sofa-lookout/quick-start-client-boot/","wordcount":244},{"author":null,"categories":null,"content":" Quick start for client Common Java Project Add the Maven dependency of the client to the application:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-client\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Lookout-client relies on the lookout-reg-server module by default (supports reporting metrics data to the lookout server). If you want to use a different type of registry (such as lookout-reg-prometheus), then add the corresponding dependency.\nBefore starting to use the SOFALookout Client, you must firstly build a global client instance (com.alipay.lookout.client.DefaultLookoutClient).\nLookoutConfig lookoutConfig = new LookoutConfig(); DefaultLookoutClient client = new DefaultLookoutClient(\u0026amp;quot;appName\u0026amp;quot;); // Choose to build the Registry you need to use (if you need multiple registry types, it is recommended to use the same lookoutConfig instance for centralized management). LookoutRegistry lookoutRegistry = new LookoutRegistry(lookoutConfig); // Client can add a registry instance (at least one) after the client is created. client.addRegistry(lookoutRegistry); // (Optional) Uniformly register the metrics of extended modules for the registry instances that have been added or will be added to the client. client.registerExtendedMetrics();  Then get the Registry instance through the client and use it:\n// The registry is a \u0026amp;quot;combination\u0026amp;quot; registry Registry registry = client.getRegistry(); //demo Id id = registry.createId(\u0026amp;quot;http_requests_total\u0026amp;quot;); Counter counter = registry.counter(id); counter.inc();  For the use of the client, see Project sample.\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/quick-start-client-java/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5dc476aa21ece4789859f1af598d4445","permalink":"/en/projects/sofa-lookout/quick-start-client-java/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-lookout/quick-start-client-java/","summary":"Quick start for client Common Java Project Add the Maven dependency of the client to the application:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa.lookout\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lookout-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lookout.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;  Lookout-client relies on the lookout-reg-server module by default (supports reporting metrics data to the lookout server). If you want to use a different type of registry (such as lookout-reg-prometheus), then add the corresponding dependency.\nBefore starting to use the SOFALookout Client, you must firstly build a global client instance (com.","tags":null,"title":"Quick start guide for common Java project","type":"projects","url":"/en/projects/sofa-lookout/quick-start-client-java/","wordcount":197},{"author":null,"categories":null,"content":" For REST，we provide a Filter to support cors now.\nSOFARPC API Usage For users who use SOFARPC API directly，they can add parameters in ServerConfig.\nMap\u0026amp;lt;String,String\u0026amp;gt; parameters=new HashMap\u0026amp;lt;String, String\u0026amp;gt;() parameters.put(RpcConstants.ALLOWED_ORIGINS,\u0026amp;quot;abc.com,cdf.com\u0026amp;quot;); serverConfig.setParameters(parameters);  XML Usage You can add this configuration to application.properties\ncom.alipay.sofa.rpc.rest.allowed.origins=a.com,b.com  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-cors/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"549f73920842ebb121abf87566761c47","permalink":"/en/projects/sofa-rpc/restful-cors/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/restful-cors/","summary":" For REST，we provide a Filter to support cors now.\nSOFARPC API Usage For users who use SOFARPC API directly，they can add parameters in ServerConfig.\nMap\u0026lt;String,String\u0026gt; parameters=new HashMap\u0026lt;String, String\u0026gt;() parameters.put(RpcConstants.ALLOWED_ORIGINS,\u0026quot;abc.com,cdf.com\u0026quot;); serverConfig.setParameters(parameters);  XML Usage You can add this configuration to application.properties\ncom.alipay.sofa.rpc.rest.allowed.origins=a.com,b.com  ","tags":null,"title":"REST Cors","type":"projects","url":"/en/projects/sofa-rpc/restful-cors/","wordcount":40},{"author":null,"categories":null,"content":"对于 REST，我们设计了一个 JAXRSProviderManager 管理器类。在服务端生效，生效时间为服务启动时。如果希望有一个通用的 异常处理类，用来处理REST的某中异常类型的信息。可以实现一个REST 的处理类。如下示例是一个拦截SofaRpcException 的通用处理器。\n@PreMatching public class CustomExceptionMapper implements ExceptionMapper\u0026amp;lt;SofaRpcException\u0026amp;gt; { @Override public Response toResponse(SofaRpcException exception) { return Response.status(500).entity(exception.getMessage()).build(); } }  并将该处理器注册到JAXRSProviderManager中，时机可以在Main方法中。具体说明可以参考RESTful-Filter。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-exception/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0ff4ef4139b228537d2ce4d52a213651","permalink":"/projects/sofa-rpc/restful-exception/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/restful-exception/","summary":"对于 REST，我们设计了一个 JAXRSProviderManager 管理器类。在服务端生效，生效时间为服务启动时。如果希望有一个通用的 异常处理类，用来处理REST的某中异常类型的","tags":null,"title":"REST Exception","type":"projects","url":"/projects/sofa-rpc/restful-exception/","wordcount":204},{"author":null,"categories":null,"content":"For REST, we designed a JAXRSProviderManager manager class. It takes effect on the server when the service starts.\ncom.alipay.sofa.rpc.server.rest.RestServer#registerProvider  For the user-defined Filter class, you can call it after the initialization is complete.\ncom.alipay.sofa.rpc.config.JAXRSProviderManager#registerCustomProviderInstance  To register filter, since the custom Filter follows REST specification, you need to implement the following interface:\njavax.ws.rs.container.ContainerResponseFilter or javax.ws.rs.container.ContainerRequestFilter  After the REST server is started, if using bare SOFARPC, you need to register filter first before starting the service. In SOFABoot environment, it is similar. The specific encoding method is as follows:\ncom.alipay.sofa.rpc.server.rest.TraceRequestFilter com.alipay.sofa.rpc.server.rest.TraceResponseFilter  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-filter/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"53eb86b2504bf3beda2aca24437d6dab","permalink":"/en/projects/sofa-rpc/restful-filter/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/restful-filter/","summary":"For REST, we designed a JAXRSProviderManager manager class. It takes effect on the server when the service starts.\ncom.alipay.sofa.rpc.server.rest.RestServer#registerProvider  For the user-defined Filter class, you can call it after the initialization is complete.\ncom.alipay.sofa.rpc.config.JAXRSProviderManager#registerCustomProviderInstance  To register filter, since the custom Filter follows REST specification, you need to implement the following interface:\njavax.ws.rs.container.ContainerResponseFilter or javax.ws.rs.container.ContainerRequestFilter  After the REST server is started, if using bare SOFARPC, you need to register filter first before starting the service.","tags":null,"title":"REST filter","type":"projects","url":"/en/projects/sofa-rpc/restful-filter/","wordcount":89},{"author":null,"categories":null,"content":"对于 REST，我们设计了一个 JAXRSProviderManager 管理器类。在服务端生效，生效时间为服务启动时。\ncom.alipay.sofa.rpc.server.rest.RestServer#registerProvider  对于用户自定义的 Filter 类，可以在初始化完成后，调用\ncom.alipay.sofa.rpc.config.JAXRSProviderManager#registerCustomProviderInstance  进行注册，其中自定义的 Filter 遵循 REST 的规范，需要实现如下接口：\njavax.ws.rs.container.ContainerResponseFilter 或者 javax.ws.rs.container.ContainerRequestFilter  REST server 启动之后，对于裸 SOFARPC 的使用，需要先注册，再启动服务。对于 SOFABoot 环境下的使用，也是类似的过程，具体的写法可以参考：\ncom.alipay.sofa.rpc.server.rest.TraceRequestFilter com.alipay.sofa.rpc.server.rest.TraceResponseFilter  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-filter/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"53eb86b2504bf3beda2aca24437d6dab","permalink":"/projects/sofa-rpc/restful-filter/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/restful-filter/","summary":"对于 REST，我们设计了一个 JAXRSProviderManager 管理器类。在服务端生效，生效时间为服务启动时。 com.alipay.sofa.rpc.server.rest.RestServer#registerProvider 对于用户自定义的 Filter 类，可以在初始化完成后，调用 com.alipay.sofa.rpc.config.JAXRSProviderManager#registerCustomProviderInstance 进行注册，其中","tags":null,"title":"REST 自定义 Filter","type":"projects","url":"/projects/sofa-rpc/restful-filter/","wordcount":152},{"author":null,"categories":null,"content":" 对于 REST，我们内置了一个跨域 Filter 的支持。\nSOFARPC API 使用 对于使用 SOFARPC API 的用户，可以在 ServerConfig 中添加一个参数表明即可\nMap\u0026amp;lt;String,String\u0026amp;gt; parameters=new HashMap\u0026amp;lt;String, String\u0026amp;gt;() parameters.put(RpcConstants.ALLOWED_ORIGINS,\u0026amp;quot;abc.com,cdf.com\u0026amp;quot;); serverConfig.setParameters(parameters);  XML 方式使用 直接通过配置\ncom.alipay.sofa.rpc.rest.allowed.origins=a.com,b.com  即可\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-cors/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"549f73920842ebb121abf87566761c47","permalink":"/projects/sofa-rpc/restful-cors/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/restful-cors/","summary":"对于 REST，我们内置了一个跨域 Filter 的支持。 SOFARPC API 使用 对于使用 SOFARPC API 的用户，可以在 ServerConfig 中添加一个参数表明即可 Map\u0026lt;String,String\u0026gt; parameters=new HashMap\u0026lt;String, String\u0026gt;() parameters.put(RpcConstants.ALLOWED_ORIGINS,\u0026quot;abc.com,cdf.com\u0026quot;); serverConfig.setParameters(parameters); XML 方式使用 直接通过配置 com.alipay.sofa.rpc.rest.allowed.origins=a.com,b.com 即可","tags":null,"title":"REST 跨域","type":"projects","url":"/projects/sofa-rpc/restful-cors/","wordcount":70},{"author":null,"categories":null,"content":"SOFARPC supports RESTful protocol, making it convenient for users to publish an interface in the manner of RESTful. * Basic usage * Custom Filter * Integrate Swagger\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f238d7f58de0c4a0e12d566ea9e09f52","permalink":"/en/projects/sofa-rpc/restful/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/restful/","summary":"SOFARPC supports RESTful protocol, making it convenient for users to publish an interface in the manner of RESTful. * Basic usage * Custom Filter * Integrate Swagger","tags":null,"title":"RESTful","type":"projects","url":"/en/projects/sofa-rpc/restful/","wordcount":27},{"author":null,"categories":null,"content":"SOFARPC 提供了 RESTful 协议的支持，可以让用户非常方便地将一个接口通过 RESTful 的方式发布出去。 * 基本使用 * 自定义 Filter * 通用异常处理 * 集成 Swagger\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f238d7f58de0c4a0e12d566ea9e09f52","permalink":"/projects/sofa-rpc/restful/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/restful/","summary":"SOFARPC 提供了 RESTful 协议的支持，可以让用户非常方便地将一个接口通过 RESTful 的方式发布出去。 * 基本使用 * 自定义 Filter * 通用异常处理 * 集成 Swagger","tags":null,"title":"RESTful 协议","type":"projects","url":"/projects/sofa-rpc/restful/","wordcount":58},{"author":null,"categories":null,"content":" 在 SOFARPC 中，使用不同的通信协议即使用不同的 Binding 即可，如果需要使用 RESTful 协议，只要将 Binding 设置为 REST 即可。\n发布服务 在定义 RESTful 的服务接口的时候，需要采用 JAXRS 标准的注解在接口上加上元信息，比如下面的接口：\n@Path(\u0026amp;quot;sample\u0026amp;quot;) public interface SampleService { @GET @Path(\u0026amp;quot;hello\u0026amp;quot;) String hello(); }   JAXRS 的标准的注解的使用方式可以参考 RESTEasy 的文档。\n 在定义好了接口之后，将接口的实现发布成一个服务，比如，通过 Annotation 的方式：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)}) public class RestfulSampleServiceImpl implements SampleService { @Override public String hello() { return \u0026amp;quot;Hello\u0026amp;quot;; } }  如果要通过其他的方式发布服务，请参考 Bolt 协议基本使用。\n通过浏览器访问服务 在发布服务之后，用户可以通过浏览器来直接访问服务，对于上面的服务，访问的地址如下：\nhttp://localhost:8341/sample/hello  SOFARPC 的 RESTful 服务的默认端口为 8341。\n引用服务 除了通过浏览器访问 SOFARPC 发布的 RESTful 服务之外，用户也可以通过 SOFARPC 标准的服务引用的方式来引用服务，比如通过 Annotation 的方式：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)) private SampleService sampleService;  如果要使用其他的方式引用服务，请参考 Bolt 协议基本使用。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-basic/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d41f976864ba8f8221f5b5d26f354d1c","permalink":"/projects/sofa-rpc/restful-basic/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/restful-basic/","summary":"在 SOFARPC 中，使用不同的通信协议即使用不同的 Binding 即可，如果需要使用 RESTful 协议，只要将 Binding 设置为 REST 即可。 发布服务 在定义 RESTful 的服务接口的时候，需要采用 JAXRS 标准的注","tags":null,"title":"RESTful 协议基本使用","type":"projects","url":"/projects/sofa-rpc/restful-basic/","wordcount":358},{"author":null,"categories":null,"content":"In SOFABoot, the RPC framework provides some configuration parameters at the application level, and supports application-level parameter configuration, such as port and thread pool, which are bound by Spring Boot\u0026amp;rsquo;s @ConfigurationProperties. The binding attribute class is com.alipay.sofa.rpc.boot.config.SofaBootRpcProperties, and the configuration prefix is as follows:\nstatic final String PREFIX = \u0026amp;quot;com.alipay.sofa.rpc\u0026amp;quot;;  Then in the application.properties file, you can currently configure the following options. Also, you can write the codes based on your own coding habits as well as according to the Spring Boot specification, camel, underline and so on.\n#Standalone fault tolerance com.alipay.sofa.rpc.aft.regulation.effective # Whether to enable standalone fault tolerance com.alipay.sofa.rpc.aft.degrade.effective # Whether to enable degradation com.alipay.sofa.rpc.aft.time.window # Time window com.alipay.sofa.rpc.aft.least.window.count # Minimum number of calls com.alipay.sofa.rpc.aft.least.window.exception.rate.multiple # minimum exception rate com.alipay.sofa.rpc.aft.weight.degrade.rate # Degradation rate com.alipay.sofa.rpc.aft.weight.recover.rate # Recovery rate com.alipay.sofa.rpc.aft.degrade.least.weight #Minimum degrading weight com.alipay.sofa.rpc.aft.degrade.max.ip.count # Maximum number of degraded IPs # bolt com.alipay.sofa.rpc.bolt.port # bolt port com.alipay.sofa.rpc.bolt.thread.pool.core.size # Number of bolt core threads com.alipay.sofa.rpc.bolt.thread.pool.max.size # Maximum number of bolt threads com.alipay.sofa.rpc.bolt.thread.pool.queue.size # bolt thread pool queue com.alipay.sofa.rpc.bolt.accepts.size # Number of connections that server allows client to establish # rest com.alipay.sofa.rpc.rest.hostname # rest hostname com.alipay.sofa.rpc.rest.port # rest port com.alipay.sofa.rpc.rest.io.thread.size # Number of rest io threads com.alipay.sofa.rpc.rest.context.path # rest context path com.alipay.sofa.rpc.rest.thread.pool.core.size # Number of rest core threads com.alipay.sofa.rpc.rest.thread.pool.max.size # Maximum number of rest threads com.alipay.sofa.rpc.rest.max.request.size # Maximum rest request size com.alipay.sofa.rpc.rest.telnet # Whether to allow rest telnet com.alipay.sofa.rpc.rest.daemon # Whether to hold the port. If true, exit with the main thread exit # dubbo com.alipay.sofa.rpc.dubbo.port # dubbo port com.alipay.sofa.rpc.dubbo.io.thread.size # dubbo io thread size com.alipay.sofa.rpc.dubbo.thread.pool.max.size # Maximum number of dubbo business threads com.alipay.sofa.rpc.dubbo.accepts.size # Number of connections that server allows client to establish com.alipay.sofa.rpc.dubbo.thread.pool.core.size #Number of dubbo core Threads com.alipay.sofa.rpc.dubbo.thread.pool.queue.size #Maximum number of dubbo threads # registry com.alipay.sofa.rpc.registry.address # Registry center address com.alipay.sofa.rpc.virtual.host # virtual host com.alipay.sofa.rpc.bound.host # bind host …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/application-rpc-config/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"bd19b2ced39a8deb802c13e525093fac","permalink":"/en/projects/sofa-rpc/application-rpc-config/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/application-rpc-config/","summary":"In SOFABoot, the RPC framework provides some configuration parameters at the application level, and supports application-level parameter configuration, such as port and thread pool, which are bound by Spring Boot\u0026rsquo;s @ConfigurationProperties. The binding attribute class is com.alipay.sofa.rpc.boot.config.SofaBootRpcProperties, and the configuration prefix is as follows:\nstatic final String PREFIX = \u0026quot;com.alipay.sofa.rpc\u0026quot;;  Then in the application.properties file, you can currently configure the following options. Also, you can write the codes based on your own coding habits as well as according to the Spring Boot specification, camel, underline and so on.","tags":null,"title":"RPC application parameter configuration","type":"projects","url":"/en/projects/sofa-rpc/application-rpc-config/","wordcount":381},{"author":null,"categories":null,"content":" ProviderConfig    Attribute Name Default value Comment     id ID Generated automatically    application Application object Empty ApplicationConfig    interfaceId Service interface (unique identifier)  Use the actual interface class for both normal calls and return calls.   uniqueId Service tag (unique identifier)     filterRef Filter configuration example  List   filter Filter configuration alias  separated by commas   registry Registry center on the server  List   methods Method-level configuration  Map\u0026amp;lt;String, MethodConfig\u0026amp;gt;   serialization Serialization protocol hessian2    register Whether to register true It depends on the implementation and may not take effect.   subscribe Whether to subscribe true It depends on the implementation and may not take effect.   proxy Proxy type javassist As well as JDK dynamic proxy   ref Service interface implementation class     server server  List, and it can be sent to multiple servers at once   delay Time for delaying service publishing  Service delay   weight Service static weight     include Included methods     exclude Methods not included     dynamic Whether to dynamically register     priority Service priority     bootstrap Service publishing starter bolt    executor Custom thread pool     timeout Execution timeout period for server     concurrents Concurrent execution request  Maximum number of parallel executable requests per method under interface. -1 indicates turning off the concurrent filter, and 0 means that filtering is enabled but not limited   cacheRef Result cache implementation class     mockRef Mock implementation class     mock Whether to enable Mock     validation Whether to enable parameter verification (jsr303)     compress Whether to start compression false    cache Whether to enable result caching false    parameters Extra attributes  Map\u0026amp;lt;String, String\u0026amp;gt;    ConsumerConfig    Attribute Name Default value Comment     id ID Generated automatically    application Application object Empty ApplicationConfig    interfaceId Service interface (unique identifier)  Use the actual interface class for both normal calls and return calls.   uniqueId Service tag (Unique identifier)     filterRef Filter configuration example  List   filter Filter configuration alias  List   registry Registry center on the server  List   methods Method-level configuration  Map\u0026amp;lt;String, MethodConfig\u0026amp;gt;   serialization Serialization protocol hessian2    register Whether to register true It depends on the implementation and may not take effect.   subscribe Whether to subscribe true It depends on the implementation and may not take effect.   proxy proxy type javassist As well as JDK dynamic proxy   protocol Call protocol bolt Currently supports bolt, rest, dubbo   directUrl Direct address  Directly connected to register   generic Whether to generalize calls false    connectTimeout Timeout period for connection establishment 3000(cover 5000)    disconnectTimeout Timeout period for disconnection 5000(cover …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration-common/","fuzzywordcount":1100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2eb5963f4785f5f828f0e15759272971","permalink":"/en/projects/sofa-rpc/configuration-common/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/sofa-rpc/configuration-common/","summary":"ProviderConfig    Attribute Name Default value Comment     id ID Generated automatically    application Application object Empty ApplicationConfig    interfaceId Service interface (unique identifier)  Use the actual interface class for both normal calls and return calls.   uniqueId Service tag (unique identifier)     filterRef Filter configuration example  List   filter Filter configuration alias  separated by commas   registry Registry center on the server  List   methods Method-level configuration  Map\u0026lt;String, MethodConfig\u0026gt;   serialization Serialization protocol hessian2    register Whether to register true It depends on the implementation and may not take effect.","tags":null,"title":"RPC publishing and reference configuration","type":"projects","url":"/en/projects/sofa-rpc/configuration-common/","wordcount":1084},{"author":null,"categories":null,"content":" ProviderConfig    属性 名称 默认值 备注     id ID 自动生成    application 应用对象 空ApplicationConfig    interfaceId 服务接口（唯一标识元素）  不管是普通调用和返回调用，这里都设置实际的接口类   uniqueId 服务标签（唯一标识元素）     filterRef 过滤器配置实例  List   filter 过滤器配置别名  多个用逗号隔开   registry 服务端注册中心  List   methods 方法级配置  Map\u0026amp;lt;String, MethodConfig\u0026amp;gt;   serialization 序列化协议 hessian2    register 是否注册 true 取决于实现，可能不生效。   subscribe 是否订阅 true 取决于实现，可能不生效。   proxy 代理类型 javassist 还有JDK动态代理   ref 服务接口实现类     server 服务端  List，可以一次发到多个服务端   delay 服务延迟发布时间  服务延迟   weight 服务静态权重     include 包含的方法     exclude 不包含的方法     dynamic 是否动态注册     priority 服务优先级     bootstrap 服务发布启动器 bolt    executor 自定义线程池     timeout 服务端执行超时时间     concurrents 并发执行请求  接口下每方法的最大可并行执行请求数，配置-1关闭并发过滤器，等于0表示开启过滤但是不限制   cacheRef 结果缓存实现类     mockRef Mock实现类     mock 是否开启Mock     validation 是否开启参数验证(jsr303)     compress 是否启动压缩 false    cache 是否启用结果缓存 false    parameters 额外属性  Map\u0026amp;lt;String, String\u0026amp;gt;    ConsumerConfig    属性 名称 默认值 备注     id ID 自动生成    application 应用对象 空ApplicationConfig    interfaceId 服务接口（唯一标识元素）  不管是普通调用和返回调用，这里都设置实际的接口类   uniqueId 服务标签（唯一标识元素）     filterRef 过滤器配置实例  List   filter 过滤器配置别名  List   registry 服务端注册中心  List   methods 方法级配置  Map\u0026amp;lt;String, MethodConfig\u0026amp;gt;   serialization 序列化协议 hessian2    register 是否注册 true 取决于实现，可能不生效。   subscribe 是否订阅 true 取决于实现，可能不生效。   proxy 代理类型 javassist 还有JDK动态代理   protocol 调用的协议 bolt 目前支持bolt，rest，dubbo   directUrl 直连地址  直连后register   generic 是否泛化调用 false    connectTimeout 建立连接超时时间 3000(cover 5000)    disconnectTimeout 断开连接等等超时时间 5000(cover 10000)    cluster 集群模式 failover    connectionHolder 连接管理器实现 all    loadBalancer 负载均衡算法 random    lazy 是否延迟建立长连接 false    sticky 是否使用粘性连接 false 跳过负载均衡算法使用上一个地址   inJVM 是否转为JVM调用 true JVM发现服务提供者，转为走本地   check 是否检查强依赖 false 无可用服务端启动失败   heartbeat 心跳间隔 30000 客户端给服务端发心跳间隔。取决于实现，可能不生效。   reconnect 重连间隔 10000 客户端重建端口长连接的间隔。取决于实现，可能不生效。   router 路由器配置别名  List   routerRef 路由器配置实例  List   bootstrap 服务引用启动器 bolt    addressWait 等待地址获取时间 -1 取决于实现，可能不生效。   timeout 调用超时时间 3000(cover 5000)    retries 失败后重试次数 0 跟集群模式有关，failover读取此参数。   invokeType 调用类型 sync    onReturn 并发执行请求数  接口下每方法的最大可并行执行请求数，\n配置-1关闭并发过滤器，等于0表示开启过滤但是不限制   cacheRef 结果缓存实现类     mockRef Mock实现类     cache 是否启用结果缓存 false    mock 是否开启Mock     validation 是否开启参数验证  基于JSR303   compress 是否启动压缩 false    parameters 额外属性  Map\u0026amp;lt;String, String\u0026amp;gt;    MethodConfig    属性 名称 默认值 备注     name 方法名     timeout 调用超时时间 null    retries 失败后重试次数 null    invokeType 调用类型 null    validation 是否开启参数验证 null 基于JSR303   onReturn 返回时调用的SofaResponseCallback null 用于实现Callback等   concurrent 并发执行请求数 null 接口下每方法的最大可并行执行请求数，配置-1关闭并发过滤器，等于0表示开启过滤但是不限制。   validation 是否开启参数验证 null    compress 是否启动压缩 null    parameters 额外属性  Map\u0026amp;lt;String, String\u0026amp;gt;    ServerConfig    id Id 默认值 备注     protocol 协议 bolt 目前支持bolt，rest，dubbo   host 主机 0.0.0.0    port 端口 12200 默认端口 bolt:12200, rest:8341, h2c:12300, dubbo:20880   contextPath 上下文路径 /    ioThreads IO线程池数 0 取决于实现，可能不生效。例如bolt默认cpu*2。0表示自动计算。   threadPoolType 业务线程池类型 cached    coreThreads 业务线程池核心大小 80(override …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration-common/","fuzzywordcount":1800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2eb5963f4785f5f828f0e15759272971","permalink":"/projects/sofa-rpc/configuration-common/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-rpc/configuration-common/","summary":"ProviderConfig 属性 名称 默认值 备注 id ID 自动生成 application 应用对象 空ApplicationConfig interfaceId 服务接口（唯一标识元素） 不管是普通调用和返回调用，这里都设置","tags":null,"title":"RPC 发布订阅配置","type":"projects","url":"/projects/sofa-rpc/configuration-common/","wordcount":1782},{"author":null,"categories":null,"content":"在 SOFABoot 的使用场景下，RPC 框架在应用层面，提供一些配置参数，支持的应用级别的参数配置，如端口，线程池等信息，都是通过 Spring Boot的@ConfigurationProperties 进行的绑定。绑定属性类是com.alipay.sofa.rpc.boot.config.SofaBootRpcProperties，配置前缀是\nstatic final String PREFIX = \u0026amp;quot;com.alipay.sofa.rpc\u0026amp;quot;;  那么在 application.properties 文件中，目前可以配置以下几个选项。其中使用者也可以根据自己的编码习惯，按照 Spring Boot的规范，按照驼峰，中划线等进行书写。\n# 单机故障剔除 com.alipay.sofa.rpc.aft.regulation.effective # 是否开启单机故障剔除功能 com.alipay.sofa.rpc.aft.degrade.effective # 是否开启降级 com.alipay.sofa.rpc.aft.time.window # 时间窗口 com.alipay.sofa.rpc.aft.least.window.count # 最小调用次数 com.alipay.sofa.rpc.aft.least.window.exception.rate.multiple # 最小异常率 com.alipay.sofa.rpc.aft.weight.degrade.rate # 降级速率 com.alipay.sofa.rpc.aft.weight.recover.rate # 恢复速率 com.alipay.sofa.rpc.aft.degrade.least.weight #降级最小权重 com.alipay.sofa.rpc.aft.degrade.max.ip.count # 最大降级 ip # bolt com.alipay.sofa.rpc.bolt.port # bolt 端口 com.alipay.sofa.rpc.bolt.thread.pool.core.size # bolt 核心线程数 com.alipay.sofa.rpc.bolt.thread.pool.max.size # bolt 最大线程数 com.alipay.sofa.rpc.bolt.thread.pool.queue.size # bolt 线程池队列 com.alipay.sofa.rpc.bolt.accepts.size # 服务端允许客户端建立的连接数 # rest com.alipay.sofa.rpc.rest.hostname # rest hostname com.alipay.sofa.rpc.rest.port # rest port com.alipay.sofa.rpc.rest.io.thread.size # rest io 线程数 com.alipay.sofa.rpc.rest.context.path # rest context path com.alipay.sofa.rpc.rest.thread.pool.core.size # rest 核心线程数 com.alipay.sofa.rpc.rest.thread.pool.max.size # rest 最大线程数 com.alipay.sofa.rpc.rest.max.request.size # rest 最大请求大小 com.alipay.sofa.rpc.rest.telnet # 是否允许 rest telnet com.alipay.sofa.rpc.rest.daemon # 是否hold住端口，true的话随主线程退出而退出 # dubbo com.alipay.sofa.rpc.dubbo.port # dubbo port com.alipay.sofa.rpc.dubbo.io.thread.size # dubbo io 线程大小 com.alipay.sofa.rpc.dubbo.thread.pool.max.size # dubbo 业务线程最大数 com.alipay.sofa.rpc.dubbo.accepts.size # dubbo 服务端允许客户端建立的连接数 com.alipay.sofa.rpc.dubbo.thread.pool.core.size #dubbo 核心线程数 com.alipay.sofa.rpc.dubbo.thread.pool.queue.size #dubbo 最大线程数 # registry com.alipay.sofa.rpc.registry.address # 注册中心地址 com.alipay.sofa.rpc.virtual.host # virtual host com.alipay.sofa.rpc.bound.host # 绑定 host com.alipay.sofa.rpc.virtual.port # virtual端口 com.alipay.sofa.rpc.enabled.ip.range # 多网卡 ip 范围 com.alipay.sofa.rpc.bind.network.interface # 绑定网卡 # h2c com.alipay.sofa.rpc.h2c.port # h2c 端口 com.alipay.sofa.rpc.h2c.thread.pool.core.size # h2c 核心线程数 com.alipay.sofa.rpc.h2c.thread.pool.max.size # h2c 最大线程数 com.alipay.sofa.rpc.h2c.thread.pool.queue.size # h2c 队列大小 com.alipay.sofa.rpc.h2c.accepts.size # 服务端允许客户端建立的连接数 # 扩展 com.alipay.sofa.rpc.lookout.collect.disable # 是否关闭 lookout # 代理 com.alipay.sofa.rpc.consumer.repeated.reference.limit # 允许客户端对同一个服务生成的引用代理数量，默认为3;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/application-rpc-config/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"bd19b2ced39a8deb802c13e525093fac","permalink":"/projects/sofa-rpc/application-rpc-config/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/application-rpc-config/","summary":"在 SOFABoot 的使用场景下，RPC 框架在应用层面，提供一些配置参数，支持的应用级别的参数配置，如端口，线程池等信息，都是通过 Spring Boot的@Config","tags":null,"title":"RPC 应用参数配置","type":"projects","url":"/projects/sofa-rpc/application-rpc-config/","wordcount":620},{"author":null,"categories":null,"content":" Raft 新特性      Strong Leader  更强的领导形式 例如日志条目只会从领导者发送到其他服务器, 这很大程度上简化了对日志复制的管理    Leader Election  使用随机定时器来选举领导者 用最简单的方式减少了选举冲突的可能性    Membership Change  新的联合一致性 (joint consensus) 方法      复制状态机 1. 复制状态机通过日志实现  每台机器一份日志 每个日志条目包含一条命令 状态机按顺序执行命令  2.应用于实际系统的一致性算法一般有以下特性  确保安全性 高可用性 不依赖时序保证一致性 一条命令能够尽可能快的在大多数节点对一轮RPC调用响应时完成  Paxos 算法的不足  算法复杂度高, 较难理解 工程复杂度高, 难以在实际环境中实现  Raft 设计原则  概念分解  Leader election Log replication Membership changes  通过减少状态数量将状态空间简化  日志不允许出现空洞, 并且 raft 限制了日志不一致的可能性 使用随机化时钟简化了领导选举的算法   Raft 一致性算法 State (状态) 在所有服务器上持久存储的(响应RPC之前稳定存储的)\n   currentTerm 服务器最后知道的任期号(从0开始递增)     votedFor 在当前任期内收到选票的候选人Id(如果没有就为null)   log[] 日志条目, 每个条目包含状态机要执行的命令以及从Leader收到日志时的任期号    在所有服务器上不稳定存在的\n   commitIndex 已知被提交的最大日志条目索引     lastApplied 已被状态机执行的最大日志条目索引    在Leader服务器上不稳定存在的\n   nextIndex[] 对于每一个follower, 记录需要发给他的下一条日志条目的索引     matchIndex[] 对于每一个follower, 记录已经复制完成的最大日志条目索引    AppendEntries RPC (日志复制) 由leader通过RPC向follower复制日志, 也会用作heartbeat\n入参\n   term Leader任期号     leaderId Leader id, 为了能帮助客户端重定向到Leader服务器   prevLogIndex 前一个日志的索引   prevLogTerm 前一个日志所属的任期   entries[] 将要存储的日志条目列表(为空时代表heartbeat, 有时候为了效率会发送超过一条)   leaderCommit Leader已提交的日志条目索引    返回值\n   term 当前的任期号, 用于leader更新自己的任期号     success 如果其他follower包含能够匹配上prevLogIndex和prevLogTerm的日志, 那么为真    接收日志的follower需要实现的\n 如果term \u0026amp;lt; currentTerm, 不接受日志并返回false 如果索引prevLogIndex处的日志的任期号与prevLogTerm不匹配, 不接受日志并返回false 如果一条已存在的日志与新的冲突(index相同但是term不同), 则删除已经存在的日志条目和他之后所有的日志条目 添加任何在已有日志中不存在的条目 如果leaderCommit \u0026amp;gt; commitIndex, 则设置commitIndex = min(leaderCommit, index of last new entry)  RequestVote RPC (投票请求) 入参\n   term 候选人的任期号     candidateId 发起投票请求的候选人id   lastLogIndex 候选人最新的日志条目索引   lastLogTerm 候选人最新日志条目对应的任期号    返回值\n   term 目前的任期号, 用于候选人更新自己     voteGranted 如果候选人收到选票, 那么为true    接收日志的follower需要实现的\n 如果term \u0026amp;lt; currentTerm, 那么拒绝投票并返回false 如果votedFor为空或者与candidateId相同, 并且候选人的日志和自己一样新或者更新, 那么就给候选人投票并返回true  服务器要遵守的规则  所有服务器:  如果commitIndex \u0026amp;gt; lastApplied, 那么将lastApplied自增并把对应日志log[lastApplied]应用到状态机 如果RPC请求或响应包含一个term T大于currentTerm, 那么将currentTerm赋值为T并立即切换状态为follower  Follower:  无条件响应来自candidate和leader的RPC 如果在选举超时之前没收到任何来自leader的AppendEntries RPC或RequestVote RPC, 那么自己转换状态为candidate  Candidate:  转变为candidate之后开始发起选举  currentTerm自增 \u0026amp;ndash;\u0026amp;gt; 重置选举计时器 \u0026amp;ndash;\u0026amp;gt; 给自己投票 \u0026amp;ndash;\u0026amp;gt; 向其他服务器发起RequestVote RPC  如果收到了来自大多数服务器的投票, 转换状态成为leader 如果收到了来自新leader的AppendEntries RPC(Heartbeat), 转换状态为follower 如果选举超时, 开始新一轮的选举  Leader:  一旦成为leader, 想其他所有服务器发送空的AppendEntries RPC(Heartbeat), 并在空闲时间重复发送以防选举超时 如果收到来自客户端的请求, 向本地日志追加条目并向所有服务器发送AppendEntries RPC, 在收到大多数响应后将该条目应用到状态机并回复响应给客户端 如果leader上一次收到的日志索引大于一个follower的nextIndex, 那么通过AppendEntries RPC将nextIndex之后的所有日志发送出去; 如果发送成功, 将follower的nextIndex和matchIndex更新, 如果由于日志不一致导致失败, 那么将nextIndex递减并重新发送 如果存在一个N \u0026amp;gt; commitIndex和半数以上的matchIndex[i] \u0026amp;gt;= N并且log[N].term == currentTerm, 将commitIndex赋值为N   一致性算法总结    Election Safety 选举安全原则: 一个任期内最多允许有一个leader     Leader Append-Only 只增加日志原则: Leader只会增加日志条目, 永远不会覆盖或删除自己的日志   Log Matching 日志匹配原则: 如果两个日志在相同的的索引位置上并且任期号相同, 那么就可以认为这个日志从头到这个索引位置之间的条目完 …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/raft-introduction/","fuzzywordcount":5200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b811e803d23b40da67657798801f8b51","permalink":"/projects/sofa-jraft/raft-introduction/","publishdate":"0001-01-01T00:00:00Z","readingtime":11,"relpermalink":"/projects/sofa-jraft/raft-introduction/","summary":"Raft 新特性 Strong Leader 更强的领导形式 例如日志条目只会从领导者发送到其他服务器, 这很大程度上简化了对日志复制的管理 Leader Election 使用随机定时器来选举领导者 用最简单","tags":null,"title":"Raft 算法解读","type":"projects","url":"/projects/sofa-jraft/raft-introduction/","wordcount":5182},{"author":null,"categories":null,"content":" Raincat 强一致性分布式事务，是基于二阶段提交+本地事务补偿机制来实现。原理介绍\n基于java语言来开发（JDK1.8），支持dubbo,motan,springcloud进行分布式事务。\n因为文件名太长，大家在拉取代码的时候执git命令：git config \u0026amp;ndash;global core.longpaths true # Features\n 框架特性\n 无缝集成spring 或 spring boot。\n 支持dubbo,motan,springcloud,等rpc框架进行分布式事务。\n 事务发起者，参与者与协调者底层基于netty长连接通信,稳定高效。\n 协调者采用eureka做注册中心，支持集群模式。\n 采用Aspect AOP 切面思想与Spring无缝集成。\n 配置简单，集成简单，源码简洁，稳定性高，已在生产环境使用。\n 内置经典的分布式事务场景demo工程，并有swagger-ui可视化界面可以快速体验。\n  事务角色\n 事务发起者（可理解为消费者 如：dubbo的消费者,springcloud的调用方）,发起分布式事务\n 事务参与者（可理解为提供者 如：dubbo的提供者,springcloud的rest服务提供者),参与事务发起者的事务\n 事务协调者（tx-manager），协调分布式事务的提交，回滚等。\n  技术方案\n 协调者（tx-manager）采用eureka作为注册中心，集群配置，达到服务的高可用，采用redis集群来分布式存储事务数据, springboot 提供rest服务，采用netty与参与者，发起者进行长连接通信。\n 发起者与协调者，采用Aspect AOP 切面思想，SPI，多线程，异步回调，线程池，netty通信等技术。\n  SPI扩展\n 本地事务恢复，支持redis，mogondb，zookeeper，file，mysql等关系型数据库 本地事务序列化保存，支持java，hessian，kryo，protostuff netty通信序列化方式，支持 hessian，kryo，protostuff   Prerequisite  JDK 1.8+ Maven 3.2.x Git RPC framework dubbo or motan or springcloud。  架构设计  架构设计图 ：  流程图 ：\n  视频源码分析 ### 环境搭建\n### 启动过程\n### 事务提交\n### 回滚恢复\n### 管理后台\nSupport  如有任何问题欢迎加入QQ群进行讨论  微信公众号   # Contribution\n","date":-62135596800,"description":"","dir":"projects/raincat/overview/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8581aebda8993d467b1930e09efa7aea","permalink":"/en/projects/raincat/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/raincat/overview/","summary":"Raincat 强一致性分布式事务，是基于二阶段提交+本地事务补偿机制来实现。原理介绍 基于java语言来开发（JDK1.8），支持dubbo,motan,","tags":null,"title":"Raincat 介绍","type":"projects","url":"/en/projects/raincat/overview/","wordcount":860},{"author":null,"categories":null,"content":" Raincat 强一致性分布式事务，是基于二阶段提交+本地事务补偿机制来实现。原理介绍\n基于java语言来开发（JDK1.8），支持dubbo,motan,springcloud进行分布式事务。\n因为文件名太长，大家在拉取代码的时候执git命令：git config \u0026amp;ndash;global core.longpaths true # Features\n 框架特性\n 无缝集成spring 或 spring boot。\n 支持dubbo,motan,springcloud,等rpc框架进行分布式事务。\n 事务发起者，参与者与协调者底层基于netty长连接通信,稳定高效。\n 协调者采用eureka做注册中心，支持集群模式。\n 采用Aspect AOP 切面思想与Spring无缝集成。\n 配置简单，集成简单，源码简洁，稳定性高，已在生产环境使用。\n 内置经典的分布式事务场景demo工程，并有swagger-ui可视化界面可以快速体验。\n  事务角色\n 事务发起者（可理解为消费者 如：dubbo的消费者,springcloud的调用方）,发起分布式事务\n 事务参与者（可理解为提供者 如：dubbo的提供者,springcloud的rest服务提供者),参与事务发起者的事务\n 事务协调者（tx-manager），协调分布式事务的提交，回滚等。\n  技术方案\n 协调者（tx-manager）采用eureka作为注册中心，集群配置，达到服务的高可用，采用redis集群来分布式存储事务数据, springboot 提供rest服务，采用netty与参与者，发起者进行长连接通信。\n 发起者与协调者，采用Aspect AOP 切面思想，SPI，多线程，异步回调，线程池，netty通信等技术。\n  SPI扩展\n 本地事务恢复，支持redis，mogondb，zookeeper，file，mysql等关系型数据库 本地事务序列化保存，支持java，hessian，kryo，protostuff netty通信序列化方式，支持 hessian，kryo，protostuff   Prerequisite  JDK 1.8+ Maven 3.2.x Git RPC framework dubbo or motan or springcloud。  架构设计  架构设计图 ：  流程图 ：\n  视频源码分析 ### 环境搭建\n### 启动过程\n### 事务提交\n### 回滚恢复\n### 管理后台\nSupport  如有任何问题欢迎加入QQ群进行讨论  微信公众号   # Contribution\n","date":-62135596800,"description":"","dir":"projects/raincat/overview/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8581aebda8993d467b1930e09efa7aea","permalink":"/projects/raincat/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/raincat/overview/","summary":"Raincat 强一致性分布式事务，是基于二阶段提交+本地事务补偿机制来实现。原理介绍 基于java语言来开发（JDK1.8），支持dubbo,motan,","tags":null,"title":"Raincat 介绍","type":"projects","url":"/projects/raincat/overview/","wordcount":860},{"author":null,"categories":null,"content":"TBD\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-register-agent/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"da6c96fadd94eedcf961d50ce7b00600","permalink":"/projects/sofa-mesh/pilot-register-agent/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-register-agent/","summary":"TBD","tags":null,"title":"Register Agent","type":"projects","url":"/projects/sofa-mesh/pilot-register-agent/","wordcount":1},{"author":null,"categories":null,"content":" Register agent TBD\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-register-agent/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"da6c96fadd94eedcf961d50ce7b00600","permalink":"/en/projects/sofa-mesh/pilot-register-agent/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-mesh/pilot-register-agent/","summary":"Register agent TBD","tags":null,"title":"Register agent","type":"projects","url":"/en/projects/sofa-mesh/pilot-register-agent/","wordcount":3},{"author":null,"categories":null,"content":" Related articles  ISSUES User manual Chinese introductory article: Ant communication framework practices  ","date":-62135596800,"description":"","dir":"projects/sofa-bolt/related-links/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6844d2a639b69fa3128132b8631f33e3","permalink":"/en/projects/sofa-bolt/related-links/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-bolt/related-links/","summary":" Related articles  ISSUES User manual Chinese introductory article: Ant communication framework practices  ","tags":null,"title":"Related articles","type":"projects","url":"/en/projects/sofa-bolt/related-links/","wordcount":12},{"author":null,"categories":null,"content":"To learn more, see https://github.com/mos/mosn/blob/master/CHANGELOG.md.\n","date":-62135596800,"description":"","dir":"projects/mosn/release-notes/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"62efb8e40401ab4612bcccaa6e942c97","permalink":"/en/projects/mosn/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/mosn/release-notes/","summary":"To learn more, see https://github.com/mos/mosn/blob/master/CHANGELOG.md.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/mosn/release-notes/","wordcount":5},{"author":null,"categories":null,"content":"To learn more, see https://github.com/mos/mosn/blob/master/CHANGELOG.md.\n","date":-62135596800,"description":"","dir":"projects/occlum/release-notes/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6b9dec1dd8c196e43129ab36a046a84f","permalink":"/en/projects/occlum/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/occlum/release-notes/","summary":"To learn more, see https://github.com/mos/mosn/blob/master/CHANGELOG.md.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/occlum/release-notes/","wordcount":5},{"author":null,"categories":null,"content":" Release history For more information, refer to: https://github.com/sofastack/sofa-ark/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-release/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"994c3569ea416ee5b0dea253f08af6be","permalink":"/en/projects/sofa-boot/sofa-ark-release/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/sofa-ark-release/","summary":"Release history For more information, refer to: https://github.com/sofastack/sofa-ark/releases","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-release/","wordcount":8},{"author":null,"categories":null,"content":"﻿## Release history For more information, refer to: https://github.com/sofastack/sofa-jarslink/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-release/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4554e362f42cbc42b9408d9507cdf689","permalink":"/en/projects/sofa-boot/sofa-jarslink-release/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-release/","summary":"﻿## Release history For more information, refer to: https://github.com/sofastack/sofa-jarslink/releases","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-release/","wordcount":9},{"author":null,"categories":null,"content":"For more information, see https://github.com/sofastack/sofa-dashboard/releases.\n","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/release-node/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3c8e6985123810c9692f47cc56b50081","permalink":"/en/projects/sofa-dashboard/release-node/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/release-node/","summary":"For more information, see https://github.com/sofastack/sofa-dashboard/releases.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-dashboard/release-node/","wordcount":5},{"author":null,"categories":null,"content":" 1.2.5 April 1, 2019\n Bugs fixed  Fixed the conflict between jmh and the unit test code. Fixed the installation failure bug that would occur when the snapshot is too large. This bug may affect the addition of new nodes.  Features  Optimized part of the LogManagerImpl code to reduce CPU usage. Corrected some spelling errors.  Breaking changes  None   We strongly recommend that you upgrade to this version.\n1.2.4 March 20, 2019\n Bugs fixed  Fixed stale read of lease read in a circumstance. Modified part of timestamps to monotonic time. Fixed the problem of the replicator being blocked in one circumstance. Resolved directory creation failures for some unit tests on Windows. Resolved process crashes caused by improper rocksdb options settings on Windows.  Features  Made the RocksDB options available for users to set. Optimized the pre-vote process, and used the lease mechanism to avoid the current term\u0026amp;rsquo;s interruption on a disconnected node (caused by network partitioning or no writes in the cluster for a long time) to improve the system availability. Updated SOFABolt to 1.5.3. Modified ReadWriteLock of the BallotBox to StampedLock, and provided the OptimisticRead implementation. Fixed a few spelling errors.  Breaking changes  None  Acknowledgements (in no particular order)  @pifuant @huangyunbin @shiftyman @slievrly   1.2.3 March 5, 2019 Released the first open source version.\n1.2.2 February 21, 2019\n Bugs fixed  Made PeerId and Endpoint immutable, to avoid concurrency problems on APIs such as getLeaderId. Upgraded sofa-common to 1.0.12. The earlier version 1.0.9 was not released to the public GitHub repository.  Features  The JRaft-RheaKV implemented auto range split. When placementDriver(pd) is enabled, the pd can calculate and issue the range split command based on state information reported by each node. When pd is disabled, RheaKVCliService is provided to allow users to manually trigger range split by using the CLI service. Provided LogExceptionHandler generic support. Added MetricThreadPoolExecutor (an updated version of LogThreadPoolExecutor) to print the uncaught exception log and record the time for task.run() and replaced all ThreadPoolExecutors in JRaft with MetricThreadPoolExecutor to record time-consumption metric statistics. This metric can be used as an important reference for adjusting the thread pool configuration in actual application.  Breaking changes  Removed the reset method of Endpoint/PeerId.   V1.2.1 January 28, 2019\n Bugs fixed  Fixed a bug that RaftGroupService may mistakenly disable the shared rpcServer. Fixed the bug of the apply-order change caused by batch write of the RheaKV state machine. Fixed the time usage API error.  Features  Merged the code of duplicate functions of Jraft and RheaKV. Reduced memory usage of the log replication request handling process on followers. Optimized the synchronized conf read/write of the RouteTable to the read/write lock. Implemented lock safe with fencing and the automatic lease …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/release-log/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9e24fb74a3cda6a600252b01f8a85db9","permalink":"/en/projects/sofa-jraft/release-log/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-jraft/release-log/","summary":"1.2.5 April 1, 2019\n Bugs fixed  Fixed the conflict between jmh and the unit test code. Fixed the installation failure bug that would occur when the snapshot is too large. This bug may affect the addition of new nodes.  Features  Optimized part of the LogManagerImpl code to reduce CPU usage. Corrected some spelling errors.  Breaking changes  None   We strongly recommend that you upgrade to this version.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-jraft/release-log/","wordcount":855},{"author":null,"categories":null,"content":"For more information, see https://github.com/sofastack/sofa-registry/releases.\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/release-notes/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d92dddf77bbbd6078f3f96ba2224a53d","permalink":"/en/projects/sofa-registry/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-registry/release-notes/","summary":"For more information, see https://github.com/sofastack/sofa-registry/releases.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-registry/release-notes/","wordcount":5},{"author":null,"categories":null,"content":"To learn more, see https://github.com/sofastack/sofa-rpc/releases.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/release-notes/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ab7d46caa6906863103b77b742ec7e84","permalink":"/en/projects/sofa-rpc/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/release-notes/","summary":"To learn more, see https://github.com/sofastack/sofa-rpc/releases.","tags":null,"title":"Release notes","type":"projects","url":"/en/projects/sofa-rpc/release-notes/","wordcount":5},{"author":null,"categories":null,"content":" This example demonstrates how to remotely report link data to Zipkin by configuring SOFATracer in an application that integrates SOFATracer.\nThe following examples demonstrate how to use them in SOFABoot/SpringBoot projects and non-SOFABoot/SpringBoot projects, respectively.\nPrepare environment To use SOFABoot, you need to prepare the basic environment first. SOFABoot relies on the following environments: + JDK7 or JDK8 + Apache Maven 3.2.5+ required for compilation\nIntroduce SOFABoot After creating a Spring Boot project, you need to introduce the SOFABoot dependency. First, you need to unzip the zip package of the Spring Boot project generated above and modify the Maven project configuration file pom.xml.\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  Replace the above with the followings:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  The ${sofa.boot.version} specifies the latest version of SOFABoot. For more about SOFABoot versions, see Release notes.\nAdd SOFATracer starter \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Application configuration Finally, add the properties to be used by SOFATracer under the project\u0026amp;rsquo;s application.properties file, including spring.application.name to indicate the name of the current application; logging.path to specify the output directory of the log.\n# Application Name spring.application.name=SOFATracerReportZipkin # logging path logging.path=./logs # open zipkin report com.alipay.sofa.tracer.zipkin.enabled=true # specify zipkin server address com.alipay.sofa.tracer.zipkin.baseUrl=http://localhost:9411  Configure Zipkin Dependencies Considering that Zipkin\u0026amp;rsquo;s data reporting capability is not the ability of SOFATracer to be enabled by default,it‘s desirable to add the following Zipkin data reporting dependencies when using SOFATracer for data reporting:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.zipkin.zipkin2\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;zipkin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.11.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.zipkin.reporter2\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;zipkin-reporter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.7.13\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;\t Start the Zipkin server Start the Zipkin server to receive the link data reported by SOFATracer and display it. Zipkin Server can be configured with reference to this document.\nRunning You can import the project into IDE and run the main method in the project to start the application. In the console, you can see the log about startup as follows:\n2018-05-12 13:12:05.868 INFO 76572 --- …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/report-to-zipkin/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d28d192386829452262116de9c32b570","permalink":"/en/projects/sofa-tracer/report-to-zipkin/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-tracer/report-to-zipkin/","summary":"This example demonstrates how to remotely report link data to Zipkin by configuring SOFATracer in an application that integrates SOFATracer.\nThe following examples demonstrate how to use them in SOFABoot/SpringBoot projects and non-SOFABoot/SpringBoot projects, respectively.\nPrepare environment To use SOFABoot, you need to prepare the basic environment first. SOFABoot relies on the following environments: + JDK7 or JDK8 + Apache Maven 3.2.5+ required for compilation\nIntroduce SOFABoot After creating a Spring Boot project, you need to introduce the SOFABoot dependency.","tags":null,"title":"Report data to Zipkin","type":"projects","url":"/en/projects/sofa-tracer/report-to-zipkin/","wordcount":465},{"author":null,"categories":null,"content":" RestTemplate Integration In this document will demonstrate how to use SOFATracer to track of RestTemplate, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!-- SOFABoot version unified management --\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026amp;rsquo;s application.properties file, including spring.application.name that indicates the name of the current application and logging.path that specifies the log output directory.\n# Application Name spring.application.name=SOFATracerSpringMVC # logging path logging.path=./logs  Add a Controller that provides RESTFul services In the project, provide a simple Controller, for example:\n@RestController public class SampleController { private final AtomicLong counter = new AtomicLong(0); @RequestMapping(\u0026amp;quot;/rest\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; rest() { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); return map; } @RequestMapping(\u0026amp;quot;/asyncrest\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; asyncrest() throws InterruptedException { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); Thread.sleep(5000); return map; } }  Construct the RestTemplate in API model to initiate a call to the RESTful service above  Construct a RestTemplate synchronous call instance  RestTemplate restTemplate = SofaTracerRestTemplateBuilder.buildRestTemplate(); ResponseEntity\u0026amp;lt;String\u0026amp;gt; responseEntity = restTemplate.getForEntity( \u0026amp;quot;http://sac.alipay.net:8080/rest\u0026amp;quot;, String.class);   Construct a RestTemplate asynchronous call instance  AsyncRestTemplate asyncRestTemplate = SofaTracerRestTemplateBuilder .buildAsyncRestTemplate(); ListenableFuture\u0026amp;lt;ResponseEntity\u0026amp;lt;String\u0026amp;gt;\u0026amp;gt; forEntity = asyncRestTemplate.getForEntity( \u0026amp;quot;http://sac.alipay.net:8080/asyncrest\u0026amp;quot;, String.class);  Get the RestTemplate in an automatic injection @Autowired RestTemplate restTemplate;  Run the project Start the SOFABoot app and see the log in the console as follows:\n2018-10-24 10:45:28.683 INFO 5081 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2018-10-24 10:45:28.733 INFO 5081 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2018-10-24 10:45:28.736 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : Started RestTemplateDemoApplication in 2.163 seconds (JVM running for 3.603)  Successful call：\n2018-10-24 10:45:28.989 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : Response is {\u0026amp;quot;count\u0026amp;quot;:1} 2018-10-24 10:45:34.014 INFO 5081 --- [ main] …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-resttemplate/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8b66d6ad488bd59ecbf113b37825d58e","permalink":"/en/projects/sofa-tracer/usage-of-resttemplate/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-tracer/usage-of-resttemplate/","summary":"RestTemplate Integration In this document will demonstrate how to use SOFATracer to track of RestTemplate, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;!-- SOFABoot version unified management --\u0026gt; \u0026lt;/dependency\u0026gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026rsquo;s application.properties file, including spring.","tags":null,"title":"RestTemplate Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-resttemplate/","wordcount":434},{"author":null,"categories":null,"content":" RestTemplate Log Format SOFATracer integrates RestTemplate and outputs the requested link log data format. The default is JSON data format.\nRestTemplate digest log（resttemplate-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.url Request URL   method Request HTTP method   result.code HTTP return status code   resp.size.bytes Response Body Size   time.cost.milliseconds Request time (ms)   current.thread.name Current thread name   remote.app remote app name   baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-10-24 10:45:28.977\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8b3154034912878910015081\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://sac.alipay.net:8080/rest\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:188,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RestTemplate stat log（resttemplate-stat.log） stat.key is the collection of statistical keywords in this period, which uniquely determines a set of statistical data, including local.app, request.url, and method field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   request.url Request URL    method  Request HTTP method   count Number of requests in this period   total.cost.milliseconds Total duration (ms) for requests in this period   success Request result: Y means success ; N indicates failure   load.test Pressure test mark: T indicates pressure test; F indicates non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-10-24 10:46:28.769\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://sac.alipay.net:8080/rest\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:5009,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-resttemplate/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c52c919080b467801700a8a1f156c513","permalink":"/en/projects/sofa-tracer/log-format-resttemplate/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/log-format-resttemplate/","summary":"RestTemplate Log Format SOFATracer integrates RestTemplate and outputs the requested link log data format. The default is JSON data format.\nRestTemplate digest log（resttemplate-digest.log） The data is output in JSON format. Each key meaning is as follows:\n   key Meaning     time Log printing time   local.app Current application name   traceId TraceId   spanId SpanId   request.url Request URL   method Request HTTP method   result.","tags":null,"title":"RestTemplate log","type":"projects","url":"/en/projects/sofa-tracer/log-format-resttemplate/","wordcount":172},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 RestTemplate 进行埋点，本示例工程地址。\n假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n依赖引入 \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  工程配置 在工程的 application.properties 文件下添加 SOFATracer 要使用的参数，包括 spring.application.name 用于标示当前应用的名称；logging.path 用于指定日志的输出目录。\n# Application Name spring.application.name=TestTemplateDemo # logging path logging.path=./logs  添加一个提供 RESTful 服务的 Controller 在工程代码中，添加一个简单的 Controller，例如：\n@RestController public class SampleController { private final AtomicLong counter = new AtomicLong(0); @RequestMapping(\u0026amp;quot;/rest\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; rest() { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); return map; } @RequestMapping(\u0026amp;quot;/asyncrest\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; asyncrest() throws InterruptedException { Map\u0026amp;lt;String, Object\u0026amp;gt; map = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); map.put(\u0026amp;quot;count\u0026amp;quot;, counter.incrementAndGet()); Thread.sleep(5000); return map; } }  以 API 方式构造 RestTemplate 发起一次对上文的 RESTful 服务的调用  构造 RestTemplate 同步调用实例  RestTemplate restTemplate = SofaTracerRestTemplateBuilder.buildRestTemplate(); ResponseEntity\u0026amp;lt;String\u0026amp;gt; responseEntity = restTemplate.getForEntity( \u0026amp;quot;http://sac.alipay.net:8080/rest\u0026amp;quot;, String.class);   构造 RestTemplate 异步调用实例  AsyncRestTemplate asyncRestTemplate = SofaTracerRestTemplateBuilder .buildAsyncRestTemplate(); ListenableFuture\u0026amp;lt;ResponseEntity\u0026amp;lt;String\u0026amp;gt;\u0026amp;gt; forEntity = asyncRestTemplate.getForEntity( \u0026amp;quot;http://sac.alipay.net:8080/asyncrest\u0026amp;quot;, String.class);  以自动注入的方式获取 RestTemplate @Autowired RestTemplate restTemplate;  运行 启动 SOFABoot 应用，将会在控制台中看到启动打印的日志：\n2018-10-24 10:45:28.683 INFO 5081 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2018-10-24 10:45:28.733 INFO 5081 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2018-10-24 10:45:28.736 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : Started RestTemplateDemoApplication in 2.163 seconds (JVM running for 3.603)  调用成功：\n2018-10-24 10:45:28.989 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : Response is {\u0026amp;quot;count\u0026amp;quot;:1} 2018-10-24 10:45:34.014 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : Async Response is {\u0026amp;quot;count\u0026amp;quot;:2} 2018-10-24 10:45:34.014 INFO 5081 --- [ main] c.a.s.t.e.r.RestTemplateDemoApplication : test finish .......  查看日志 在上面的 application.properties 里面，我们配置的日志打印目录是 ./logs 即当前应用的根目录（我们可以根据自己的实践需要进行配置），在当前工程的根目录下可以看到类似如下结构的日志文件：\n./logs ├── spring.log └── tracelog ├── resttemplate-digest.log ├── resttemplate-stat.log ├── spring-mvc-digest.log ├── spring-mvc-stat.log ├── static-info.log └── tracer-self.log  示例中通过构造两个 RestTemplate（一个同步一个异步） 发起对同一个 RESTful 服务的调用，调用完成后可以在 restTemplate-digest.log 中看到类似如下的日志： …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-resttemplate/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8b66d6ad488bd59ecbf113b37825d58e","permalink":"/projects/sofa-tracer/usage-of-resttemplate/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-resttemplate/","summary":"在本文档将演示如何使用 SOFATracer 对 RestTemplate 进行埋点，本示例工程地址。 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作： 依赖引入 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt;","tags":null,"title":"RestTemplate 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-resttemplate/","wordcount":610},{"author":null,"categories":null,"content":" SOFATracer 集成 RestTemplate 后输出请求的链路数据格式，默认为 JSON 数据格式。\nRestTemplate 摘要日志（resttemplate-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   request.url 请求地址   method http method   req.size.bytes 请求大小   resp.size.bytes 响应大小   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:33:10.336\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9271567477985327100211176\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;SimpleAsyncTaskExecutor-1\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;5009ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8801/asyncrest\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RestTemplate 统计日志（resttemplate-stat.log） stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、request.url、和 method 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   request.url 请求 URL    method  请求 HTTP 方法   count 本段时间内请求次数   total.cost.milliseconds 本段时间内的请求总耗时（ms）   success 请求结果：Y 表示成功；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:34:04.130\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8801/asyncrest\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:5009,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-resttemplate/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c52c919080b467801700a8a1f156c513","permalink":"/projects/sofa-tracer/log-format-resttemplate/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-resttemplate/","summary":"SOFATracer 集成 RestTemplate 后输出请求的链路数据格式，默认为 JSON 数据格式。 RestTemplate 摘要日志（resttemplate-digest.log） 以 JSON 格式输出的数据，相应 key 的","tags":null,"title":"RestTemplate 日志","type":"projects","url":"/projects/sofa-tracer/log-format-resttemplate/","wordcount":342},{"author":null,"categories":null,"content":" SOFARPC supports a framework-level retry strategy when the cluster mode is FailOver (SOFARPC uses FailOver mode by default). Retry is only initiated if there is a framework-level exception or a timeout exception on the server. If the business itself throws an exception, the service will not be called again. SOFARPC does not perform any retry by default.\n Note: Although the system will retry calling in case of timeout exception, the server still needs to guarantee the idempotency of the service. Otherwise there may be risks.\n Use XML If you subscribe to the service using XML, you can set the number of retries by setting the retries parameter of sofa:global-attrs:\n\u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;retriesServiceReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.retries.RetriesService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs retries=\u0026amp;quot;2\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Use Annotation If you are using Annotation, you can set the retries attribute of @SofaReferenceBinding annotation:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, retries = 2)) private SampleService sampleService;  Use API in Spring environment If you are using the API in Spring environment, you can call the setRetries method of BoltBindingParam:\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setRetries(2);  Use API in non-Spring environment If you are using the bare API of SOFARPC directly in non-Spring environment, you can call the setRetries method of ConsumerConfig:\nConsumerConfig\u0026amp;lt;RetriesService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;RetriesService\u0026amp;gt;(); consumerConfig.setRetries(2);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/retry-invoke/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d60b44aa8f1b49ab6c1bbc55593a91da","permalink":"/en/projects/sofa-rpc/retry-invoke/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/retry-invoke/","summary":"SOFARPC supports a framework-level retry strategy when the cluster mode is FailOver (SOFARPC uses FailOver mode by default). Retry is only initiated if there is a framework-level exception or a timeout exception on the server. If the business itself throws an exception, the service will not be called again. SOFARPC does not perform any retry by default.\n Note: Although the system will retry calling in case of timeout exception, the server still needs to guarantee the idempotency of the service.","tags":null,"title":"Retry strategy","type":"projects","url":"/en/projects/sofa-rpc/retry-invoke/","wordcount":205},{"author":null,"categories":null,"content":" SOFAJRaft 2019 年 4-7 月开发计划  (p1) Telnet 服务（或其他，越简单越好），作为一种在线排查问题的手段，主要提供以下几个功能  Raft_stat: 以 node 节点为 root，能列出大部分甚至所有相关 stat Metrics: 展示当前节点最新的所有 metrics 指标度量(虽然日志里有相关数据但是相对分散)  (p1) 扩展点：引入 SPI 机制，先列出几个扩展点  LogStorage LogEntry codec RaftMetaStorage Metric 指标度量  (p1) 对于 multi-raft-group 场景，提供一个 manual rebalance api 用于平衡各个节点的 leaders 数量 (p2) 文档国际化 (p2) 添加 Learner 角色，只用于同步数据不参与投票 (p3) RheaKV 完成 jepsen 验证\n  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/road-map/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d39cc6e615d623f8dfc320f32dcbdfa6","permalink":"/projects/sofa-jraft/road-map/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-jraft/road-map/","summary":"SOFAJRaft 2019 年 4-7 月开发计划 (p1) Telnet 服务（或其他，越简单越好），作为一种在线排查问题的手段，主要提供以下几个功能 Raft_stat: 以 node 节点为 root，能列出大部分甚至所有","tags":null,"title":"Road Map","type":"projects","url":"/projects/sofa-jraft/road-map/","wordcount":194},{"author":null,"categories":null,"content":" Roadmap Version 1.5.1  Fixed code style problems in the project: https://github.com/alipay/sofa-bolt/issues/85 Fixed known bugs in the project: https://github.com/alipay/sofa-bolt/issues/82 The RPC layer supports message list dispatching from the I/O thread: https://github.com/alipay/sofa-bolt/pull/84  Version 1.6.0 Overall goal  Unify lifecycle APIs for all components Extract and incorporate network component APIs Converge configuration methods and enhance configuration scalability  Unify lifecycle APIs for all components In the current Bolt version, APIs of lifecycle management components are named inconsistently, for example:\n ReconnectManager does not need startup or initialization, and the disabling method is stop. The initialization method for DefaultConnectionMonitor of is start, and the disabling method is destroy. The initialization method forRpcClient init, and the disabling method is shutdown. The initialization method forRpcTaskScanner is start, and the disabling method is shutdown.  We plan to unify lifecycle APIs of all components in V1.6.0:\n For components that are subject to lifecycle management, which require initialization before use and must release resources after use, their startup/shutdown APIs are to be unified.  Extract and incorporate network component APIs Network operations of Bolt are mainly performed by using the remoting class, which is provided as an abstract class. We plan to converge methods of this class, and provide them in the form of APIs in the future. There are a few advantages of doing so:\n Standardized usage Stable service Convenient internal code iteration  Taking the ReconnectManager as an example. It provides the public addCancelUrl method, which is not called in the Bolt project. This may cause problems:\n IDE will give a warning. Users may get confused on whether they should delete this method.  We plan to solve the these problems in V1.6.0 by extracting a set of stable APIs, which are convenient for users to use, helpful to improve code readability, and can lay a solid foundation for future iterations.\nConverge configuration methods and enhance configuration scalability Currently, Bolt supports the following configuration methods:\n ProtocolSwitch: supports protocol configuration (enabling or disabling CRC validation), and creates configuration objects by static means. GlobalSwitch: offers instance-level configuration, and offers GlobalSwitch configuration items to every AbstractConfigurableInstance. The default value is taken from the SystemProperty, and the configuration can be adjusted through an API. ConfigItem: enumerates Netty-related configuration items that cannot be inherited or extended before you modify the source code. ConfigManager: reads SystemProperty configurations by static means. Configs: defines the configuration item names and specifies their default values.  Generally, Bolt\u0026amp;rsquo;s configuration items look to be loose and scattered and are hard for users to extend their usage. …","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-roadmap/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3d4eac90b5c8e657d14eb885ab1f9a92","permalink":"/en/projects/sofa-bolt/sofa-bolt-roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-bolt/sofa-bolt-roadmap/","summary":"Roadmap Version 1.5.1  Fixed code style problems in the project: https://github.com/alipay/sofa-bolt/issues/85 Fixed known bugs in the project: https://github.com/alipay/sofa-bolt/issues/82 The RPC layer supports message list dispatching from the I/O thread: https://github.com/alipay/sofa-bolt/pull/84  Version 1.6.0 Overall goal  Unify lifecycle APIs for all components Extract and incorporate network component APIs Converge configuration methods and enhance configuration scalability  Unify lifecycle APIs for all components In the current Bolt version, APIs of lifecycle management components are named inconsistently, for example:","tags":null,"title":"Roadmap","type":"projects","url":"/en/projects/sofa-bolt/sofa-bolt-roadmap/","wordcount":539},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-roadmap/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c4532d11cef15d8fe3ff5e04c7b08f90","permalink":"/en/projects/sofa-boot/sofa-ark-roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/en/projects/sofa-boot/sofa-ark-roadmap/","summary":"","tags":null,"title":"Roadmap","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-roadmap/","wordcount":0},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-roadmap/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f1d0bf15efba08535f9574e1c8344cab","permalink":"/en/projects/sofa-boot/sofa-jarslink-roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-roadmap/","summary":"","tags":null,"title":"Roadmap","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-roadmap/","wordcount":0},{"author":null,"categories":null,"content":" Development plans of SOFAJRaft from April to July 2019  (p1) Implement the Telnet service (or similar equivalents, the simpler the better) as an online troubleshooting means. It should be able to provide the following functions:  Raft_stat: List most or all stats of a Raft node. Metrics: Uniformly display the latest values of all metrics for the current node (the related data is scattered in the log).  (p1) Extension points: introduce the SPI mechanism. Some of the extension points are listed as follows:  LogStorage LogEntry codec RaftMetaStorage Metrics  (p1) Provide a manual rebalance API for the multi-raft-group scenario to balance the number of leaders on each node. (p2) Translate the document into multiple languages. (p2) Add a learner role that only replicates data and does not vote. (p3) Complete jepsen tests for RheaKV.  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/road-map/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d39cc6e615d623f8dfc320f32dcbdfa6","permalink":"/en/projects/sofa-jraft/road-map/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-jraft/road-map/","summary":"Development plans of SOFAJRaft from April to July 2019  (p1) Implement the Telnet service (or similar equivalents, the simpler the better) as an online troubleshooting means. It should be able to provide the following functions:  Raft_stat: List most or all stats of a Raft node. Metrics: Uniformly display the latest values of all metrics for the current node (the related data is scattered in the log).  (p1) Extension points: introduce the SPI mechanism.","tags":null,"title":"Roadmap","type":"projects","url":"/en/projects/sofa-jraft/road-map/","wordcount":132},{"author":null,"categories":null,"content":" Task list Some of the existing internal features will be available in subsequent iterations.\nThe features that have been implemented are listed in the following table. You are welcome to claim the tasks and make contributions.\n   Task type Task Degree of difficulty Claimant and time Planned completion time Progress Related issues     Documentation Document translation Low       Code Flexible persistent connection management Low    #56   Code etcd registry center implementation Medium @wynn5a\n2018-6   #153   Code eureka registry center implementation Medium @liufeiit\n2018-4   #52   Code gRPC support High    #57   Code CXF protocol High    #58   Code TLS support High        Version iteration Plan v5.5.0  Support JSON serialization Support H2 TLS Implement flexible connection pool Integrate Hystrix Support Consul registry center  v5.6.0  Support GRPC communication layer Support etcd registry center Support SOFAMesh Implement BOLT version negotiation and CRC verification  v5.7.0  Support Telnet built-in instructions Support SpringBoot 2.0 Support Mock function Support encryption  v5.8.0  Support authorization Support SofaRegistry Support Reactive  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/roadmap/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6064fc180911f520f6d1590b88595693","permalink":"/en/projects/sofa-rpc/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/roadmap/","summary":"Task list Some of the existing internal features will be available in subsequent iterations.\nThe features that have been implemented are listed in the following table. You are welcome to claim the tasks and make contributions.\n   Task type Task Degree of difficulty Claimant and time Planned completion time Progress Related issues     Documentation Document translation Low       Code Flexible persistent connection management Low    #56   Code etcd registry center implementation Medium @wynn5a","tags":null,"title":"Roadmap","type":"projects","url":"/en/projects/sofa-rpc/roadmap/","wordcount":152},{"author":null,"categories":null,"content":" Tasks The following table lists the features that have not yet been implemented. We encourage you to claim the tasks and make a contribution.\n   Type Task Difficulty Claimed by and on Planned completion time Progress Related issues     Document SOFADashboard Parameter Configuration Guide Simple       Code Support for SOFARegistry Medium       Code Support for Docker Medium       Code Support for Kubernetes Medium       Code Support for Apollo Medium       Code Frontend optimization Medium        ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/roadmap/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a740c874742b504de9011b07f3a4ddb5","permalink":"/en/projects/sofa-dashboard/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/roadmap/","summary":" Tasks The following table lists the features that have not yet been implemented. We encourage you to claim the tasks and make a contribution.\n   Type Task Difficulty Claimed by and on Planned completion time Progress Related issues     Document SOFADashboard Parameter Configuration Guide Simple       Code Support for SOFARegistry Medium       Code Support for Docker Medium       Code Support for Kubernetes Medium       Code Support for Apollo Medium       Code Frontend optimization Medium        ","tags":null,"title":"Roadmap and task claim","type":"projects","url":"/en/projects/sofa-dashboard/roadmap/","wordcount":67},{"author":null,"categories":null,"content":" Roadmap Tasks We have some internal implementations of some new features, which will be released along with the iterations when sorted out.\nFeatures that are not implemented yet are listed in the following table. We encourage you to claim the tasks and contribute to SOFARegistry.\n   Type Task Difficulty Claimed by and on Planned completion time Progress Related issues     Document Document Translation Low       Code Support for Spring Cloud Medium       Code Data self-check High       Code Blacklist filtering Medium       Code SOFARegistry Dashboard High       Code Support for other microservice frameworks Medium       Code Support for Docker \u0026amp;amp; Kubernetes High       Code Multi-language client support High        Version iteration plan v5.3.0  Support for Spring Cloud Data self-check Blacklist filtering  v5.4.0  SOFARegistry Dashboard Support for other microservice frameworks  v5.5.0  Support for Docker \u0026amp;amp; Kubernetes Multi-language client support  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/roadmap/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b0ab45d52ba3eb7db590a4f5e4197c9e","permalink":"/en/projects/sofa-registry/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-registry/roadmap/","summary":"Roadmap Tasks We have some internal implementations of some new features, which will be released along with the iterations when sorted out.\nFeatures that are not implemented yet are listed in the following table. We encourage you to claim the tasks and contribute to SOFARegistry.\n   Type Task Difficulty Claimed by and on Planned completion time Progress Related issues     Document Document Translation Low       Code Support for Spring Cloud Medium       Code Data self-check High       Code Blacklist filtering Medium       Code SOFARegistry Dashboard High       Code Support for other microservice frameworks Medium       Code Support for Docker \u0026amp; Kubernetes High       Code Multi-language client support High        Version iteration plan v5.","tags":null,"title":"Roadmap and task claims","type":"projects","url":"/en/projects/sofa-registry/roadmap/","wordcount":128},{"author":null,"categories":null,"content":" SOFA-RPC Interface Sectioon  Introduce the jar packages into your interface project.  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.\npublic interface HelloService { @Hmily void say(String hello); }  The project with SOFA-RPC implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration\n Step 3 ： Add the specific annotation to the implementation method. you need to complete the development of confirm and cancel method, if in TCC mode.\n  Introduce The Maven dependency Spring-Namespace  Introduce the hmily-sofa-rpc dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-sofa-rpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  make the configuration in the XML configuration file as below:\n  \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot  Introduce the hmily-spring-boot-starter-sofa-rpc dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-sofa-rpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  Add annotations on the implementation interface We have completed the integration described above,and the next we will talk about the specific implementation.\nTCC Mode  Add @HmilyTCC (confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation to the concrete implementation of the interface method identified by \u0026amp;lsquo;@Hmily\u0026amp;rsquo;.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be consistent with the identification method.\n The TCC mode should ensure the idempotence of the confirm and cancel methods,Users need to develop these two methods by themselves,The confirmation and rollback behavior of all transactions are completely up tp users.The Hmily framework is just responsible for making calls.\n  public class HelloServiceImpl implements HelloService { …","date":-62135596800,"description":"SOFA-RPC User Guide","dir":"projects/hmily/user-rpc/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b0291d256039472c980741163fd918a8","permalink":"/en/projects/hmily/user-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/hmily/user-rpc/","summary":"SOFA-RPC Interface Sectioon  Introduce the jar packages into your interface project.  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Add the @Hmily annotation on the interface method in which you need to perform Hmily distributed transactions.\npublic interface HelloService { @Hmily void say(String hello); }  The project with SOFA-RPC implementation  Step 1 ： Introduce the jar package of the hmily dependency\n Step 2 ： Add Hmily configuration","tags":null,"title":"SOFA-RPC User Guide","type":"projects","url":"/en/projects/hmily/user-rpc/","wordcount":515},{"author":null,"categories":null,"content":" AntCoreTest (ACTS) is a white-box test framework developed by Ant Financial based on years\u0026amp;rsquo; testing knowledge and experience with the financial-level distributed architecture for the purpose of providing enterprises with a highly efficient, precise, and automated interface testing services. In addition to general testing capabilities such as data-driven testing provided by conventional open source frameworks like TestNG, ACTS offers new features such as model-driven testing, visualized editing, and a standard process engine to assist engineers with efficient and high quality test case compilation as well as standard and precise test validation for interface testing.\nACTS is a next generation testing framework based on the data model-driven testing engine. ACTS is applicable to context environments that require the integration of TestNg and Spring. ACTS uses the YAML file as the data carrier and builds data model drivers upon it, providing features such as the all-in-one editor, precise validation, and efficient test case management to significantly improve testing efficiency.\nOperating principle  Upon the start of the test script, ActsDataProvider starts the tested method (the method annotated by @Test), loads the corresponding test case data file (YAML file), and converts the data into corresponding PrepareData objects.\n When runTest starts running, it passes PrepareData and test case names to ACTS. ACTS then assembles such information into the ActsRuntimeContext class, transmits it in the entire process, and initializes the TestUnitHandler. The running period of the runTest process method consists of the following stages:\n  | Action | Method | | :\u0026amp;mdash; | :\u0026amp;mdash; | | Clear | clear(actsRuntimeContext) | | Prepare | prepare(actsRuntimeContext) | | Execute | execute(actsRuntimeContext) | | Check | check(actsRuntimeContext) |\nDescription:\n Clear: Clean up the preparation data and validation data to avoid the negative impact of dirty data on the test script. Prepare: Prepare data such as DB data. Execute: Call the tested method, and capture the corresponding information, such as responses and exception messages. Check: Validate the corresponding information such as the responses, DB data, and exception messages based on the test data.  Features ACTS provides the following features:\n2.1 All-in-one editor The ACTS framework separates the test data from the test code, and provides the visual editor ACTS IDE. ACTS IDE can help you quickly enter, view, and manage the test case data, which significantly reduces repetitive coding.\n2.2 Precise validation To improve data fill-in efficiency and reduce omission of check points among the expectation data, such as response expectations and database expectations, the ACTS framework provides a run and backfill function. In addition, ACTS uses validation rule flags to implement precise validation of the expectation data.\n2.3 Flexible scalability ACTS provides a rich variety of APIs, which are encapsulated …","date":-62135596800,"description":"","dir":"projects/sofa-acts/overview/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ac57071cd0d40a63359d476d05344c61","permalink":"/en/projects/sofa-acts/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-acts/overview/","summary":"AntCoreTest (ACTS) is a white-box test framework developed by Ant Financial based on years\u0026rsquo; testing knowledge and experience with the financial-level distributed architecture for the purpose of providing enterprises with a highly efficient, precise, and automated interface testing services. In addition to general testing capabilities such as data-driven testing provided by conventional open source frameworks like TestNG, ACTS offers new features such as model-driven testing, visualized editing, and a standard process engine to assist engineers with efficient and high quality test case compilation as well as standard and precise test validation for interface testing.","tags":null,"title":"SOFAActs overview","type":"projects","url":"/en/projects/sofa-acts/overview/","wordcount":556},{"author":null,"categories":null,"content":" ACTS（AntCoreTest）源于蚂蚁金服多年金融级分布式架构工程的测试实践的积累与沉淀，是一款白盒测试框架，旨在为企业提供高效、精细化的接口自动化测试。 与现有的诸如 TestNG 等开源框架相比，ACTS 除了具备通用的数据自动化驱动等测试能力外，还具有契合快速的互联网发展和复杂的分布式金融系统特点的模型驱动、可视化编辑和标准流程引擎等新特性，可辅助工程师高效、高质量地完成接口测试用例编写以及标准化精准化测试验证。\nACTS 是基于数据模型驱动测试引擎执行的的新一代测试框架（如图1所示），适配 TestNg+Spring 的测试上下文环境，以 YAML 为数据载体并在此上构建数据模型驱动，实现了一站式编辑、精细化校验和高效用例管理等，可以有效提高测试效率。\n运行原理  测试脚本启动的时，ActsDataProvider 会启动被测方法（被 @Test 注解的方法），加载对应的用例数据文件(以 YAML 文件承载)，然后转换成对应的 PrepareData 对象； runTest 开始执行时会传入 PrepareData 和用例名称，ACTS 根据这些信息组装出 ActsRuntimeContext 上下文并在整个过程中传递，同时初始化 TestUnitHandler 测试处理器。runTest -\u0026amp;gt; process 方法执行期包含如下四个子流程：\n   说明 方法     清理 clear(actsRuntimeContext)   准备 prepare(actsRuntimeContext)   执行 execute(actsRuntimeContext)   检查 check(actsRuntimeContext)     方法功能说明： + 清理阶段：清理准备数据、校验数据，防止脏数据对测试脚本产生影响； + 准备阶段：准备 DB 数据等； + 执行阶段：调用被测方法，捕获返回结果和异常等信息； + 检查阶段：根据测试数据，校验返回结果、DB 数据和异常信息等内容。\n功能描述 ACTS 提供了以下能力：\n2.1 一站式编辑 框架实现了测试数据与测试代码的分离，同时配套提供可视化编辑器 ACTS IDE，通过 ACTS IDE 可以快速地录入、查看和管理用例数据，有效减少重复性编码。\n2.2 精细化校验 为了提高返回结果、DB 数据等期望数据的填写效率和减少检验点遗漏，框架提供了预跑返填功能；同时在 ACTS 校验规则标签的标记下，实现期望 DB 数据、期望结果等数据的精细化校验。\n2.3 灵活可扩展 ACTS 提供了丰富的 API ，其封装于 ActsRuntimeContext 类中，借助 API 可快速获取和设置自定义参数、用例入参、期望结果等，满足用户对用例数据的自定义操作；\n同时，框架的 ActsTestBase 测试基类对外暴露各个执行阶段方法，包括 prepare，execute，check，clear 等，例如在测试类中通过重写 process 方法可将整个测试脚本重新编排。\n2.4 统一配置能力 配置文件中提供丰富的配置能力以定制化框架的个性需求。\n应用场景 基于 SOFABoot 搭建的应用，在 Intellij IDEA 开发环境下快速编写和执行接口测试用例。推荐使用 Intellij IDEA 2017 以便能更好地兼容 ACTS IDE。\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/overview/","fuzzywordcount":1000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ac57071cd0d40a63359d476d05344c61","permalink":"/projects/sofa-acts/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-acts/overview/","summary":"ACTS（AntCoreTest）源于蚂蚁金服多年金融级分布式架构工程的测试实践的积累与沉淀，是一款白盒测试框架，旨在为企业提供高效、精细化","tags":null,"title":"SOFAActs 介绍","type":"projects","url":"/projects/sofa-acts/overview/","wordcount":998},{"author":null,"categories":null,"content":" SOFAArk offers a variety of methods to support multi-application (module) consolidation and deployment, including command line-based control and API-based control. SOFAArk control is an implementation of SOFADashboard\u0026amp;rsquo;s control over APIs. SOFAArk control is implemented by pushing commands to and parsing commands in ZooKeeper.\nSOFAArk control mainly provides the following functions:\n Plug-in registration: registers the ark-biz package with SOFADashboard as basic data processors. Application association: binds the ark-biz package with host applications. Plug-in details: On the plug-in details page, you can view the information about all host applications that are associated with the current ark-biz package, as well as the status information of the ark-biz package in these host applications. Command push: On the plug-in details page, you can push some commands for specific applications and IP addresses, such as install and uninstall. When these commands are written to a ZooKeeper node, all host applications that listen to this node will parse the commands and perform related operations.  Plug-in registration Register the ark-biz package with SOFADashboard:\nEnter basic information of the plug-in\nAfter successful registration, the plug-in is displayed on the module list as follows.\nApplication association Click Associate application in the Actions column of a plug-in on the module list to associate it with an application.\nClick Associate application in the Actions column of the plug-in to associate it with an application.\nPlug-in details Click Details in the Actions column of a plug-in to view all apps and app instances associated with the current plug-in.\n Version switch  After switching the plug-in to V2.0.0, the status information is empty, because the plug-in V2.0.0 has not been installed in the host application.\nCommand push SOFADashboard supports command push in two dimensions:\n Application-based command push, where all instances of the specified application listen to this command IP-based and group-based command push for single-IP address scenarios  IP-based command push Click Install. The page is refreshed after about 1s to 1.5s.\n Application-based command push is similar.\n ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/ark-console/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b42cffbb8e55a4c47412e49de0e9b228","permalink":"/en/projects/sofa-dashboard/ark-console/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-dashboard/ark-console/","summary":"SOFAArk offers a variety of methods to support multi-application (module) consolidation and deployment, including command line-based control and API-based control. SOFAArk control is an implementation of SOFADashboard\u0026rsquo;s control over APIs. SOFAArk control is implemented by pushing commands to and parsing commands in ZooKeeper.\nSOFAArk control mainly provides the following functions:\n Plug-in registration: registers the ark-biz package with SOFADashboard as basic data processors. Application association: binds the ark-biz package with host applications.","tags":null,"title":"SOFAArk control","type":"projects","url":"/en/projects/sofa-dashboard/ark-console/","wordcount":321},{"author":null,"categories":null,"content":" SOFAArk 是一款基于 Java 实现的轻量级类隔离容器，主要提供类隔离和应用(模块)合并部署能力，由蚂蚁金服公司开源贡献；\n在大型软件开发过程中，通常会推荐底层功能插件化，业务功能模块化的开发模式，以期达到低耦合、高内聚、功能复用的优点。基于此，SOFAArk 提供了一套较为规范化的插件化、模块化的开发方案，产品能力主要包括：\n 定义类加载模型，运行时底层插件、业务应用(模块)之间均相互隔离，单一插件和应用(模块)由不同的 ClassLoader 加载，可以有效避免相互之间的包冲突，提升插件和模块功能复用能力； 定义插件开发规范，提供 maven 打包工具，简单快速将多个二方包打包成插件（Ark Plugin，以下简称 Plugin） 定义模块开发规范，提供 maven 打包工具，简单快速将应用打包成模块 (Ark Biz，以下简称 Biz) 针对 Plugin、Biz 提供标准的编程界面，包括服务、事件、扩展点等机制 支持多 Biz 的合并部署，开发阶段将多个 Biz 打包成可执行 Fat Jar，或者运行时使用 API 或配置中心(Zookeeper)动态地安装卸载 Biz  基于以上能力，SOFAArk 可以帮助解决依赖包冲突、多应用(模块)合并部署等场景问题。\n场景 包冲突 日常使用 Java 开发，常常会遇到包依赖冲突的问题，尤其当应用变得臃肿庞大，包冲突的问题也会变得更加棘手，导致各种各样的报错，例如 LinkageError, NoSuchMethodError 等；实际开发中，可以采用多种方法来解决包冲突问题，比较常见的是类似 Spring Boot 的做法，统一管理应用所有依赖包的版本，保证这些三方包不存在依赖冲突；这种做法只能有效避免包冲突问题，不能根本上解决包冲突的问题；如果某个应用的确需要在运行时使用两个相互冲突的包，例如 protobuf2 和 protobuf3，那么类似 Spring Boot 的做法依然解决不了问题。\n为了彻底解决包冲突的问题，需要借助类隔离机制，使用不同的 ClassLoader 加载不同版本的三方依赖，进而隔离包冲突问题； OSGI 作为业内最出名的类隔离框架，自然是可以被用于解决上述包冲突问题，但是 OSGI 框架太过臃肿，功能繁杂；为了解决包冲突问题，引入 OSGI 框架，有牛刀杀鸡之嫌，且反而使工程变得更加复杂，不利于开发；\nSOFAArk 采用轻量级的类隔离方案来解决日常经常遇到的包冲突问题，在蚂蚁金服内部服务于整个 SOFABoot 技术体系，弥补 Spring Boot 没有的类隔离能力。SOFAArk 提出了一种特殊的包结构 \u0026amp;ndash; Ark Plugin，在遇到包冲突时，用户可以使用 Maven 插件将若干冲突包打包成 Plugin，运行时由独立的 PluginClassLoader 加载，从而解决包冲突。\n假设如下场景，如果工程需要引入两个三方包：A 和 B，但是 A 需要依赖版本号为 0.1 的 C 包，而恰好 B 需要依赖版本号为 0.2 的 C 包，且 C 包的这两个版本无法兼容：\n此时，即可使用 SOFAArk 解决该依赖冲突问题；只需要把 A 和版本为 0.1 的 C 包一起打包成一个 Ark 插件，然后让应用工程引入该插件依赖即可；\n合并部署 复杂项目通常需要跨团队协作开发，各自负责不同的组件，而众所周知，协调跨团队合作开发会遇到不少问题；比如各自技术栈不统一导致的依赖冲突，又比如往同一个 Git 仓库提交代码常常导致 merge 冲突。因此，如果能让每个团队将负责的功能组件当成一个个单独的应用开发，运行时合并部署，通过统一的编程界面交互，那么将极大的提升开发效率及应用可扩展性。SOFAArk 提出了一种特殊的包结构 \u0026amp;ndash; Ark Biz，用户可以使用 Maven 插件将应用打包成 Biz，允许多 Biz 在 SOFAArk 容器之上合并部署，并通过统一的编程界面交互。\n静态合并部署 SOFAArk 提供了静态合并部署能力，在开发阶段，应用可以将其他应用打成的 Biz 包通过 Maven 依赖的方式引入，而当自身被打成可执行 Fat Jar 时，可以将其他应用 Biz 包一并打入，启动时，则会根据优先级依次启动各应用。每个 Biz 使用独立的 BizClassLoader 加载，不需要考虑相互依赖冲突问题，Biz 之间则通过 SofaService/SofaReference JVM 服务进行交互。\n动态合并部署 动态合并部署区别于静态合并部署最大的一点是，运行时通过 API 或者配置中心（Zookeeper）来控制 Biz 的部署和卸载。动态合并部署的设计理念图如下：\n无论是静态还是动态合并部署都会有宿主应用（master biz）的概念, 如果 Ark 包只打包了一个 Biz，则该 Biz 默认成为宿主应用；如果 Ark 包打包了多个 Biz 包，需要配置指定宿主应用。宿主应用不允许被卸载，一般而言，宿主应用会作为流量入口的中台系统，具体的服务实现会放在不同的动态 Biz 中，供宿主应用调用。宿主应用可以使用 SOFAArk 提供的客户端 API 实现动态应用的部署和卸载。除了 API, SOFAArk 提供了 Config Plugin，用于对接配置中心（目前支持 Zookeeper），运行时接受动态配置；Config Plugin 会解析下发的配置，控制动态应用的部署和卸载。\n原理 SOFAArk 包含三个概念，Ark Container, Ark Plugin 和 Ark Biz; 运行时逻辑结构图如下:\n在介绍这三个概念之前，先介绍上述 Ark 包概念；Ark 包是满足特定目录格式要求的可运行 Fat Jar，使用官方提供的 Maven 插件 sofa-ark-maven-plugin 可以将单个或多个应用打包成标准格式的 Ark 包；使用 java -jar 命令即可在 SOFAArk 容器之上启动所有应用；Ark 包通常包含 Ark Container、Ark Plugin 和 Ark Biz；以下我们针对这三个概念简单做下名词解释：\n Ark Container: SOFAArk 容器，负责 Ark 包启动运行时的管理；Ark Plugin 和 Ark Biz 运行在 SOFAArk 容器之上；容器具备管理插件和应用的功能；容器启动成功后，会自动解析 classpath 包含的 Ark Plugin 和 Ark Biz 依赖，完成隔离加载并按优先级依次启动之；\n Ark Plugin: Ark 插件，满足特定目录格式要求的 Fat Jar，使用官方提供的 Maven 插件 sofa-ark-plugin-maven-plugin 可以将一个或多个普通的 Java jar 打包成一个标准格式的 Ark Plugin；Ark Plugin 会包含一份配置文件，通常包括插件类导入导出配置、资源导入导出配置、插件启动优先级等；运行时，SOFAArk 容器会使用独立的 PluginClassLoader加载插件，并根据插件配置构建类加载索引表、资源加载索引表，使插件和插件之 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-readme/","fuzzywordcount":2600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cdb6729fc7a63954b7559c8ea319f550","permalink":"/projects/sofa-boot/sofa-ark-readme/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/projects/sofa-boot/sofa-ark-readme/","summary":"SOFAArk 是一款基于 Java 实现的轻量级类隔离容器，主要提供类隔离和应用(模块)合并部署能力，由蚂蚁金服公司开源贡献； 在大型软件开发过程中，通常会推荐底层","tags":null,"title":"SOFAArk 介绍","type":"projects","url":"/projects/sofa-boot/sofa-ark-readme/","wordcount":2577},{"author":null,"categories":null,"content":" SOFAArk 本身提供了多种方式来支持多应用(模块)合并部署 ，包括基于命令行的管控，基于 API 的管控等；SOFAARK 管控是 SOFADashboard 针对 API 的管控的一种实现。通过面向 Zookeeper 进行命令的推送和命令的解析执行。\nSOFAArk 管控主要包括以下功能：\n 插件注册：将 ark-biz 插件注册到 SOFADashboard，作为基础数据 关联应用：将 ark-biz 插件与宿主应用进行绑定 插件详情：通过插件详情页，可以看下当前 ark-biz 插件下所有关联的宿主应用信息，以及宿主应用中的ark-biz 状态信息 命令推送：插件详情页，可以针对应用维度、分组维度、IP 维度 推送一些指令，比如 install、uninstall 等等，当这些命令被写入到 Zookeeper 的某个节点上时，所有监听此节点的 宿主应用均会解析此指令，并进行相关的操作  插件管理 插件注册 将 ark-biz 插件注册到 SOFADashboard：\n插件删除 添加插件版本 插件版本 biz 包路径 删除版本 关联应用 点击模块列表操作菜单栏中的关联应用，可以将一个应用与插件进行绑定：\n动态管控 点击插件列表后面的 详情 按钮，可以查看当前插件下所有应用信息和应用实例信息。\n命令推送 SOFADashboard 提供三种维度的命令推送\n 基于应用维度，当前应用所有的实例都会监听到此命令变更 基于IP 维度，分组维度的单 ip 场景  动态模块详情 点击 状态详细按钮，左侧栏将会展开 \u0026amp;ldquo;抽屉\u0026amp;rdquo; 展示详情状态数据\n","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/ark-console/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b42cffbb8e55a4c47412e49de0e9b228","permalink":"/projects/sofa-dashboard/ark-console/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-dashboard/ark-console/","summary":"SOFAArk 本身提供了多种方式来支持多应用(模块)合并部署 ，包括基于命令行的管控，基于 API 的管控等；SOFAARK 管控是 SOFADashboard 针对 API 的管控的一种实现。通过面","tags":null,"title":"SOFAArk 管控","type":"projects","url":"/projects/sofa-dashboard/ark-console/","wordcount":541},{"author":null,"categories":null,"content":" SOFAArk 的配置目录不是必须存在，如果需要，统一放在工程根目录 ${baseDir}/conf/ark 下，执行 sofa-ark-maven-plugin 打包，将会自动将该目录下的配置打包至 Ark 包，例如 Ark 包目录为：\n. ├── META-INF │ └── MANIFEST.MF ├── SOFA-ARK │ ├── biz │ │ └── demo-0.0.1-SNAPSHOT-ark-biz.jar │ └── container │ └── sofa-ark-all-0.6.0-SNAPSHOT.jar ├── com │ └── alipay │ └── sofa │ └── ark │ ├── ... │ └── conf └── ark ├── bootstrap-dev.properties ├── bootstrap.properties └── log └── logback-conf.xml  注意事项：如果应用中包含 SOFAArk 配置，打包时需要注意 baseDir 配置，用于指定工程根目录，具体参考文档\n上述 conf/ark 目录中可以配置 SOFAArk 容器启动配置以及日志配置，下面介绍配置的使用.\nconf/ark/bootstrap.properties 是 SOFAArk 容器默认启动配置文件，配置内容包括：日志配置、plugin 激活和钝化配置、biz 激活和钝化配置.\n日志配置 SOFAArk 容器日志内部实现使用 logback, 日志配置参数包括： + logging.path \u0026amp;gt; 容器日志目录根路径，这里只影响 SOFAArk 容器日志路径，不影响应用日志，应用自身日志由自身配置决定，默认打印在 ${user.admin}/logs 目录\n logging.level.com.alipay.sofa.ark \u0026amp;gt; 设置 SOFAArk 容器日志级别，默认为 INFO\n logging.config.com.alipay.sofa.ark \u0026amp;gt; 指定自定义日志配置文件名，用于覆盖 SOFAArk 容器自带的日志配置文件。建议自定义配置文件放在 conf/ark/log 目录中\n sofa.middleware.log.com.alipay.sofa.ark.console \u0026amp;gt; 配置容器日志是否打印在 console，默认为 false.\n sofa.middleware.log.com.alipay.sofa.ark.console.level \u0026amp;gt; 配合上述配置项使用，如果打印在 console ，该配置项用于配置 SOFAArk 容器打印在 console 的日志级别\n  插件配置  ark.plugin.active.include \u0026amp;gt; 指定激活哪些插件，多个插件使用 \u0026amp;lsquo;,\u0026amp;rsquo; 分隔；默认激活 Ark 包中所有的插件。\n ark.plugin.active.exclude \u0026amp;gt; 指定排除哪些插件，多个插件使用 \u0026amp;lsquo;,\u0026amp;rsquo; 分隔；默认为空\n  注：如果同时配置了这两个属性，以 ark.plugin.active.include 为准\nbiz配置  ark.biz.active.include \u0026amp;gt; 指定激活哪些 Biz，多个 Biz 使用 \u0026amp;lsquo;,\u0026amp;rsquo; 分隔；默认激活 Ark 包中所有的 Biz.\n ark.biz.active.exclude \u0026amp;gt; 指定排除哪些 Biz，多个 Biz 使用 \u0026amp;lsquo;,\u0026amp;rsquo; 分隔；默认为空\n com.alipay.sofa.ark.master.biz \u0026amp;gt; 指定宿主 Biz 名，如果 Ark 包中只有一个 Biz，则不用设置，默认设置为宿主 Biz; 否则需要显示设置\n  注：如果同时配置了前两个属性，以 ark.biz.active.include 为准\n动态配置 SOFAArk 提供了对接 Zookeeper 的插件，目前用于动态接收 Biz 指令，目前只支持 Zookeeper，配置格式如下：\ncom.alipay.sofa.ark.config.address=zookeeper://ip:port?key1=value1\u0026amp;amp;key2=value2  特别注意，SOFAArk 有一个默认的逻辑，如果用户配置了 com.alipay.sofa.ark.config.address，且 Ark 包中打入了多个 Biz，则只会启动宿主应用(master biz)；这样做的原因是如果配置了动态配置，SOFAArk 会优先根据动态配置控制 Biz 的部署。\nProfile 机制 默认 SOFAArk 容器使用 bootstrap.properties 配置，实际开发中，可能根据运行环境加载不同的配置，SOFAArk 提供了 profile 机制. 指定 profile 值，SOFAArk 容器会加载 bootstrap-${profile}.properties 配置文件。指定 profile 的配置有两种方式： + 通过 -D VM 参数传入，例如：-Dark.profile=dev,dev2 多个值使用 \u0026amp;lsquo;,\u0026amp;rsquo; 隔开。 + 通过应用启动参数传入，例如：java -jar demo-executable-ark.jar -Aprofile=dev,dev2 多个值使用 \u0026amp;lsquo;,\u0026amp;rsquo; 隔开。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-config/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"70dd9c389e65ee3f89573cf93bd466ec","permalink":"/projects/sofa-boot/sofa-ark-ark-config/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-config/","summary":"SOFAArk 的配置目录不是必须存在，如果需要，统一放在工程根目录 ${baseDir}/conf/ark 下，执行 sofa-ark-maven-plugin 打包，将会自动将该目录下的配置打包至 Ark 包，例如 Ark 包目录为： . ├── META-INF │ └─","tags":null,"title":"SOFAArk 配置","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-config/","wordcount":1036},{"author":null,"categories":null,"content":" Introduction SOFABolt is a network communication framework implemented based on Netty and developed by Ant Finance.\n Netty was developed to let Java programmers focus more on the implementation of network communication-based business logic, and not worry excessively about network low-level NIO implementation or network problems that are difficult to debug. SOFABolt was developed to let middleware developers focus more on the implementation of products\u0026amp;rsquo; functional performance, and not on making the communication framework\u0026amp;rsquo;s wheels over and over again.  Bolt takes its name from a Disney movie character. Bolt is a light, easy-to-use, high-performance, and flexibly scalable communication framework based on the Netty best practices. In the past few years, we have solved a lot of problems in terms of network communication for microservices and message oriented middleware. We have accumulated a lot of experience and have been constantly optimizing and improving our solutions. We hope that our solutions can be incorporated into the SOFABolt base component to serve more network communication scenarios. At present, SOFABolt has already been put to use in many Ant Middleware products, such as microservice products (SOFARPC), message queue, distributed transactions, distributed switches, and configuration centers.\nMultiple languages supported  node Python cpp  ","date":-62135596800,"description":"","dir":"projects/sofa-bolt/overview/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5ee08df7c4bbd2c3be846e16f3bc81b1","permalink":"/en/projects/sofa-bolt/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-bolt/overview/","summary":"Introduction SOFABolt is a network communication framework implemented based on Netty and developed by Ant Finance.\n Netty was developed to let Java programmers focus more on the implementation of network communication-based business logic, and not worry excessively about network low-level NIO implementation or network problems that are difficult to debug. SOFABolt was developed to let middleware developers focus more on the implementation of products\u0026rsquo; functional performance, and not on making the communication framework\u0026rsquo;s wheels over and over again.","tags":null,"title":"SOFABolt overview","type":"projects","url":"/en/projects/sofa-bolt/overview/","wordcount":196},{"author":null,"categories":null,"content":" 功能架构 SOFABolt　的基础功能：  基础通信功能 ( remoting-core )  基于 Netty 高效的网络 IO 与线程模型运用 连接管理 (无锁建连，定时断链，自动重连) 基础通信模型 ( oneway，sync，future，callback ) 超时控制 批量解包与批量提交处理器 心跳与 IDLE 事件处理  协议框架 ( protocol-skeleton )  命令与命令处理器 编解码处理器 心跳触发器  私有协议定制实现 - RPC 通信协议 ( protocol-implementation )  RPC 通信协议的设计 灵活的反序列化时机控制 请求处理超时 FailFast 机制 用户请求处理器 ( UserProcessor ) 双工通信   用法1 将 SOFABolt 用作一个远程通信框架，使用者可以不用关心如何实现一个私有协议的细节，直接使用我们内置的 RPC 通信协议。可以非常简单的启动客户端与服务端，同时注册一个用户请求处理器，即可完成远程调用。同时，像连接管理、心跳等基础功能特性都默认可以使用。 当前支持的调用类型如下图所示：\n 示例 Demo 请参考我们的 用户手册  用法2 将 SOFABolt 用作一个协议框架，使用者可以复用基础的通信模型、协议包含的接口定义等基础功能。然后根据自己设计的私有协议自定义 Command 类型、Command 处理器、编解码处理器等。如下图所示，RPC 和消息的 Command 定义结构：\n","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-functions/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"fde29139cbd8b786326a6479e52814dd","permalink":"/projects/sofa-bolt/sofa-bolt-functions/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-bolt/sofa-bolt-functions/","summary":"功能架构 SOFABolt 的基础功能： 基础通信功能 ( remoting-core ) 基于 Netty 高效的网络 IO 与线程模型运用 连接管理 (无锁建连，定时断链，自动重连) 基础通信模型 ( oneway，","tags":null,"title":"SOFABolt 功能介绍","type":"projects","url":"/projects/sofa-bolt/sofa-bolt-functions/","wordcount":450},{"author":null,"categories":null,"content":" 参与贡献 开放代码允许在签署协议之后,提交贡献代码.\n版权协议 对 SOFABolt 代码的修改和变更，需要遵守版权协议。\n准备工作  贡献代码前需要先了解git工具的使用和GitHub网站的使用。 git 工具用法可以查看git官方书籍,需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章Git协作流程  GitHub 贡献代码流程 提issue 不论你是修复 Bolt 的bug还是新增 Bolt 的功能，在你提交代码之前，在 Bolt 的GitHub上提交一个 issue, 描述你要修复的问题或者要增加的功能。这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突,产生重复工作。Bolt 的维护人员会对你提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少pull request被拒绝的情况。  获取源码 要修改或新增功能，在提issue后，点击左上角的fork按钮，复制一份 Bolt 主干代码到你的代码仓库。\n拉分支 Bolt 所有修改都在分支上进行，修改完后提交 pull request ， 在code review 后由项目维护人员 merge 到主干。 因此，在获取源码步骤介绍后，你需要：\n 下载代码到本地,这一步你可以选择git/https方式.  git clone https://github.com/sofastack/sofa-bolt.git   拉分支准备修改代码  git branch add_xxx_feature   执行完上述命令后，你的代码仓库就切换到相应分支了。执行如下命令可以看到你当前分支：  git branch -a   如果你想切换回主干，执行下面命令:  git checkout -b master   如果你想切换回分支，执行下面命令：  git checkout -b \u0026amp;quot;branchName\u0026amp;quot;   想直接从github上拉取分支到本地  git clone -b branchname https://xxx.git  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致 Bolt 通过 Maven插件来保持代码格式一致.在提交代码前,务必本地执行  mvn clean package   补充单元测试代码 新有修改应该通过已有的单元测试. 应该提供新的单元测试来证明以前的代码存在bugs，而新的代码已经解决了这些bugs.  你可以用如下命令运行所有测试\nmvn clean test  也可以通过IDE来辅助运行.\n其它注意事项  请保持你编辑的代码的原有风格,尤其是空格换行等. 对于无用的注释, 请直接删除 对逻辑和功能不容易被理解的地方添加注释. 及时更新文档 修改完代码后，执行如下命令提交所有修改到本地:  git commit -am \u0026#39;添加xx功能\u0026#39;  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到github上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面你是通过fork来做的,那么那么这里的 origin 是push到你的代码仓库，而不是 Bolt 的代码仓库.\n提交合并代码到主干的请求 在你的代码提交到GitHub后，你就可以发送请求来把你改好的代码合入 Bolt 主干代码了。此时你需要进入你的 GitHub 上的对应仓库，按右上角的 pull request按钮。选择目标分支,一般就是主干master, 系统会通知 Bolt 的人员，Bolt 人员会 review 你的代码，符合要求后就会合入主干，成为 Bolt 主干代码的一部分。\n代码review 在你提交代码后，你的代码会被指派给维护人员review,请保持耐心。如果在数天后，仍然没有人对你的提交给予任何回复，可以在pull request下面留言,并@对应的人员. 对于代码review的意见会提交到对应issue。如果觉得建议是合理的，也请你把这些建议更新到你的补丁中。\n合并代码到主干 在代码 Bolt 通过后，就由 Bolt 维护人员操作合入主干了。这一步不用参与,review合并之后,你会收到合并成功的提示.\nContributing to SOFABolt SOFABolt is released under the Apache 2.0 license, and follows a very standard Github development process, using Github tracker for issues and merging pull requests into master . If you would like to contribute something, or simply want to hack on the code this document should help you get started.\nSign the Contributor License Agreement Before we accept a non-trivial patch or pull request we will need you to sign the Contributor License Agreement. Signing the contributor’s agreement does not grant anyone commit rights to the main repository, but it does mean that we can accept your contributions, and you will get an author credit if we do. Active contributors might be asked to join the core team, and given the ability to merge pull requests.\nCode Conventions None of these is essential for a pull request, but they will all help.\n we provided a code formatter file, it will formatting automatically your project when during process of building.\n Make sure all new .java files to have a simple Javadoc class comment with at least an @author tag identifying you, and preferably at least a paragraph on what the class is for.\n Add the …","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-contribution/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c044ad534cf99e4d6d400113b490f816","permalink":"/projects/sofa-bolt/sofa-bolt-contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-bolt/sofa-bolt-contribution/","summary":"参与贡献 开放代码允许在签署协议之后,提交贡献代码. 版权协议 对 SOFABolt 代码的修改和变更，需要遵守版权协议。 准备工作 贡献代码前需要先了解git工具的使","tags":null,"title":"SOFABolt 参与贡献","type":"projects","url":"/projects/sofa-bolt/sofa-bolt-contribution/","wordcount":1650},{"author":null,"categories":null,"content":" 发展路线 Version 1.5.1  修复项目中代码风格的问题：https://github.com/alipay/sofa-bolt/issues/85 修复项目中已知的BUG：https://github.com/alipay/sofa-bolt/issues/82 RPC 层支持从 IO 线程派发 message list：https://github.com/alipay/sofa-bolt/pull/84  Version 1.6.0 整体目标  统一生命周期组件 抽象并沉淀网络组件的API 收敛配置入口\u0026amp;amp;增强配置的可扩展性  统一生命周期组件 在1.5.x的Bolt版本中，管理组件的生命周期相关的API命名并不统一，比如：\n ReconnectManager不需要启动或者初始化，关闭方法为stop DefaultConnectionMonitor初始化方法为start，关闭的方法为destroy RpcClient初始化方法为init，关闭的方法为shutdown RpcTaskScanner初始化的方法为start，关闭方法为shutdown  在1.6.0版本里，统一了所有组件的生命周期接口：\n 对于有生命周期的组件，即使用前需要进行初始化，使用完毕需要释放资源的，统一提供startup/shutdown接口  抽象并沉淀网络组件的API Bolt中remoting类是网络操作的主要入口，目前以抽象类的形式提供，后续希望对方法进行收敛，暴露对应的接口：\n 标准化，规范使用 沉淀接口，保持稳定 收敛入口，便于内部的代码迭代  在1.5.x的版本中，ReconnectManager类尽管提供了public的addCancelUrl方法，但是这个方法在Bolt项目中没有调用：\n IDE会给出警告 给用户造成困惑：这个方法可否删除？  在1.6.0版本中解决了以上的问题，抽象出一套稳定的API，便于用户使用、提升代码可读性，同时也为后续的迭代打下基础。\n收敛配置入口\u0026amp;amp;增强配置的可扩展性 1.5.x版本的Bolt配置入口有以下几个：\n ProtocolSwitch：协议配置（是否开启CRC校验），通过静态的方法创建配置对象 GlobalSwitch：实例级配置，每个AbstractConfigurableInstance拥有自己的GlobalSwitch配置，默认值取自SystemProperty，可以通过API调整配置 ConfigItem：Netty相关的配置项的枚举，不可以继承拓展（用户需要修改源码） ConfigManager：配置读取入口，通过静态方法读取SystemProperty的配置 Configs：配置项名称的定义和配置项的默认值  整体上看Bolt的配置项比较零散，且对用户来说难以拓展使用，有以接口暴露的配置项、有以静态方法暴露的配置项，配置项可以通过系统参数配置也可以通过API执行配置。\n且Bolt配置项存在相互影响的问题，比如一个产品同时使用了RPC和消息，而RPC和消息底层都依赖于Bolt，那么基于SystemProperty的配置将无法做到RPC和消息的配置隔离。\n在1.6.0版本中对配置模块进行了调整，在兼容当前版本配置的情况下：\n 收敛配置入口，提供统一的配置的编程界面（以类似Netty的Option的方式进行配置） 支持配置隔离，不同的Bolt实例使用不同的配置项 提升配置的可扩展性  ","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-roadmap/","fuzzywordcount":1400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3d4eac90b5c8e657d14eb885ab1f9a92","permalink":"/projects/sofa-bolt/sofa-bolt-roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-bolt/sofa-bolt-roadmap/","summary":"发展路线 Version 1.5.1 修复项目中代码风格的问题：https://github.com/alipay/sofa-bolt/issues/85 修复项目中已","tags":null,"title":"SOFABolt 发展路线","type":"projects","url":"/projects/sofa-bolt/sofa-bolt-roadmap/","wordcount":1356},{"author":null,"categories":null,"content":" 介绍 SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。\n 为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于网络底层 NIO 的实现以及处理难以调试的网络问题，Netty 应运而生。 为了让中间件开发者能将更多的精力放在产品功能特性实现上，而不是重复地一遍遍制造通信框架的轮子，SOFABolt 应运而生。  Bolt 名字取自迪士尼动画-闪电狗，是一个基于 Netty 最佳实践的轻量、易用、高性能、易扩展的通信框架。 这些年我们在微服务与消息中间件在网络通信上解决过很多问题，积累了很多经验，并持续的进行着优化和完善，我们希望能把总结出的解决方案沉淀到 SOFABolt 这个基础组件里，让更多的使用网络通信的场景能够统一受益。 目前该产品已经运用在了蚂蚁中间件的微服务 (SOFARPC)、消息中心、分布式事务、分布式开关、以及配置中心等众多产品上。\n多语言  node python cpp  ","date":-62135596800,"description":"","dir":"projects/sofa-bolt/overview/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5ee08df7c4bbd2c3be846e16f3bc81b1","permalink":"/projects/sofa-bolt/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-bolt/overview/","summary":"介绍 SOFABolt 是蚂蚁金融服务集团开发的一套基于 Netty 实现的网络通信框架。 为了让 Java 程序员能将更多的精力放在基于网络通信的业务逻辑实现上，而不是过多的纠结于","tags":null,"title":"SOFABolt 概述","type":"projects","url":"/projects/sofa-bolt/overview/","wordcount":369},{"author":null,"categories":null,"content":" 用户指南 maven coordinator \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;bolt\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   check release note for version\n 1. 基础功能 1.1 实现用户请求处理器 (UserProcessor) 我们提供了两种用户请求处理器，SyncUserProcessor 与 AsyncUserProcessor。 二者的区别在于，前者需要在当前处理线程以return返回值的形式返回处理结果；而后者，有一个 AsyncContext 存根，可以在当前线程，也可以在异步线程，调用 sendResponse 方法返回处理结果。示例可参考如下两个类：\n 同步请求处理器 异步请求处理器  1.2 实现连接事件处理器 (ConnectionEventProcessor) 我们提供了两种事件监听，建连事件（ConnectionEventType.CONNECT）与断连事件（ConnectionEventType.CLOSE），用户可以创建自己的事件处理器，并注册到客户端或者服务端。客户端与服务端，都可以监听到各自的建连与断连事件。\n 处理连接建立事件 处理连接断开事件  1.3 客户端与服务端初始化 (RpcClient，RpcServer) 我们提供了一个 RpcClient 与 RpcServer，经过简单的必要功能初始化，或者功能开关，即可使用。一个最简单的例子如下：\n 客户端初始化示例 服务端初始化示例  1.4 基础通信模型 我们提供了四种通信模型：\n1.Oneway 调用\n当前线程发起调用后，不关心调用结果，不做超时控制，只要请求已经发出，就完成本次调用。注意 Oneway 调用不保证成功，而且发起方无法知道调用结果。因此通常用于可以重试，或者定时通知类的场景，调用过程是有可能因为网络问题，机器故障等原因，导致请求失败。业务场景需要能接受这样的异常场景，才可以使用。请参考示例。\n2. Sync 同步调用\n当前线程发起调用后，需要在指定的超时时间内，等到响应结果，才能完成本次调用。如果超时时间内没有得到结果，那么会抛出超时异常。这种调用模式最常用。注意要根据对端的处理能力，合理设置超时时间。请参考示例。\n3. Future调用\n当前线程发起调用，得到一个 RpcResponseFuture 对象，当前线程可以继续执行下一次调用。可以在任意时刻，使用 RpcResponseFuture 对象的 get() 方法来获取结果，如果响应已经回来，此时就马上得到结果；如果响应没有回来，则会阻塞住当前线程，直到响应回来，或者超时时间到。请参考示例。\n4. Callback异步调用\n当前线程发起调用，则本次调用马上结束，可以马上执行下一次调用。发起调用时需要注册一个回调，该回调需要分配一个异步线程池。待响应回来后，会在回调的异步线程池，来执行回调逻辑。请参考示例。\n1.5 日志打印 SOFABolt 只依赖 SLF4J 作为日志门面。同时提供了 log4j、log4j2、logback 三种日志模板，使用者只需要在运行时依赖某一种日志实现，我们依赖的 sofa-common-tools 组件，会在运行时动态感知是哪一种日志实现，同时加载正确的日志模板，进行打印。日志会打印在 ~/logs/bolt/ 目录下面，包括如下几种日志：\n common-default.log：默认日志，打印一些客户端、服务器启动、关闭等通信过程的普通日志 common-error.log：异常日志，框架运行过程中出现的异常 connection-event.log：连接事件日志 remoting-rpc.log：RPC 协议相关的日志  关于日志依赖，可以参考日志实现依赖参考\n2. 进阶功能 2.1 请求上下文 在调用过程中，我们还提供了带 InvokeContext 的接口，并一路传递下去，可以在自定义序列化器，用户请求处理器中获得。我们分为两种场景来使用请求上下文：\n 客户端：用户可以设置一些针对本次请求生效的参数，比如序列化类型，是否开启crc等机制。同时可以从上下文中获取建连耗时，连接信息等。 服务端：用户可以从用户请求处理器中获得请求到达后的排队耗时，连接信息等 注意：客户端与服务端的上下文是独立的，即客户端设置的上下文只在客户端可见，对服务端不可见；反之同理。 使用示例  2.2 双工通信 除了服务端可以注册用户请求处理器，我们的客户端也可以注册用户请求处理器。此时，服务端就可以发起对客户端的调用，也可以使用 1.4 提到了任何一种通信模型。\n 示例1：使用 Connection 对象的双工通信，注意使用 Connection 对象的双工通信，服务端需要通过事件监听处理器或者用户请求处理器，自己保存好 Connection 对象。 示例2：使用 Address 的双工通信，注意使用 Address 方式的双工通信，需要在初始化 RpcServer 时，打开 manageConnection 开关，表示服务端会根据客户端发起的建连，维护一份地址与连接的映射关系。默认不需要双工通信的时候，这个功能是关闭的。  2.3 建立多连接与连接预热 通常来说，点对点的直连通信，客户端和服务端，一个 IP 一个连接对象就够用了。不管是吞吐能力还是并发度，都能满足一般业务的通信需求。而有一些场景，比如不是点对点直连通信，而是经过了 LVS VIP，或者 F5 设备的连接，此时，为了负载均衡和容错，会针对一个 URL 地址建立多个连接。我们提供如下方式来建立多连接，即发起调用时传入的 URL 增加如下参数 127.0.0.1:12200?_CONNECTIONNUM=30\u0026amp;amp;_CONNECTIONWARMUP=true，表示针对这个 IP 地址，需要建立30个连接，同时需要预热连接。其中预热与不预热的区别是：\n 预热：即第一次调用（比如 Sync 同步调用），就建立30个连接 不预热：每一次调用，创建一个连接，直到创建满30个连接 使用示例  2.4 自动断连与重连 通常 RPC 调用过程，是不需要断链与重连的。因为每次 RPC 调用过程，都会校验是否有可用连接，如果没有则新建一个。但有一些场景，是需要断链和保持长连接的：\n 自动断连：比如通过 LVS VIP 或者 F5 建立多个连接的场景，因为网络设备的负载均衡机制，有可能某一些连接固定映射到了某几台后端的 RS 上面，此时需要自动断连，然后重连，靠建连过程的随机性来实现最终负载均衡。注意，开启了自动断连的场景，通常需要配合重连使用。 重连：比如客户端发起建连后，由服务端来通过双工通信，发起请求到客户端。此时如果没有重连机制，则无法实现。 使用示例，注意考虑一个进程可能会有多个 SOFABolt 的通信实例，我们提供了全局开关以及用户开关两种开关方式： …","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-handbook/","fuzzywordcount":3600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2a0a2e3c7749dbcdceea064f6f850e33","permalink":"/projects/sofa-bolt/sofa-bolt-handbook/","publishdate":"0001-01-01T00:00:00Z","readingtime":8,"relpermalink":"/projects/sofa-bolt/sofa-bolt-handbook/","summary":"用户指南 maven coordinator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bolt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; check release note for version 1. 基础功能 1.1 实现用户请求处理器 (UserProcessor) 我们提供了两种用户请求处理器，SyncUserProcessor 与 Async","tags":null,"title":"SOFABolt 用户手册","type":"projects","url":"/projects/sofa-bolt/sofa-bolt-handbook/","wordcount":3516},{"author":null,"categories":null,"content":" 相关链接  ISSUES 用户手册 中文介绍文章: 蚂蚁通信框架实践  ","date":-62135596800,"description":"","dir":"projects/sofa-bolt/related-links/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6844d2a639b69fa3128132b8631f33e3","permalink":"/projects/sofa-bolt/related-links/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-bolt/related-links/","summary":"相关链接 ISSUES 用户手册 中文介绍文章: 蚂蚁通信框架实践","tags":null,"title":"SOFABolt 相关链接","type":"projects","url":"/projects/sofa-bolt/related-links/","wordcount":24},{"author":null,"categories":null,"content":" ﻿# Upgrade SOFABoot from 2.3.x/2.4.x to 2.5.x SOFABoot 2.3.x/2.4.x is developed based on Spring Boot 1.4.2.RELEASE, SOFABoot 2.5.x is developed based on Spring Boot 1.5.x. When upgrading SOFABoot 2.3.x/2.4.x to SOFABoot 2.5.x, we should pay special attention to the differences between the Spring Boot 1.5.x upgrade and the Spring Boot 1.4.x upgrade.\nRenamed Spring Boot Starters  spring-boot-starter-ws \u0026amp;ndash;\u0026amp;gt; spring-boot-starter-web-services spring-boot-starter-redis \u0026amp;ndash;\u0026amp;gt; spring-boot-starter-data-redis  Endpoint Security Control Spring Boot 1.5.x has security control over all sensitive endpoints by default, that is, endpoints such as /beans and /dump, which were previously accessible by default in version 1.4.x, are not accessible in version 1.5.x. To access such endpoints, we need to configure Spring Boot as follows: \u0026amp;gt; management.security.enabled=false\nOnly /health, /info, and /docs are accessible by default in version 1.5.x. Please refer to official description for details: + endpoints + Accessing sensitive endpoints\nApplicationEvent Change ApplicationStartedEvent in 1.4.x has been renamed ApplicationStartingEvent in Spring Boot 1.5.x. The version 1.5.x remains forward compatible. Note that the ApplicationStartedEvent event has a completely different meaning in version 2.x.\n** Users who have upgraded the SOFABoot to the version 2.5.x are strongly advised to change ApplicationStartedEvent to ApplicationStartingEvent to avoid compatibility issues when upgrading SOFABoot to the version 3.0.x in the future.**\nProperty renaming  server.max-http-post-size \u0026amp;ndash;\u0026amp;gt; server.tomcat.max-http-post-size spring.data.neo4j.session.scope is removed  Refer to the configuration of Spring Boot 1.5.x changelog\nSummary The above are the major points worthy of notice when we upgrade SOFABoot 2.3.x/2.4.x to SOFABoot 2.5.x. For detailed information, refer to the Release Report of Spring Boot 1.5.x.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/upgrade_2_5_x/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4b7dd4287b00106684831d2a8524a6f7","permalink":"/en/projects/sofa-boot/upgrade_2_5_x/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/upgrade_2_5_x/","summary":"﻿# Upgrade SOFABoot from 2.3.x/2.4.x to 2.5.x SOFABoot 2.3.x/2.4.x is developed based on Spring Boot 1.4.2.RELEASE, SOFABoot 2.5.x is developed based on Spring Boot 1.5.x. When upgrading SOFABoot 2.3.x/2.4.x to SOFABoot 2.5.x, we should pay special attention to the differences between the Spring Boot 1.5.x upgrade and the Spring Boot 1.4.x upgrade.\nRenamed Spring Boot Starters  spring-boot-starter-ws \u0026ndash;\u0026gt; spring-boot-starter-web-services spring-boot-starter-redis \u0026ndash;\u0026gt; spring-boot-starter-data-redis  Endpoint Security Control Spring Boot 1.","tags":null,"title":"SOFABoot 2.5.x upgrade","type":"projects","url":"/en/projects/sofa-boot/upgrade_2_5_x/","wordcount":251},{"author":null,"categories":null,"content":" SOFABoot 2.3.x/2.4.x 升级到 2.5.x SOFABoot 2.3.x/2.4.x 基于 Spring Boot 1.4.2.RELEASE 版本开发，SOFABoot 2.5.x 则是基于 Spring Boot 1.5.x 版本开发。 从 SOFABoot 2.3.x/2.4.x 升级到 SOFABoot 2.5.x 需要重点考虑 Spring Boot 1.5.x 相较 Spring Boot 1.4.x 的升级注意点。\n重命名的 spring boot starters  spring-boot-starter-ws \u0026amp;ndash;\u0026amp;gt; spring-boot-starter-web-services spring-boot-starter-redis \u0026amp;ndash;\u0026amp;gt; spring-boot-starter-data-redis  endpoint 安全性控制 Spring Boot 1.5.x 对所有 Sensitive Endpoint 默认进行了安全管控，即之前在 1.4.x 默认能访问的诸如 /beans, /dump 等 endpoints 在 1.5.x 版本均不能访问。如果需要访问，需要配置： \u0026amp;gt; management.security.enabled=false\n默认情况下，在 1.5.x 只有 /health, /info, /docs 能够访问。详细请参考官方描述： + endpoints + Accessing sensitive endpoints\nApplicationEvent 变更 Spring Boot 1.5.x 将 1.4.x 中的 ApplicationStartedEvent 重命名为 ApplicationStartingEvent，在 1.5.x 仍然保持向前兼容。需要格外注意的是，在 2.x 版本中，ApplicationStartedEvent 事件意义完全不一样。\n强烈建议升级到 SOFABoot 2.5.x 的用户，将应用中使用的 ApplicationStartedEvent 变更为 ApplicationStartingEvent；避免今后升级至 SOFABoot 3.0.x 出现兼容性问题\nProperty 重命名  server.max-http-post-size \u0026amp;ndash;\u0026amp;gt; server.tomcat.max-http-post-size spring.data.neo4j.session.scope 被移除  具体参考 Spring Boot 1.5.x 配置的 changelog\n总结 以上总结了从 SOFABoot 2.3.x/2.4.x 升级到 SOFABoot 2.5.x 的几个主要注意点，详细可以参考 Spring Boot 1.5.x 的发布报告\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/upgrade_2_5_x/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4b7dd4287b00106684831d2a8524a6f7","permalink":"/projects/sofa-boot/upgrade_2_5_x/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/upgrade_2_5_x/","summary":"SOFABoot 2.3.x/2.4.x 升级到 2.5.x SOFABoot 2.3.x/2.4.x 基于 Spring Boot 1.4.2.RELEASE 版本开发，SOFABoot 2.5.x 则是基于 Spring Boot 1.5.x 版本开发。 从 SOFABoot 2.3.x/2.4.x 升级到 SOFABoot 2.5.x 需要重点考虑 Spring Boot 1.5.x 相较 Spring Boot 1.4.x 的升级注意点。 重命","tags":null,"title":"SOFABoot 2.5.x 升级注意事项","type":"projects","url":"/projects/sofa-boot/upgrade_2_5_x/","wordcount":404},{"author":null,"categories":null,"content":" ﻿## Preface As a Spring Boot-based development framework open sourced by Ant Financial, SOFABoot provides capabilities such as Readiness Check, class isolation, and log space isolation. In addition to enhancing the Spring Boot, SOFABoot provides users with the capability to easily use SOFA middleware in Spring Boot.\nWe have received a lot of feedback from community users since SOFABoot was open sourced in April 2018. We are also very pleased to see many community users take an active part in building the SOFAStack open source, which greatly increases our determination to prosper SOFAStack community and ecosystem. Here, we announce the release of the SOFABoot 3.0, which is developed based on Spring Boot 2.0. SOFABoot 3.0 allows us to seamlessly integrate the extension capability of SOFABoot with official components of Spring Boot 2.x. In addition, SOFABoot 3.0 is compatible with Spring Cloud components, which allows us to easily integrate Spring Cloud components like Zuul and Config in the SOFABoot framework.\nBelow are the major changes of SOFABoot 3.0 compared with SOFABoot 2.x.\nUpgrade Spring Boot to version 2.x Upgrade Spring Boot in SOFABoot 3.0 to version 2.0. As the Spring Boot community recently announced that the maintenance for version 1.x will end in August 2019, we will focus on SOFABoot 3.x in the future and will release SOFABoot 3.1 with Spring Boot upgraded to version 2.1 soon.\nSpring Cloud compatible Some components are not compatible with SOFABoot in SOFABoot 2.x. In SOFABoot 3.x, we have run thorough compatibility tests on Spring Cloud components and fixed all problems found to ensure good compatibility between SOFABoot 3.x and Spring Cloud.\nWebFlux framework compatible Spring Boot 2.x introduces the WebFlux framework. SOFABoot 3.x is compatible with WebFlux in two major aspects; + Health Check is compatible with the ReactiveHealthIndicator extension interface. The Readiness Check will include the implementation of the interface extension; + Compatible with buried points of WebFlux web requests. Point burying logs and files are compatible with common MVC requests. For detailed information, refer to MVC point burying request.\nJDK version support SOFABoot 3.x must run on JDK 8 or higher versions and does not support JDK 6 and JDK 7.\nHealth Check SOFABoot adds Readiness Check capability to Spring Boot\u0026amp;rsquo;s Health Check capability, to ensure that all components and operations are in a healthy state before the application goes into services. Compared with SOFABoot 2.x, SOFABoot 3.0 features great adjustments to the Health Check. It abandons some internal compatibility logic of Ant Financial and uses a friendlier coding scheme. Besides, the Health Check of SOFABoot 3.0 offers extensions in various scenarios, supports the \u0026amp;lsquo;ReactiveHealthIndicator\u0026amp;rsquo; extension interface introduced in Spring Boot 2.x, and provides more Health Check extension features.\nAdjust the Readiness Check Endpoint path The Endpoint for checking the …","date":-62135596800,"description":"","dir":"projects/sofa-boot/upgrade_3_x/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"91ba09adf6bc42aaf70645b9a19b409b","permalink":"/en/projects/sofa-boot/upgrade_3_x/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-boot/upgrade_3_x/","summary":"﻿## Preface As a Spring Boot-based development framework open sourced by Ant Financial, SOFABoot provides capabilities such as Readiness Check, class isolation, and log space isolation. In addition to enhancing the Spring Boot, SOFABoot provides users with the capability to easily use SOFA middleware in Spring Boot.\nWe have received a lot of feedback from community users since SOFABoot was open sourced in April 2018. We are also very pleased to see many community users take an active part in building the SOFAStack open source, which greatly increases our determination to prosper SOFAStack community and ecosystem.","tags":null,"title":"SOFABoot 3.0 upgrade","type":"projects","url":"/en/projects/sofa-boot/upgrade_3_x/","wordcount":910},{"author":null,"categories":null,"content":" 前言 SOFABoot 是蚂蚁金服开源的基于 Spring Boot 的研发框架，它在 Spring Boot 的基础上，提供了诸如 Readiness Check，类隔离，日志空间隔离等能力。在增强了 Spring Boot 的同时，SOFABoot 提供了让用户可以在 Spring Boot 中非常方便地使用 SOFA 中间件的能力。\n自今年 4 月份 SOFABoot 开源至今，我们收到了非常多来自社区同学的反馈，也非常高兴的看到很多社区同学积极的参与到 SOFAStack 开源共建，这极大了鼓舞了我们建设 SOFAStack 开源社区的决心，力图把 SOFAStack 社区和生态建设更加繁荣。在此，我们宣布推出 SOFABoot 3.0 版本，SOFABoot 3.0 是基于 Spring Boot 2.0 版本开发。在 SOFABoot 3.0 中，可以将 SOFABoot 扩展能力和 Spring Boot 2.x 官方组件无缝集成。此外，我们在 SOFABoot 3.0 中兼容了 Spring Cloud 组件集成，可以很方便地在 SOFABoot 框架中集成 Spring Cloud 组件，如 Zuul, Config 等。\n以下，本文将详细介绍 SOFABoot 3.0 相较 SOFABoot 2.x 的变更。\nSpring Boot 升级 2.x SOFABoot 3.0 版本升级 Spring Boot 版本至 2.0。鉴于Spring Boot 社区最近刚公告 1.x 版本将维护至明年 8 月份为止，未来，我们也将主力维护 SOFABoot 3.x 版本，近期也将发布 SOFABoot 3.1 升级 Spring Boot 版本至 2.1。\nSpring Cloud 兼容 在 SOFABoot 2.x 中，存在部分组件和 SOFABoot 兼容性问题。在 SOFABoot 3.x 中，对 Spring Cloud 各组件进行了比较完备的兼容性测试和问题修复，保证了 SOFABoot 3.x 与 Spring Cloud 良好的兼容性。\nWebFlux 框架兼容 Spring Boot 2.x 引入了 WebFlux 框架，SOFABoot 3.x 主要在两个方面兼容了 WebFlux 框架； + 健康检查兼容了 ReactiveHealthIndicator 扩展接口，业务对这个接口的扩展实现将会纳入到 Readiness Check； + 兼容对 WebFlux 网络请求进行埋点，埋点日志格式和文件保持对普通 MVC 请求兼容，详细参考MVC 埋点请求\nJDK 版本支持 SOFABoot 3.x 最低要求运行在 JDK 8 及其以上版本，不支持 JDK 6，7。\n健康检查 SOFABoot 为 Spring Boot 的健康检查能力增加了 Readiness Check 能力，以确保应用在正常对外服务前，所有组件及业务本身处于健康状态。相较于与 SOFABoot 2.x, SOFABoot 3.0 在健康检查做了很大的重构，主要是剥离了部分蚂蚁金服内部兼容逻辑，采用更加友好的编码方案；其次，SOFABoot 3.0 健康检查提供了多种不同场景下的健康检查扩展形式，支持 Spring Boot 2.x 引入的 ReactiveHealthIndicator 扩展接口，丰富了健康检查扩展特性。\n调整 Readiness Check Endpoint 路径 在 SOFABoot 2.x 中，查看健康检查结果的 Endpoint 为 /health/readiness，而在 SOFABoot 3.0 中，变更为 /actuator/readiness。\n扩展接口变更 在 SOFABoot 3.x 中，提供四种方式扩展健康检查，分别是 + HealthChecker + HealthIndicator(Spring Boot 原生) + ReactiveHealthIndicator(Spring Boot 原生) + ReadinessCheckCallback\n这四个接口的扩展实现执行顺序是 HealthChecker \u0026amp;gt; HealthIndicator, ReactiveHealthIndicator \u0026amp;gt; ReadinessCheckCallback，相同接口的扩展实现执行顺序则遵循 Spring Boot 标准的方案。即扩展类可以额外实现两个标准的 Order 接口：\n org.springframework.core.Ordered org.springframework.core.PriorityOrdered  或者使用注解\n org.springframework.core.annotation.Order  这些接口的扩展实现处理结果将会在健康检查结果查中展现。\n删除 SofaBootBeforeHealthCheckEvent 事件 在 SOFABoot 2.x 中，我们没有提供支持对健康检查扩展实现进行排序，导致用户无法预期自身扩展执行时机。如上述，SOFABoot 3.x 支持各组件扩展实现的排序，因此该事件可以统一使用 HealthChecker 接口和高优先级顺序实现替代。其次，在 SOFABoot 2.x 中，SofaBootBeforeHealthCheckEvent 事件的处理逻辑结果并不会反应在健康检查结果中，使用 HealthChecker 替代之后，这部分逻辑处理自然变成健康检查的一部分，可供查看。\n删除 DefaultHealthChecker 接口 使用 JDK8 默认方法特性，删除 DefaultHealthChecker 接口，用户可以直接使用 HealthChecker 接口替代 DefaultHealthChecker.\n删除 SofaBootMiddlewareAfterReadinessCheckCallback 和 SofaBootAfterReadinessCheckCallback 接口 在 SOFABoot 2.x 中，这两个接口是两种场景下的健康检查回调；推荐 SOFABoot 官方 Starter 使用 SofaBootMiddlewareAfterReadinessCheckCallback，而业务应用推荐使用SofaBootAfterReadinessCheckCallback，框架将优先执行 SofaBootMiddlewareAfterReadinessCheckCallback 的扩展实现，然后执行 SofaBootAfterReadinessCheckCallback 的扩展实现。这样的设计有两个缺陷： + 两个接口本质没有区别，但是隐藏了先后顺序逻辑，给用户引入了额外的学习成本； + 只考虑了 SofaBootMiddlewareAfterReadinessCheckCallback 和 SofaBootAfterReadinessCheckCallback 两个接口的顺序，但无法保证相同接口实现的执行顺序。\nSOFABoot 3.x …","date":-62135596800,"description":"","dir":"projects/sofa-boot/upgrade_3_x/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"91ba09adf6bc42aaf70645b9a19b409b","permalink":"/projects/sofa-boot/upgrade_3_x/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/upgrade_3_x/","summary":"前言 SOFABoot 是蚂蚁金服开源的基于 Spring Boot 的研发框架，它在 Spring Boot 的基础上，提供了诸如 Readiness Check，类隔离，日志空间隔离等能力。在增强了 Spring Boot 的同时，SOFA","tags":null,"title":"SOFABoot 3.0 升级注意事项","type":"projects","url":"/projects/sofa-boot/upgrade_3_x/","wordcount":1687},{"author":null,"categories":null,"content":" SOFABoot supports modular isolation. But in actual usage scenarios, There is one case that beans in one module sometimes need to open some entries for another module to expand. SOFABoot draws on and uses the Nuxeo Runtime project and the nuxeo project and expands on it, provides the ability to extend points with Spring, We call it Extension Point.\nUsage Using extension point capabilities in SOFABoot requires the following three steps:\nDefine a bean that provides extension capabilities When using the SOFABoot extension point capability, you first need to define an interface that needs to be extended, like:\npackage com.alipay.sofa.boot.test; public interface IExtension { String say(); }  Define the implementation of this interface:\npackage com.alipay.sofa.boot.test.impl; public class ExtensionImpl implements IExtension { private String word; @Override public String say() { return word; } public void setWord(String word) { this.word = word; } public void registerExtension(Extension extension) throws Exception { Object[] contributions = extension.getContributions(); String extensionPoint = extension.getExtensionPoint(); if (contributions == null) { return; } for (Object contribution : contributions) { if (\u0026amp;quot;word\u0026amp;quot;.equals(extensionPoint)) { setWord(((ExtensionDescriptor) contribution).getValue()); } } } }  Here you can see that there is a method: registerExtension, you can temporarily ignore this method, and later will introduce its specific role.\nIn the module\u0026amp;rsquo;s Spring configuration file, we add configuration of this bean:\n\u0026amp;lt;bean id=\u0026amp;quot;extension\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.boot.test.impl.ExtensionImpl\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;word\u0026amp;quot; value=\u0026amp;quot;Hello, world\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt;  Defining extension points There is a field word in the above bean. In practice, we want this field to be overridden by other module customizations. Here we expose it as an extension point.\nFirst, you need a class to describe this extension point:\n@XObject(\u0026amp;quot;word\u0026amp;quot;) public class ExtensionDescriptor { @XNode(\u0026amp;quot;value\u0026amp;quot;) private String value; public String getValue() { return value; } }  Then define the extension point in xml:\n\u0026amp;lt;sofa:extension-point name=\u0026amp;quot;word\u0026amp;quot; ref=\u0026amp;quot;extension\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:object class=\u0026amp;quot;com.alipay.sofa.boot.test.extension.ExtensionDescriptor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:extension-point\u0026amp;gt;  among them: - name is the name of the extension point - ref is the bean to which the extension point is applied - object is a concrete description of the contribution point of the extension point. This description is done by XMap (XMap is used to map Java objects and XML files. It is recommended to search XMap documents on the Internet to understand XMap)\nDefining extension implements The above has defined the extension point, and we can extend this bean at this point:\n\u0026amp;lt;sofa:extension bean=\u0026amp;quot;extension\u0026amp;quot; point=\u0026amp;quot;word\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:content\u0026amp;gt; \u0026amp;lt;word\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/extension/","fuzzywordcount":1300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"93225b6c1f2b68f2047a7cf49b76650b","permalink":"/en/projects/sofa-boot/extension/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/extension/","summary":"SOFABoot supports modular isolation. But in actual usage scenarios, There is one case that beans in one module sometimes need to open some entries for another module to expand. SOFABoot draws on and uses the Nuxeo Runtime project and the nuxeo project and expands on it, provides the ability to extend points with Spring, We call it Extension Point. Usage Using extension point capabilities in SOFABoot requires the following three","tags":null,"title":"SOFABoot Extension Point","type":"projects","url":"/en/projects/sofa-boot/extension/","wordcount":1279},{"author":null,"categories":null,"content":" Spring 框架从 3.1.X 版本开始提供了 profile 功能: Bean Definition Profiles，SOFABoot 支持模块级 profile 能力，即在各个模块启动的时候决定模块是否能够启动。\n使用 Module-Profile 激活 module 使用 SOFABoot 的 profile 功能，需要在 application.properties 文件增加 com.alipay.sofa.boot.active-profiles 字段，该字段的值为逗号分隔的字符串，表示允许激活的 profile 列表，指定该字段后，SOFABoot 会为每个可以激活的模块指定此字段表示的 profile 列表。\nSOFABoot 模块的 sofa-module.properties 文件支持 Module-Profile 字段，该字段的值为逗号分隔的字符串，表示当前模块允许在哪些 profile 激活。Module-Profile 支持取反操作， !dev 表示 com.alipay.sofa.boot.active-profiles 不包含 dev 时被激活。\n当应用未指定 com.alipay.sofa.boot.active-profiles 参数时，表示应用所有模块均可启动。SOFABoot 模块未指定 Module-Profile 时，表示当前 SOFABoot 模块可以在任何 profile 启动。\n使用例子 激活 dev SOFABoot 模块 application.properties 中增加配置如下：\ncom.alipay.sofa.boot.active-profiles=dev  该配置表示激活 profile 为 dev 的模块。\n在每个需要限定为 dev profile 被激活模块的 sofa-module.properties 文件中增加如下配置：\nModule-Profile=dev  配置多个激活 profile application.properties 中增加配置如下：\ncom.alipay.sofa.boot.active-profiles=dev,test  该配置表示激活 profile 为 dev 或者 test 的模块。\n在 SOFABoot 模块的 sofa-module.properties 文件中增加如下配置：\nModule-Profile=test,product  该配置表示当 com.alipay.sofa.boot.active-profiles 包含 test 或者 product 时激活模块，由于当前指定的 com.alipay.sofa.boot.active-profiles 为 dev,test ，此模块将被激活。\nModule-Profile 取反 application.properties 中增加配置如下：\ncom.alipay.sofa.boot.active-profiles=dev  该配置表示激活 profile 为 dev 的模块。\n在 SOFABoot 模块的 sofa-module.properties 文件中增加如下配置：\nModule-Profile=!product  该配置表示当 com.alipay.sofa.boot.active-profiles 不包含 product 时激活模块，由于当前指定的 com.alipay.sofa.boot.active-profiles 为 dev ，此模块将被激活。\n设置激活模块 Spring 上下文的 spring.profiles.active 属性 application.properties 中增加配置如下：\ncom.alipay.sofa.boot.active-profiles=dev,test  该配置表示激活 profile 为 dev 或者 test 的模块，当一个模块满足上面的激活条件时，这个模块就会被启动，同时 Spring 上下文的环境信息 spring.profiles.active 也被设置为了 dev,test ，这样如下的配置 beanId 为 devBeanId 和 testBeanId 的bean都会被激活。\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xmlns:jdbc=\u0026amp;quot;http://www.springframework.org/schema/jdbc\u0026amp;quot; xmlns:jee=\u0026amp;quot;http://www.springframework.org/schema/jee\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\u0026amp;quot; default-autowire=\u0026amp;quot;byName\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;beans profile=\u0026amp;quot;dev\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;devBeanId\u0026amp;quot; class=\u0026amp;quot;com.alipay.cloudenginetest.sofaprofilesenv.DemoBean\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;name\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;value\u0026amp;gt;demoBeanDev\u0026amp;lt;/value\u0026amp;gt; \u0026amp;lt;/property\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt; \u0026amp;lt;beans profile=\u0026amp;quot;test\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;testBeanId\u0026amp;quot; class=\u0026amp;quot;com.alipay.cloudenginetest.sofaprofilesenv.DemoBean\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;name\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;value\u0026amp;gt;demoBeanTest\u0026amp;lt;/value\u0026amp;gt; \u0026amp;lt;/property\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofaboot-profile/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b29d568fa057cad0b440790d5cc65d07","permalink":"/projects/sofa-boot/sofaboot-profile/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofaboot-profile/","summary":"Spring 框架从 3.1.X 版本开始提供了 profile 功能: Bean Definition Profiles，SOFABoot 支持模块级 profile 能力，即在各个模块启动的时候决定模块是否能够启动。 使用 Module-Profile 激","tags":null,"title":"SOFABoot Profile","type":"projects","url":"/projects/sofa-boot/sofaboot-profile/","wordcount":666},{"author":null,"categories":null,"content":" Background kc-sofastack-demo has introduced how to quickly build an e-commerce microservice application and has implemented the service calling link tracking and application status monitoring.\nIn e-commerce system, the platforms often are not satisfied with the default product listing order, and always want to arrange some products in the conspicuous places. Also, there are some cases where the platforms would like to show different products to different users based on the collected user behaviors.\nBased on the background of kc-sofastack-demo, this guide will implement sorting the products dynamically based on the total amount of products of each onsite attendee.\nDemo content Implement the dynamic change of product sorting via the dynamic module capability provided by SOFABoot and the dynamic module control capability of SOFADashboard.\nImplement the change of application behavior without restarting the host and without changing the application configuration.\nThe project architecture is as follows:\nTasks 1. Preparation Clone the demo from GitHub to local\ngit clone https://github.com/sofastack-guides/kc-sofastack-dynamic-demo.git  Then, import the project into IDEA or Eclipse.\n2. Package SOFABoot project as Ark JAR As shown in the following screenshot, add the Ark package plugin in the POM file and configure it:\nStep 1: Copy the Ark plugin and configuration to the specified positions in the above screenshot \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;0.6.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;!-- package configuration of ark-biz JAR --\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!-- Whether to package, install and publish ark biz. The default value is false. For details, see Ark Biz documentation.--\u0026amp;gt; \u0026amp;lt;attach\u0026amp;gt;true\u0026amp;lt;/attach\u0026amp;gt; \u0026amp;lt;!-- The directory for ark package and ark biz package, defaulting to the build directory of project--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;!-- The priority of starting ark-biz package. The smaller the value, the higher the priority.--\u0026amp;gt; \u0026amp;lt;priority\u0026amp;gt;200\u0026amp;lt;/priority\u0026amp;gt; \u0026amp;lt;!--Set the root directory of application, used to read ${base.dir}/conf/ark/bootstrap.application configuration file and defaulting to ${project.basedir}--\u0026amp;gt; \u0026amp;lt;baseDir\u0026amp;gt;../\u0026amp;lt;/baseDir\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  Step 2: Run mvn clean package to package the project. The successfully packaged JAR file is as shown in the following screenshot:\n3. Build host application In the downloaded project, dynamic-stock-mng is the host application model. In this task, we will build dynamic-stock-mng as the host application of dynamic module.\nStep 1: Introduce Ark …","date":-62135596800,"description":"This guide introduce how to implement the merged deployment and dynmaic module push provided by SOFAArck based on the Ark control function of SOFADashboard.","dir":"guides/kc-sofastack-dynamic-demo/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8bfd4a50e21ce9fc867b1cf18a8c9af3","permalink":"/en/guides/kc-sofastack-dynamic-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/guides/kc-sofastack-dynamic-demo/","summary":"Background kc-sofastack-demo has introduced how to quickly build an e-commerce microservice application and has implemented the service calling link tracking and application status monitoring.\nIn e-commerce system, the platforms often are not satisfied with the default product listing order, and always want to arrange some products in the conspicuous places. Also, there are some cases where the platforms would like to show different products to different users based on the collected user behaviors.","tags":null,"title":"SOFABoot dynamic module practice","type":"guides","url":"/en/guides/kc-sofastack-dynamic-demo/","wordcount":630},{"author":null,"categories":null,"content":" SOFABoot is a development framework open sourced by Ant Financial which is based on Spring Boot, provides capabilities such as Readiness Check, class isolation, and log space isolation. In addition to enhancing the Spring Boot, SOFABoot provides users with the capability to easily use SOFA middleware in Spring Boot.\nYou can view all the release notes in Release History. The correspondence between SOFABoot version and Spring Boot version is as follows:\n   SOFABoot version Spring Boot version     2.3.x 1.4.2.RELEASE   2.4.x 1.4.2.RELEASE   2.5.x 1.5.16.RELEASE   3.0.x 2.0.3.RELEASE   3.1.0 2.1.0.RELEASE    That is, the SOFABoot 2.3.x and 2.4.x series are based on Spring Boot 1.4.2.RELEASE; SOFABoot 2.5.x series are based on Spring Boot 1.5.x; SOFABoot 3.x series are based on Spring Boot 2.x. You can view and get the codes of all revisions in Release History. In addition, to facilitate users in the community to learn the latest development version of SOFABoot, we will release the SNAPSHOT version, which is a branch of the current development. To successfully pull the SNAPSHOT package from the central repository, it\u0026amp;rsquo;s necessary to add the following profile configuration to the local maven setting.xml file:\n\u0026amp;lt;profile\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;activation\u0026amp;gt; \u0026amp;lt;activeByDefault\u0026amp;gt;true\u0026amp;lt;/activeByDefault\u0026amp;gt; \u0026amp;lt;/activation\u0026amp;gt; \u0026amp;lt;repositories\u0026amp;gt; \u0026amp;lt;repository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/repository\u0026amp;gt; \u0026amp;lt;/repositories\u0026amp;gt; \u0026amp;lt;pluginRepositories\u0026amp;gt; \u0026amp;lt;pluginRepository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/pluginRepository\u0026amp;gt; \u0026amp;lt;/pluginRepositories\u0026amp;gt; \u0026amp;lt;/profile\u0026amp;gt;  Feature Description Based on Spring Boot, SOFABoot provides the following capabilities:\n Capability of expanding the Health Check of Spring Boot: Provide the Readiness Check based on the Health Check of Spring Boot, to ensure a secure launch of application examples. Capability of log space isolation: The middleware framework automatically finds the application\u0026amp;rsquo;s logs and realizes dependence on the logs and independent log printing, avoiding binding the middleware and the application logs. The capability is achieved through sofa-common-tools. Capability of providing class isolation: Provide class isolation based on the SOFAArk framework, making it easy for users to solve various class conflicts. Capability of providing modular development: Based on the Spring context isolation, provide modular development capability, with a separate Spring context for each SOFABoot module, to avoid BeanId conflicts between different SOFABoot modules. Integrated management of middleware: manage in a unified manner, provide a unified and easy-to-use …","date":-62135596800,"description":"","dir":"projects/sofa-boot/overview/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"fe6aed461c61b86dfed846a2dc0b7dcb","permalink":"/en/projects/sofa-boot/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/overview/","summary":"SOFABoot is a development framework open sourced by Ant Financial which is based on Spring Boot, provides capabilities such as Readiness Check, class isolation, and log space isolation. In addition to enhancing the Spring Boot, SOFABoot provides users with the capability to easily use SOFA middleware in Spring Boot.\nYou can view all the release notes in Release History. The correspondence between SOFABoot version and Spring Boot version is as follows:","tags":null,"title":"SOFABoot overview","type":"projects","url":"/en/projects/sofa-boot/overview/","wordcount":461},{"author":null,"categories":null,"content":" ﻿Since 3.1.X Spring framework has started to support the profile function: Bean Definition Profiles, SOFABoot support modular-level profiling, it will determine whether a module can be started when each module is getting started.\nActivating Module Using Module-Profile To enable the SOFABoot profiling, we need to add the com.alipay.sofa.boot.active-profiles field in the application.properties file. The value of this field is a comma-separated string denoting a list of profiles allowed to be activated. After specifying it, SOFABoot will specify a profile list represented by the field for each module that can be activated.\nThe sofa-module.properties file of the SOFABoot module supports the Module-Profile field, which points to a comma-separated string of values representing which profiles are allowed to be activated. Module-Profile supports the inversion operation, !dev indicates that com.alipay.sofa.boot.active-profiles is activated when it does not contain dev.\nIf the value of the com.alipay.sofa.boot.active-profiles field is not specified in the application, all modules are allowed to be started. If the Module-Profile is not specified in the SOFABoot module, the current SOFABoot module can be started with any profile.\nExample Activating the dev SOFABoot Module Add the following configurations to the application.properties file:\ncom.alipay.sofa.boot.active-profiles=dev  With this configuration, the module with dev profile will be activated.\nAdd the following configuration to each sofa-module.properties file where modules with dev profile need to be activated.\nModule-Profile=dev  Configuring Multiple Activation Profiles Add the following configurations to the application.properties file:\ncom.alipay.sofa.boot.active-profiles=dev,test  With this configuration, the modules with dev or test profile will be activated.\nAdd the following configuration to the SOFABoot\u0026amp;rsquo;s sofa-module.properties file:\nModule-Profile=test,product  With this configuration, the module will be activated when the com.alipay.sofa.boot.active-profiles contains test or product. Since the com.alipay.sofa.boot.active-profiles is specified as dev and test, this module will be activated.\nThe Inverted Module-Profile Add the following configurations to the application.properties file:\ncom.alipay.sofa.boot.active-profiles=dev  With this configuration, the module with dev profile will be activated.\nAdd the following configuration to the SOFABoot\u0026amp;rsquo;s sofa-module.properties file:\nModule-Profile=!product  This will activate the module when the com.alipay.sofa.boot.active-profiles does not contain product. Since it is specified as dev, this module will be activated.\nSet the spring.profiles.active property that is used to activate the Spring context of the module. Add the following configurations to the application.properties file:\ncom.alipay.sofa.boot.active-profiles=dev,test  With this configuration, the modules with dev or test profile will be activated. If a module meets those …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofaboot-profile/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b29d568fa057cad0b440790d5cc65d07","permalink":"/en/projects/sofa-boot/sofaboot-profile/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-boot/sofaboot-profile/","summary":"﻿Since 3.1.X Spring framework has started to support the profile function: Bean Definition Profiles, SOFABoot support modular-level profiling, it will determine whether a module can be started when each module is getting started.\nActivating Module Using Module-Profile To enable the SOFABoot profiling, we need to add the com.alipay.sofa.boot.active-profiles field in the application.properties file. The value of this field is a comma-separated string denoting a list of profiles allowed to be activated.","tags":null,"title":"SOFABoot profile","type":"projects","url":"/en/projects/sofa-boot/sofaboot-profile/","wordcount":450},{"author":null,"categories":null,"content":" SOFABoot 是蚂蚁金服开源的基于 Spring Boot 的研发框架，它在 Spring Boot 的基础上，提供了诸如 Readiness Check，类隔离，日志空间隔离等能力。在增强了 Spring Boot 的同时，SOFABoot 提供了让用户可以在 Spring Boot 中非常方便地使用 SOFA 中间件的能力。\n你可以在发布历史中查看所有的发布报告，SOFABoot 版本和 Spring Boot 版本对应关系如下：\n   SOFABoot 版本 Spring Boot 版本     2.3.x 1.4.2.RELEASE   2.4.x 1.4.2.RELEASE   2.5.x 1.5.16.RELEASE   3.0.x 2.0.3.RELEASE   3.1.x 2.1.0.RELEASE   3.2.x 2.1.0.RELEASE   3.3.0～3.3.1 2.1.11.RELEASE   3.3.2 及以后 2.1.13.RELEASE    即 SOFABoot 2.3.x 和 2.4.x 系列版本构建在 Spring Boot 1.4.2.RELEASE 基础之上；SOFABoot 2.5.x 系列版本构建在 Spring Boot 1.5.x 基础之上；SOFABoot 3.x 系列版本将构建在 Spring Boot 2.x 基础之上。你可以在发布历史中查看获取所有的历史版本代码。另外为了方便社区同学能够基于最新开发版本的 SOFABoot 进行开发学习，我们会发布当前开发分支的 SNAPSHOT 版本。为顺利从中央仓库拉取 SNAPSHOT 包，需要在本地 maven setting.xml 文件增加如下 profile 配置:\n\u0026amp;lt;profile\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;activation\u0026amp;gt; \u0026amp;lt;activeByDefault\u0026amp;gt;true\u0026amp;lt;/activeByDefault\u0026amp;gt; \u0026amp;lt;/activation\u0026amp;gt; \u0026amp;lt;repositories\u0026amp;gt; \u0026amp;lt;repository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/repository\u0026amp;gt; \u0026amp;lt;/repositories\u0026amp;gt; \u0026amp;lt;pluginRepositories\u0026amp;gt; \u0026amp;lt;pluginRepository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/pluginRepository\u0026amp;gt; \u0026amp;lt;/pluginRepositories\u0026amp;gt; \u0026amp;lt;/profile\u0026amp;gt;  目前 SOFABoot 最新版本为 3.1.0，基于 Spring Boot 2.1.0.RELEASE, 支持 JDK11。\n功能描述 SOFABoot 在 Spring Boot 基础上，提供了以下能力：\n 扩展 Spring Boot 健康检查的能力：在 Spring Boot 健康检查能力基础上，提供了 Readiness Check 的能力，保证应用实例安全上线。 提供模块化开发的能力：基于 Spring 上下文隔离提供模块化开发能力，每个 SOFABoot 模块使用独立的 Spring 上下文，避免不同 SOFABoot 模块间的 BeanId 冲突。 增加模块并行加载和 Spring Bean 异步初始化能力，加速应用启动； 增加日志空间隔离的能力：中间件框架自动发现应用的日志实现依赖并独立打印日志，避免中间件和应用日志实现绑定，通过 sofa-common-tools 实现。 增加类隔离的能力：基于 SOFAArk 框架提供类隔离能力，方便使用者解决各种类冲突问题。 增加中间件集成管理的能力：统一管控、提供中间件统一易用的编程接口、每一个 SOFA 中间件都是独立可插拔的组件。 提供完全兼容 Spring Boot的能力：SOFABoot 基于 Spring Boot 的基础上进行构建，并且完全兼容 Spring Boot。  应用场景 SOFABoot 本身就脱胎于蚂蚁金服内部对于 Spring Boot 的实践，补充了 Spring Boot 在大规模金融级生产场景下一些不足的地方，所以 SOFABoot 特别适合于这样的场景。\n当然，SOFABoot 的每个组件都是可选的，用户可以灵活选择其中的功能来使用，比如如果仅仅想在 Spring Boot 下面引入 SOFA 中间件，可以不需引入 SOFABoot 中的类隔离能力。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/overview/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"fe6aed461c61b86dfed846a2dc0b7dcb","permalink":"/projects/sofa-boot/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/overview/","summary":"SOFABoot 是蚂蚁金服开源的基于 Spring Boot 的研发框架，它在 Spring Boot 的基础上，提供了诸如 Readiness Check，类隔离，日志空间隔离等能力。在增强了 Spring Boot 的同时，SOFABo","tags":null,"title":"SOFABoot 介绍","type":"projects","url":"/projects/sofa-boot/overview/","wordcount":874},{"author":null,"categories":null,"content":" SOFABoot 提供了类隔离框架 SOFAArk, 弥补了 Spring Boot 在类隔离能力上的缺失，用以解决在实际开发中常见的类冲突、包冲突问题，详细请参考 SOFAArk。\n在 SOFABoot 工程中使用类隔离能力，只需两步操作；配置 sofa-ark-maven-plugin 打包插件以及引入 sofa-ark-springboot-starter 类隔离框架依赖；\n配置 Maven 打包插件 官方提供了 Maven 插件 - sofa-ark-maven-plugin ，只需要简单的配置项，即可将 SpringBoot 工程打包成标准格式规范的可执行 Ark 包，插件坐标为：\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  配置模板如下：\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--specify destination where executable-ark-jar will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;!-- all class exported by ark plugin would be resolved by ark biz in default, if configure denyImportClasses, then it would prefer to load them by ark biz itself --\u0026amp;gt; \u0026amp;lt;denyImportClasses\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.SampleClass1\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.SampleClass2\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;/denyImportClasses\u0026amp;gt; \u0026amp;lt;!-- Corresponding to denyImportClasses, denyImportPackages is package-level --\u0026amp;gt; \u0026amp;lt;denyImportPackages\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;org.springframework\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;/denyImportPackages\u0026amp;gt; \u0026amp;lt;!-- denyImportResources can prevent resource exported by ark plugin with accurate name to be resolved --\u0026amp;gt; \u0026amp;lt;denyImportResources\u0026amp;gt; \u0026amp;lt;resource\u0026amp;gt;META-INF/spring/test1.xml\u0026amp;lt;/resource\u0026amp;gt; \u0026amp;lt;resource\u0026amp;gt;META-INF/spring/test2.xml\u0026amp;lt;/resource\u0026amp;gt; \u0026amp;lt;/denyImportResources\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  插件配置项解释：\n outputDirectory: 执行 mvn package 命令后，指定打出来的 ark 包存放目录，默认存放至 ${project.build.directory} arkClassifier: 执行 mvn depleoy 命令后，指定发布到仓库的 ark 包的maven坐标的 classifer 值, 默认为空；我们推荐配置此配置项用于和普通的 Fat Jar 加以名字上区别； denyImportClasses: 默认情况下，应用会优先加载 ark plugin 导出的类，使用该配置项，可以禁止应用从 ark plugin 加载其导出类； denyImportPackages: 对应上述的 denyImportClasses, 提供包级别的禁止导入； denyImportResources: 默认情况下，应用会优先加载 ark plugin 导出的资源，使用该配置项，可以禁止应用从 ark plugin 加载其导出资源；  添加类隔离框架依赖 在实际开发中，为了在跑测试用例时使用 SOFABoot 类隔离能力，需要在 SOFABoot 工程中添加如下的依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-springboot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  根据 SpringBoot 依赖即服务的原则，添加该依赖之后，应用启动之前，会优先启动 SOFABoot 类隔离容器；\nSOFABoot 的类隔离框架会自动检测应用中是否有引入 Ark Plugin（即需要被隔离的jar包，详情请参考 SOFAArk）, 并隔离加载；例如为了避免 SOFABoot 官方提供的 SOFARPC 组件和应用产生依赖冲突，SOFABoot提供了 SOFARPC 组件对应的 ark plugin 版，用户如果需要隔离 SOFARPC，只需要添加如下组件：\n\u0026amp;lt;dependency\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/classloader-isolation/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e007416ab008c1dd4b886433dbf8af01","permalink":"/projects/sofa-boot/classloader-isolation/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/classloader-isolation/","summary":"SOFABoot 提供了类隔离框架 SOFAArk, 弥补了 Spring Boot 在类隔离能力上的缺失，用以解决在实际开发中常见的类冲突、包冲突问题，详细请参考 SOFAArk。 在 SOFABoot 工程中使用类","tags":null,"title":"SOFABoot 使用类隔离","type":"projects","url":"/projects/sofa-boot/classloader-isolation/","wordcount":1667},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"本指南将基于 SOFADashboard 的 ARK 管控能力来实现 SOFAArk 提供的合并部署和动态模块推送的功能。","dir":"guides/kc-sofastack-dynamic-demo/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8bfd4a50e21ce9fc867b1cf18a8c9af3","permalink":"/guides/kc-sofastack-dynamic-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/kc-sofastack-dynamic-demo/","summary":"","tags":null,"title":"SOFABoot 动态模块实践","type":"guides","url":"/guides/kc-sofastack-dynamic-demo/","wordcount":0},{"author":null,"categories":null,"content":" SOFABoot 支持模块化隔离，在实际的使用场景中，一个模块中的 bean 有时候需要开放一些入口，供另外一个模块扩展。SOFABoot 借鉴和使用了 Nuxeo Runtime 项目 以及 nuxeo 项目，并在上面扩展，与 Spring 融合，提供扩展点的能力。\n使用 在 SOFABoot 中使用扩展点能力，需要以下三个步骤：\n定义提供扩展能力的 bean 在使用 SOFABoot 扩展点能力时，首先需要定一个需要被扩展的 bean，先定一个接口：\npackage com.alipay.sofa.boot.test; public interface IExtension { String say(); }  定义这个接口的实现：\npackage com.alipay.sofa.boot.test.impl; public class ExtensionImpl implements IExtension { private String word; @Override public String say() { return word; } public void setWord(String word) { this.word = word; } public void registerExtension(Extension extension) throws Exception { Object[] contributions = extension.getContributions(); String extensionPoint = extension.getExtensionPoint(); if (contributions == null) { return; } for (Object contribution : contributions) { if (\u0026amp;quot;word\u0026amp;quot;.equals(extensionPoint)) { setWord(((ExtensionDescriptor) contribution).getValue()); } } } }  在这里可以看到有一个方法：registerExtension ，暂时可以先不用管这个方法，后续会介绍其具体的作用。\n在模块的 Spring 配置文件中，我们把这个 bean 给配置起来：\n\u0026amp;lt;bean id=\u0026amp;quot;extension\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.boot.test.impl.ExtensionImpl\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;word\u0026amp;quot; value=\u0026amp;quot;Hello, world\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt;  定义扩展点 在上面的 bean 中有一个字段 word ，在实际中，我们希望这个字段能够被其他的模块自定义进行覆盖，这里我们将其以扩展点的形式暴露出来。\n首先需要一个类去描述这个扩展点：\n@XObject(\u0026amp;quot;word\u0026amp;quot;) public class ExtensionDescriptor { @XNode(\u0026amp;quot;value\u0026amp;quot;) private String value; public String getValue() { return value; } }  然后在 xml 中定义扩展点：\n\u0026amp;lt;sofa:extension-point name=\u0026amp;quot;word\u0026amp;quot; ref=\u0026amp;quot;extension\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:object class=\u0026amp;quot;com.alipay.sofa.boot.test.extension.ExtensionDescriptor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:extension-point\u0026amp;gt;  其中： - name 为扩展点的名字 - ref 为扩展点所作用在的 bean - object 为扩展点的贡献点具体的描述，这个描述是通过 XMap 的方式来进行的(XMap 的作用是将 Java 对象和 XML 文件进行映射，这里建议通过在网上搜索下 XMap 的文档来了解 XMap)\n定义扩展 上述已经将扩展点定义好了，此时我们就可以对这个 bean 进行扩展了:\n\u0026amp;lt;sofa:extension bean=\u0026amp;quot;extension\u0026amp;quot; point=\u0026amp;quot;word\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:content\u0026amp;gt; \u0026amp;lt;word\u0026amp;gt; \u0026amp;lt;value\u0026amp;gt;newValue\u0026amp;lt;/value\u0026amp;gt; \u0026amp;lt;/word\u0026amp;gt; \u0026amp;lt;/sofa:content\u0026amp;gt; \u0026amp;lt;/sofa:extension\u0026amp;gt;  其中： - bean 为扩展所作用在的 bean - point 为扩展点的名字 - content 里面的内容为扩展的定义，其会通过 XMap 将内容解析为：扩展点的贡献点具体的描述对象，在这里即为 com.alipay.sofa.boot.test.extension.ExtensionDescriptor 对象\n到这里，我们可以回头看一开始在 com.alipay.sofa.boot.test.impl.ExtensionImpl 中定义的 registerExtension 方法了，SOFABoot 在解析到贡献点时，会调用被扩展 bean 的 registerExtension 方法，其中包含了用户定义的贡献点处理逻辑，在上述的例子中，获取用户定义的 value 值，并将其设置到 word 字段中覆盖 bean 中原始定义的值。\n此时，调用 extension bean 的 say() 方法，可以看到返回扩展中定义的值: newValue 。\nXMap 支持和扩展 上述的例子中只是一个很简单的扩展，其实 XMap 包含了非常丰富的描述能力，包括 List, Map 等，这些可以通过查看 XMap 的文档来了解。\n在 SOFABoot 中，除了 XMap 原生的支持以外，还扩展了跟 Spring 集成的能力： - 通过 XNode 扩展出了 XNodeSpring - 通过 XNodeList 扩展出了 XNodeListSpring - 通过 XNodeMap 扩展出了 XNodeMapSpring\n这部分的扩展能力，让扩展点的能力更加丰富，描述对象中可以直接指向一个 SpringBean(用户配置 bean 的名字，SOFABoot 会根据名字从 spring 上下文中获取到 bean)，这里举一个使用 XNodeListSpring 的例子，依然是上述描述的三个步骤：\n定义提供扩展能力的 bean 接口定义：\n在这个接口里，返回一个 list，目标是这个 list 能够被通过扩展的方式填充\npackage com.alipay.sofa.boot.test; public interface IExtension { …","date":-62135596800,"description":"","dir":"projects/sofa-boot/extension/","fuzzywordcount":2000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"93225b6c1f2b68f2047a7cf49b76650b","permalink":"/projects/sofa-boot/extension/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-boot/extension/","summary":"SOFABoot 支持模块化隔离，在实际的使用场景中，一个模块中的 bean 有时候需要开放一些入口，供另外一个模块扩展。SOFABoot 借鉴和使用了 Nuxeo Runtime 项目 以及 nuxeo 项","tags":null,"title":"SOFABoot 拓展点","type":"projects","url":"/projects/sofa-boot/extension/","wordcount":1959},{"author":null,"categories":null,"content":" 本文档将演示了如何在 SOFABoot 环境下应用 SOFARPC 进行服务的发布和引用。\n您可以直接在工程下找到本文档的示例代码。注意,示例代码中需要本地安装 zookeeper 环境,如果没有安装.需要将application.properties中的com.alipay.sofa.rpc.registry.address 配置注释掉.走本地文件注册中心的方式\n创建工程  环境准备：SOFABoot 需要 JDK7 或者 JDK8 ，需要采用 Apache Maven 2.2.5 或者以上的版本来编译。 工程构建：SOFABoot 构建在 Spring Boot 之上。因此可以使用 Spring Boot 的工程生成工具 来生成一个标准的Spring Boot 工程。 引入 SOFABoot 环境：生成的 Spring Boot 标准工程直接使用的 Spring Boot 的 parent 依赖，改为 SOFABoot 提供的 parent 依赖，该parent 提供并管控了多种 SOFABoot 提供的 starter。\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  替换为：\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  这里的 ${sofa.boot.version} 指定具体的 SOFABoot 版本，参考发布历史\n 配置 application.properties ：application.properties 是 SOFABoot 工程中的配置文件。这里需要配置一个必不可少的配置项，即应用名。\nspring.application.name=AppName  引入 RPC Starter：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   声明 SOFABoot 的 xsd 文件：在要使用的 XML 配置文件中将头部 xsd 文件的声明设置为如下。这样就能够使用 SOFABoot 定义的 XML 元素进行开发。 xml \u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xmlns:sofa=\u0026amp;quot;http://sofastack.io/schema/sofaboot\u0026amp;quot; xmlns:context=\u0026amp;quot;http://www.springframework.org/schema/context\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://sofastack.io/schema/sofaboot http://sofastack.io/schema/sofaboot.xsd\u0026amp;quot; default-autowire=\u0026amp;quot;byName\u0026amp;quot;\u0026amp;gt;    定义服务接口与实现 public interface HelloSyncService { String saySync(String string); }  public class HelloSyncServiceImpl implements HelloSyncService { @Override public String saySync(String string) { return string; } }  服务端发布服务 在 xml 文件中编写如下配置。Spring 上下文在刷新时，SOFABoot 就将该服务实现注册到了服务器上，以 bolt 协议与客户端进行通信地址，并将地址等元数据发布到了注册中心(这里默认使用的本地文件作为注册中心)。\n\u0026amp;lt;bean id=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  客户端引用服务 在 xml 文件中编写如下配置。Spring 上下文刷新时，SOFABoot 会生成一个RPC的代理 bean，即 personReferenceBolt 。这样就可以直接在代码中使用该 bean 进行远程调用了。\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;helloSyncServiceReference\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  运行 在 SpringBoot 的启动类中编码如下。其中利用 ImportResource 将上述的xml文件加载。\n@ImportResource({ \u0026amp;quot;classpath*:rpc-sofa-boot-starter-samples.xml\u0026amp;quot; }) …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/getting-started-with-sofa-boot/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0dd5e0e5116473aee630cba38679d493","permalink":"/projects/sofa-rpc/getting-started-with-sofa-boot/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/getting-started-with-sofa-boot/","summary":"本文档将演示了如何在 SOFABoot 环境下应用 SOFARPC 进行服务的发布和引用。 您可以直接在工程下找到本文档的示例代码。注意,示例代码中需要本地安装 zookeeper 环境,如果没有","tags":null,"title":"SOFABoot 方式快速入门","type":"projects","url":"/projects/sofa-rpc/getting-started-with-sofa-boot/","wordcount":815},{"author":null,"categories":null,"content":"在xml方式中发布和引用服务的方式如下。 sofa:service 元素表示发布服务， sofa:reference 元素表示引用服务。 sofa:binding 表示服务发布或引用的协议。\n\u0026amp;lt;bean id=\u0026amp;quot;personServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;personServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  一个服务也可以通过多种协议进行发布，如下：\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;personServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;sofa:binding.dubbo/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  服务引用\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  也可以是其他的协议\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceRest\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-xml/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9192a93415bee3070a9be62c0f693949","permalink":"/projects/sofa-rpc/programing-sofa-boot-xml/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/programing-sofa-boot-xml/","summary":"在xml方式中发布和引用服务的方式如下。 sofa:service 元素表示发布服务， sofa:reference 元素表示引用服务。 sofa:binding 表示服务发布或引用的协议。 \u0026lt;bean id=\u0026quot;personServiceImpl\u0026quot; class=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonServiceImpl\u0026quot;/\u0026gt; \u0026lt;sofa:service ref=\u0026quot;personServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;/sofa:service\u0026gt; 一个服务也可以通","tags":null,"title":"SOFABoot 环境 XML 配置使用","type":"projects","url":"/projects/sofa-rpc/programing-sofa-boot-xml/","wordcount":113},{"author":null,"categories":null,"content":"SOFABoot 为 RPC 服务的发布和引用提供了一套编程 API 方式，方便直接在代码中发布和引用 RPC 服务，与 Spring 的 ApplicationContextAware 类似，为使用编程 API 方式，首先需要实现 ClientFactoryAware 接口获取编程组件 API：\npublic class ClientFactoryBean implements ClientFactoryAware { private ClientFactory clientFactory; @Override public void setClientFactory(ClientFactory clientFactory) { this.clientFactory = clientFactory; } }  以 DirectService 为例，看下如何使用 clientFactory 通过编程 API 方式发布 RPC 服务：\nServiceClient serviceClient = clientFactory.getClient(ServiceClient.class); ServiceParam serviceParam = new ServiceParam(); serviceParam.setInterfaceType(DirectService.class); serviceParam.setInstance(new DirectServiceImpl()); List\u0026amp;lt;BindingParam\u0026amp;gt; params = new ArrayList\u0026amp;lt;BindingParam\u0026amp;gt;(); BindingParam serviceBindingParam = new BoltBindingParam(); params.add(serviceBindingParam); serviceParam.setBindingParams(params); serviceClient.service(serviceParam);  上面的代码中\n 首先通过 clientFactory 获得 ServiceClient 对象 然后构造 ServiceParam 对象，ServiceParam 对象包含发布服务所需参数，通过 setInstance 方法来设置需要被发布成 RPC 服务的对象，setInterfaceType 来设置服务的接口 最后，调用 ServiceClient 的 service 方法，发布一个 RPC 服务  通过编程 API 方式引用 RPC 服务的代码也是类似的：\nReferenceClient referenceClient = clientFactory.getClient(ReferenceClient.class); ReferenceParam\u0026amp;lt;DirectService\u0026amp;gt; referenceParam = new ReferenceParam\u0026amp;lt;DirectService\u0026amp;gt;(); referenceParam.setInterfaceType(DirectService.class); BindingParam refBindingParam = new BoltBindingParam(); referenceParam.setBindingParam(refBindingParam); DirectService proxy = referenceClient.reference(referenceParam); proxy.sayDirect(\u0026amp;quot;hello\u0026amp;quot;);  同样，引用一个 RPC 服务只需从 ClientFactory 中获取一个 ReferenceClient ，然后和发布一个服务类似，构造出一个 ReferenceParam，然后设置好服务的接口，最后调用 ReferenceClient 的 reference 方法即可。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-api/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2679388dc3459714f869d8f8a71739d7","permalink":"/projects/sofa-rpc/programing-sofa-boot-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/programing-sofa-boot-api/","summary":"SOFABoot 为 RPC 服务的发布和引用提供了一套编程 API 方式，方便直接在代码中发布和引用 RPC 服务，与 Spring 的 ApplicationContextAware 类似，为使用编程 API 方式，首先需要实现 ClientFactoryAware 接口获取编程组件","tags":null,"title":"SOFABoot 环境动态 API 使用","type":"projects","url":"/projects/sofa-rpc/programing-sofa-boot-api/","wordcount":374},{"author":null,"categories":null,"content":" 这部分介绍在 SOFABoot 环境下,完整的 SOFARPC 服务发布与引用说明\n发布服务 \u0026amp;lt;bean id=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot; unique-id=\u0026amp;quot;\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs registry=\u0026amp;quot;\u0026amp;quot; serialize-type=\u0026amp;quot;\u0026amp;quot; filter=\u0026amp;quot;\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; thread-pool-ref=\u0026amp;quot;\u0026amp;quot; warm-up-time=\u0026amp;quot;60000\u0026amp;quot; warm-up-weight=\u0026amp;quot;10\u0026amp;quot; weight=\u0026amp;quot;100\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:binding.rest\u0026amp;gt; \u0026amp;lt;/sofa:binding.rest\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;     属性 名称 默认值 备注     id ID bean名    class 类 无    ref 服务接口实现类     interface 服务接口（唯一标识元素）  不管是普通调用和返回调用，这里都设置实际的接口类   unique-id 服务标签（唯一标识元素）     filter 过滤器配置别名  多个用逗号隔开   registry 服务端注册中心  逗号分隔   timeout 服务端执行超时时间     serialize-type 序列化协议 hessian2,protobuf    thread-pool-ref 服务端当前接口使用的线程池 无     weight 服务静态权重     warm-up-weight 服务预热权重     warm-up-time 服务预热时间  单位毫秒    引用服务 \u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;helloSyncServiceReference\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot; unique-id=\u0026amp;quot;\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs type=\u0026amp;quot;sync\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; callback-ref=\u0026amp;quot;\u0026amp;quot; callback-class=\u0026amp;quot;\u0026amp;quot; address-wait-time=\u0026amp;quot;1000\u0026amp;quot; connect.num=\u0026amp;quot;1\u0026amp;quot; check=\u0026amp;quot;false\u0026amp;quot; connect.timeout=\u0026amp;quot;1000\u0026amp;quot; filter=\u0026amp;quot;\u0026amp;quot; generic-interface=\u0026amp;quot;\u0026amp;quot; idle.timeout=\u0026amp;quot;1000\u0026amp;quot; idle.timeout.read=\u0026amp;quot;1000\u0026amp;quot; lazy=\u0026amp;quot;false\u0026amp;quot; loadBalancer=\u0026amp;quot;\u0026amp;quot; registry=\u0026amp;quot;\u0026amp;quot; retries=\u0026amp;quot;1\u0026amp;quot; serialize-type=\u0026amp;quot;\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;xxx:12200\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;hello\u0026amp;quot; callback-class=\u0026amp;quot;\u0026amp;quot; callback-ref=\u0026amp;quot;\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; type=\u0026amp;quot;sync\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;     属性 名称 默认值 备注     id ID 自动生成    jvm-first 是否优先本地 true    interface 服务接口（唯一标识元素）  不管是普通调用和返回调用，这里都设置实际的接口类   unique-id 服务标签（唯一标识元素）     type 调用方式 sync callback,sync,future,oneway   filter 过滤器配置别名  List   registry 服务端注册中心  List   method 方法级配置  说明同上   serialize-type 序列化协议 hessian2    target-url 直连地址  直连后register   generic-interface 泛化接口     connect.timeout 建立连接超时时间 3000(cover 5000)    connect.num 连接数 1    idle.timeout 空闲超时时间     idle.timeout.read 读空闲超时时间     loadBalancer 负载均衡算法 random    lazy 是否延迟建立长连接 false    address-wait-time 等待地址获取时间 -1 取决于实现，可能不生效。   timeout 调用超时时间 3000(cover 5000)    retries 失败后重试次数 0 跟集群模式有关，failover读取此参数。   callback-class callback 回调类 无 callback 才可用   callback-ref callback 回调类 无 callback 才可用    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/rpc-config-xml-explain/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4b5110e9eb6cf6c6f287aef0fd210047","permalink":"/projects/sofa-rpc/rpc-config-xml-explain/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/rpc-config-xml-explain/","summary":"这部分介绍在 SOFABoot 环境下,完整的 SOFARPC 服务发布与引用说明 发布服务 \u0026lt;bean id=\u0026quot;helloSyncServiceImpl\u0026quot; class=\u0026quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncServiceImpl\u0026quot;/\u0026gt; \u0026lt;sofa:service ref=\u0026quot;helloSyncServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026quot; unique-id=\u0026quot;\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt\u0026gt; \u0026lt;sofa:global-attrs registry=\u0026quot;\u0026quot; serialize-type=\u0026quot;\u0026quot; filter=\u0026quot;\u0026quot; timeout=\u0026quot;3000\u0026quot; thread-pool-ref=\u0026quot;\u0026quot; warm-up-time=\u0026quot;60000\u0026quot; warm-up-weight=\u0026quot;10\u0026quot; weight=\u0026quot;100\u0026quot;/\u0026gt; \u0026lt;/sofa:binding.bolt\u0026gt; \u0026lt;sofa:binding.rest\u0026gt; \u0026lt;/sofa:binding.rest\u0026gt; \u0026lt;/sofa:service\u0026gt; 属性 名称 默认值 备注 id ID bean名 class 类 无 ref 服","tags":null,"title":"SOFABoot 环境发布订阅说明","type":"projects","url":"/projects/sofa-rpc/rpc-config-xml-explain/","wordcount":518},{"author":null,"categories":null,"content":" 注解服务发布与服务引用 除了常规的 xml 方式发布服务外,我们也支持在SOFABoot 环境下,注解方式的发布与引用,同 xml 类似,我们有 @SofaService 和 @SofaReference,同时对于多协议,存在@SofaServiceBinding 和 @SofaReferenceBinding 注解\n服务发布 如果要发布一个 RPC 服务. 我们只需要在 Bean 上面打上@SofaService注解.指定接口和协议类型即可\n@SofaService(interfaceType = AnnotationService.class, bindings = { @SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;) }) @Component public class AnnotationServiceImpl implements AnnotationService { @Override public String sayAnnotation(String stirng) { return stirng; } }  服务引用 对于需要引用远程服务的 bean, 只需要在属性,或者方法上,打上Reference 的注解即可，支持 bolt, dubbo, rest 协议。\n@Component public class AnnotationClientImpl { @SofaReference(interfaceType = AnnotationService.class, binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)) private AnnotationService annotationService; public String sayClientAnnotation(String str) { String result = annotationService.sayAnnotation(str); return result; } }  使用演示 可以在sample 工程目录的annotation 子项目中进行验证测试.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-annotation/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2c3afd33cbce4f5aa2473716b3afe5a6","permalink":"/projects/sofa-rpc/programing-sofa-boot-annotation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/programing-sofa-boot-annotation/","summary":"注解服务发布与服务引用 除了常规的 xml 方式发布服务外,我们也支持在SOFABoot 环境下,注解方式的发布与引用,同 xml 类似,我们有 @SofaService 和 @SofaR","tags":null,"title":"SOFABoot 环境注解使用","type":"projects","url":"/projects/sofa-rpc/programing-sofa-boot-annotation/","wordcount":313},{"author":null,"categories":null,"content":" SOFADashboard is designed to implement unified management over SOFA framework components, including service governance and SOFAArk control. All technology stacks used by SOFADashboard are developed and constructed based on open-source community products, such as Ant Design Pro, SOFABoot, Spring, and MyBatis.\nCurrently, service governance and SOFAArk control of SOFADashboard are dependent on ZooKeeper. Therefore, you need to ensure the ZooKeeper service is available when you decide to use SOFADashboard. You also need to ensure that MySQL is available, because SOFAArk control and deployment uses MySQL for resource data storage.\nArchitecture Currently, service governance and SOFAArk control of SOFADashboard are implemented upon ZooKeeper-based programming.\n SOFADashboard backend corresponds to the sofa-dashboard-backend project. It is the server end project of SOFADashboard, responsible for data interaction between ZooKeeper and MySQL and for providing the rest API to the SOFADashboard frontend. SOFADashboard frontend corresponds to the sofa-dashboard-frontend project. It is the frontend project of SOFADashboard. It provides UIs for interaction with users. Application  rpc provider: service provider of SOFARPC, which registers services with ZooKeeper. rpc consumer: service consumer of SOFARPC, which subscribes to services on ZooKeeper. client: SOFADashboard client, which is available upon the installation of the sofa-dashboard-client package. Currently, the SOFADashboard client only supports registration of health-check status and port information of applications with ZooKeeper. Later on, it will evolve into SOFABoot client, and report more diversified application data. ark-biz host app: see SOFAArk .   ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/overview/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f0664d0ca7fc1fa87e67847525081993","permalink":"/en/projects/sofa-dashboard/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-dashboard/overview/","summary":"SOFADashboard is designed to implement unified management over SOFA framework components, including service governance and SOFAArk control. All technology stacks used by SOFADashboard are developed and constructed based on open-source community products, such as Ant Design Pro, SOFABoot, Spring, and MyBatis.\nCurrently, service governance and SOFAArk control of SOFADashboard are dependent on ZooKeeper. Therefore, you need to ensure the ZooKeeper service is available when you decide to use SOFADashboard. You also need to ensure that MySQL is available, because SOFAArk control and deployment uses MySQL for resource data storage.","tags":null,"title":"SOFADashboard overview","type":"projects","url":"/en/projects/sofa-dashboard/overview/","wordcount":231},{"author":null,"categories":null,"content":" SOFADashboard 致力于对 SOFA 框架中组件进行统一管理，包括服务治理、SOFAArk 管控等。SOFADashboard 本身所用技术栈均基于开源社区产品来开发构建，包括：Ant Design Pro、SOFABoot、Spring、MyBatis 等。\n目前，SOFADashboard 中的服务治理、SOFAArk 管控等需要依赖于 Zookeeper，因此如果您需要使用 SOFADashboard 那么请确保 Zookeeper 服务可用；另外 SOFAArk 管控部署需要依赖 MySQL 进行资源数据存储，因此也需要保证 MySQL 可以正常使用。\n架构简图 SOFADashboard 目前服务治理与 SOFAArk 管控都是面向 Zookeeper 来编程实现的。\n SOFADashboard backend : 对应 sofa-dashboard-backend 工程，是 SOFADashboard 的服务端工程，负责与 Zookeeper 和 MySQL 进行数据交互，并且为 SOFADashboard frontend 提供 rest 接口。 SOFADashboard frontend : 对应 sofa-dashboard-frontend 工程，是 SOFADashboard 的前端工程，用于提供与用户交互的 UI 界面。 app 应用  rpc provider : SOFARPC 的服务提供方，会将服务注册到 Zookeeper 上。 rpc consumer : SOFARPC 的服务消费方，会从 Zookeeper 上订阅服务。 client : SOFADashboard 客户端，引入 sofa-dashboard-client 包即可。目前仅提供将应用的健康检查状态及端口信息注册到 Zookeeper ，后面将会演化成 SOFABoot client，上报更丰富的应用数据。 ark-biz 宿主应用: 参考 SOFAArk 。   ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/overview/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f0664d0ca7fc1fa87e67847525081993","permalink":"/projects/sofa-dashboard/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/overview/","summary":"SOFADashboard 致力于对 SOFA 框架中组件进行统一管理，包括服务治理、SOFAArk 管控等。SOFADashboard 本身所用技术栈均基于开源社区产品来开发构建","tags":null,"title":"SOFADashboard 介绍","type":"projects","url":"/projects/sofa-dashboard/overview/","wordcount":431},{"author":null,"categories":null,"content":" This topic is a part of the Braft document. To read the Braft document, click here. The Raft algorithm and its application are comprehensively described in the Braft document. As JRaft is designed on the basis of Braft, we strongly recommend that you read the Braft document first to understand the basic principles and application of the Raft algorithm.\nDistributed consensus Distributed consensus is a very fundamental problem in a distributed system. Simply put, it is about how to reach a consensus on a specific value among multiple servers, and ensure that the decision is not overthrown regardless of what failures may occur on these servers. Assume that, all processes of a distributed system needs to determine a value V. If the system has the following properties, we consider it solves the problem of distributed consensus:\n Termination: All normal processes will determine the specific value of V, and there is no process that keeps running in a loop. Validity: A value V\u0026amp;rsquo; determined by normal processes must have been proposed by one of them. For example, a random number generator does not have this property. Agreement: All normal processes choose the same value.  Consensus state machine Assume we have an infinitely incrementing sequence (system) a[1, 2, 3…]. If for any integer i, the value of a[i] meets the distributed consensus requirement, the system meets the requirement of a consensus state machine. Basically, all systems are subject to continuous operations, and reaching consensus on a single value is definitely not enough. To make sure all replicas of a real-life system are consistent, we usually convert the operations into entries of a write-ahead-log(WAL). Then, we make sure all replicas of the system reach a consensus on the WAL entries, so that each process will perform operations corresponding to the WAL entries in order. As a result, the replicas are in consistent states.\nRAFT RAFT is a new and easy-to-understand distributed consensus replication protocol proposed by Diego Ongaro and John Ousterhout of Stanford University as a central coordination component of the RAMCloudproject. Raft is a leader-based multi-Paxos variant that provides a more complete and straightforward protocol description than existing protocols such as Paxos, Zab, and Viewstamped Replication. It also provides a clear description for adding and deleting nodes. In Raft, replicated state machines are the most important and fundamental to distributed systems. Raft allows commands to be replicated and executed in order, and ensures that the states of nodes remain consistent when their initial states are the same. A system is fully functional (available) as long as a majority of nodes function properly. It allows non-Byzantine conditions, including network delays, packet loss, and reordering, but does not allow tampering with any messages.\nRaft can solve the distributed consensus and partitioning problems, but cannot solve the availability problem. Raft covers …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/overview/","fuzzywordcount":700,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"eff7088d010aefabdffa2858e88d76c0","permalink":"/en/projects/sofa-jraft/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-jraft/overview/","summary":"This topic is a part of the Braft document. To read the Braft document, click here. The Raft algorithm and its application are comprehensively described in the Braft document. As JRaft is designed on the basis of Braft, we strongly recommend that you read the Braft document first to understand the basic principles and application of the Raft algorithm.\nDistributed consensus Distributed consensus is a very fundamental problem in a distributed system.","tags":null,"title":"SOFAJRaft overview","type":"projects","url":"/en/projects/sofa-jraft/overview/","wordcount":645},{"author":null,"categories":null,"content":" 本介绍内容来自 braft 文档，原文链接请参见这里。braft 的关于算法和应用本身的文档非常优秀，由于 jraft 脱胎自 braft，我们强烈推荐阅读上述文档以了解 raft 算法的基本原理和应用。\n分布式一致性 分布式一致性 (distributed consensus) 是分布式系统中最基本的问题，用来保证一个分布式系统的可靠性以及容灾能力。简单的来讲，就是如何在多个机器间对某一个值达成一致, 并且当达成一致之后，无论之后这些机器间发生怎样的故障，这个值能保持不变。 抽象定义上， 一个分布式系统里的所有进程要确定一个值 v，如果这个系统满足如下几个性质， 就可以认为它解决了分布式一致性问题, 分别是:\n Termination: 所有正常的进程都会决定 v 具体的值，不会出现一直在循环的进程。 Validity: 任何正常的进程确定的值 v\u0026amp;rsquo;, 那么 v\u0026amp;rsquo; 肯定是某个进程提交的。比如随机数生成器就不满足这个性质。 Agreement: 所有正常的进程选择的值都是一样的。  一致性状态机 对于一个无限增长的序列 a[1, 2, 3…], 如果对于任意整数 i, a[i] 的值满足分布式一致性，这个系统就满足一致性状态机的要求。 基本上所有的系统都会有源源不断的操作, 这时候单独对某个特定的值达成一致是不够的。为了真实系统保证所有的副本的一致性，通常会把操作转化为 write-ahead-log(简称WAL)。然后让系统的所有副本对WAL保持一致，这样每个进程按照顺序执行WAL里的操作，就能保证最终的状态是一致的。\nRAFT RAFT 是一种新型易于理解的分布式一致性复制协议，由斯坦福大学的 Diego Ongaro 和 John Ousterhout 提出，作为 RAMCloud 项目中的中心协调组件。Raft 是一种 Leader-Based 的 Multi-Paxos 变种，相比 Paxos、Zab、View Stamped Replication 等协议提供了更完整更清晰的协议描述，并提供了清晰的节点增删描述。 Raft 作为复制状态机，是分布式系统中最核心最基础的组件，提供命令在多个节点之间有序复制和执行，当多个节点初始状态一致的时候，保证节点之间状态一致。系统只要多数节点存活就可以正常处理，它允许消息的延迟、丢弃和乱序，但是不允许消息的篡改（非拜占庭场景）。\nRaft 可以解决分布式理论中的 CP，即一致性和分区容忍性，并不能解决 Available 的问题。其中包含分布式系统中一些通常的功能：\n Leader Election Log Replication Membership Change Log Compaction  RAFT 可以做什么 通过 RAFT 提供的一致性状态机，可以解决复制、修复、节点管理等问题，极大的简化当前分布式系统的设计与实现，让开发者只关注于业务逻辑，将其抽象实现成对应的状态机即可。基于这套框架，可以构建很多分布式应用：\n 分布式锁服务，比如 Zookeeper 分布式存储系统，比如分布式消息队列、分布式块系统、分布式文件系统、分布式表格系统等 高可靠元信息管理，比如各类 Master 模块的 HA  JRAFT 一个纯 Java 的 Raft 算法实现库, 基于百度 braft 实现而来, 使用 Java 重写了所有功能, 支持:\n Leader election and priority-based semi-deterministic leader election. Replication and recovery. Snapshot and log compaction. Read-only member (learner). Membership management. Fully concurrent replication. Fault tolerance. Asymmetric network partition tolerance. Workaround when quorate peers are dead. Replication pipeline optimistic Linearizable read, ReadIndex/LeaseRead.  联系我们 更多讨论欢迎加入钉钉讨论群：30315793\n","date":-62135596800,"description":"","dir":"projects/sofa-jraft/overview/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"eff7088d010aefabdffa2858e88d76c0","permalink":"/projects/sofa-jraft/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-jraft/overview/","summary":"本介绍内容来自 braft 文档，原文链接请参见这里。braft 的关于算法和应用本身的文档非常优秀，由于 jraft 脱胎自 braft，我们强烈推荐阅读上述文档以了","tags":null,"title":"SOFAJRaft 介绍","type":"projects","url":"/projects/sofa-jraft/overview/","wordcount":1132},{"author":null,"categories":null,"content":" SOFALookout 是蚂蚁金服开源的一款解决系统的度量和监控问题的轻量级中间件服务。它提供的服务包括：Metrics 的埋点、收集、加工、存储与查询等。该开源项目包括了两个独立部分，分别是客户端与服务器端服务。\n1.客户端部分 SOFALookout Client 是一个 Java 的 SDK，可以帮助开发者在项目代码中进行 metrics 埋点。通过它也可以查看该 JAVA 应用的实时的状态信息。\n +------------------+ Reg: API: | dimension meters +--------+ +------------------+ | flatmap +---------------------------+ +-----------\u0026amp;gt; | Default/DropwizardMetrics| | +---------------------------+ | | http +--------------+ +-----------\u0026amp;gt; |Lookout server| | +--------------+ +----------------------+ | add common tags dimension EXTS: | JVM,OS,GC... +----+ +----------------------+  2.服务器端服务 SOFALookout Server 可以帮助我们解决分布式环境下系统状态度量的问题，它提供丰富的协议接入支持，包括自有SDK（SOFALookout Client）上报协议，还支持 Prometheus 的数据协议（推模式和拉模式），Metricbeat 协议（版本是6），Opentsdb写入协议。 Lookout Server 兼容和增强了 Prometheus 的数据及元数据查询的 RESTful API。同样对应 PromQL 我们也基本实现了兼容和增强（不包括 Alert 相关语法）。\n2.1.Metrics 服务器端主要特性:  适配社区主要 Metrics 数据源协议写入（比如: Prometheus，Metricbeat等）； 数据的存储支持扩展，暂时开源版默认支持 Elasticsearch,并且透明和自动化了相关运维操作； 遵循 Prometheus 查询 API 的标准以及支持 PromQL，并进行了适当改进； 自带数据查询的控制台，并支持 Grafana 进行数据可视化； 使用简单，支持单一进程运行整个服务器端模块。  2.2.Metrics 服务器端工作机制: +----------------+ | Lookout Client +-----+ +----------------+ | +----------------+ | | Prometheus SDK +-----+ +-------------------+ +----------------------+ +------------------+ +-----------+ +----------------+ +--\u0026amp;gt; Lookout Gateway +---\u0026amp;gt; DB(ES/InfluxDB...) \u0026amp;lt;-----+ Lookout Server \u0026amp;lt;----+ Grafana | +----------------+ | +-------------------+ +----------------------+ +------------------+ +-----------+ | Metricbeat +-----+ +----------------+ | +----------------+ | | ... +-----+ +----------------+  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/overview/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8a8a8ef02ca95d4d11e3e4b195bbae70","permalink":"/projects/sofa-lookout/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-lookout/overview/","summary":"SOFALookout 是蚂蚁金服开源的一款解决系统的度量和监控问题的轻量级中间件服务。它提供的服务包括：Metrics 的埋点、收集、加工、存储与查询等。该开源项","tags":null,"title":"SOFALookout 介绍","type":"projects","url":"/projects/sofa-lookout/overview/","wordcount":602},{"author":null,"categories":null,"content":" 1.使用本机 ES 服务  1)本地启动 ES  docker run -d --name es -p 9200:9200 -p 9300:9300 -e \u0026amp;quot;discovery.type=single-node\u0026amp;quot; elasticsearch:5.6  版本：V5，V6\n 2)检查 ES 是否健康  http://localhost:9200/_cat/health?v   3)启动 Lookout 服务  执行 all-in-one-bootstrap 编译后的 fat-jar 包，如何获得，见文末备注部分：\njava -Dcom.alipay.sofa.ark.master.biz=lookoutall -jar lookout-all-in-one-bootstrap-1.6.0-executable-ark.jar   注意 -Dcom.alipay.sofa.ark.master.biz=lookoutall 是必须的, 用于设置 sofa-ark 的 master biz。\n 4)最后进行功能验证\n  查询 （Gateway）的 metrics 作为功能验证，访问“localhost:9090”，在查询框输入：\njvm.memory.heap.used{app=\u0026amp;quot;gateway\u0026amp;quot;}  最后，也可以使用 grafana\n2.使用远程 ES 服务 总体步骤和“使用本机 ES 服务”类似，唯一不同的是，需要指定配置文件。\njava -Dcom.alipay.sofa.ark.master.biz=lookoutall -Dlookoutall.config-file=abc.properties \\ -jar lookout-all-in-one-bootstrap-1.6.0-executable-ark.jar  -Dlookoutall.config-file（如果你本地启动 ES 测试的话则该配置项可以忽略！），该配置项制定的文件暂时只能引用文件系统上的 properties 文件(没有像 spring-boot 支持那么丰富），配置项必须以应用名开头，从而提供隔离能力。\n例如：在fat-jar同目录下创建一个abc.properties配置文件, 用于存放存放配置文件(下面列出了必须的配置项,用于指向使用的 ES 服务地址）：\ngateway.metrics.exporter.es.host=localhost gateway.metrics.exporter.es.port=9200 metrics-server.spring.data.jest.uri=http://localhost:9200  备注 如何获得 all-in-one-bootstrap 编译后的 fat-jar。\n方式1：本地编译\n./boot/all-in-one-bootstrap/build.sh   打包结果在boot/all-in-one-bootstrap/target/allinone-executable.jar\n 方式2：发布报告中附件获取\n临时方式（针对 1.6.0）暂时提供一个 v1.6.0的snapshot包，下载后（保证ES服务已经单独启动）运行：\njava -Dcom.alipay.sofa.ark.master.biz=lookoutall -jar lookout-all-1.6.0.snapshot.jar  方式3：使用docker镜像\n服务端默认会连接到 localhost:9200 的ES实例, 而我所用的开发机器是MacOS，无法使用 --net=host 模式启动容器，因此在容器内无法通过 localhost:9200 连接ES，需要使用如下方式绕过去：\n编辑一个配置文件，比如 foo.properties：\ngateway.metrics.exporter.es.host=es metrics-server.spring.data.jest.uri=http://es:9200  在 foo.properties 所在的目录下运行 all-in-one 镜像：\ndocker run -it \\ --name allinone \\ --link es:es \\ -p 7200:7200 \\ -p 9090:9090 \\ -v $PWD/foo.properties:/home/admin/deploy/foo.properties \\ -e JAVA_OPTS=\u0026amp;quot;-Dlookoutall.config-file=/home/admin/deploy/foo.properties\u0026amp;quot; \\ -e JAVA_OPTS=\u0026amp;quot;...定制JVM系统属性...\u0026amp;quot; \\ xzchaoo/lookout-allinone:1.6.0-SNAPSHOT   这里利用了docker的\u0026amp;ndash;link参数使得应用可以访问到ES实例 这里做测试用，所以不用-d参数在后台运行\n ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/quick-start-metrics-server/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7c12565c66342c2f8e963cf1c1e26db5","permalink":"/projects/sofa-lookout/quick-start-metrics-server/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-lookout/quick-start-metrics-server/","summary":"1.使用本机 ES 服务 1)本地启动 ES docker run -d --name es -p 9200:9200 -p 9300:9300 -e \u0026quot;discovery.type=single-node\u0026quot; elasticsearch:5.6 版本：V5，V6 2)检查 ES 是否健康 http://localhost:9200/_cat/health?v 3)启动 Lookout 服务 执行 all-in-one-bootstrap 编译后的 fat-jar 包，如何获得，见文","tags":null,"title":"SOFALookout 服务端快速开始","type":"projects","url":"/projects/sofa-lookout/quick-start-metrics-server/","wordcount":808},{"author":null,"categories":null,"content":" This repository is deprecated. It will contribute to istio directly instead of developing in a forked repo. Please go to see Istio’s doc.\nSOFAMesh is a large-scale implementation scheme of Service Mesh based on Istio. On the basis of inheriting the powerful functions and rich features of Istio, in order to meet the performance requirements in large-scale deployments and to respond to the actual situation in the implementation, the following improvements are made:\n MOSN written in Golang instead of Envoy Merge Mixer to data plane to resolve performance bottlenecks Enhance Pilot for more flexible service discovery mechanism Added support for SOFA RPC, Dubbo  The initial version was contributed by Ant Financial and Alibaba UC Business Unit.\nThe following figure shows the architectural differences between SOFAMesh and Istio:\nMain components MOSN In SOFAMesh, the data pane adopts Golang to write a module called MOSN (Modular Open Smart Network), and replaces Envoy with MOSN to integrate with Istio to implement the functions of Sidecar. MOSN is fully compatible with Envoy\u0026amp;rsquo;s APIs.\nSOFAMesh Pilot SOFAMesh greatly expands and enhances the Pilot module in Istio:\n Add an Adapter for SOFA Registry to provide solutions for super large-scale service registration and discovery; Add data synchronization modules to enable data exchange between multiple service registry centers; Add Open Service Registry API to provide standardized service registration.  Together with Pilot and MOSN, SOFAMesh provides the ability to enable traditional intrusive frameworks (such as Spring Cloud, Dubbo and SOFARPC) and Service Mesh products to communicate with each other, thus it can smoothly evolve and transit to Service Mesh.\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/overview/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e44a7cb73f7b68217663bd75655f43d7","permalink":"/en/projects/sofa-mesh/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-mesh/overview/","summary":"This repository is deprecated. It will contribute to istio directly instead of developing in a forked repo. Please go to see Istio’s doc.\nSOFAMesh is a large-scale implementation scheme of Service Mesh based on Istio. On the basis of inheriting the powerful functions and rich features of Istio, in order to meet the performance requirements in large-scale deployments and to respond to the actual situation in the implementation, the following improvements are made:","tags":null,"title":"SOFAMesh overview","type":"projects","url":"/en/projects/sofa-mesh/overview/","wordcount":260},{"author":null,"categories":null,"content":" 该项目仓库已弃用。该项目将直接向 Istio 贡献，不会继续在 fork 的仓库中开发，请转至 Istio 官网。\nSOFAMesh 是基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案。在继承 Istio 强大功能和丰富特性的基础上，为满足大规模部署下的性能要求以及应对落地实践中的实际情况，有如下改进：\n 采用 Golang 编写的 MOSN 取代 Envoy 合并 Mixer 到数据平面以解决性能瓶颈 增强 Pilot 以实现更灵活的服务发现机制 增加对 SOFA RPC、Dubbo 的支持  初始版本由蚂蚁金服和阿里大文娱UC事业部携手贡献。\n下图展示了SOFAMesh 和 Istio 在架构上的不同：\n主要组件 MOSN 在 SOFAMesh 中，数据面我们采用 Golang 语言编写了名为 MOSN（Modular Open Smart Network）的模块来替代 Envoy 与 Istio 集成以实现 Sidecar 的功能，同时 MOSN 完全兼容 Envoy 的 API。\nSOFA Pilot SOFAMesh 中大幅扩展和增强 Istio 中的 Pilot 模块：\n 增加 SOFA Registry 的 Adapter，提供超大规模服务注册和发现的解决方案 增加数据同步模块，以实现多个服务注册中心之间的数据交换 增加 Open Service Registry API，提供标准化的服务注册功能  MOSN 和 Pilot 配合，将可以提供让传统侵入式框架（如 Spring Cloud、Dubbo、SOFARPC 等）和 Service Mesh 产品可以相互通讯的功能，以便可以平滑的向 Service Mesh 产品演进和过渡。\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/overview/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e44a7cb73f7b68217663bd75655f43d7","permalink":"/projects/sofa-mesh/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/overview/","summary":"该项目仓库已弃用。该项目将直接向 Istio 贡献，不会继续在 fork 的仓库中开发，请转至 Istio 官网。 SOFAMesh 是基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案。在继承 Istio 强","tags":null,"title":"SOFAMesh 介绍","type":"projects","url":"/projects/sofa-mesh/overview/","wordcount":474},{"author":null,"categories":null,"content":" SOFARPC Metrics SOFARPC currently measures two metrics.\nServer thread pool    metric name metric tags specification     rpc.bolt.threadpool.config bolt thread pool configuration Mainly includes thread pool configuration information for RPC server   rpc.bolt.threadpool.active.count  Running thread of the current thread pool   rpc.bolt.threadpool.idle.count  Idle thread of the current thread pool   rpc.bolt.threadpool.queue.size  Tasks in the queue of the current thread pool    Client call information    metric name metric tags specification     rpc.consumer.service.stats.fail_count.count app,service,method,protocol,invoke_type,target_app Failure count of a certain interface   rpc.consumer.service.stats.fail_count.rate app,service,method,protocol,invoke_type,target_app Number of failures per second of a certain interface   rpc.consumer.service.stats.fail_time.elapPerExec app,service,method,protocol,invoke_type,target_app Average time per failed execution of a certain interface   rpc.consumer.service.stats.fail_time.max app,service,method,protocol,invoke_type,target_app Maximum failure time of a certain interface   rpc.consumer.service.stats.fail_time.totalTime app,service,method,protocol,invoke_type,target_app Total failure time of a certain interface   rpc.consumer.service.stats.request_size.max app,service,method,protocol,invoke_type,target_app Maximum request size of a certain interface   rpc.consumer.service.stats.request_size.rate app,service,method,protocol,invoke_type,target_app Average request size per second of a certain interface   rpc.consumer.service.stats.request_size.totalAmount app,service,method,protocol,invoke_type,target_app Total request amount of a certain interface   rpc.consumer.service.stats.response_size.max app,service,method,protocol,invoke_type,target_app Maximum response size of a certain interface   rpc.consumer.service.stats.response_size.rate app,service,method,protocol,invoke_type,target_app Average response size per second of a certain interface   rpc.consumer.service.stats.response_size.totalAmount app,service,method,protocol,invoke_type,target_app Total response amount of a certain interface   rpc.consumer.service.stats.total_count.count app,service,method,protocol,invoke_type,target_app Total number of calls of a certain interface   rpc.consumer.service.stats.total_count.count_service_sum_30000 app,service,method,protocol,invoke_type,target_app Total call information of a certain interface   rpc.consumer.service.stats.total_count.rate app,service,method,protocol,invoke_type,target_app Number of calls per second of a certain interface   rpc.consumer.service.stats.total_time.elapPerExec app,service,method,protocol,invoke_type,target_app Average time per execution of a certain interface   rpc.consumer.service.stats.total_time.max app,service,method,protocol,invoke_type,target_app Maximum total time of a certain interface   rpc.consumer.service.stats.total_time.totalTime …","date":-62135596800,"description":"","dir":"projects/sofa-lookout/sofarpc-metrics/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1f444349855508f4b111d8f2d2b5e43d","permalink":"/en/projects/sofa-lookout/sofarpc-metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-lookout/sofarpc-metrics/","summary":"SOFARPC Metrics SOFARPC currently measures two metrics.\nServer thread pool    metric name metric tags specification     rpc.bolt.threadpool.config bolt thread pool configuration Mainly includes thread pool configuration information for RPC server   rpc.bolt.threadpool.active.count  Running thread of the current thread pool   rpc.bolt.threadpool.idle.count  Idle thread of the current thread pool   rpc.bolt.threadpool.queue.size  Tasks in the queue of the current thread pool    Client call information    metric name metric tags specification     rpc.","tags":null,"title":"SOFARPC Metrics","type":"projects","url":"/en/projects/sofa-lookout/sofarpc-metrics/","wordcount":332},{"author":null,"categories":null,"content":"  SOFARPC 目前度量了两个指标。\n 服务端线程池    metric name metric tags specification     rpc.bolt.threadpool.config bolt 线程池配置 主要包括 rpc 服务端的线程池配置信息   rpc.bolt.threadpool.active.count  当前线程池的运行线程   rpc.bolt.threadpool.idle.count  当前线程池的空闲线程   rpc.bolt.threadpool.queue.size  当前线程池的队列中的任务    客户端调用信息    metric name metric tags specification     rpc.consumer.service.stats.fail_count.count app,service,method,protocol,invoke_type,target_app 某个具体接口失败次数   rpc.consumer.service.stats.fail_count.rate app,service,method,protocol,invoke_type,target_app 某个具体接口每秒失败   rpc.consumer.service.stats.fail_time.elapPerExec app,service,method,protocol,invoke_type,target_app 某个具体接口每秒执行时间   rpc.consumer.service.stats.fail_time.max app,service,method,protocol,invoke_type,target_app 某个具体接口失败时间最大值   rpc.consumer.service.stats.fail_time.totalTime app,service,method,protocol,invoke_type,target_app 某个具体接口失败时间总值   rpc.consumer.service.stats.request_size.max app,service,method,protocol,invoke_type,target_app 某个具体接口请求大小最大值   rpc.consumer.service.stats.request_size.rate app,service,method,protocol,invoke_type,target_app 某个具体接口每秒平均请求大小   rpc.consumer.service.stats.request_size.totalAmount app,service,method,protocol,invoke_type,target_app 某个具体接口请求大小总金额   rpc.consumer.service.stats.response_size.max app,service,method,protocol,invoke_type,target_app 某个具体接口响应大小最大值   rpc.consumer.service.stats.response_size.rate app,service,method,protocol,invoke_type,target_app 某个具体接口每秒平均响应大小   rpc.consumer.service.stats.response_size.totalAmount app,service,method,protocol,invoke_type,target_app 某个具体接口响应大小总金额   rpc.consumer.service.stats.total_count.count app,service,method,protocol,invoke_type,target_app 某个具体接口总的调用数目   rpc.consumer.service.stats.total_count.count_service_sum_30000 app,service,method,protocol,invoke_type,target_app 某个具体接口总的调用信息   rpc.consumer.service.stats.total_count.rate app,service,method,protocol,invoke_type,target_app 某个具体接口每秒调用次数   rpc.consumer.service.stats.total_time.elapPerExec app,service,method,protocol,invoke_type,target_app 某个具体接口平均每次指定时间   rpc.consumer.service.stats.total_time.max app,service,method,protocol,invoke_type,target_app 某个具体接口总时间最大值   rpc.consumer.service.stats.total_time.totalTime app,service,method,protocol,invoke_type,target_app 某个具体接口总时间    服务端被调用信息    metric name metric tags specification     rpc.provider.service.stats.fail_count.count app,service,method,protocol,caller_app 某个具体接口总的被调用失败次数   rpc.provider.service.stats.fail_count.rate app,service,method,protocol,caller_app 某个具体接口每秒失败次数   rpc.provider.service.stats.fail_time.elapPerExec app,service,method,protocol,caller_app 某个具体接口每次失败失败   rpc.provider.service.stats.fail_time.max app,service,method,protocol,caller_app 某个具体接口失败次数最大值   rpc.provider.service.stats.fail_time.totalTime app,service,method,protocol,caller_app 某个具体接口失败总时间   rpc.provider.service.stats.total_count.count app,service,method,protocol,caller_app 某个具体接口总的调用次数   rpc.provider.service.stats.total_count.rate app,service,method,protocol,caller_app 某个具体接口每秒调用次 …","date":-62135596800,"description":"","dir":"projects/sofa-lookout/sofarpc-metrics/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1f444349855508f4b111d8f2d2b5e43d","permalink":"/projects/sofa-lookout/sofarpc-metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/sofarpc-metrics/","summary":"SOFARPC 目前度量了两个指标。 服务端线程池 metric name metric tags specification rpc.bolt.threadpool.config bolt 线程池配置 主要包括 rpc 服务端的线程池配置信息 rpc.bolt.threadpool.active.count 当前线程池的运行线程 rpc.bolt.threadpool.idle.count 当前线程池的空闲线程 rpc.bolt.threadpool.queue.size 当前","tags":null,"title":"SOFARPC Metrics 指标","type":"projects","url":"/projects/sofa-lookout/sofarpc-metrics/","wordcount":487},{"author":null,"categories":null,"content":" SOFARPC is divided into two layers from bottom to top:\n Core layer: It contains the core components of RPC (such as various interfaces, APIs and common packages) and some common implementations (such as random load balancing algorithms). Function implementation layer: All users of the function implementation layer are equal, and all functions are implemented based on the extension mechanism.  The internal version specific for Ant Financial just has some internal extension based on the open source version.\nOf course, you can add your own third-party extension. See Extension mechanism for more information.\nModule division The implementation classes of each module only appear in the modules. Generally, the modules don\u0026amp;rsquo;t depend on each other. The modules that require cross dependency have been abstracted into the core or common modules.\nCurrently, SOFARPC is divided into the following modules:\nThe main modules and their corresponding dependencies are as follows:\n   Module Submodule Definition Description Dependency     all  Publish and packing module  All modules that need to be packaged   bom  Dependency control module Control dependency version None   example  Sample module  all   test  Test module Include integration test all   core api API module Include various basic process interfaces, messages, contexts, extension interfaces and others Common   core common Public module Include utils and data structure exception   core exception Exception module Include various exception interfaces and others common   bootstrap  Startup implementation module Include start class, service publish or reference logic, and registry operations core   proxy  Proxy implementation module Generate interface implementation proxy core   Client  Client implementation module Send request, receive response, maintain connections, routing, and implement load balancing, synchronization, asynchronization and other operations    server  Server implementation module Start listening, receive requests, send responses, distribute business threads, and implement other operations    filter  Interceptor implementation module Implement various interceptors for server and client core   codec  Coding and encoding implementation module Implement compression, serialization and other operations core   protocol  Protocol implementation module Package and process protocol and conduct negotiation core   transport  Network transmission implementation module Establish TCP connection, process sticky data packets, and distribute requested response objects    registry  Registry center implementation module Implement registration centers, such as ZooKeeper core    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/structure-intro/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1902232a50d57df7ab5b2c7eea1f8caa","permalink":"/en/projects/sofa-rpc/structure-intro/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/structure-intro/","summary":"SOFARPC is divided into two layers from bottom to top:\n Core layer: It contains the core components of RPC (such as various interfaces, APIs and common packages) and some common implementations (such as random load balancing algorithms). Function implementation layer: All users of the function implementation layer are equal, and all functions are implemented based on the extension mechanism.  The internal version specific for Ant Financial just has some internal extension based on the open source version.","tags":null,"title":"SOFARPC architecture","type":"projects","url":"/en/projects/sofa-rpc/structure-intro/","wordcount":347},{"author":null,"categories":null,"content":" SOFARPC Log Format After SOFARPC (v5.4.0 and above) is integrated in SOFATracer, the link data is output in JSON format by default. Each field meaning is as follows:\nRPC client digest log (rpc-client-digest.log)  Log printing time TraceId SpanId Span type Current appName Protocol type (bolt, rest) Service interface information Method name Current thread name Calling type (sync, callback, oneway, future) Routing record (DIRECT, REGISTRY) Target IP Target appName Local machine IP Return code (00=success; 01=business exception; 02=RPC logic error; 03=timeout failure;04=routing failure) Request serialization time (in ms) Response deserialization time (in ms) Response size (in Byte) Request size (in Byte) Client connection duration (in ms) Total call duration (in ms) Local client port Transparently transmitted baggage data (kv format)  Example:\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:03:20.708\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526807000498100185597\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;router.record\u0026amp;quot;:\u0026amp;quot;DIRECT\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;samples\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;,\u0026amp;quot;local.client.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;req.serialize.time\u0026amp;quot;:\u0026amp;quot;33\u0026amp;quot;,\u0026amp;quot;resp.deserialize.time\u0026amp;quot;:\u0026amp;quot;39\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;client.conn.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;client.elapse.time\u0026amp;quot;:\u0026amp;quot;155\u0026amp;quot;,\u0026amp;quot;local.client.port\u0026amp;quot;:\u0026amp;quot;59774\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RPC server digest log (rpc-server-digest.log)  Log printing time TraceId SpanId Span type Service interface information Method name Source IP Source appName Protocol (bolt, rest) Current appName Current thread name Return code (00=success; 01=business exception; 02=RPC logic error) Server thread pool waiting time (in ms) Business processing duration (in ms) Response serialization time (in ms) Request deserialization time (in ms) Response size (in Byte) Request size (in Byte) Transparently transmitted baggage data (kv format)  Example:\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-sofarpc/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ad45177e2719b9a22f4bfb07a0481905","permalink":"/en/projects/sofa-tracer/log-format-sofarpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/log-format-sofarpc/","summary":"SOFARPC Log Format After SOFARPC (v5.4.0 and above) is integrated in SOFATracer, the link data is output in JSON format by default. Each field meaning is as follows:\nRPC client digest log (rpc-client-digest.log)  Log printing time TraceId SpanId Span type Current appName Protocol type (bolt, rest) Service interface information Method name Current thread name Calling type (sync, callback, oneway, future) Routing record (DIRECT, REGISTRY) Target IP Target appName Local machine IP Return code (00=success; 01=business exception; 02=RPC logic error; 03=timeout failure;04=routing failure) Request serialization time (in ms) Response deserialization time (in ms) Response size (in Byte) Request size (in Byte) Client connection duration (in ms) Total call duration (in ms) Local client port Transparently transmitted baggage data (kv format)  Example:","tags":null,"title":"SOFARPC log","type":"projects","url":"/en/projects/sofa-tracer/log-format-sofarpc/","wordcount":259},{"author":null,"categories":null,"content":" 项目简介 SOFARPC 是蚂蚁金服开源的一款基于 Java 实现的 RPC 服务框架，为应用之间提供远程服务调用能力，具有高可伸缩性，高容错性，目前蚂蚁金服所有的业务的相互间的 RPC 调用都是采用 SOFARPC。SOFARPC 为用户提供了负载均衡，流量转发，链路追踪，链路数据透传，故障剔除等功能。\nSOFARPC 还支持不同的协议，目前包括 bolt，RESTful，dubbo，H2C 协议进行通信。其中 bolt 是蚂蚁金融服务集团开放的基于 Netty 开发的网络通信框架。\n基本原理  当一个 SOFARPC 的应用启动的时候，如果发现当前应用需要发布 RPC 服务的话，那么 SOFARPC 会将这些服务注册到服务注册中心上。如图中 Service 指向 Registry。 当引用这个服务的 SOFARPC 应用启动时，会从服务注册中心订阅到相应服务的元数据信息。服务注册中心收到订阅请求后，会将发布方的元数据列表实时推送给服务引用方。如图中 Registry 指向 Reference。 当服务引用方拿到地址以后，就可以从中选取地址发起调用了。如图中 Reference 指向 Service。  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/overview/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"62f0806ad40fcaaeab6a82470b14a2e2","permalink":"/projects/sofa-rpc/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/overview/","summary":"项目简介 SOFARPC 是蚂蚁金服开源的一款基于 Java 实现的 RPC 服务框架，为应用之间提供远程服务调用能力，具有高可伸缩性，高容错性，目前蚂蚁金服所有的业务的相互","tags":null,"title":"SOFARPC 介绍","type":"projects","url":"/projects/sofa-rpc/overview/","wordcount":402},{"author":null,"categories":null,"content":" 架构图 SOFARPC 从下到上分为两层：\n 核心层：包含了我们的 RPC 的核心组件（例如我们的各种接口、API、公共包）以及一些通用的实现（例如随机等负载均衡算法）。 功能实现层：所有的功能实现层的用户都是平等的，都是基于扩展机制实现的。  蚂蚁内部使用的版本也只是开源版本上增加一些内部扩展而已。\n当然你也可以增加自己三方扩展，参见：扩展机制\n模块划分 各个模块的实现类都只在自己模块中出现，一般不交叉依赖。需要交叉依赖的全部已经抽象到core或者common模块中。\n目前模块划分如下:\n主要模块及其依赖如下：\n   模块名 子模块名 中文名 说明 依赖     all  发布打包模块  需要打包的全部模块   bom  依赖管控模块 依赖版本管控 无   example  示例模块  all   test  测试模块 包含集成测试 all   core api API模块 各种基本流程接口、消息、上下文、扩展接口等 common   core common 公共模块 utils、数据结构 exception   core exception 异常模块 各种异常接口等 common   bootstrap  启动实现模块 启动类，发布或者引用服务逻辑、以及registry的操作 core   proxy  代理实现模块 接口实现代理生成 core   client  客户端实现模块 发送请求、接收响应、连接维护、路由、负载均衡、同步异步等 core   server  服务端实现模块 启动监听、接收请求，发送响应、业务线程分发等 core   filter  拦截器实现模块 服务端和客户端的各种拦截器实现 core   codec  编解码实现模块 例如压缩，序列化等 core   protocol  协议实现模块 协议的包装处理、协商 core   transport  网络传输实现模块 TCP连接的建立，数据分包粘包处理，请求响应对象分发等 core   registry  注册中心实现模块 实现注册中心，例如zk等 core    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/structure-intro/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1902232a50d57df7ab5b2c7eea1f8caa","permalink":"/projects/sofa-rpc/structure-intro/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/structure-intro/","summary":"架构图 SOFARPC 从下到上分为两层： 核心层：包含了我们的 RPC 的核心组件（例如我们的各种接口、API、公共包）以及一些通用的实现（例如随机等负载均衡算法）","tags":null,"title":"SOFARPC 工程架构介绍","type":"projects","url":"/projects/sofa-rpc/structure-intro/","wordcount":598},{"author":null,"categories":null,"content":" 本文档将演示了如何应用 SOFARPC 进行服务的发布和引用。 本例将在本地模拟服务端启动监听一个端口并发布一个服务，客户端引用该服务进行直连调用。\n您可以直接在工程下找到本文档的示例代码。\n创建工程 需要安装 JDK 6 及以上 和 Maven 3 以上.\n我们新建一个 Maven 工程，并引入 SOFARPC 的依赖。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-rpc-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;最新版本\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  注：最新版本可以从 https://github.com/sofastack/sofa-rpc/releases 里找到。\n编写服务端实现 第一步：创建接口\n/** * Quick Start demo interface */ public interface HelloService { String sayHello(String string); }  第二步：创建接口实现\n/** * Quick Start demo implement */ public class HelloServiceImpl implements HelloService { @Override public String sayHello(String string) { System.out.println(\u0026amp;quot;Server receive: \u0026amp;quot; + string); return \u0026amp;quot;hello \u0026amp;quot; + string + \u0026amp;quot; ！\u0026amp;quot;; } }  第三步：编写服务端代码\n/** * Quick Start Server */ public class QuickStartServer { public static void main(String[] args) { ServerConfig serverConfig = new ServerConfig() .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) // 设置一个协议，默认bolt .setPort(12200) // 设置一个端口，默认12200 .setDaemon(false); // 非守护线程 ProviderConfig\u0026amp;lt;HelloService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) // 指定接口 .setRef(new HelloServiceImpl()) // 指定实现 .setServer(serverConfig); // 指定服务端 providerConfig.export(); // 发布服务 } }  编写客户端实现 第一步：拿到服务端接口\n一般服务端会通过jar的形式将接口类提供给客户端。而在本例中，由于服务端和客户端在一个工程所以跳过。\n第二步：编程客户端代码\n/** * Quick Start client */ public class QuickStartClient { public static void main(String[] args) { ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) // 指定接口 .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) // 指定协议 .setDirectUrl(\u0026amp;quot;bolt://127.0.0.1:12200\u0026amp;quot;); // 指定直连地址 // 生成代理类 HelloService helloService = consumerConfig.refer(); while (true) { System.out.println(helloService.sayHello(\u0026amp;quot;world\u0026amp;quot;)); try { Thread.sleep(2000); } catch (Exception e) { } } } }  运行 分别启动服务端和客户端，观察运行效果。\n服务端将打印：\n Server receive: world\n 客户端将打印：\n hello world ！\n 更多 更多示例请参考：example\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/getting-started-with-rpc/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"192d252b0b36266622284b68d10e9fe4","permalink":"/projects/sofa-rpc/getting-started-with-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/getting-started-with-rpc/","summary":"本文档将演示了如何应用 SOFARPC 进行服务的发布和引用。 本例将在本地模拟服务端启动监听一个端口并发布一个服务，客户端引用该服务进行直连调用。 您可以直接","tags":null,"title":"SOFARPC 方式快速入门","type":"projects","url":"/projects/sofa-rpc/getting-started-with-rpc/","wordcount":563},{"author":null,"categories":null,"content":" SOFATracer 集成在 SOFARPC(5.4.0及之后的版本) 后输出链路数据的格式，默认为 JSON 数据格式，具体的字段含义解释如下：\nRPC 客户端 摘要日志（ rpc-client-digest.log）  日志打印时间 TraceId SpanId Span 类型 当前 appName 协议类型(bolt,rest) 服务接口信息 方法名 当前线程名 调用类型(sync,callback,oneway,future) 路由记录(DIRECT,REGISTRY) 目标ip 目标 appName 本机ip 返回码(00=成功/01=业务异常/02=RPC逻辑错误/03=超时失败/04=路由失败) 请求序列化时间(单位ms) 响应反序列化时间(单位ms) 响应大小(单位Byte) 请求大小(单位Byte) 客户端连接耗时(单位ms) 调用总耗时(单位ms) 本地客户端端口 透传的 baggage 数据 (kv 格式)  样例：\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:03:20.708\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526807000498100185597\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;router.record\u0026amp;quot;:\u0026amp;quot;DIRECT\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;samples\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;,\u0026amp;quot;local.client.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;req.serialize.time\u0026amp;quot;:\u0026amp;quot;33\u0026amp;quot;,\u0026amp;quot;resp.deserialize.time\u0026amp;quot;:\u0026amp;quot;39\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;client.conn.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;client.elapse.time\u0026amp;quot;:\u0026amp;quot;155\u0026amp;quot;,\u0026amp;quot;local.client.port\u0026amp;quot;:\u0026amp;quot;59774\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RPC 服务端 摘要日志（ rpc-server-digest.log）  日志打印时间 TraceId SpanId Span 类型 服务接口信息 方法名 来源ip 来源 appName 协议(bolt,rest) 当前 appName 当前线程名 返回码(00=成功/01=业务异常/02=RPC逻辑错误) 服务端线程池等待时间(单位ms) 业务处理耗时(单位ms) 响应序列化时间(单位ms) 请求反序列化时间(单位ms) 响应大小(单位Byte) 请求大小(单位Byte) 透传的 baggage 数据 (kv 格式)  样例：\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:00:53.312\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526806853032100185011\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;server\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;SOFA-BOLT-BIZ-12200-5-T1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;server.pool.wait.time\u0026amp;quot;:\u0026amp;quot;3\u0026amp;quot;,\u0026amp;quot;biz.impl.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;resp.serialize.time\u0026amp;quot;:\u0026amp;quot;4\u0026amp;quot;,\u0026amp;quot;req.deserialize.time\u0026amp;quot;:\u0026amp;quot;38\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RPC 客户端 统计日志（ rpc-client-stat.log）  日志打印时间 日志关键key 方法信息 客户端 appName 服务接口信息 调用次数 总耗时(单位ms) 调用结果(Y/N)  样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-05-18 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-sofarpc/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ad45177e2719b9a22f4bfb07a0481905","permalink":"/projects/sofa-tracer/log-format-sofarpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/log-format-sofarpc/","summary":"SOFATracer 集成在 SOFARPC(5.4.0及之后的版本) 后输出链路数据的格式，默认为 JSON 数据格式，具体的字段含义解释如下： RPC 客户端 摘要日志（ rpc-c","tags":null,"title":"SOFARPC 日志","type":"projects","url":"/projects/sofa-tracer/log-format-sofarpc/","wordcount":705},{"author":null,"categories":null,"content":"SOFARPC already supports using SOFARegistry as a service registry. Suppose you have deployed SOFARegistry Server locally according to SOFARegistry\u0026amp;rsquo;s Quick Start, and the service discovery port is set to 9603 by default.\nTo use SOFARegistry as a service registry in SOFARPC, you only need to add the following configuration to application.properties:\ncom.alipay.sofa.rpc.registry.address=sofa://127.0.0.1:9603  The current version of SOFARegistry is supported:\nSOFARPC: 5.5.2, SOFABoot: 2.6.3。\nBecause of the time of SOFABoot, users need to specify the version of rpc starter.\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;5.5.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  SOFARPC integration verification SOFARegistry server version: 5.2.0。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-sofa/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"65085018ce2b2b2ef452993bb79a69de","permalink":"/en/projects/sofa-rpc/registry-sofa/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-sofa/","summary":"SOFARPC already supports using SOFARegistry as a service registry. Suppose you have deployed SOFARegistry Server locally according to SOFARegistry\u0026rsquo;s Quick Start, and the service discovery port is set to 9603 by default.\nTo use SOFARegistry as a service registry in SOFARPC, you only need to add the following configuration to application.properties:\ncom.alipay.sofa.rpc.registry.address=sofa://127.0.0.1:9603  The current version of SOFARegistry is supported:\nSOFARPC: 5.5.2, SOFABoot: 2.6.3。\nBecause of the time of SOFABoot, users need to specify the version of rpc starter.","tags":null,"title":"SOFARegistry","type":"projects","url":"/en/projects/sofa-rpc/registry-sofa/","wordcount":90},{"author":null,"categories":null,"content":" Product introduction SOFARegistry is a production-level, low-latency, and highly available service registry powered by Ant Financial. SOFARegistry was developed on the basis ConfigServer of Taobao. After more than ten years of business development of Ant Financial, SOFARegistry has evolved into the fifth generation architecture. Currently, SOFARegistry not only provides full support to Ant Financial and its numerous partners, but also embraces the open source community. Built on an AP architecture, SOFARegistry support s message push in seconds. It also adopts a layered architecture to support infinite horizontal scaling.\nFeatures High scalability SOFARegistry adopts a layered architecture and partition-based data storage to break the single machine performance and capacity bottleneck, and to support the theoretical \u0026amp;ldquo;infinite horizontal scaling\u0026amp;rdquo;. It has been providing reliable services to the Ant Financial production environment which has a massive number of nodes and services.\nLow latency By virtue of the SOFABolt communication framework, SOFARegistry implements TCP long connection-based heartbeat detection among nodes, and the customized push mode to send service messages between upstream and downstream nodes in seconds.\nHighly available Unlike CP-architecture based registry products such as ZooKeeper, Consul, and Etcd, SOFARegistry adopts the AP architecture based on the service characteristics of service discovery, which significantly improves the availability of the registry in the case of failures caused by network partitioning. SOFARegistry takes many measures, such as multi-replica clusters, to prevent service unavailability arising from node failures.\nArchitecture SOFARegistry has four roles: Client, SessionServer, DataServer, and MetaServer, each with unique capabilities and responsibilities. They are combined to provide external services. The relationships and structures of them are explained as follows.\nClient A client provides basic APIs to allow applications to access SOFARegistry. The client provides JAR packages to application systems, so that they can call the service subscription and publishing features of SOFARegistry.\nSessionServer The SessionServer grants clients access to SessionServer, and accepts service publishing and subscription requests from clients. It also serves as an intermediate layer to forward the published data to DataServer for storage. The SessionServer can be infinitely scaled up to support connection with large amounts of clients.\nDataServer The DataServer is responsible for storing data published by clients. The data is stored by dataId through consistent hashing. DataServer supports multi-replica backup to ensure high availability of the data. The Data can also be infinitely scaled up to support large amounts of data.\nMetaServer The MetaServer is responsible for maintaining the consistency lists of the SessionServer and DataServer within the cluster, and immediately notify other nodes in the …","date":-62135596800,"description":"","dir":"projects/sofa-registry/overview/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d444761f1ad8b0c52e3505926176b13f","permalink":"/en/projects/sofa-registry/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-registry/overview/","summary":"Product introduction SOFARegistry is a production-level, low-latency, and highly available service registry powered by Ant Financial. SOFARegistry was developed on the basis ConfigServer of Taobao. After more than ten years of business development of Ant Financial, SOFARegistry has evolved into the fifth generation architecture. Currently, SOFARegistry not only provides full support to Ant Financial and its numerous partners, but also embraces the open source community. Built on an AP architecture, SOFARegistry support s message push in seconds.","tags":null,"title":"SOFARegistry overview","type":"projects","url":"/en/projects/sofa-registry/overview/","wordcount":429},{"author":null,"categories":null,"content":" 项目简介 SOFARegistry 是蚂蚁金服开源的一个生产级、高时效、高可用的服务注册中心。SOFARegistry 最早源自于淘宝的 ConfigServer，十年来，随着蚂蚁金服的业务发展，注册中心架构已经演进至第五代。目前 SOFARegistry 不仅全面服务于蚂蚁金服的自有业务，还随着蚂蚁金融科技服务众多合作伙伴，同时也兼容开源生态。SOFARegistry 采用 AP 架构，支持秒级时效性推送，同时采用分层架构支持无限水平扩展。\n产品特点 高可扩展性 采用分层架构、数据分片存储等方式，突破单机性能与容量瓶颈，接近理论上的“无限水平扩展”。经受过蚂蚁金服生产环境海量节点数与服务数的考验。\n高时效性 借助 SOFABolt 通信框架，实现基于TCP长连接的节点判活与推模式的变更推送，服务上下线通知时效性在秒级以内。\n高可用性 不同于 Zookeeper、Consul、Etcd 等 CP 架构注册中心产品，SOFARegistry 针对服务发现的业务特点，采用 AP 架构，最大限度地保证网络分区故障下注册中心的可用性。通过集群多副本等方式，应对自身节点故障。\n架构 服务注册中心分为四个角色，客户端（Client）、会话服务器（SessionServer）、数据服务器（DataServer）、元数据服务器（MetaServer），每个角色司职不同能力组合后共同提供对外服务能力，各部分关系和结构如下：\nClient 提供应用接入服务注册中心的基本 API 能力，应用系统通过依赖客户端 JAR 包，通过编程方式调用服务注册中心的服务订阅和服务发布能力。\nSessionServer 会话服务器，提供客户端接入能力，接受客户端的服务发布及服务订阅请求，并作为一个中间层将发布数据转发 DataServer 存储。SessionServer 可无限扩展以支持海量客户端连接。\nDataServer 数据服务器，负责存储客户端发布数据，数据存储按照数据 ID 进行一致性 hash 分片存储，支持多副本备份，保证数据高可用。DataServer 可无限扩展以支持海量数据量。\nMetaServer 元数据服务器，负责维护集群 SessionServer 和 DataServer 的一致列表，在节点变更时及时通知集群内其他节点。MetaServer 通过 SOFAJRaft 保证高可用和一致性。\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/overview/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d444761f1ad8b0c52e3505926176b13f","permalink":"/projects/sofa-registry/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-registry/overview/","summary":"项目简介 SOFARegistry 是蚂蚁金服开源的一个生产级、高时效、高可用的服务注册中心。SOFARegistry 最早源自于淘宝的 ConfigServer，十年来","tags":null,"title":"SOFARegistry 介绍","type":"projects","url":"/projects/sofa-registry/overview/","wordcount":840},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"本指南为 SOFAStack 多个组件的 Demo 合集。","dir":"guides/sofastack-demos/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6a1fada81ea88116efa0e30539da60a1","permalink":"/guides/sofastack-demos/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/sofastack-demos/","summary":"","tags":null,"title":"SOFAStack Demos","type":"guides","url":"/guides/sofastack-demos/","wordcount":0},{"author":null,"categories":null,"content":" Since SOFARPC 5.4.0, the SOFATracer function is integrated, which is enabled by default. It can output the data information in the link.\nBy default, the output data is in JSON format. The involved fields are as follows:\nRPC client digest Log (rpc-client-digest.log)  Log printing time TraceId SpanId Span type Current appName Protocol type Service interface information Method name Current thread name Calling type Routing record Target IP Local machine IP Return code Request serialization duration Response deserialization duration Response size (in Byte) Request size (in Byte) Client connection duration Total duration for call Local client port Transparently transmitted baggage data (kv format)  Example:\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:03:20.708\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526807000498100185597\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;router.record\u0026amp;quot;:\u0026amp;quot;DIRECT\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;,\u0026amp;quot;local.client.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;req.serialize.time\u0026amp;quot;:\u0026amp;quot;33\u0026amp;quot;,\u0026amp;quot;resp.deserialize.time\u0026amp;quot;:\u0026amp;quot;39\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;client.conn.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;client.elapse.time\u0026amp;quot;:\u0026amp;quot;155\u0026amp;quot;,\u0026amp;quot;local.client.port\u0026amp;quot;:\u0026amp;quot;59774\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RPC server digest log (rpc-server-digest.log)  Log printing time TraceId SpanId Span type Service interface information Method name Source IP Source appName Protocol Local appName Current thread name Return code Server thread pool waiting time Business processing duration Response serialization duration Request deserialization duration Response size (in Byte) Request size (in Byte) Transparently transmitted baggage data (kv format)  Example:\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/sofatracer-usage/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"dde9dc7a759b7c0c272d67bac1b315d4","permalink":"/en/projects/sofa-rpc/sofatracer-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/sofatracer-usage/","summary":"Since SOFARPC 5.4.0, the SOFATracer function is integrated, which is enabled by default. It can output the data information in the link.\nBy default, the output data is in JSON format. The involved fields are as follows:\nRPC client digest Log (rpc-client-digest.log)  Log printing time TraceId SpanId Span type Current appName Protocol type Service interface information Method name Current thread name Calling type Routing record Target IP Local machine IP Return code Request serialization duration Response deserialization duration Response size (in Byte) Request size (in Byte) Client connection duration Total duration for call Local client port Transparently transmitted baggage data (kv format)  Example:","tags":null,"title":"SOFATracer","type":"projects","url":"/en/projects/sofa-rpc/sofatracer-usage/","wordcount":222},{"author":null,"categories":null,"content":" SOFATracer Tools Get Span through SOFATracer context In the process of a distributed link call, the component that integrates SOFATracer generates a Span and caches it in the SOFATracer context. And the context is cached in ThreadLocal. You can get the current SOFATracer context in the following way:\nSofaTraceContext sofaTraceContext = SofaTraceContextHolder.getSofaTraceContext();  Through the SOFATracer context SofaTraceContext, you can add, delete, modify, check, and empty the cached Spans. As the developers responsible for integrating components, we will add, delete, modify and check the SOFATracer context to integrate distributed link tracking. However, as the application developer to directly use SOFATracer, you only need to get the corresponding Span. That is to say, you only need to use the following method after getting the context:\nSofaTracerSpan sofaTracerSpan = sofaTraceContext.getCurrentSpan();  Get information through Span When using the SOFATracer plugin component, such as Spring MVC, the component integrates the capabilities of SOFATracer. So it can get all the information in the Span after getting Span. The specific acquisition method example (it demands that Span is not empty, namely that the corresponding component has integrated SOFATracer) is as follow:\nGet TraceId and SpanId: SofaTracerSpanContext sofaTracerSpanContext = currentSpan.getSofaTracerSpanContext(); String traceId = sofaTracerSpanContext.getTraceId(); String spanId = sofaTracerSpanContext.getSpanId();  Get Tags and Logs in OpenTracing specification Get Tags:\nMap\u0026amp;lt;String, String\u0026amp;gt; tagsStr = sofaTracerSpan.getTagsWithStr(); Map\u0026amp;lt;String, Boolean\u0026amp;gt; tagsBool = sofaTracerSpan.getTagsWithBool(); Map\u0026amp;lt;String, Number\u0026amp;gt; tagsNumber = sofaTracerSpan.getTagsWithNumber();  Get Logs:\nList \u0026amp;lt;LogData\u0026amp;gt; logDataList = sofaTracerSpan.getLogs ();  Process transparently transmitted data Baggage element is a collection of key-value pairs that carries data to be transparently transmitted. In SOFATracer, Baggage data is divided into sysBaggage and bizBaggage; sysBaggage mainly refers to transparently transmitted system data, and bizBaggage mainly refers to transparently transmitted business data.\nConfigure and get BaggageItem BaggageItem is a data element in the Baggage collection.\n Configure the corresponding BaggageItem data through the standard interface:  String baggageKey = \u0026amp;quot;key\u0026amp;quot;; String baggageVal = \u0026amp;quot;val\u0026amp;quot;; sofaTracerSpan.setBaggageItem(baggageKey,baggageVal);   Get the corresponding BaggageItem data through the standard interface:  String baggageKey = \u0026amp;quot;key\u0026amp;quot;; String baggageValue = sofaTracerSpan.getBaggageItem(baggageKey);  Note: Configuring and getting Baggage data through the standard interface is actually operated on bizBaggage.\nConfigure and get \u0026amp;lsquo;Baggage\u0026amp;rsquo; data 1, Configure \u0026amp;lsquo;Baggage\u0026amp;rsquo; data\nSofaTracerSpanContext sofaTracerSpanContext = sofaTracerSpan.getSofaTracerSpanContext(); Map\u0026amp;lt;String, String\u0026amp;gt; bizBaggage …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/utils/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"d40318a5bd3ee5e1573f8770ea649dba","permalink":"/en/projects/sofa-tracer/utils/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-tracer/utils/","summary":"SOFATracer Tools Get Span through SOFATracer context In the process of a distributed link call, the component that integrates SOFATracer generates a Span and caches it in the SOFATracer context. And the context is cached in ThreadLocal. You can get the current SOFATracer context in the following way:\nSofaTraceContext sofaTraceContext = SofaTraceContextHolder.getSofaTraceContext();  Through the SOFATracer context SofaTraceContext, you can add, delete, modify, check, and empty the cached Spans. As the developers responsible for integrating components, we will add, delete, modify and check the SOFATracer context to integrate distributed link tracking.","tags":null,"title":"SOFATracer Tools","type":"projects","url":"/en/projects/sofa-tracer/utils/","wordcount":443},{"author":null,"categories":null,"content":" SOFATracer configuration item After introducing SOFATracer, you can add related configuration items in Spring Boot configuration file application.properties to customize the behaviors of SOFATracer.\nFor SOFATracer log output directory, you can configure logging.path in application.properties, then the log output path is ${logging.path}/tracelog; if logging.path is not configured, the default output path is ${user.home}/logs/tracelog.\n   Configuration item Description Default value     logging.path log output directory SOFATracer output logs to logging.path directory in priority; If the directory is not configured, log will be output to ${user.home} by default.   com.alipay.sofa.tracer.disableDigestLog Disable all integrated SOFATracer summary log printing false   com.alipay.sofa.tracer.disableConfiguration[${logType}] Disable specific SOFATracer summary log printing of ${logType}. ${logType} indicates the log type, such as spring-mvc-digest.log false   com.alipay.sofa.tracer.tracerGlobalRollingPolicy SOFATracer log rolling policy yyyy-MM-dd：roll by day；yyyy-MM-dd_HH：roll by hour;Logs are not rolled by day by default.   com.alipay.sofa.tracer.tracerGlobalLogReserveDay Retention days of SOFATracer logs Retained for 7 days by default.   com.alipay.sofa.tracer.statLogInterval Time interval of statistical logs, unit: second Output statistical logs once every 60 seconds by default   com.alipay.sofa.tracer.baggageMaxLength Maximum length for retaining penetration data Default: 1024   com.alipay.sofa.tracer.zipkin.enabled Whether to enable SOFATracer remote data reporting to Zipkin true: enable; false: disable. Disabled by default.   com.alipay.sofa.tracer.zipkin.baseUrl The address Zipkin address to which SOFATracer remotely reports data, which works only in the case of com.alipay.sofa.tracer.zipkin.enabled=true Format: http: //${host}:${port}   com.alipay.sofa.tracer.springmvc.filterOrder Order validated by SOFATrace Filter intergrated in SpringMVC -2147483647(org.springframework.core.Ordered#HIGHEST_PRECEDENCE + 1)   com.alipay.sofa.tracer.springmvc.urlPatterns URL Pattern paths validated by SOFATrace Filter intergrated in SpringMVC /*: All validated    ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/configuration/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1a6bf2b7aa168440544f8d0d69358869","permalink":"/en/projects/sofa-tracer/configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/configuration/","summary":"SOFATracer configuration item After introducing SOFATracer, you can add related configuration items in Spring Boot configuration file application.properties to customize the behaviors of SOFATracer.\nFor SOFATracer log output directory, you can configure logging.path in application.properties, then the log output path is ${logging.path}/tracelog; if logging.path is not configured, the default output path is ${user.home}/logs/tracelog.\n   Configuration item Description Default value     logging.path log output directory SOFATracer output logs to logging.","tags":null,"title":"SOFATracer configuration items","type":"projects","url":"/en/projects/sofa-tracer/configuration/","wordcount":231},{"author":null,"categories":null,"content":" SOFATracer 是蚂蚁金服开发的基于 OpenTracing 规范 的分布式链路跟踪系统，其核心理念就是通过一个全局的 TraceId 将分布在各个服务节点上的同一次请求串联起来。通过统一的 TraceId 将调用链路中的各种网络调用情况以日志的方式记录下来同时也提供远程汇报到 Zipkin 进行展示的能力，以此达到透视化网络调用的目的。\n功能描述 基于 OpenTracing 规范提供分布式链路跟踪解决方案 基于 OpenTracing 规范 并扩展其能力提供链路跟踪的解决方案。各个框架或者组件可以基于此实现，通过在各个组件中埋点的方式来提供链路跟踪的能力。\n提供异步落地磁盘的日志打印能力 基于 Disruptor 高性能无锁循环队列，提供异步打印日志到本地磁盘的能力。框架或者组件能够在接入时，在异步日志打印的前提下可以自定义日志文件的输出格式。SOFATracer 提供两种类似的日志打印类型即摘要日志和统计日志，摘要日志：每一次调用均会落地磁盘的日志；统计日志：每隔一定时间间隔进行统计输出的日志。\n支持日志自清除和滚动能力 异步落地磁盘的 SOFATracer 日志支持自清除和滚动能力，支持按照按照天清除和按照小时或者天滚动的能力\n基于 SLF4J MDC 的扩展能力 SLF4J 提供了 MDC（Mapped Diagnostic Contexts）功能，可以支持用户定义和修改日志的输出格式以及内容。SOFATracer 集成了 SLF4J MDC 功能，方便用户在只简单修改日志配置文件即可输出当前 Tracer 上下文的 TraceId 和 SpanId。\n界面展示能力 SOFATracer 可以将链路跟踪数据远程上报到开源产品 Zipkin 做分布式链路跟踪的展示。\n统一配置能力 配置文件中提供丰富的配置能力以定制化应用的个性需求。\n应用场景 解决在实施大规模微服务架构时的链路跟踪问题，达到透视化网络调用的目的，并可用于故障的快速发现，服务治理等。\n组件埋点 目前 SOFATracer 支持 Spring MVC、标准 JDBC 接口实现的数据库连接池(DBCP、Druid、c3p0、tomcat、HikariCP、BoneCP)、HttpClient、Dubbo、Spring Cloud OpenFeign 等开源组件，其他开源组件（如 MQ、Redis）埋点支持在开发中。\n   支持组件 接入文档 支持版本     Spring MVC doc link 2.1.0   DBCP doc link 2.2.0   Druid doc link 2.2.0   c3p0 doc link 2.2.0   HikariCP doc link 2.2.0   HttpClient doc link 2.2.0   OkHttp doc link 2.3.2   Dubbo doc link 2.4.0   Redis TODO    MQ TODO     ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/overview/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e8d6bf5eec6c5ce1e41d461743f2c4f1","permalink":"/projects/sofa-tracer/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/overview/","summary":"SOFATracer 是蚂蚁金服开发的基于 OpenTracing 规范 的分布式链路跟踪系统，其核心理念就是通过一个全局的 TraceId 将分布在各个服务节点上的同一次请求串联起来。通过统一的 TraceId 将调","tags":null,"title":"SOFATracer 介绍","type":"projects","url":"/projects/sofa-tracer/overview/","wordcount":843},{"author":null,"categories":null,"content":" 通过 SOFATracer 上下文获取 Span 在一次分布式链路调用过程中，在集成了 SOFATracer 的组件会产生一个 Span 并会缓存到 SOFATracer 的上下文中，这个上下文是缓存在 ThreadLocal 中的，作为使用者可以通过如下的方式随时获取到当前 SOFATracer 的上下文：\nSofaTraceContext sofaTraceContext = SofaTraceContextHolder.getSofaTraceContext();  SOFATracer 上下文 SofaTraceContext 通过这个实例，可以对其缓存的 Span 执行增、删、改、查和清空操作。作为组件集成的同学在集成过程中我们会对 SOFATracer 上下文做增、删、改和查等操作来集成分布式链路跟踪的能力；但是作为直接使用 SOFATracer 的应用开发者，我们只需要能够获取相应的 Span 即可，即只需要在获取上下文后使用如下的方法：\nSofaTracerSpan sofaTracerSpan = sofaTraceContext.getCurrentSpan();  通过 Span 获取信息 在使用相应的组件如 Spring MVC 时，该组件集成了 SOFATracer 的能力后可以在获取到 Span 后获取到 Span 中的所有信息，具体获取方式示例（前提 Span 不为空即相应组件已经集成 SOFATracer）：\n获取 TraceId 和 SpanId ： SofaTracerSpanContext sofaTracerSpanContext = currentSpan.getSofaTracerSpanContext(); String traceId = sofaTracerSpanContext.getTraceId(); String spanId = sofaTracerSpanContext.getSpanId();  获取 OpenTracing 规范中的 Tags 和 Logs 获取 Tags:\nMap\u0026amp;lt;String, String\u0026amp;gt; tagsStr = sofaTracerSpan.getTagsWithStr(); Map\u0026amp;lt;String, Boolean\u0026amp;gt; tagsBool = sofaTracerSpan.getTagsWithBool(); Map\u0026amp;lt;String, Number\u0026amp;gt; tagsNumber = sofaTracerSpan.getTagsWithNumber();  获取 Logs:\nList\u0026amp;lt;LogData\u0026amp;gt; logDataList = sofaTracerSpan.getLogs();  透传数据处理 Baggage 元素是一个键值对集合，其携带的是需要透传的数据。SOFATracer 中将 Baggage 数据分为 sysBaggage 和 bizBaggage；sysBaggage 主要是指系统维度的透传数据，bizBaggage 主要是指业务的透传数据。\n设置和获取 BaggageItem BaggageItem 是 Baggage集合中的数据元素。\n1、通过标准接口设置相应的 BaggageItem 数据：\nString baggageKey = \u0026amp;quot;key\u0026amp;quot;; String baggageVal = \u0026amp;quot;val\u0026amp;quot;; sofaTracerSpan.setBaggageItem(baggageKey,baggageVal);  2、通过标准接口获取相应的 BaggageItem 数据：\nString baggageKey = \u0026amp;quot;key\u0026amp;quot;; String baggageValue = sofaTracerSpan.getBaggageItem(baggageKey);  注：当通过标准接口进行设置和获取 Baggage 数据时，实际上操作的对象均为 bizBaggage\n设置和获取 \u0026amp;lsquo;Baggage\u0026amp;rsquo; 数据 1、设置 \u0026amp;lsquo;Baggage\u0026amp;rsquo; 数据\nSofaTracerSpanContext sofaTracerSpanContext = sofaTracerSpan.getSofaTracerSpanContext(); Map\u0026amp;lt;String, String\u0026amp;gt; bizBaggage = new HashMap\u0026amp;lt;String, String\u0026amp;gt;(); bizBaggage.put(\u0026amp;quot;bizKey\u0026amp;quot;,\u0026amp;quot;bizVal\u0026amp;quot;); sofaTracerSpanContext.addBizBaggage(bizBaggage); Map\u0026amp;lt;String, String\u0026amp;gt; sysBaggage = new HashMap\u0026amp;lt;String, String\u0026amp;gt;(); sysBaggage.put(\u0026amp;quot;sysKey\u0026amp;quot;,\u0026amp;quot;sysVal\u0026amp;quot;); sofaTracerSpanContext.addSysBaggage(sysBaggage);  2、获取 \u0026amp;lsquo;Baggage\u0026amp;rsquo; 数据\nSofaTracerSpanContext sofaTracerSpanContext = sofaTracerSpan.getSofaTracerSpanContext(); //获取 bizBaggage Map\u0026amp;lt;String, String\u0026amp;gt; bizBaggages = sofaTracerSpanContext.getBizBaggage(); //获取 sysBaggage Map\u0026amp;lt;String, String\u0026amp;gt; sysBaggages = sofaTracerSpanContext.getSysBaggage();  遍历 Baggage 数据 OpenTracing 规范中 SpanContext 接口提供了 baggageItems() 方法，可以通过这个方法来遍历所有的 baggage 元素。SOFATracer 在 SofaTracerSpanContext 类中对 baggageItems() 方法进行了具体实现。\nIterable\u0026amp;lt;Map.Entry\u0026amp;lt;String, String\u0026amp;gt;\u0026amp;gt; entrySet = sofaTracerSpanContext.baggageItems();  注：遍历 Baggage 数据返回的是 sysBaggage 和 bizBaggage 的合集。\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/utils/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d40318a5bd3ee5e1573f8770ea649dba","permalink":"/projects/sofa-tracer/utils/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/utils/","summary":"通过 SOFATracer 上下文获取 Span 在一次分布式链路调用过程中，在集成了 SOFATracer 的组件会产生一个 Span 并会缓存到 SOFATracer 的上下文中，这个上下文是缓存在 ThreadLocal 中的，作为使用者可以通","tags":null,"title":"SOFATracer 工具类","type":"projects","url":"/projects/sofa-tracer/utils/","wordcount":738},{"author":null,"categories":null,"content":" 应用在引入 SOFATracer 后，可以在 Spring Boot 的配置文件 application.properties 中添加相关配置项来定制 SOFATracer 的相关行为。\nSOFATracer 的日志输出目录，可以在 application.properties 中配置 logging.path 的路径，那么其日志输出路径为 ${logging.path}/tracelog；如果没有配置 logging.path，那么 SOFATracer 的默认输出路径为 ${user.home}/logs/tracelog。\nSpringBoot 工程配置    SOFATracer 配置项 说明 默认值     logging.path 日志输出目录 SOFATracer 会优先输出到 logging.path 目录下；如果没有配置日志输出目录，那默认输出到 ${user.home}   com.alipay.sofa.tracer.disableDigestLog 是否关闭所有集成 SOFATracer 组件摘要日志打印 false   com.alipay.sofa.tracer.disableConfiguration[${logType}] 关闭指定 ${logType} 的 SOFATracer 组件摘要日志打印。${logType}是指具体的日志类型，如：spring-mvc-digest.log false   com.alipay.sofa.tracer.tracerGlobalRollingPolicy SOFATracer 日志的滚动策略 .yyyy-MM-dd：按照天滚动；.yyyy-MM-dd_HH：按照小时滚动。默认不配置按照天滚动   com.alipay.sofa.tracer.tracerGlobalLogReserveDay SOFATracer 日志的保留天数 默认保留 7 天   com.alipay.sofa.tracer.statLogInterval 统计日志的时间间隔，单位：秒 默认 60 秒统计日志输出一次   com.alipay.sofa.tracer.baggageMaxLength 透传数据能够允许存放的最大长度 默认值 1024   com.alipay.sofa.tracer.zipkin.enabled 是否开启 SOFATracer 远程上报数据到 Zipkin true：开启上报；false：关闭上报。默认不上报   com.alipay.sofa.tracer.zipkin.baseUrl SOFATracer 远程上报数据到 Zipkin 的地址，com.alipay.sofa.tracer.zipkin.enabled=true时配置此地址才有意义 格式：http://${host}:${port}   com.alipay.sofa.tracer.springmvc.filterOrder SOFATracer 集成在 SpringMVC 的 Filter 生效的 Order -2147483647（org.springframework.core.Ordered#HIGHEST_PRECEDENCE + 1）   com.alipay.sofa.tracer.springmvc.urlPatterns SOFATracer 集成在 SpringMVC 的 Filter 生效的 URL Pattern 路径 /* 全部生效   com.alipay.sofa.tracer.jsonOutput 是否以json格式输出日志 true，如果期望较少日志空间占用，可以使用非 json 格式输出（日志顺序与JSON 格式顺序一致）    非SpringBoot 工程配置 在非 SpringBoot 工程中，可以通过在 classpath 下新建一个 sofa.tracer.properties 配置文件，配置项如下：\n   SOFATracer 配置项 说明 默认值     disable_middleware_digest_log 是否关闭中间件组件摘要日志打印 false   disable_digest_log 关闭摘要日志打印。 false   tracer_global_rolling_policy SOFATracer 日志的滚动策略 .yyyy-MM-dd：按照天滚动；.yyyy-MM-dd_HH：按照小时滚动。默认不配置按照天滚动   tracer_global_log_reserve_day SOFATracer 日志的保留天数 默认保留 7 天   stat_log_interval 统计日志的时间间隔，单位：秒 默认 60 秒统计日志输出一次   tracer_penetrate_attribute_max_length 透传数据能够允许存放的最大长度 默认值 1024   tracer_async_appender_allow_discard 是否允许丢失日志 false   tracer_async_appender_is_out_discard_number 丢失日志数 0   spring.application.name 应用名 ``   tracer_sampler_strategy_name_key 采样策略名 ``   tracer_sampler_strategy_custom_rule_class_name 采样规则 spi 实现的类的全限定名 ``   tracer_sampler_strategy_percentage_key 采样比率    com.alipay.sofa.tracer.jsonOutput 是否以json格式输出日志 true，如果期望较少日志空间占用，可以使用非 json 格式输出（日志顺序与JSON 格式顺序一致）    ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/configuration/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1a6bf2b7aa168440544f8d0d69358869","permalink":"/projects/sofa-tracer/configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-tracer/configuration/","summary":"应用在引入 SOFATracer 后，可以在 Spring Boot 的配置文件 application.properties 中添加相关配置项来定制 SOFATracer 的相关行为。 SOFATracer 的日志输出目录，可以在 application.properties 中配置 logging.path 的路径，那么其日志输出路径为 ${","tags":null,"title":"SOFATracer 配置项","type":"projects","url":"/projects/sofa-tracer/configuration/","wordcount":1004},{"author":null,"categories":null,"content":" 在SOFARPC(5.4.0及之后的版本) 后的版本中，我们集成了SOFATracer的功能，默认开启，可以输出链路中的数据信息。\n默认为 JSON 数据格式，具体的字段含义解释如下：\nRPC 客户端 摘要日志（ rpc-client-digest.log）  日志打印时间 TraceId SpanId Span 类型 当前 appName 协议类型 服务接口信息 方法名 当前线程名 调用类型 路由记录 目标ip 本机ip 返回码 请求序列化时间 响应反序列化时间 响应大小(单位Byte) 请求大小(单位Byte) 客户端连接耗时 调用总耗时 本地客户端端口 透传的 baggage 数据 (kv 格式)  样例：\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:03:20.708\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526807000498100185597\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;main\u0026amp;quot;,\u0026amp;quot;invoke.type\u0026amp;quot;:\u0026amp;quot;sync\u0026amp;quot;,\u0026amp;quot;router.record\u0026amp;quot;:\u0026amp;quot;DIRECT\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;,\u0026amp;quot;local.client.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;req.serialize.time\u0026amp;quot;:\u0026amp;quot;33\u0026amp;quot;,\u0026amp;quot;resp.deserialize.time\u0026amp;quot;:\u0026amp;quot;39\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;client.conn.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;client.elapse.time\u0026amp;quot;:\u0026amp;quot;155\u0026amp;quot;,\u0026amp;quot;local.client.port\u0026amp;quot;:\u0026amp;quot;59774\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  RPC 服务端 摘要日志（ rpc-server-digest.log）  日志打印时间 TraceId SpanId Span 类型 服务接口信息 方法名 来源ip 来源 appName 协议 本应用 appName 当前线程名 返回码 服务端线程池等待时间 业务处理耗时 响应序列化时间 请求反序列化时间 响应大小(单位Byte) 请求大小(单位Byte) 透传的 baggage 数据 (kv 格式)  样例：\n{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 17:00:53.312\u0026amp;quot;,\u0026amp;quot;tracerId\u0026amp;quot;:\u0026amp;quot;1e27326d1526806853032100185011\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;server\u0026amp;quot;,\u0026amp;quot;service\u0026amp;quot;:\u0026amp;quot;com.alipay.sofa.tracer.examples.sofarpc.direct.DirectService:1.0\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;sayDirect\u0026amp;quot;,\u0026amp;quot;remote.ip\u0026amp;quot;:\u0026amp;quot;127.0.0.1\u0026amp;quot;,\u0026amp;quot;remote.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;protocol\u0026amp;quot;:\u0026amp;quot;bolt\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerRPC\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;SOFA-BOLT-BIZ-12200-5-T1\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;00\u0026amp;quot;,\u0026amp;quot;server.pool.wait.time\u0026amp;quot;:\u0026amp;quot;3\u0026amp;quot;,\u0026amp;quot;biz.impl.time\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;resp.serialize.time\u0026amp;quot;:\u0026amp;quot;4\u0026amp;quot;,\u0026amp;quot;req.deserialize.time\u0026amp;quot;:\u0026amp;quot;38\u0026amp;quot;,\u0026amp;quot;resp.size\u0026amp;quot;:\u0026amp;quot;170\u0026amp;quot;,\u0026amp;quot;req.size\u0026amp;quot;:\u0026amp;quot;582\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,{\u0026amp;quot;timestamp\u0026amp;quot;:\u0026amp;quot;2018-05-20 …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/sofatracer-usage/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dde9dc7a759b7c0c272d67bac1b315d4","permalink":"/projects/sofa-rpc/sofatracer-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/sofatracer-usage/","summary":"在SOFARPC(5.4.0及之后的版本) 后的版本中，我们集成了SOFATracer的功能，默认开启，可以输出链路中的数据信息。 默认为 JSON 数据","tags":null,"title":"SOFATracer 链路追踪","type":"projects","url":"/projects/sofa-rpc/sofatracer-usage/","wordcount":529},{"author":null,"categories":null,"content":" SOFATracer is a distributed link tracing system based on OpenTracing specification developed by Ant Financial. Its core concept is to concatenate the same request distributed on each service node with a global TraceId. By the unified TraceId, it can record the various network call information in the call link in logs, and can remotely report the call records to Zipkin for presentation, thus implementing perspective network call.\nFeatures Distributed link tracing solution based on OpenTracing specification SOFATracer is a solution that provides link tracing based on and improved from the OpenTracing specification. Based on this implementation, each framework or component can provide the ability to link tracking by burying points.\nProvide asynchronous log printing to disks Based on high-performance lock-free loop queue of Disruptor, SOFATracer provides the ability to print logs asynchronously to local disk. The introduced framework or component can customize the output format of the log file under the premise of asynchronous log printing. SOFATracer provides two types of logs, digest log and statistical log. Digest log: logs that are printed to disk upon each call. Statistical log: logs that are printed at regular intervals.\nSupport automatic log cleanup and scrolling Asynchronous SOFATracer log supports automatic cleanup and scrolling, and supports cleaning by day and scrolling by hour or day.\nExtended based on SLF4J MDC SLF4J provides MDC (Mapped Diagnostic Contexts), which supports user to define and modify the output log format and content. SOFATracer integrates the SLF4J MDC function, which allows user to output the TraceId and SpanId of the current Tracer context by simply modifying the log configuration file.\nInterface presentation SOFATracer can remotely report link tracing data to the open-source product Zipkin for distributed link tracing presentation.\nUnified configuration The profile file provides various configuration options for you to customize the individual requirements of the application.\nScenario SOFATracer solves the problem of link tracing when implementing large-scale microservice architecture, achieves perspective network call, and can be used to rapidly Failures Discovery, Service Governance, and so on.\nComponent event tracking At present, SOFATracer supports Spring MVC, database connection pool (DBCP, Druid, c3p0, tomcat, HikariCP, BoneCP) acheived by standard JDBC interface, HttpClient and other open-source components. Event tracking for other open-source components (such as MQ, Redis) is still in development.\n   Component Document Version     Spring MVC doc link 2.1.0   DBCP doc link 2.2.0   Druid doc link 2.2.0   C3p0 doc link 2.2.0   HikariCP doc link 2.2.0   HttpClient doc link 2.2.0   RestTemplate doc link 2.3.0   OkHttp doc link 2.3.2   Dubbo doc link 2.4.0   OpenFeign doc link 3.0.4   Redis TODO    MQ TODO     ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/overview/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e8d6bf5eec6c5ce1e41d461743f2c4f1","permalink":"/en/projects/sofa-tracer/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/overview/","summary":"SOFATracer is a distributed link tracing system based on OpenTracing specification developed by Ant Financial. Its core concept is to concatenate the same request distributed on each service node with a global TraceId. By the unified TraceId, it can record the various network call information in the call link in logs, and can remotely report the call records to Zipkin for presentation, thus implementing perspective network call.\nFeatures Distributed link tracing solution based on OpenTracing specification SOFATracer is a solution that provides link tracing based on and improved from the OpenTracing specification.","tags":null,"title":"SOFATracker overview","type":"projects","url":"/en/projects/sofa-tracer/overview/","wordcount":421},{"author":null,"categories":null,"content":" Sampling Currently,SOFATracer provides two sampling modes. One is the fixed sampling rate based on BitSet. The other is the sampling provided to the user to customize the implementation sampling.The following example shows how to use it.\nThis example is based on the tracer-sampled-with-springmvc project,Except for application.properties, everything else is the same.\nSampling mode based on fixed sampling rate Add sampling related configuration items in application.properties #Sampling rate 0~100 com.alipay.sofa.tracer.samplerPercentage=100 #Sampling type name com.alipay.sofa.tracer.samplerName=PercentageBasedSampler  Verification  When the sample rate is set to 100, the digest log is printed each time. When the sample rate is set to 0, the digest log is not printed. Print by probability when the sampling rate is set between 0 and 100.  The result is verified by requesting 10 times.\n1、When the sample rate is set to 100, the digest log is printed each time.\nStart the project and enter in the browser:http://localhost:8080/springmvc ,And refresh the address 10 times, check the log as follows:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:47.643\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173568757510019269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:68,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-1\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:50.980\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173569097710029269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:3,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-2\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:51.542\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173569153910049269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:3,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-4\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/sampler/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"48856a040da01abc84213934c1c5fce4","permalink":"/en/projects/sofa-tracer/sampler/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/sampler/","summary":"Sampling Currently,SOFATracer provides two sampling modes. One is the fixed sampling rate based on BitSet. The other is the sampling provided to the user to customize the implementation sampling.The following example shows how to use it.\nThis example is based on the tracer-sampled-with-springmvc project,Except for application.properties, everything else is the same.\nSampling mode based on fixed sampling rate Add sampling related configuration items in application.properties #Sampling rate 0~100 com.alipay.sofa.tracer.samplerPercentage=100 #Sampling type name com.","tags":null,"title":"Sampling modes","type":"projects","url":"/en/projects/sofa-tracer/sampler/","wordcount":426},{"author":null,"categories":null,"content":" 1. Integrated deployment 1.1 Scale up registry-integration Assume that three registry-integration servers have been deployed currently, which are namely node1, node2, and node 3. The new node to be added to the cluster is node 4.\nOperation steps:\nStep 1. Deploy the new registry-integration node\nFirst, deploy registry-integration.tgz on node4 by referencing the Deployment topic. Note that you need to set the nodes.metaNode configuration item on node4 to a 4-server endpoint list:\nnodes.metaNode=DefaultDataCenter:\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;,\u0026amp;lt;node3\u0026amp;gt;,\u0026amp;lt;node4\u0026amp;gt;  In this step, after node4 is started, visit curl http://\u0026amp;lt;node4\u0026amp;gt;:9615/health/check. The status will be unhealthy, because node4 has not been added to the cluster yet. To add it to the cluster, perform the next step.\nStep 2. Call the changePeer operation to add a new node to a cluster\nRun the changePeer command on one of the existing servers (node1, node2, and node3), to modify the current cluster from a three-node cluster (node1, node2, and node3) to a four-node cluster (node1, node2, node3, and node4):\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;,\u0026amp;lt;node3\u0026amp;gt;,\u0026amp;lt;node4\u0026amp;gt;\u0026amp;quot;  After completing this step, visit curl http://\u0026amp;lt;node4\u0026amp;gt;:9615/health/check. The status will be healthy.\n1.2 Scale down registry-integration Assume that you have three servers in one cluster, which are respectively node1, node2, and node3, and you want to scale down node3.\n1.2.1 Smooth scale-down Operation steps:\nStep 1. Call the changePeer operation to remove a node\nRun the changePeer command on either node1 or node2 to change the cluster list from \u0026amp;ldquo;node1, node2, node3\u0026amp;rdquo; to \u0026amp;ldquo;node1,node2\u0026amp;rdquo;. This removes node3 from the endpoint list of the cluster:\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;\u0026amp;quot;  After completing this step, visit curl http://\u0026amp;lt;node3\u0026amp;gt;:9615/health/check. The status will be unhealthy, because node3 has already been removed from the cluster.\nStep 2. Close node3\nThis step is optional, because node3 has already been removed from the cluster, and it does not affect the cluster even if it is still running.\n1.2.2 Handling of node failure If node3 is no longer functional, you need to remove it from the cluster.\nOperation steps:\nStep 1. Call the changePeer operation to remove a node\nRun the changePeer command on either node1 or node2 to change the cluster list from \u0026amp;ldquo;node1, node2, node3\u0026amp;rdquo; to \u0026amp;ldquo;node1,node2\u0026amp;rdquo;. This removes node3 from the endpoint list of the cluster:\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;\u0026amp;quot;  2. Independent deployment 2.1 Scale up registry-meta Assume that you have already deployed three registry-meta servers, which are respectively metaNode1, metaNode2, and metaNode3. The new node to be added to the …","date":-62135596800,"description":"","dir":"projects/sofa-registry/scale/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"57de6dc4da1292063ff25ecea9ffbd08","permalink":"/en/projects/sofa-registry/scale/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-registry/scale/","summary":"1. Integrated deployment 1.1 Scale up registry-integration Assume that three registry-integration servers have been deployed currently, which are namely node1, node2, and node 3. The new node to be added to the cluster is node 4.\nOperation steps:\nStep 1. Deploy the new registry-integration node\nFirst, deploy registry-integration.tgz on node4 by referencing the Deployment topic. Note that you need to set the nodes.metaNode configuration item on node4 to a 4-server endpoint list:","tags":null,"title":"Scaling","type":"projects","url":"/en/projects/sofa-registry/scale/","wordcount":954},{"author":null,"categories":null,"content":" Quickly understand ACTS scripts Do you have to frequently compile test cases? Are you frustrated by the following problems?\n You have to repeat assertEquals, which is definitely not creative. Missing an assert may lead to false success, while mistaking one may ruin your mood. If the scenario is complex, the test code may be longer than the service code, which is painful. You have to migrate utility classes every time you start writing test cases for a new application.  A TestNG test case is shown on the left side, and an ACTS test case on the right. Repeated coding is gone, and the code size is significantly reduced. Unlike ordinary test scripts, ACTS scripts inherit from the ActsTestBase class, which is encapsulated with data loading methods, driving methods, execution engines, and validation rules. Users do not have to clean or prepare data, run test cases, or validate results. ACTS implements zero coding for simple services, which greatly reduces the coding and maintenance costs.\nGenerate test scripts Prerequisites: Be sure to use Maven to compile your project and generate the object model. Otherwise, ACTS IDE may encounter unexpected errors, such as edit failures and incorrect data.\nRight click a the method defined in the interface and select ACTS Function \u0026amp;gt; Generate Test Case.\nRun test script Method: Right click the tested method in ACTS script, and select TestNG to run the test script as shown in the following figure.\nSpecify a test script to run  Set test_only＝^T in src/test/resource/config/acts-config.properties to run only the test case whose name starts with T. You can also replace ^T with other regular expressions.\n In this case, you can modify the name of the test case that you want to run by adding T in front of its name. ACTS only runs a test case whose name starts with T.\n  Split test cases of the test script ACTS stores all test case data of a test script in the same YAML file by default. You can determine whether to store test case data by test script or by test case by configuring the option spilt_yaml_by_case. It is set to false by default, which means all test case data of the same test script is stored in one YAML file.\nYou can set spilt_yaml_by_case=true in acts-config.properties to store each test case of a new test script in a separate YAML file that is named after the case ID. This reduces the chances of file conflicts in the case where multiple developers work on the same interface.\nIn addition, ACTS provides a utility class that allows you split a legacy YAML file of a specified test script under a specified path by test case. See the following.\nBaseDataUtil.saveYamlDataToCaseByCase\n Note: Before the split, we recommend that you rename the original YAML file for backup, and then use the test case editor to check whether the content of the split files is correct. The original YAML file must be deleted if the split files are correct, because they cannot coexist.  Coding for data preparation ACTS provides context APIs …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-script/","fuzzywordcount":900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"0d20739dedad1f11277bd02ed65329c3","permalink":"/en/projects/sofa-acts/usage-script/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/en/projects/sofa-acts/usage-script/","summary":"Quickly understand ACTS scripts Do you have to frequently compile test cases? Are you frustrated by the following problems?\n You have to repeat assertEquals, which is definitely not creative. Missing an assert may lead to false success, while mistaking one may ruin your mood. If the scenario is complex, the test code may be longer than the service code, which is painful. You have to migrate utility classes every time you start writing test cases for a new application.","tags":null,"title":"Scripts","type":"projects","url":"/en/projects/sofa-acts/usage-script/","wordcount":825},{"author":null,"categories":null,"content":" SEATA Demo for SOFAStack Cloud Native Workshop on KubeCon China 2019\nAT mode 1.Introduce maven dependencies Introduce the following dependencies into the POM file of the parent project (seata-demo-at/pom.xml):\n... \u0026amp;lt;properties\u0026amp;gt; ... \u0026amp;lt;seata.version\u0026amp;gt;0.6.1\u0026amp;lt;/seata.version\u0026amp;gt; \u0026amp;lt;netty4.version\u0026amp;gt;4.1.24.Final\u0026amp;lt;/netty4.version\u0026amp;gt; \u0026amp;lt;/properties\u0026amp;gt; ... \u0026amp;lt;dependencyManagement\u0026amp;gt; \u0026amp;lt;dependencies\u0026amp;gt; ... \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.seata\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;seata-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${seata.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.seata\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;seata-server\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${seata.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;javax.servlet\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;servlet-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.netty\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;netty-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${netty4.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;/dependencies\u0026amp;gt; \u0026amp;lt;/dependencyManagement\u0026amp;gt;  Introduce the following dependencies into the POM file of the stock-mng project (seata-demo-at/stock-mng/pom.xml):\n\u0026amp;lt;dependencies\u0026amp;gt; .... \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.seata\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;seata-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.netty\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;netty-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependencies\u0026amp;gt;  Introduce the following dependencies into the POM file of the balance-mng-impl project (seata-demo-at/balance-mng/balance-mng-impl/pom.xml):\n\u0026amp;lt;dependencies\u0026amp;gt; .... \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.seata\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;seata-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.seata\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;seata-server\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.netty\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;netty-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependencies\u0026amp;gt;  2. Use Seata\u0026amp;rsquo;s DataSourceProxy to proxy actual data source and configure GlobalTransactionScanner to scan @GlobalTransaction annotation Add the following java snippet to the main methods in BalanceMngApplication and StockMngApplication classes:\n... import io.seata.rm.datasource.DataSourceProxy; import io.seata.spring.annotation.GlobalTransactionScanner; ... @Configuration public static class DataSourceConfig { @Bean @Primary @ConfigurationProperties(prefix = \u0026amp;quot;spring.datasource.hikari\u0026amp;quot;) public DataSource dataSource(DataSourceProperties properties) { HikariDataSource dataSource = createDataSource(properties, HikariDataSource.class); if (StringUtils.hasText(properties.getName())) { dataSource.setPoolName(properties.getName()); } return new DataSourceProxy(dataSource); } @SuppressWarnings(\u0026amp;quot;unchecked\u0026amp;quot;) …","date":-62135596800,"description":"This guide introduces how to use the AT mode and TCC mode of the open-source distributed transaction framework Seata to solve the final consistency of service data.","dir":"guides/kc-seata-demo/","fuzzywordcount":1500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"60071a0eb44bf0901fb187eefd63ccdb","permalink":"/en/guides/kc-seata-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/en/guides/kc-seata-demo/","summary":"SEATA Demo for SOFAStack Cloud Native Workshop on KubeCon China 2019\nAT mode 1.Introduce maven dependencies Introduce the following dependencies into the POM file of the parent project (seata-demo-at/pom.xml):\n... \u0026lt;properties\u0026gt; ... \u0026lt;seata.version\u0026gt;0.6.1\u0026lt;/seata.version\u0026gt; \u0026lt;netty4.version\u0026gt;4.1.24.Final\u0026lt;/netty4.version\u0026gt; \u0026lt;/properties\u0026gt; ... \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.seata\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;seata-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${seata.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.seata\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;seata-server\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${seata.version}\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.netty\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;netty-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${netty4.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt;  Introduce the following dependencies into the POM file of the stock-mng project (seata-demo-at/stock-mng/pom.","tags":null,"title":"Seata distributed transaction practice","type":"guides","url":"/en/guides/kc-seata-demo/","wordcount":1464},{"author":null,"categories":null,"content":"SOFABoot RPC Starter provides a variety of registry center options as well as convenient configurations.\nCurrently, bolt, rest, and dubbo all support Zookeeper as registry center. In addition, bolt and rest support the local file system as registry center, which is generally used for testing.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-usage/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5a1a4619c8ac4a9fc27b8576472aed9f","permalink":"/en/projects/sofa-rpc/registry-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-usage/","summary":"SOFABoot RPC Starter provides a variety of registry center options as well as convenient configurations.\nCurrently, bolt, rest, and dubbo all support Zookeeper as registry center. In addition, bolt and rest support the local file system as registry center, which is generally used for testing.","tags":null,"title":"Select Service Registry","type":"projects","url":"/en/projects/sofa-rpc/registry-usage/","wordcount":45},{"author":null,"categories":null,"content":"When using the Bolt communication protocol, SOFARPC can choose different serialization protocols, which can be hessian2 or protobuf currently.\nBy default, SOFARPC uses hessian2 as the serialization protocol. If you need to set the serialization protocol to protobuf, you need to configure the following settings when publishing the service:\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofarpc.demo.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs serialize-type=\u0026amp;quot;protobuf\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  That is to add the \u0026amp;lt;sofa:global-attrs\u0026amp;gt; tag to the \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; tag and set the serialize-type attribute to protobuf.\nCorrespondingly, when referencing the service, you also need to change the serialization protocol to protobuf. The setting method is similar to publishing the service:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sofarpc.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleServiceRef\u0026amp;quot; jvm-first=\u0026amp;quot;false\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs serialize-type=\u0026amp;quot;protobuf\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Currently, when you use Annotation for service reference, it is not yet supported to set serialization protocol. But this will be supported in future versions. For details, see ISSUE: https://github.com/sofastack/sofa-boot/issues/278\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/serialization/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"87e2faa84c2c7a7605243dc096bc4e17","permalink":"/en/projects/sofa-rpc/serialization/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/serialization/","summary":"When using the Bolt communication protocol, SOFARPC can choose different serialization protocols, which can be hessian2 or protobuf currently.\nBy default, SOFARPC uses hessian2 as the serialization protocol. If you need to set the serialization protocol to protobuf, you need to configure the following settings when publishing the service:\n\u0026lt;sofa:service ref=\u0026quot;sampleService\u0026quot; interface=\u0026quot;com.alipay.sofarpc.demo.SampleService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt\u0026gt; \u0026lt;sofa:global-attrs serialize-type=\u0026quot;protobuf\u0026quot;/\u0026gt; \u0026lt;/sofa:binding.bolt\u0026gt; \u0026lt;/sofa:service\u0026gt;  That is to add the \u0026lt;sofa:global-attrs\u0026gt; tag to the \u0026lt;sofa:binding.bolt\u0026gt; tag and set the serialize-type attribute to protobuf.","tags":null,"title":"Serialization protocol","type":"projects","url":"/en/projects/sofa-rpc/serialization/","wordcount":138},{"author":null,"categories":null,"content":" Deployment SOFARegistry supports two types of deployment modes, which are integrated deployment and independent deployment. This topic describes the simplest integrated single-node deployment. For more information about deployment modes, see the Deployment topic.\nDeployment steps 1. Download the source code or installation package. Download the source code. git clone https://github.com/sofastack/sofa-registry.git cd sofa-registry mvn clean package -DskipTests cp server/distribution/integration/target/registry-integration.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-integration tar -zxvf registry-integration.tgz -C registry-integration cd registry-integration  Download the installation package. You can download the latest registry-integration-$version.tar.gz package from Releases.\nmkdir registry-integration tar -zxvf registry-integration-$version.tar.gz -C registry-integration cd registry-integration  2. Start registry-integration. Linux/Unix/Mac Startup command: sh bin/startup.sh\nWindows Double click the startup.bat file under the bin directory.\n3. Check the running status. You can access the healthcheck API provided by these three roles, or view logs/registry-startup.log to check the running status.\n# View the healthcheck API of the meta role: $ curl http://localhost:9615/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... raftStatus:Leader\u0026amp;quot;} # View the healthcheck API of the data role: $ curl http://localhost:9622/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... status:WORKING\u0026amp;quot;} # View the healthcheck API of the session role: $ curl http://localhost:9603/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;...\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/server-quick-start/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b620900b56ba04f4668838846a97698a","permalink":"/en/projects/sofa-registry/server-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-registry/server-quick-start/","summary":"Deployment SOFARegistry supports two types of deployment modes, which are integrated deployment and independent deployment. This topic describes the simplest integrated single-node deployment. For more information about deployment modes, see the Deployment topic.\nDeployment steps 1. Download the source code or installation package. Download the source code. git clone https://github.com/sofastack/sofa-registry.git cd sofa-registry mvn clean package -DskipTests cp server/distribution/integration/target/registry-integration.tgz \u0026lt;somewhere\u0026gt; cd \u0026lt;somewhere\u0026gt; \u0026amp;\u0026amp; mkdir registry-integration tar -zxvf registry-integration.tgz -C registry-integration cd registry-integration  Download the installation package.","tags":null,"title":"Server deployment","type":"projects","url":"/en/projects/sofa-registry/server-quick-start/","wordcount":176},{"author":null,"categories":null,"content":" 本文是关于 MOSN server 配置的说明。\n虽然 MOSN 的配置结构里 servers 是一个 server 数组，但是目前最多只支持配置一个server。\nserver 描述的 MOSN 的基本的全局参数如下所示。\n{ \u0026amp;quot;default_log_path\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;default_log_level\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;global_log_roller\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;graceful_timeout\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;processor\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;listeners\u0026amp;quot;:[] }  default_log_path 默认的错误日志文件路径，支持配置完整的日志路径，以及标准输出（stdout）和标准错误（stderr）。\n 如果配置为空，则默认输出到标准错误（stderr）。  default_log_level 默认的错误日志级别，支持DEBUG、INFO、WARN、ERROR、FATAL。\n 如果配置为空，则默认为 INFO。  global_log_roller  日志轮转配置，会对所有的日志生效，如 tracelog、accesslog、defaultlog。 字符串配置，支持两种模式的配置，一种是按时间轮转，一种是按日志大小轮转。同时只能有一种模式生效。 按照日志大小轮转\n size， 表示日志达到多少 M 进行轮转。 age，表示最多保存多少天的日志。 keep，表示最多保存多少个日志。 compress，表示日志是否压缩，on 为压缩，off 为不压缩。\n\u0026amp;quot;global_log_roller\u0026amp;quot;: \u0026amp;quot;size=100 age=10 keep=10 compress=off\u0026amp;quot;   按照时间轮转\n time，表示每个多少个小时轮转一次。\n\u0026amp;quot;global_log_roller\u0026amp;quot;:\u0026amp;quot;time=1\u0026amp;quot;    graceful_timeout  Duration String 的字符串配置，表示 MOSN 在进行平滑升级时，等待连接关闭的最大时间。 如果没有配置，默认为 30s。  processor MOSN 使用的 GOMAXPROCS 数量 - 如果没有配置，默认为 CPU 数量。 - 如果配置为 0，等价于没有配置。\nListeners 一组 Listener 的配置。\n","date":-62135596800,"description":"","dir":"projects/mosn/configuration/server/overview/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3b43981b2aebeca5879d566d8264f6b6","permalink":"/projects/mosn/configuration/server/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/mosn/configuration/server/overview/","summary":"本文是关于 MOSN server 配置的说明。 虽然 MOSN 的配置结构里 servers 是一个 server 数组，但是目前最多只支持配置一个server。 server 描述的 MOSN 的基本的全局参数如下所示。 { \u0026quot;default_log_path\u0026quot;:\u0026quot;\u0026quot;,","tags":null,"title":"Server 配置说明","type":"projects","url":"/projects/mosn/configuration/server/overview/","wordcount":528},{"author":null,"categories":null,"content":"If you want to extend a registry center, you should take a look at the abstract classes of the registry center.\npackage com.alipay.sofa.rpc.registry; @Extensible(singleton = false) public abstract class Registry implements Initializable, Destroyable { public abstract boolean start(); public abstract void register(ProviderConfig config); public abstract void unRegister(ProviderConfig config); public abstract void batchUnRegister(List\u0026amp;lt;ProviderConfig\u0026amp;gt; configs); public abstract List\u0026amp;lt;ProviderGroup\u0026amp;gt; subscribe(ConsumerConfig config); public abstract void unSubscribe(ConsumerConfig config); public abstract void batchUnSubscribe(List\u0026amp;lt;ConsumerConfig\u0026amp;gt; configs); }  You can see the main necessary interfaces.\nStart the registry client and maintain the connection; Destroy the registry client and release resources; Publish service and cache publish information; Unpublish service and delete cache; Subscribe to service list, return data synchronously or asynchronously, and receive notifications upon changes Unsubscribe service list and delete cache  Other interfaces:\nWhen the registry center node is disconnected, the local call is not affected. Switch from one disconnected registry center node to another one by itself. After switching to another registry center node, the system resumes the registration and subscription information automatically. The registry data is cached to the local file. Even if no registry center node is connected, the service provider and caller can restart and call normally.  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-extension-guide/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c952ecbea16f7ae68ad095ab8baf0583","permalink":"/en/projects/sofa-rpc/registry-extension-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-extension-guide/","summary":"If you want to extend a registry center, you should take a look at the abstract classes of the registry center.\npackage com.alipay.sofa.rpc.registry; @Extensible(singleton = false) public abstract class Registry implements Initializable, Destroyable { public abstract boolean start(); public abstract void register(ProviderConfig config); public abstract void unRegister(ProviderConfig config); public abstract void batchUnRegister(List\u0026lt;ProviderConfig\u0026gt; configs); public abstract List\u0026lt;ProviderGroup\u0026gt; subscribe(ConsumerConfig config); public abstract void unSubscribe(ConsumerConfig config); public abstract void batchUnSubscribe(List\u0026lt;ConsumerConfig\u0026gt; configs); }  You can see the main necessary interfaces.","tags":null,"title":"Service Registry extension guide","type":"projects","url":"/en/projects/sofa-rpc/registry-extension-guide/","wordcount":192},{"author":null,"categories":null,"content":" SOFADashboard\u0026amp;rsquo;s service governance mainly manages SOFARPC services.\nConsole The service governance console mainly provides two basic functions: service name query and service information display. When you click the hyperlink of a service ID, you are redirected to the details page of the service.\nService provider details page Service consumer details page ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/governance/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e547baf489fd5d125be9e67a366854b6","permalink":"/en/projects/sofa-dashboard/governance/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/governance/","summary":" SOFADashboard\u0026rsquo;s service governance mainly manages SOFARPC services.\nConsole The service governance console mainly provides two basic functions: service name query and service information display. When you click the hyperlink of a service ID, you are redirected to the details page of the service.\nService provider details page Service consumer details page ","tags":null,"title":"Service governance","type":"projects","url":"/en/projects/sofa-dashboard/governance/","wordcount":51},{"author":null,"categories":null,"content":" The basic configuration for SOFARPC service publishing and reference is described in the \u0026amp;ldquo;Programming Interface\u0026amp;rdquo; chapter. Here are some of the features of service publishing and referencing.\nOne service publishes multiple protocols In SOFARPC, a service can be published as multiple protocols, which allows the callers to call the service provider using different protocols.\nIf you use the Java API, you can build multiple ServerConfigs as follows to set different protocols for different ServerConfigs and then assign these ServerConfigs to ProviderConfig:\nList\u0026amp;lt;ServerConfig\u0026amp;gt; serverConfigs = new ArrayList\u0026amp;lt;ServerConfig\u0026amp;gt;(); serverConfigs.add(serverConfigA); serverConfigs.add(serverConfigB); providerConfig.setServer(serverConfigs);  If you use XML, add multiple bindings directly in the \u0026amp;lt;sofa:service\u0026amp;gt; tag:\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleFacadeImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.bean.SampleFacade\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;sofa:binding.dubbo/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  If you use annotation, add multiple bindings in @SofaService:\n@SofaService ( interfaceType = SampleService.class, bindings = { @SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;), @SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;) } ) public class SampleServiceImpl implements SampleService { // ... }  One service registers multiple registry centers If you use the API, build multiple RegistryConfigs and assign them to ProviderConfig:\nList\u0026amp;lt;RegistryConfig\u0026amp;gt; registryConfigs = new ArrayList\u0026amp;lt;RegistryConfig\u0026amp;gt;(); registryConfigs.add(registryA); registryConfigs.add(registryB); providerConfig.setRegistry(registryConfigs);  Method-level parameter settings In the Java API mode, you can set the corresponding parameters by calling the set method of the MethodConfig object, as shown below:\nMethodConfig methodConfigA = new MethodConfig(); MethodConfig methodConfigB = new MethodConfig(); List\u0026amp;lt;MethodConfig\u0026amp;gt; methodConfigs = new ArrayList\u0026amp;lt;MethodConfig\u0026amp;gt;(); methodConfigs.add(methodConfigA); methodConfigs.add(methodConfigB); providerConfig.setMethods(methodConfigs); //server settings consumerConfig.setMethods(methodConfigs); //Client settings  In the XML mode, you can use the \u0026amp;lt;sofa:method\u0026amp;gt; tag in the corresponding binding to set the corresponding parameters:\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs timeout=\u0026amp;quot;3000\u0026amp;quot; address-wait-time=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Call timeout; address wait time. --\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;127.0.0.1:22000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Directly connected address --\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;sayName\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Method level configuration --\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleFacadeImpl\u0026amp;quot; …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/publish-and-reference/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6a78b8b84b226eaf1e6d2b1ff1d15fee","permalink":"/en/projects/sofa-rpc/publish-and-reference/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/publish-and-reference/","summary":"The basic configuration for SOFARPC service publishing and reference is described in the \u0026ldquo;Programming Interface\u0026rdquo; chapter. Here are some of the features of service publishing and referencing.\nOne service publishes multiple protocols In SOFARPC, a service can be published as multiple protocols, which allows the callers to call the service provider using different protocols.\nIf you use the Java API, you can build multiple ServerConfigs as follows to set different protocols for different ServerConfigs and then assign these ServerConfigs to ProviderConfig:","tags":null,"title":"Service publishing and reference","type":"projects","url":"/en/projects/sofa-rpc/publish-and-reference/","wordcount":348},{"author":null,"categories":null,"content":" This document describes the complete SOFARPC service publishing and reference in the SOFABoot environment.\nPublish service \u0026amp;lt;bean id=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;helloSyncServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot; unique-id=\u0026amp;quot;\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs registry=\u0026amp;quot;\u0026amp;quot; serialize-type=\u0026amp;quot;\u0026amp;quot; filter=\u0026amp;quot;\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; thread-pool-ref=\u0026amp;quot;\u0026amp;quot; warm-up-time=\u0026amp;quot;60000\u0026amp;quot; warm-up-weight=\u0026amp;quot;10\u0026amp;quot; weight=\u0026amp;quot;100\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:binding.rest\u0026amp;gt; \u0026amp;lt;/sofa:binding.rest\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;     Attribute Name Default value Comment     id ID bean名    class Class None    ref Service interface implementation class     interface Service interface (unique identifier)  Use actual interface class for both normal calls and return calls   unique-id Service tag (unique identifier)     filter Filter configuration alias  Separated by commas   registry Server registry center  Separated by commas   timeout Execution timeout period on the server     serialize-type Serialization protocol hessian2,protobuf    thread-pool-ref Thread pool used by the current interface of the server None    weight Service static weight     warm-up-weight Service warm-up weight     warm-up-time Service warm-up time  Unit: millisecond    Reference service \u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;helloSyncServiceReference\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026amp;quot; unique-id=\u0026amp;quot;\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs type=\u0026amp;quot;sync\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; callback-ref=\u0026amp;quot;\u0026amp;quot; callback-class=\u0026amp;quot;\u0026amp;quot; address-wait-time=\u0026amp;quot;1000\u0026amp;quot; connect.num=\u0026amp;quot;1\u0026amp;quot; check=\u0026amp;quot;false\u0026amp;quot; connect.timeout=\u0026amp;quot;1000\u0026amp;quot; filter=\u0026amp;quot;\u0026amp;quot; generic-interface=\u0026amp;quot;\u0026amp;quot; idle.timeout=\u0026amp;quot;1000\u0026amp;quot; idle.timeout.read=\u0026amp;quot;1000\u0026amp;quot; lazy=\u0026amp;quot;false\u0026amp;quot; loadBalancer=\u0026amp;quot;\u0026amp;quot; registry=\u0026amp;quot;\u0026amp;quot; retries=\u0026amp;quot;1\u0026amp;quot; serialize-type=\u0026amp;quot;\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;xxx:12200\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;hello\u0026amp;quot; callback-class=\u0026amp;quot;\u0026amp;quot; callback-ref=\u0026amp;quot;\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot; type=\u0026amp;quot;sync\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;     Attribute Name Default value Comment     id ID Generated automatically    jvm-first Whether to call the service of local machine first true    interface Service interface (unique identifier)  Use actual interface class for both normal calls and return calls   unique-id Service tag (unique identifier)     local-first whether refer to the service via jvm call true set it to false if this is to call a remote service via rpc   type Calling type sync callback,sync,future,oneway   filter Filter configuration alias  List   registry Server …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/rpc-config-xml-explain/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"4b5110e9eb6cf6c6f287aef0fd210047","permalink":"/en/projects/sofa-rpc/rpc-config-xml-explain/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/rpc-config-xml-explain/","summary":"This document describes the complete SOFARPC service publishing and reference in the SOFABoot environment. Publish service \u0026lt;bean id=\u0026quot;helloSyncServiceImpl\u0026quot; class=\u0026quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncServiceImpl\u0026quot;/\u0026gt; \u0026lt;sofa:service ref=\u0026quot;helloSyncServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.rpc.samples.invoke.HelloSyncService\u0026quot; unique-id=\u0026quot;\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt\u0026gt; \u0026lt;sofa:global-attrs registry=\u0026quot;\u0026quot; serialize-type=\u0026quot;\u0026quot; filter=\u0026quot;\u0026quot; timeout=\u0026quot;3000\u0026quot; thread-pool-ref=\u0026quot;\u0026quot; warm-up-time=\u0026quot;60000\u0026quot; warm-up-weight=\u0026quot;10\u0026quot; weight=\u0026quot;100\u0026quot;/\u0026gt; \u0026lt;/sofa:binding.bolt\u0026gt; \u0026lt;sofa:binding.rest\u0026gt; \u0026lt;/sofa:binding.rest\u0026gt; \u0026lt;/sofa:service\u0026gt; Attribute Name Default value Comment id ID bean名 class Class None ref Service interface implementation class interface Service interface (unique identifier) Use actual interface class for both normal calls","tags":null,"title":"Service publishing and reference in SOFABoot","type":"projects","url":"/en/projects/sofa-rpc/rpc-config-xml-explain/","wordcount":356},{"author":null,"categories":null,"content":" Sidecar 模式是 Service Mesh 中习惯采用的模式，是容器设计模式的一种，在 Service Mesh 出现之前该模式就一直存在，本文将为您讲解 Sidecar 模式。\n什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。如图所示，Sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\n就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观察性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 Sidecar 模式具有以下优势：\n 将与应用业务逻辑无关的功能抽象到共同基础设施降低了微服务代码的复杂度。\n 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。\n 降低应用程序代码和底层平台的耦合度。\n  Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在 Service Mesh 中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述使用了节点代理和 Sidecar 模式的 Service Mesh 架构。\n使用 Sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 Sidecar 副本。在 Sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 Sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n","date":-62135596800,"description":"","dir":"projects/mosn/concept/sidecar-pattern/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"38b45799d69d52f24f26008cd2ad7da5","permalink":"/projects/mosn/concept/sidecar-pattern/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/mosn/concept/sidecar-pattern/","summary":"Sidecar 模式是 Service Mesh 中习惯采用的模式，是容器设计模式的一种，在 Service Mesh 出现之前该模式就一直存在，本文将为您讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为","tags":null,"title":"Sidecar 模式","type":"projects","url":"/projects/mosn/concept/sidecar-pattern/","wordcount":602},{"author":null,"categories":null,"content":" Since SOFARPC 5.4.0, the link analysis feature of Skywalking is supported. You can use it as needed. The Skywalking must be 6.0.0-alpha and above.\nThis document does not cover the backend deployment. If you need it, please refer to the official Skywalking documentation.\nInstall Java agent  Locate the agent directory in the downloaded Skywalking release package.\n Set agent.service_name in config/agent.config, which can be any English character. Generally, it can be your own system name.\n Set the collector.backend_service Skywalking backend address in config/agent.config, which defaults to 127.0.0.1:11800. It is used for local verification.\n Add -javaagent:/path/to/skywalking-package/agenxt/skywalking-agent.jar to the application, which must be placed before the -jar parameter. The skywalking-agent can be gotten in official release package. The new directory structure is as follows:\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins sofa-rpc-plugin-6.0.0-alpha.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar  Note: Ensure that the plugins/sofa-rpc-plugin-**.jar file exists.\n Start the application. After a period of RPC calls, you can view the UI to observe the calling link.\n  More For more relevant documents, please refer to\nSkywalking Agent installation documentation Skywalking Backend deployment documentation\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/skywalking-usage/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3cd5fb45f1b981d0a8c54c8ce43b190b","permalink":"/en/projects/sofa-rpc/skywalking-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/skywalking-usage/","summary":"Since SOFARPC 5.4.0, the link analysis feature of Skywalking is supported. You can use it as needed. The Skywalking must be 6.0.0-alpha and above.\nThis document does not cover the backend deployment. If you need it, please refer to the official Skywalking documentation.\nInstall Java agent  Locate the agent directory in the downloaded Skywalking release package.\n Set agent.service_name in config/agent.config, which can be any English character. Generally, it can be your own system name.","tags":null,"title":"Skywalking","type":"projects","url":"/en/projects/sofa-rpc/skywalking-usage/","wordcount":181},{"author":null,"categories":null,"content":" SOFARPC 在5.4.0 及之后的版本中，已经支持 Skywalking 的链路分析的功能，用户可以根据需要进行使用，其中Skywalking 的版本 要求6.0.0-alpha及以上。本文档，不涉及后端的部署，如有需要，可查看 Skywalking 官方文档。\n安装 Java agent 1.在下载的 Skywalking 的release 包中找到 agent 目录。\n2.在config/agent.config 中设置 agent.service_name，可以是任何英文字符，一般可以设置为自己的系统名。\n3.在config/agent.config 中设置 collector.backend_service Skywalking 的后端地址，默认指向 127.0.0.1:11800,这个是为了本地验证的。\n4.给应用程序添加 -javaagent:/path/to/skywalking-package/agenxt/skywalking-agent.jar，其中注意，一定要放在 -jar 参数之前。 Agent 在 kywalking 的 官方 release 包. 新的目录结构如下.\n+-- agent +-- activations apm-toolkit-log4j-1.x-activation.jar apm-toolkit-log4j-2.x-activation.jar apm-toolkit-logback-1.x-activation.jar ... +-- config agent.config +-- plugins sofa-rpc-plugin-6.0.0-alpha.jar apm-feign-default-http-9.x.jar apm-httpClient-4.x-plugin.jar ..... skywalking-agent.jar  注意，确保plugins/sofa-rpc-plugin-**.jar 文件存在。\n5.启动应用程序，经过一段时间RPC调用后，可以查看 UI 来观察链路。\n更多 更多文档请参考\n Skywalking Agent 安装文档 Skywalking Backend 部署文档  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/skywalking-usage/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3cd5fb45f1b981d0a8c54c8ce43b190b","permalink":"/projects/sofa-rpc/skywalking-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/skywalking-usage/","summary":"SOFARPC 在5.4.0 及之后的版本中，已经支持 Skywalking 的链路分析的功能，用户可以根据需要进行使用，其中Skywalking 的版本 要求6.0.0-alpha","tags":null,"title":"Skywalking 链路分析","type":"projects","url":"/projects/sofa-rpc/skywalking-usage/","wordcount":482},{"author":null,"categories":null,"content":" What is the Soul？ This is an asynchronous, high-performance, cross-language, responsive API gateway.I hope that something can protect your micro service like a soul.After referring to excellent gateways such as Kong, Spring-Cloud-Gateway, soul was born standing on the shoulders of giants!\n# Features\n Support various languages (http protocol), support dubbo, spring cloud protocol.\n Plugin design idea, plugin hot swap, easy to expand.\n Flexible flow filtering to meet various flow control.\n Built-in rich plugin support, authentication, limiting, fuse, firewall, etc.\n Dynamic flow configuration, high performance, gateway consumption is 1~2ms.\n Support cluster deployment, A/B Test, blue-green release.\n  # Architecture diagram\nPrerequisite  JDK 1.8+ Maven 3.2.x Git mysql  ","date":-62135596800,"description":"Soul Introduction","dir":"projects/soul/overview/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5d30d134c5a14a47c55803eebfb34cd1","permalink":"/en/projects/soul/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/soul/overview/","summary":"What is the Soul？ This is an asynchronous, high-performance, cross-language, responsive API gateway.I hope that something can protect your micro service like a soul.After referring to excellent gateways such as Kong, Spring-Cloud-Gateway, soul was born standing on the shoulders of giants!\n# Features\n Support various languages (http protocol), support dubbo, spring cloud protocol.\n Plugin design idea, plugin hot swap, easy to expand.\n Flexible flow filtering to meet various flow control.","tags":null,"title":"Soul Introduction","type":"projects","url":"/en/projects/soul/overview/","wordcount":105},{"author":null,"categories":null,"content":" What is the Soul？ 这是一个异步的，高性能的，跨语言的，响应式的API网关。我希望能够有一样东西像灵魂一样，保护您的微服务。参考了Kong，Spring-Cloud-Gateway等优秀的网关后，站在巨人的肩膀上，Soul由此诞生！\n# Features\n 支持各种语言(http协议)，支持 dubbo，springcloud协议。\n 插件化设计思想，插件热插拔，易扩展。\n 灵活的流量筛选，能满足各种流量控制。\n 内置丰富的插件支持，鉴权，限流，熔断，防火墙等等。\n 流量配置动态化，性能极高，网关消耗在 1~2ms。\n 支持集群部署，支持 A/B Test，蓝绿发布。\n  # 架构图\nPrerequisite  JDK 1.8+ Maven 3.2.x Git mysql  ","date":-62135596800,"description":"soul介绍","dir":"projects/soul/overview/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5d30d134c5a14a47c55803eebfb34cd1","permalink":"/projects/soul/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/soul/overview/","summary":"What is the Soul？ 这是一个异步的，高性能的，跨语言的，响应式的API网关。我希望能够有一样东西像灵魂一样，保护您的微服务。参考了Kong，Sp","tags":null,"title":"Soul 介绍","type":"projects","url":"/projects/soul/overview/","wordcount":269},{"author":null,"categories":null,"content":" SOFABoot 提供了模块并行启动以及 Spring Bean 异步初始化能力，用于加快应用启动速度。本文介绍如何使用 SOFABoot 异步初始化 Spring Bean 能力以提高应用启动速度。\n使用场景 在实际使用 Spring/Spring Boot 开发中，一些 Bean 在初始化过程中执行准备操作，如拉取远程配置、初始化数据源等等。在应用启动期间，这些 Bean 会增加 Spring 上下文刷新时间，导致应用启动耗时变长。\n为了加速应用启动，SOFABoot 通过配置可选项，将 Bean 的初始化方法（init-method）使用单独线程异步执行，加快 Spring 上下文加载过程，提高应用启动速度。\n引入依赖 在工程的 pom.xml 文件中，引入如下 starter：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  使用方法 异步初始化 Bean 的原理是开启单独线程负责执行 Bean 的初始化方法（init-method）。因此，除了引入上述依赖，还需要在 Bean 的 XML 定义中配置 async-init=\u0026amp;ldquo;true\u0026amp;rdquo; 属性，用于指定是否异步执行该 Bean 的初始化方法，例如：\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xmlns:sofa=\u0026amp;quot;http://sofastack.io/schema/sofaboot\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://sofastack.io/schema/sofaboot http://sofastack.io/schema/sofaboot.xsd\u0026amp;quot; default-autowire=\u0026amp;quot;byName\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;!-- async init test --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;testBean\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.runtime.beans.TimeWasteBean\u0026amp;quot; init-method=\u0026amp;quot;init\u0026amp;quot; async-init=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt;  属性配置 SOFABoot 异步初始化能力提供两个属性配置，用于指定负责异步执行 Bean 初始化方法（init-method）的线程池大小：\n com.alipay.sofa.boot.asyncInitBeanCoreSize：线程池基本大小，默认值为 CPU 核数加一。 com.alipay.sofa.boot.asyncInitBeanMaxSize：线程池中允许的最大线程数大小，默认值为 CPU 核数加一。  此配置可以通过 VM -D 参数或者 Spring Boot 配置文件 application.yml 设置。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/bean-async-init/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1b7c2d94076ffb7ac96f64a557067917","permalink":"/projects/sofa-boot/bean-async-init/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/bean-async-init/","summary":"SOFABoot 提供了模块并行启动以及 Spring Bean 异步初始化能力，用于加快应用启动速度。本文介绍如何使用 SOFABoot 异步初始化 Spring Bean 能力以提高应用启动速度。 使用场景 在实际使用","tags":null,"title":"Spring Bean 异步初始化","type":"projects","url":"/projects/sofa-boot/bean-async-init/","wordcount":578},{"author":null,"categories":null,"content":" Spring-Cloud User Guide  Step 1: Introduce the jar packages\n Step 2: Introduce the Hmily configuration\n Step 3: Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n Step 4: Add @Hmily annotation on the feignClient call method(Consumer side).\n  1.Introduce The Maven dependency Spring-Namespace  Introduce the hmily-springcloud dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-springcloud\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   make the configuration in the XML configuration file as below:  \u0026amp;lt;!--Configure the base packages that the Hmily framework need to scan --\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Configure the bean parameters for Hmily startup --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot-Starter  Introduce the hmily-spring-boot-starter-springcloud dependency  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-springcloud\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  3. Add annotations on the service implementation method TCC Mode  Add @HmilyTCC (confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation to the concrete implementation of the interface method identified by \u0026amp;lsquo;@Hmily\u0026amp;rsquo;.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be consistent with the identification method.\n The TCC mode should ensure the idempotence of the confirm and cancel methods,Users need to develop these two methods by themselves,The confirmation and rollback behavior of all transactions are completely up tp users.The Hmily framework is just responsible for making calls.\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  TAC Mode(Under development, not released)  Add @HmilyTAC annotation to the …","date":-62135596800,"description":"Hmily-Spring Cloud Distributed Transaction User Guide","dir":"projects/hmily/user-springcloud/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"752ffdae4261d81c1ae7318b180838bb","permalink":"/en/projects/hmily/user-springcloud/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/hmily/user-springcloud/","summary":"Spring-Cloud User Guide  Step 1: Introduce the jar packages\n Step 2: Introduce the Hmily configuration\n Step 3: Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n Step 4: Add @Hmily annotation on the feignClient call method(Consumer side).\n  1.Introduce The Maven dependency Spring-Namespace  Introduce the hmily-springcloud dependency  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-springcloud\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   make the configuration in the XML configuration file as below:  \u0026lt;!","tags":null,"title":"Spring Cloud User Guide","type":"projects","url":"/en/projects/hmily/user-springcloud/","wordcount":541},{"author":null,"categories":null,"content":" In this document will demonstrate how to use SOFATracer to track of SpringMVC, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026amp;rsquo;s application.properties file, including spring.application.name that indicates the name of the current application and logging.path that specifies the log output directory.\n# Application Name spring.application.name=SOFATracerSpringMVC # logging path logging.path=./logs  Add a simple Controller In the project code, add a simple Controller, for example:\n@RestController public class SampleRestController { private static final String TEMPLATE = \u0026amp;quot;Hello, %s!\u0026amp;quot;; private final AtomicLong counter = new AtomicLong(); /** * http://localhost:8080/springmvc * @param name name * @return map */ @RequestMapping(\u0026amp;quot;/springmvc\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; springmvc(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;SOFATracer SpringMVC DEMO\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; resultMap = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); resultMap.put(\u0026amp;quot;success\u0026amp;quot;, true); resultMap.put(\u0026amp;quot;id\u0026amp;quot;, counter.incrementAndGet()); resultMap.put(\u0026amp;quot;content\u0026amp;quot;, String.format(TEMPLATE, name)); return resultMap; } }  Run the project Start Current SOFABoot Application. You will see the log about startup in the console:\n2018-05-11 11:55:11.932 INFO 66490 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: \u0026#39;SpringMvcOpenTracingFilter\u0026#39; to urls: [/*] 2018-05-11 11:55:13.961 INFO 66490 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2018-05-11 11:55:13.970 INFO 66490 --- [ main] c.a.s.t.e.springmvc.DemoApplication : Started DemoApplication in 8.361 seconds (JVM running for 9.34)  You can access the REST service by visiting http://localhost:8080/springmvc in your browser. You can see the result similar to the followings:\n{ content: \u0026amp;quot;Hello, SOFATracer SpringMVC DEMO!\u0026amp;quot;, id: 1, success: true }  View log In the application.properties, the log printing directory we configured is ./logs, which is the root directory of the current application (we can configure it based on actual situation). In the root directory, you can see log files in the structure similar to the followings:\n./logs ├── spring.log └── tracelog ├── spring-mvc-digest.log ├── spring-mvc-stat.log ├── static-info.log └── tracer-self.log  Every time you visit http://localhost:8080/springmvc, SOFATracer will log the digest log. You can open the spring-mvc-digest.log file to see the specific log content. As for the meaning of each output field, you can refer to here.\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-05-17 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-mvc/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6c6389a6f994f43f08cbdf4a49d1755a","permalink":"/en/projects/sofa-tracer/usage-of-mvc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/usage-of-mvc/","summary":"In this document will demonstrate how to use SOFATracer to track of SpringMVC, this example address.\nAssuming you have built a simple Spring Web project based on SOFABoot, Then you can be operated by the following steps:\nIntroduce dependency \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Project Configuration Then, add the parameters to be used by SOFATracer in the project\u0026rsquo;s application.properties file, including spring.application.name that indicates the name of the current application and logging.","tags":null,"title":"Spring MVC Integration","type":"projects","url":"/en/projects/sofa-tracer/usage-of-mvc/","wordcount":356},{"author":null,"categories":null,"content":" SpringMVC Log Format After integrating SpringMVC, SOFATracer will output the link data format of the MVC requests, which is JSON by default.\nSpring MVC digest log (spring-mvc-digest.log) Data is ouput in JSON format. The meaning of each key is as follows:\n   Key Meaning     Time Log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Request.url Request URL   Method Request HTTP method   Result.code HTTP return status code   req.size.bytes Request body size   resp.size.bytes Response body size   Time.cost.milliseconds Request time (ms)   Current.thread.name Current thread name   Baggage Transparently transmitted baggage data    Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-06-03 16:44:05.829\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SpringMvcJsonOutput\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;c0a80d9e1528015445828101064625\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:63933/greeting\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:0,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:50,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:1,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-auto-1-exec-10\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Spring MVC statistical log (spring-mvc-stat.log) stat.key is a collection of statistical keywords in this period., which uniquely determines a set of statistical data, including local.app, request.url, and method field.\n  Key Meaning   time Log printing time   stat.key local.app Current application name   request.url Request URL    method  Request HTTP method   count Number of requests in this period   total.cost.milliseconds Total duration for (ms) requests in this period   success Request result: Y means success (the result code starting with 1 and 2 indicates success, and 302 indicates that the redirection is successful, and others indicate failure); N indicates failure   load.test Pressure test mark: T indicates pressure test; F indicates non-pressure test   Example:\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-06-03 16:44:02.473\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:63933/greeting\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SpringMvcJsonOutput\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:5,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:149,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;Y\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-springmvc/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"af13acf5aee90a32f0fa143996063a91","permalink":"/en/projects/sofa-tracer/log-format-springmvc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/log-format-springmvc/","summary":"SpringMVC Log Format After integrating SpringMVC, SOFATracer will output the link data format of the MVC requests, which is JSON by default.\nSpring MVC digest log (spring-mvc-digest.log) Data is ouput in JSON format. The meaning of each key is as follows:\n   Key Meaning     Time Log printing time   Local.app Current application name   traceId TraceId   spanId SpanId   Request.","tags":null,"title":"Spring MVC log","type":"projects","url":"/en/projects/sofa-tracer/log-format-springmvc/","wordcount":200},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 对 SpringMVC 进行埋点，本示例工程地址。\n假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n依赖引入 \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  工程配置 在工程的 application.properties 文件下添加 SOFATracer 要使用的参数，包括spring.application.name 用于标示当前应用的名称；logging.path 用于指定日志的输出目录。\n# Application Name spring.application.name=SOFATracerSpringMVC # logging path logging.path=./logs  添加一个提供 RESTful 服务的 Controller 在工程代码中，添加一个简单的 Controller，例如：\n@RestController public class SampleRestController { private static final String TEMPLATE = \u0026amp;quot;Hello, %s!\u0026amp;quot;; private final AtomicLong counter = new AtomicLong(); /** * http://localhost:8080/springmvc * @param name name * @return map */ @RequestMapping(\u0026amp;quot;/springmvc\u0026amp;quot;) public Map\u0026amp;lt;String, Object\u0026amp;gt; springmvc(@RequestParam(value = \u0026amp;quot;name\u0026amp;quot;, defaultValue = \u0026amp;quot;SOFATracer SpringMVC DEMO\u0026amp;quot;) String name) { Map\u0026amp;lt;String, Object\u0026amp;gt; resultMap = new HashMap\u0026amp;lt;String, Object\u0026amp;gt;(); resultMap.put(\u0026amp;quot;success\u0026amp;quot;, true); resultMap.put(\u0026amp;quot;id\u0026amp;quot;, counter.incrementAndGet()); resultMap.put(\u0026amp;quot;content\u0026amp;quot;, String.format(TEMPLATE, name)); return resultMap; } }  运行 启动 SOFABoot 应用，将会在控制台中看到启动打印的日志：\n2018-05-11 11:55:11.932 INFO 66490 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: \u0026#39;SpringMvcOpenTracingFilter\u0026#39; to urls: [/*] 2018-05-11 11:55:13.961 INFO 66490 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2018-05-11 11:55:13.970 INFO 66490 --- [ main] c.a.s.t.e.springmvc.DemoApplication : Started DemoApplication in 8.361 seconds (JVM running for 9.34)  可以通过在浏览器中输入 http://localhost:8080/springmvc 来访问 REST 服务，结果类似如下：\n{ content: \u0026amp;quot;Hello, SOFATracer SpringMVC DEMO!\u0026amp;quot;, id: 1, success: true }  查看日志 在上面的 application.properties 里面，我们配置的日志打印目录是 ./logs 即当前应用的根目录（我们可以根据自己的实践需要配置），在当前工程的根目录下可以看到类似如下结构的日志文件：\n./logs ├── spring.log └── tracelog ├── spring-mvc-digest.log ├── spring-mvc-stat.log ├── static-info.log └── tracer-self.log  通过访问 http://localhost:8080/springmvc SOFATracer 会记录每一次访问的摘要日志，可以打开 spring-mvc-digest.log 看到具体的输出内容。\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:33:10.336\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9271567477985327100211176\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;server\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8801-exec-2\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;5006ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8801/asyncrest\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/usage-of-mvc/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6c6389a6f994f43f08cbdf4a49d1755a","permalink":"/projects/sofa-tracer/usage-of-mvc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/usage-of-mvc/","summary":"在本文档将演示如何使用 SOFATracer 对 SpringMVC 进行埋点，本示例工程地址。 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作： 依赖引入 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tracer-sofa-boot-starter\u0026lt;/artifactId\u0026gt;","tags":null,"title":"Spring MVC 埋点接入","type":"projects","url":"/projects/sofa-tracer/usage-of-mvc/","wordcount":514},{"author":null,"categories":null,"content":" SOFATracer 集成 SpringMVC 后输出 MVC 请求的链路数据格式，默认为 JSON 数据格式。\nSpring MVC 摘要日志（spring-mvc-digest.log） 以 JSON 格式输出的数据，相应 key 的含义解释如下：\n   key 表达含义     time 日志打印时间   local.app 当前应用名   traceId TraceId   spanId SpanId   span.kind Span 类型   result.code 状态码   current.thread.name 当前线程名   time.cost.milliseconds span 耗时   request.url 请求地址   method http method   req.size.bytes 请求大小   resp.size.bytes 响应大小   sys.baggage 系统透传的 baggage 数据   biz.baggage 业务透传的 baggage 数据    样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:33:10.336\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9271567477985327100211176\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;server\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8801-exec-2\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;5006ms\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8801/asyncrest\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}  Spring MVC 统计日志（spring-mvc-stat.log） stat.key 即本段时间内的统计关键字集合，统一关键字集合唯一确定一组统计数据，包含local.app、request.url、和 method 字段.\n  key 表达含义   time 日志打印时间   stat.key local.app 当前应用名   request.url 请求 URL    method  请求 HTTP 方法   count 本段时间内请求次数   total.cost.milliseconds 本段时间内的请求总耗时（ms）   success 请求结果：Y 表示成功(1 开头和 2 开头的结果码算是成功的，302表示的重定向算成功，其他算是失败的)；N 表示失败   load.test 压测标记：T 是压测；F 不是压测   样例：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-03 10:34:04.129\u0026amp;quot;,\u0026amp;quot;stat.key\u0026amp;quot;:{\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;RestTemplateDemo\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8801/asyncrest\u0026amp;quot;},\u0026amp;quot;count\u0026amp;quot;:1,\u0026amp;quot;total.cost.milliseconds\u0026amp;quot;:5006,\u0026amp;quot;success\u0026amp;quot;:\u0026amp;quot;true\u0026amp;quot;,\u0026amp;quot;load.test\u0026amp;quot;:\u0026amp;quot;F\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/log-format-springmvc/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"af13acf5aee90a32f0fa143996063a91","permalink":"/projects/sofa-tracer/log-format-springmvc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/log-format-springmvc/","summary":"SOFATracer 集成 SpringMVC 后输出 MVC 请求的链路数据格式，默认为 JSON 数据格式。 Spring MVC 摘要日志（spring-mvc-digest.log） 以 JSON 格式输出的数据，相应 key 的","tags":null,"title":"Spring MVC 日志","type":"projects","url":"/projects/sofa-tracer/log-format-springmvc/","wordcount":380},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-dubbo Module and Run Build with Maven Run with EurekaServerApplication.java in hmily-demo-springcloud-eureka project. Configuring（hmily-demo-springcloud-account module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   run SpringCloudHmilyAccountApplication.java  Run hmily-demo-springcloud-inventory(refer to simillar instructions above). Run hmily-demo-springcloud-order(refer to simillar instructions above). Access on http://127.0.0.1:8884/swagger-ui.html for more. ","date":-62135596800,"description":"Hmily-SpringCloud Quick Start for Distributed Transactions","dir":"projects/hmily/quick-start-springcloud/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f8a46f1745f2b3558183e9457b3e8385","permalink":"/en/projects/hmily/quick-start-springcloud/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/quick-start-springcloud/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-dubbo Module and Run Build with Maven Run with EurekaServerApplication.java in hmily-demo-springcloud-eureka project. Configuring（hmily-demo-springcloud-account module for instance）  Configure with your business database (account module for instance)  spring: datasource: driver-class-name: com.","tags":null,"title":"SpringCloud Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-springcloud/","wordcount":151},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n使用你的工具 idea 打开项目，找到hmily-demo-springcloud项目, 进行maven构建。 启动 hmily-demo-springcloud-eureka项目中的 EurekaServerApplication.java。 修改项目配置（hmily-demo-springcloud-account为列子）  修改业务数据库(account项目为列子)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: 你的用户名 password: 你的密码   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   run SpringCloudHmilyAccountApplication.java  启动hmily-demo-springcloud-inventory 参考上述。 启动hmily-demo-springcloud-order 参考上述。 访问：http://127.0.0.1:8884/swagger-ui.html。 ","date":-62135596800,"description":"Hmily-SpringCloud分布式事务体验","dir":"projects/hmily/quick-start-springcloud/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f8a46f1745f2b3558183e9457b3e8385","permalink":"/projects/hmily/quick-start-springcloud/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/quick-start-springcloud/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 使用你的工具 idea 打开项目，找到hmily-demo","tags":null,"title":"SpringCloud快速体验","type":"projects","url":"/projects/hmily/quick-start-springcloud/","wordcount":552},{"author":null,"categories":null,"content":" Spring-Cloud 用户指南  步骤一: 引入依赖jar包\n 步骤二：引入hmily配置\n 步骤三：在具体的实现方法上（服务提供端），加上HmilyTCC or HmilyTAC 注解\n 步骤四：在feignClient调用方法上（消费方），加上Hmily\n  1.引入依赖 Spring-Namespace  引入依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-springcloud\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在xml中进行如下配置  \u0026amp;lt;!--配置扫码hmily框架的包--\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--配置Hmily启动的bean参数--\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot-Starter  引入依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-springcloud\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.新增hmily配置  在项目的 resource 添加文件名为:hmily.yml 的配置文件\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  3. 服务实现方添加注解 TCC模式  只需要在参与hmily分布式事务调用的具体实现方法上加@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  TAC模式（在开发，未发布）  只需要在参与分布式事务调用的具体实现方法上加@HmilyTAC  服务消费端（FeignClient）  在服务被调用方的@FeignClient 接口方法上加上 @Hmily注解。  @FeignClient(value = \u0026amp;quot;helle-service\u0026amp;quot;) public interface HelloService { @Hmily @RequestMapping(\u0026amp;quot;/helle-service/sayHello\u0026amp;quot;) void say(String hello); }  重要注意事项 在调用任何RPC调用之前，当你需要聚合rpc调用成为一次分布式事务的时候，需要在聚合RPC调用的方法上，先行添加 @HmilyTCC 或者 @HmilyTAC 注解,表示开启全局事务。\n负载均衡  如果服务部署了几个节点， 负载均衡算法最好使用 hmily自带, 这样 try, confirm, cancel 调用会落在同一个节点 充分利用了缓存，提搞了效率。在你的yaml配置如下：   hmily.ribbon.rule.enabled = true  开启hystrix  如果用户配置了feign.hystrix.enabled = true, 默认使用线程池模式， 将会开启 HmilyHystrixConcurrencyStrategy 它在hystrix使用线程池模式的时候，能够照样通过threadLoacl 进行RPC传参数。   设置永不重试  需要进行分布式事务的SpringCloud微服务的调用方需要设置不重试，如下是参考：  ribbon: MaxAutoRetriesNextServer : 0 MaxAutoRetries: 0  异常  try, confirm, cancel 方法的所有异常不要自行catch 任何异常都应该抛出给 Hmily框架处理。   ","date":-62135596800,"description":"Hmily-SpringCloud分布式事务用户指南","dir":"projects/hmily/user-springcloud/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"752ffdae4261d81c1ae7318b180838bb","permalink":"/projects/hmily/user-springcloud/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/hmily/user-springcloud/","summary":"Spring-Cloud 用户指南 步骤一: 引入依赖jar包 步骤二：引入hmily配置 步骤三：在具体的实现方法上（服务提供端），加上HmilyTCC or HmilyTAC 注解 步骤四：在","tags":null,"title":"SpringCloud用户指南","type":"projects","url":"/projects/hmily/user-springcloud/","wordcount":1031},{"author":null,"categories":null,"content":" 本文将向您展示 MOSN 的 TLS 安全能力。\n证书方案 MOSN 支持通过 Istio Citadel 的证书签发方案，基于 Istio 社区的 SDS （Secret Discovery Service）方案为 Sidecar 配置证书，支持证书动态发现和热更新能力。为了支持更高级的安全能力，MOSN 没有使用 Citadel 的证书自签发能力，而是通过对接内部 KMS 系统获取证书。同时提供证书缓存和证书推送更新能力。\n我们先来看看 MOSN 证书方案的架构图，如下图所示：\n各组件职能如下：\n Pilot：负责 Policy、SDS 配置下发，为简化复杂度，图中未标出 Citadel：Citadel 作为 Certificate Provider ，同时作为 MCP Server 为 Citadel Agent 提供 Pod、CR等资源 Citadel Agent：提供 SDS Server 服务，为MOSN、DB Sidecar、Security Sidecar 提供Certificate和CR下发能力 KMS：密钥管理系统负责证书签发  证书获取流程 对整体架构有个大致理解后，我们分解下 Sidecar 获取证书的流程，如下图所示：\n补充说明下图中的每一步环节：\n Citadel 与 Citadel agent（nodeagent）组件通过MCP协议（Mesh Configuration Protocol）同步Pod 和 CR 信息，避免 citadel agent 直接请求 API Server 导致 API Server 负载过高 MOSN 通过Unix Domain Socket 方式向 Citadel Agent 发起 SDS 请求 Citadel Agent 会进行防篡改校验，并提取appkey Citadel Agent 携带 appkey 请求 Citadel 签发证书 Citadel 检查证书是否已缓存，如果缓存证书未过期，Citadel 将直接响应缓存证书 证书不在缓存中，Citadel 会基于 appkey 构造证书签发请求，向 KMS 申请签发证书 KMS 会将签发的证书响应回Citadel，另外 KMS 也支持证书过期轮换通知 Citadel 收到证书后，会将证书传递给到对应的 Citadel Agent Citadel Agent 收到证书后，会在内存中缓存证书，并将证书下发给到 MOSN  ","date":-62135596800,"description":"","dir":"projects/mosn/concept/tls/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"47ccccf9690bc65fa437463a8f5e55b6","permalink":"/projects/mosn/concept/tls/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/mosn/concept/tls/","summary":"本文将向您展示 MOSN 的 TLS 安全能力。 证书方案 MOSN 支持通过 Istio Citadel 的证书签发方案，基于 Istio 社区的 SDS （Secret Discovery Service）方案为 Sidecar 配置证书，支持证书","tags":null,"title":"TLS 安全链路","type":"projects","url":"/projects/mosn/concept/tls/","wordcount":654},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nSetting tars nodes Build tars nodes with following information refering to here:\n APP: TestInventory, Server Name: InventoryApp, OBJ: InventoryObj, Port: 29740 APP: HmilyAccount, Server Name: AccountApp, OBJ: AccountObj, Port: 10386  With nodes built, run mvn clean package packaging command respectively under hmily-demo-tars-springboot-account and hmily-demo-tars-springboot-inventory directories, and publish with outputs on previous nodes set. Refer to here for details.\nOpen with Your Favourite Editor (IDEA) and Locate on hmily-demo-tars Module Configuring（hmily-demo-tars-account module for instance）  Configure with your business database(account module for instance)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Replace 192.168.41.102 globally with tars platform IP address within file(s) suffixed with config.conf under rescouces directory, and append an -Dconfig=\u0026amp;lt;file\u0026amp;gt; parameter to bootstrap parameters with previous file location(s)\n run TarsHmilyAccountApplication.java\n  Run hmily-demo-tars-springboot-inventory(refer to simillar instructions above). Run hmily-demo-tars-springboot-order(refer to simillar instructions above). Access on http://127.0.0.1:18087/swagger-ui.html for more. ","date":-62135596800,"description":"Tars Quick Start","dir":"projects/hmily/quick-start-tars/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"76bbac5c6c6575e871bc59f8373e9890","permalink":"/en/projects/hmily/quick-start-tars/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/quick-start-tars/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nSetting tars nodes Build tars nodes with following information refering to here:\n APP: TestInventory, Server Name: InventoryApp, OBJ: InventoryObj, Port: 29740 APP: HmilyAccount, Server Name: AccountApp, OBJ: AccountObj, Port: 10386  With nodes built, run mvn clean package packaging command respectively under hmily-demo-tars-springboot-account and hmily-demo-tars-springboot-inventory directories, and publish with outputs on previous nodes set.","tags":null,"title":"Tars Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-tars/","wordcount":227},{"author":null,"categories":null,"content":" Tars User Guide  Step 1: Introduce the jar packages\n Step 2: Introduce the Hmily configuration\n Step 3: Add @Hmily annotation on the auto-generated Servant interface method which required the Hmily Distributed Transaction.\n Step 4: Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n  Introduce The Maven dependency Spring-Namespace\n Introduce the hmily-tars dependency   \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-tars\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   make the configuration in the XML configuration file as below:\n\u0026amp;lt;!--Configure the base packages that the Hmily framework need to scan --\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- set up to enable the aspectj-autoproxy --\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- Configure the bean parameters for Hmily startup --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyCommunicatorBeanPostProcessor\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.tars.spring.TarsHmilyCommunicatorBeanPostProcessor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;tarsHmilyStartupBean\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.tars.spring.TarsHmilyFilterStartupBean\u0026amp;quot;/\u0026amp;gt;  Spring-Boot\n Introduce the hmily-spring-boot-starter-tars dependency\nxml \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-tars\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;    Introduce the Hmily configuration  new a configuration file named hmily.yml under the resource directory of the current project\n the specific parameter configuration can refer to configuration detail,Local configuration mode, Zookeeper configuration mode, nacos configuration mode,apollo configuration mode\n  Add annotations on the service implementation interface We have completed the integration described above,and the next we will talk about the specific implementation.\nTCC Mode  Add @HmilyTCC (confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;) annotation to the concrete implementation of the interface method identified by \u0026amp;lsquo;@Hmily\u0026amp;rsquo;.\n confirmMethod : the method name for confirm，The method parameter list and return type should be consistent with the identification method.\n cancelMethod : the method for cancel，The method parameter list and return type should be consistent with the identification method.\n The TCC mode should ensure the idempotence of the confirm and cancel methods,Users need to develop these two methods by themselves,The confirmation and rollback behavior of all transactions are completely up tp users.The Hmily framework is just responsible for making calls.\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = …","date":-62135596800,"description":"Tars User Guide","dir":"projects/hmily/user-tars/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"6ab504fb449aa59762cf68f052d17f16","permalink":"/en/projects/hmily/user-tars/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/hmily/user-tars/","summary":"Tars User Guide  Step 1: Introduce the jar packages\n Step 2: Introduce the Hmily configuration\n Step 3: Add @Hmily annotation on the auto-generated Servant interface method which required the Hmily Distributed Transaction.\n Step 4: Add @HmilyTCC or @HmilyTAC annotation on the concrete implementation method(Service provider).\n  Introduce The Maven dependency Spring-Namespace\n Introduce the hmily-tars dependency   \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-tars\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   make the configuration in the XML configuration file as below:","tags":null,"title":"Tars User Guide","type":"projects","url":"/en/projects/hmily/user-tars/","wordcount":352},{"author":null,"categories":null,"content":" SOFAArk 容器提供了一个简单的 telnet 服务端小工具，用于运行时查看容器状态，目前支持查看 Plugin 和 Biz 相关信息。\n使用方式 使用 telnet 连接服务端，端口号为 1234， 例如：\n telnet localhost 1234\n 进入交互界面:\n➜ telnet localhost 1234 Trying 127.0.0.1... Connected to localhost. Escape character is \u0026#39;^]\u0026#39;. sofa-ark\u0026amp;gt; sofa-ark\u0026amp;gt; sofa-ark\u0026amp;gt;  目前支持查看 Plugin 和 Biz 相关信息，相关命令可参考提示信息：\nsofa-ark\u0026amp;gt;h Plugin Command Tips: USAGE: plugin [option...] [pluginName...] SAMPLE: plugin -m plugin-A plugin-B -h Shows the help message. -a Shows all plugin name. -m Shows the meta info of specified pluginName. -s Shows the service info of specified pluginName. -d Shows the detail info of specified pluginName. Biz Command Tips: USAGE: biz [option...] [arguments...] SAMPLE: biz -m bizIdentityA bizIdentityB. -h Shows the help message. -a Shows all biz. -m Shows the meta info of specified bizIdentity. -s Shows the service info of specified bizIdentity. -d Shows the detail info of specified bizIdentity. -i Install biz of specified bizIdentity or bizUrl. -u Uninstall biz of specified bizIdentity. -o Switch biz of specified bizIdentity. sofa-ark\u0026amp;gt;  Plugin 命令 如提示信息所说，plugin 支持查看插件相关信息，包括类(资源)导入导出配置、插件打包配置等。例如：\nsofa-ark\u0026amp;gt;plugin -md runtime-sofa-boot-plugin PluginName: runtime-sofa-boot-plugin Version: 3.1.3 Priority: 1500 Activator: com.alipay.sofa.runtime.integration.activator.SofaRuntimeActivator Export Packages: com.alipay.sofa.runtime.api.*,com.alipay.sofa.runtime.client.*,com.alipay.sofa.runtime.component.*,com.alipay.sofa.runtime.constants.*,com.alipay.sofa.runtime.integration.*,com.alipay.sofa.runtime.model.*,com.alipay.sofa.runtime.service.component,com.alipay.sofa.runtime.service.helper,com.alipay.sofa.runtime.spi.client,com.alipay.sofa.runtime.spi.component,com.alipay.sofa.runtime.spi.health,com.alipay.sofa.runtime.spi.log,com.alipay.sofa.runtime.spi.binding,com.alipay.sofa.runtime.spi.util,org.aopalliance.aop,org.aopalliance.intercept Import Packages: \\ Export Classes: com.alipay.sofa.runtime.service.binding.JvmBinding,com.alipay.sofa.runtime.SofaFramework,com.alipay.sofa.runtime.SofaRuntimeProperties,com.alipay.sofa.runtime.service.binding.JvmBindingParam,com.alipay.sofa.runtime.spi.service.ServiceProxy Import Classes: \\ Export Resources: \\ Import Resources: \\ GroupId: com.alipay.sofa ArtifactId: runtime-sofa-boot-plugin Version: 3.1.3 URL: jar:file:/Users/qilong.zql/.m2/repository/com/alipay/sofa/runtime-sofa-boot-plugin/3.1.3/runtime-sofa-boot-plugin-3.1.3.jar!/ ClassLoader: com.alipay.sofa.ark.container.service.classloader.PluginClassLoader@420a63fb ClassPath: …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-telnet/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f23d585e5439569b47ed83f7bc955b22","permalink":"/projects/sofa-boot/sofa-ark-ark-telnet/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-telnet/","summary":"SOFAArk 容器提供了一个简单的 telnet 服务端小工具，用于运行时查看容器状态，目前支持查看 Plugin 和 Biz 相关信息。 使用方式 使用 telnet 连接服务端，端口号为 1234， 例如：","tags":null,"title":"Telnet 指令","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-telnet/","wordcount":513},{"author":null,"categories":null,"content":" Ark package The Executed Fat Jar that meets the specific directory format requirements can use the officially provided Maven plug-in (sofa-Ark-maven-plugin) to package the engineering application into a standard-format Ark package. Start the application on top of the SOFAArk container with the java -jar command. The Ark package usually contains the Ark Container, Ark Plugin dependency (if any), merged deployed Ark Biz (if any), and the Ark Biz of the application itself. For details, refer to the Ark package；\nArk Container The Ark container (Ark Plugin and Ark Biz) runs on top of the SOFAArk container. The container has the ability to manage multiple plug-ins and applications. After successful start, the container will resolve the configuration of Ark Plugin and Ark Biz, complete loading of the isolation and start them in turn based on their priorities. For details, refer to SOFAArk container startup；\nArk Plugin The Ark plug-in, which meets the specific fat jar directory format requirements, can use the officially provided Maven plug-in (sofa-Ark-plugin-maven-plugin) to package one or multiple common Java Jar packages into a standard-format Ark Plugin. Ark Plugin will contain a configuration file that usually contains the import and export configuration of plug-in classes and resources and the priority of plug-in startup. When running, the Ark container will use an independent PluginClassLoader to load the plug-ins, and build the index table for class loading according to the plug-in configuration, so that the plug-ins are isolated from each other and from the applications. For details, refer to Ark Plugin；\nArk Biz The Ark module, which meets the specific fat jar directory format requirements, can use the officially provided Maven plug-in (sofa-Ark-maven-plugin) to package an engineering application into a standard-format Ark Biz package. The Ark Biz package has two roles: one is to be the organizational unit of the engineering application module and its dependent packages, and the other is to be used as a common jar package dependency by other applications to start multiple Ark Biz packages in the same SOFAArk container. Multiple Ark Biz packages share the Ark Container and Ark Plugin. For details, refer to Ark Biz；\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-terminology/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b6d0ed10afe9d04bc00307017ffba7c5","permalink":"/en/projects/sofa-boot/sofa-ark-terminology/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-ark-terminology/","summary":"Ark package The Executed Fat Jar that meets the specific directory format requirements can use the officially provided Maven plug-in (sofa-Ark-maven-plugin) to package the engineering application into a standard-format Ark package. Start the application on top of the SOFAArk container with the java -jar command. The Ark package usually contains the Ark Container, Ark Plugin dependency (if any), merged deployed Ark Biz (if any), and the Ark Biz of the application itself.","tags":null,"title":"Terminologies","type":"projects","url":"/en/projects/sofa-boot/sofa-ark-terminology/","wordcount":351},{"author":null,"categories":null,"content":" Explanation of Terms    Terminology Description     TraceId TraceId refers to the ID that represents the unique request in SOFATracer. This ID is generally generated by the first system in the cluster that processes the request and is passed over the network to the next requested system in distributed calls.   SpanId SpanId represents the location or level of the request in the entire call link. For example, the system A calls system B, C, and D in sequence when processing a request. Then the SpanId of the three calls are respectively: 0.1, 0.2, 0.3. If system B continues to call system E and F, the SpanIds of the two calls are: 0.1.1, 0.1.2.    For other related terminologies, see OpenTracing specification.\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/explanation/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8ba307b0679e918f7ac68c7efb7e53f7","permalink":"/en/projects/sofa-tracer/explanation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-tracer/explanation/","summary":"Explanation of Terms    Terminology Description     TraceId TraceId refers to the ID that represents the unique request in SOFATracer. This ID is generally generated by the first system in the cluster that processes the request and is passed over the network to the next requested system in distributed calls.   SpanId SpanId represents the location or level of the request in the entire call link.","tags":null,"title":"Terminologies","type":"projects","url":"/en/projects/sofa-tracer/explanation/","wordcount":118},{"author":null,"categories":null,"content":" General terminology    Term Description     Service A software function provided over the network with specific business logic processing capabilities.   Service provider A computer node that provides services over the network.   Service consumer A computer node that receives services through the network. The same computer node can both be the service provider of some services and the service consumer of others.   Service discovery The process in which the service consumer obtains the network address of the service provider.   Service registry A software system that provides service discovery functions to help service consumers obtain network addresses of service providers.   Data center An independent physical area with a fixed physical location, stable power supply, and reliable network. A data center is usually an important factor that you want to consider in high availability design. Generally, deployment in the same data center features higher network quality, lower latency, but limited disaster recovery capability. However, deployment across different data centers features lower network quality, higher latency, but better disaster recovery capability.    SOFARegistry terminology    Terminology Description     SOFARegistry A registry product open sourced by Ant Financial to provide service discovery based on the \u0026amp;ldquo;publishing-subscription\u0026amp;rdquo; mode. In addition to service discovery, SOFARegistry is applicable to more general \u0026amp;ldquo;publishing-subscription\u0026amp;rdquo; scenarios.   Data In the context of service discovery, data specifically refers to the network address and some additional information of the service provider. In other circumstances, it also refers to information published to SOFARegistry.   Zone The key concept of the zone-based architecture. In the context of service discovery, a zone is a collection of publishing and subscription requests. When you publish or subscribe to a service, you need to specify the zone name. For more information, see Active geo-redundant zone-based architecture solution.   Publisher A node that publishes data to SOFARegistry. In the context of service discovery, the service provider is the publisher of the \u0026amp;ldquo;service provider\u0026amp;rsquo;s network address and additional information\u0026amp;rdquo;.   Subscriber A node that subscribes to data from SOFARegistry. In the context of service discovery, the service consumer is the subscriber of the \u0026amp;ldquo;service provider\u0026amp;rsquo;s network address and additional information\u0026amp;rdquo;.   Data ID A string that is used to identify the data. In the context of service discovery, DataId usually consists of the service port name, protocol, and version number. It is used as an identifier of the service.   Group ID A string that is used for grouping data. It can be used in conjunction with DataId and InstanceId as a namespace identifier of data. Two services may be considered one same service only when their DataIds, GroupIds, and InstanceIds are identical.   Instance ID A …","date":-62135596800,"description":"","dir":"projects/sofa-registry/terminology/","fuzzywordcount":600,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b678a49547c55f2a70e2d94dbce5b4a2","permalink":"/en/projects/sofa-registry/terminology/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/en/projects/sofa-registry/terminology/","summary":"General terminology    Term Description     Service A software function provided over the network with specific business logic processing capabilities.   Service provider A computer node that provides services over the network.   Service consumer A computer node that receives services through the network. The same computer node can both be the service provider of some services and the service consumer of others.","tags":null,"title":"Terminology","type":"projects","url":"/en/projects/sofa-registry/terminology/","wordcount":508},{"author":null,"categories":null,"content":" Unit test Place the unit test cases in the modules developed by yourself.\nIf the cases rely on a third-party server (such as ZooKeeper), you must manually add the profile. See the registry-zookeeper module code.\nIf the cases rely on other modules and integration test is required, place them in the test/test-intergrated module.\nIf the cases also rely on a third-party server (such as ZooKeeper), place them in the test-intergrated-3rd module.\nPerformance test Close the following projects that are closed by default:\n-Dcontext.attachment.enable=false -Dserialize.blacklist.enable=false -Ddefault.tracer= -Dlogger.impl=com.alipay.sofa.rpc.log.SLF4JLoggerImpl -Dmultiple.classloader.enable=false -Devent.bus .enable=false\nA pressure test on BOLT+hessian has been done.\n Server: 4C8G virtual machine; gigabit network; jdk1.8.0_111;\n Client: 50 concurrent requests\n     Protocol Request Response Server TPS Average RT (ms)     bolt+hessian 1KB string 1KB string Directly return 10000 1.93   bolt+hessian 1KB string 1KB string Directly return 20000 4.13   bolt+hessian 1KB string 1KB string Directly return 30000 7.32   bolt+hessian 1KB string 1KB string Directly return 40000 15.78   bolt+hessian 1KB string 1KB string Directly return 50000 (Close to the utmost limit, error rate: 0.3%) 26.51    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/test/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ccda7c2372a7f55d61f682b72d3b1dc2","permalink":"/en/projects/sofa-rpc/test/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/test/","summary":"Unit test Place the unit test cases in the modules developed by yourself.\nIf the cases rely on a third-party server (such as ZooKeeper), you must manually add the profile. See the registry-zookeeper module code.\nIf the cases rely on other modules and integration test is required, place them in the test/test-intergrated module.\nIf the cases also rely on a third-party server (such as ZooKeeper), place them in the test-intergrated-3rd module.","tags":null,"title":"Test","type":"projects","url":"/en/projects/sofa-rpc/test/","wordcount":169},{"author":null,"categories":null,"content":" When using the Bolt protocol for communication, invoke timeout defaults is 3 seconds. You can configure the timeout when referencing the service, and can also configure the timeout period from the dimension of service or method respectively. SOFARPC timeout can be set in milliseconds.\nService If you need to set the timeout from the dimension of service when publishing a service, just configure the timeout parameter to the corresponding value.\nUse XML If you reference the service using XML, set the value of the timeout attribute of the \u0026amp;lt;sofa:global-attrs\u0026amp;gt; tag under the \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; tag:\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs timeout=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Use Annotation If you reference the service using Annotation, set the value of the timeout attribute of @SofaReferenceBinding:\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, timeout = 2000)) private SampleService sampleService;  Use API in Spring environment If you reference the service in Spring or Spring Boot environment, just set the value of timeout attribute of BoltBindingParam:\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setTimeout(2000)  Use API in non-Spring environment If you reference service using the bare API of SOFARPC directly in non-Spring environment, just set the timeout attribute of ConsumerConfig:\nConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;SampleService\u0026amp;gt;() .setInterfaceId(SampleService.class.getName()) .setRegistry(registryConfig) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;) .setTimeout(2000);  Method If you want to adjust the timeout for a certain method in a service individually, you can set the timeout period from the dimension of method.\nFor a method, the timeout period of the method is prioritized. If not set, the timeout period of the service will be used.\nUse XML If you reference service using XML, just set the timeout attribute of the corresponding \u0026amp;lt;sofa: method\u0026amp;gt;:\n\u0026amp;lt;Sofa: Reference interface = \u0026amp;quot;com.example.demo .SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;hello\u0026amp;quot; timeout=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Use Annotation No method is available for setting the method-level timeout with Annotation currenty.\nUse API in Spring environment To reference a service in Spring or Spring Boot environment, you can just set the value of timeout attribute of RpcBindingMethodInfo\nBoltBindingParam boltBindingParam = new BoltBindingParam(); RpcBindingMethodInfo rpcBindingMethodInfo = new RpcBindingMethodInfo(); rpcBindingMethodInfo.setName(\u0026amp;quot;hello\u0026amp;quot;); rpcBindingMethodInfo.setTimeout(2000); List\u0026amp;lt;RpcBindingMethodInfo\u0026amp;gt; rpcBindingMethodInfos = new ArrayList\u0026amp;lt;\u0026amp;gt;(); …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/bolt-timeout/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"cf14f73dc0c4672a9255ef55b56de419","permalink":"/en/projects/sofa-rpc/bolt-timeout/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/bolt-timeout/","summary":"When using the Bolt protocol for communication, invoke timeout defaults is 3 seconds. You can configure the timeout when referencing the service, and can also configure the timeout period from the dimension of service or method respectively. SOFARPC timeout can be set in milliseconds.\nService If you need to set the timeout from the dimension of service when publishing a service, just configure the timeout parameter to the corresponding value.","tags":null,"title":"Timeout control","type":"projects","url":"/en/projects/sofa-rpc/bolt-timeout/","wordcount":388},{"author":null,"categories":null,"content":" TraceId generation rule SOFATracer uses TraceId to concatenate the call logs of a request on each server. The TraceId is typically generated by the first server that receives the request. The generation rule is: server IP + generated time + incremental sequence + current process ID, such as:\n0ad1348f1403169275002100356696   The first 8 digits 0ad1348f is the IP of the machine that generates TraceId. This is a hexadecimal number, in which every two digits represents a part of IP. Based on the number, we can get a common IP address like 10.209.52.143 by converting every two digits into a decimal number. According to this rule, you can also figure out the first server that the request goes through. The next 13 digits 1403169275002 is the time to generate the TraceId. The next 4 digits 1003 is an auto-incrementing sequence that increases from 1000 to 9000. After reaching 9000, it returns to 1000 and then restarts to increase. The last 5 digits 56696 is the current process ID. Its role in tracerId is to prevent the TraceId conflicts caused by multiple processes in a single machine.   Currently, TraceId\u0026amp;rsquo;s generated rules refer to Taobao\u0026amp;rsquo;s Hawkeye components.\n SpanId generation rule The SpanId in SOFATracer represents where the current call is in the entire calling link. If a Web system A receives a user request, then in the SOFATracer MVC log of this system, the recorded SpanId is 0, which means the root node of the entire call. If the system A processes this request and needs to call system B, C, and D through RPC, then the SpanIds in the SOFATracer RPC client log of system A are 0.1, 0.2, and 0.3 respectively. And in the SOFATracer RPC server logs of the system B, C, and D, the SpanIds are also 0.1, 0.2 and 0.3 respectively. If system C calls system E and F when processing the request, then in the corresponding SOFATracer RPC client log of system C, the SpanIds are 0.2.1 and 0.2.2. And the SpanIds in the SOFATracer RPC server logs of system E and F are also 0.2.1 and 0.2.2. As we can known from above, if all SpanIds in a call can be collected to compose a complete link tree.\nWe assume that the TraceId generated in a distributed call is 0a1234 (much longer in practice). Then, according to the generation process of SpanId, the call link tree is as shown in the following figure:\n Currently, SpanId\u0026amp;rsquo;s generated rules refer to Taobao\u0026amp;rsquo;s Hawkeye components.\n ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/traceid-generated-rule/","fuzzywordcount":500,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"8f0ef8df65deec2a4fa6591a316aa5e8","permalink":"/en/projects/sofa-tracer/traceid-generated-rule/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-tracer/traceid-generated-rule/","summary":"TraceId generation rule SOFATracer uses TraceId to concatenate the call logs of a request on each server. The TraceId is typically generated by the first server that receives the request. The generation rule is: server IP + generated time + incremental sequence + current process ID, such as:\n0ad1348f1403169275002100356696   The first 8 digits 0ad1348f is the IP of the machine that generates TraceId. This is a hexadecimal number, in which every two digits represents a part of IP.","tags":null,"title":"TraceId and spanId generation rule","type":"projects","url":"/en/projects/sofa-tracer/traceid-generated-rule/","wordcount":415},{"author":null,"categories":null,"content":" TraceId 生成规则 SOFATracer 通过 TraceId 来将一个请求在各个服务器上的调用日志串联起来，TraceId 一般由接收请求经过的第一个服务器产生，产生规则是： 服务器 IP + 产生 ID 时候的时间 + 自增序列 + 当前进程号 ，比如：\n0ad1348f1403169275002100356696  前 8 位 0ad1348f 即产生 TraceId 的机器的 IP，这是一个十六进制的数字，每两位代表 IP 中的一段，我们把这个数字，按每两位转成 10 进制即可得到常见的 IP 地址表示方式 10.209.52.143，大家也可以根据这个规律来查找到请求经过的第一个服务器。 后面的 13 位 1403169275002 是产生 TraceId 的时间。 之后的 4 位 1003 是一个自增的序列，从 1000 涨到 9000，到达 9000 后回到 1000 再开始往上涨。 最后的 5 位 56696 是当前的进程 ID，为了防止单机多进程出现 TraceId 冲突的情况，所以在 TraceId 末尾添加了当前的进程 ID。\n TraceId 目前的生成的规则参考了阿里的鹰眼组件。\n SpanId 生成规则 SOFATracer 中的 SpanId 代表本次调用在整个调用链路树中的位置，假设一个 Web 系统 A 接收了一次用户请求，那么在这个系统的 SOFATracer MVC 日志中，记录下的 SpanId 是 0，代表是整个调用的根节点，如果 A 系统处理这次请求，需要通过 RPC 依次调用 B，C，D 三个系统，那么在 A 系统的 SOFATracer RPC 客户端日志中，SpanId 分别是 0.1，0.2 和 0.3，在 B，C，D 三个系统的 SOFATracer RPC 服务端日志中，SpanId 也分别是 0.1，0.2 和 0.3；如果 C 系统在处理请求的时候又调用了 E，F 两个系统，那么 C 系统中对应的 SOFATracer RPC 客户端日志是 0.2.1 和 0.2.2，E，F 两个系统对应的 SOFATracer RPC 服务端日志也是 0.2.1 和 0.2.2。根据上面的描述，我们可以知道，如果把一次调用中所有的 SpanId 收集起来，可以组成一棵完整的链路树。\n我们假设一次分布式调用中产生的 TraceId 是 0a1234（实际不会这么短），那么根据上文 SpanId 的产生过程，有下图：\n SpanId 目前的生成的规则参考了阿里的鹰眼组件。\n ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/traceid-generated-rule/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8f0ef8df65deec2a4fa6591a316aa5e8","permalink":"/projects/sofa-tracer/traceid-generated-rule/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/traceid-generated-rule/","summary":"TraceId 生成规则 SOFATracer 通过 TraceId 来将一个请求在各个服务器上的调用日志串联起来，TraceId 一般由接收请求经过的第一个服务器产生，产生规则是： 服务器 IP + 产","tags":null,"title":"TraceId 和 SpanId 生成规则","type":"projects","url":"/projects/sofa-tracer/traceid-generated-rule/","wordcount":707},{"author":null,"categories":null,"content":"By default, SOFARPC has integrated SOFATracer. Also, you can use other APM products, such as Skywalking, to achieve the corresponding functions. For details, see the relevant documents:\n SOFATracer Skywalking  If you want to disable the tracing ability of SOFARPC, you can do it in two ways.\nIf you are using rpc-sofa-boot-starter in SOFABoot or Spring Boot environment, you can add a configuration com.alipay.sofa.rpc.defaultTracer= in application.properties.\nIf you are using sofa-rpc-all directly, you can add the following code in the main method of your application before publish any SOFARPC service or create any SOFARPC reference.\nRpcConfigs.putValue(RpcOptions.DEFAULT_TRACER, \u0026amp;quot;\u0026amp;quot;);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/tracing-usage/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"5f944f87d827ae060fb0528f6715af97","permalink":"/en/projects/sofa-rpc/tracing-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/tracing-usage/","summary":"By default, SOFARPC has integrated SOFATracer. Also, you can use other APM products, such as Skywalking, to achieve the corresponding functions. For details, see the relevant documents:\n SOFATracer Skywalking  If you want to disable the tracing ability of SOFARPC, you can do it in two ways.\nIf you are using rpc-sofa-boot-starter in SOFABoot or Spring Boot environment, you can add a configuration com.alipay.sofa.rpc.defaultTracer= in application.properties.\nIf you are using sofa-rpc-all directly, you can add the following code in the main method of your application before publish any SOFARPC service or create any SOFARPC reference.","tags":null,"title":"Tracing","type":"projects","url":"/en/projects/sofa-rpc/tracing-usage/","wordcount":96},{"author":null,"categories":null,"content":" Use client API In the design of SOFALookout client, API is decoupled from the implementation. If you need to log the events based on the SOFALookout API, you only need to add the lookout-api Maven dependency to the pom.xml file in your application/project. If the dependencies (such as client dependencies or SOFABoot (Spring Boot) Starter) do not exist, the API package uses NoopRegistry automatically, to replace all the locations of which the events are logged.\n1.Introduce API dependency \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.About ID Compared to the traditional metrics library\u0026amp;rsquo;s single-dimensional information description, Lookout metrics provides tag capability that supports multi-dimensional descriptions. ID class, the unique identification of Lookout metrics, consists of name and tags.\nId basicId = registry.createId(\u0026amp;quot;rpc.provider.service.stats\u0026amp;quot;); id = basicId.withTag(\u0026amp;quot;service\u0026amp;quot;, \u0026amp;quot;com.alipay.demo.demoService\u0026amp;quot;) .withTag(\u0026amp;quot;method\u0026amp;quot;, \u0026amp;quot;sayHi\u0026amp;quot;) .withTag(\u0026amp;quot;protocol\u0026amp;quot;, \u0026amp;quot;tr\u0026amp;quot;) .withTag(\u0026amp;quot;alias\u0026amp;quot;, \u0026amp;quot;group1\u0026amp;quot;);  The above is a simple example of ID introducing how to create ID and how to tag. Note that every time you tag, a new ID object is generated and returned.\n Do not proactively cache Id or the specific Metric object, since Lookout\u0026amp;rsquo;s Registry has already recorded. When using a same Id (with the same name and tags), the existing Id and its corresponding Metric object will be reused.\n 2.1 Priority tag (optional) PRIORITY enumeration level: HIGH, NORMAL, LOW.\nid.withTag(LookoutConstants.LOW_PRIORITY_TAG);  It is recommended that you do not add this tag, the default level will be NORMAL. The level represents the collection interval (HIGH: 2s, NORMAL: 30s, LOW: 1min).\n2.2 About tags  General tags, such as local IP, data center, and other details, will be attached and no need to be specified separately. In a non-SOFABoot project, you must manually add tags to the client, especially the app tag which specifies the app name: app=appName. key contains only lowercase letters, numbers, and underscores. (especially the metrics at runtime, such as Counter, Timer, and DistributeSummary) The values ​​of a tag shall be within a stable finite set. Try to use as few tags as possible to prevent the number of metrics from exceeding the maximum limit. For example: In RPC service, the value of method\u0026amp;rsquo;s two tags shall be as few as possible. The counterexample is that each RPC call has a separate tag-value. Therefore, the overall principle is that there should be as few custom tags as possible, and the number of sets of the values ​​should be as small as possible. Specialized TAG name \u0026amp;ldquo;priority\u0026amp;rdquo; indicates priority. The tag key reserved by the system is _*_. Starting with an underscore and ending with an …","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-api/","fuzzywordcount":1000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"76574f2435a3565fe1fc50831ff9ab0c","permalink":"/en/projects/sofa-lookout/use-guide-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/en/projects/sofa-lookout/use-guide-api/","summary":"Use client API In the design of SOFALookout client, API is decoupled from the implementation. If you need to log the events based on the SOFALookout API, you only need to add the lookout-api Maven dependency to the pom.xml file in your application/project. If the dependencies (such as client dependencies or SOFABoot (Spring Boot) Starter) do not exist, the API package uses NoopRegistry automatically, to replace all the locations of which the events are logged.","tags":null,"title":"Use API","type":"projects","url":"/en/projects/sofa-lookout/use-guide-api/","wordcount":911},{"author":null,"categories":null,"content":" SOFARPC Service publishing The process of service publishing involves three classes RegistryConfig, ServerConfig, ProviderConfig.\n RegistryConfig\nRegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181\u0026amp;quot;)  RegistryConfig represents the registry center. As above, the address and port of the service registry center is 127.0.0.1:2181, and the protocol is Zookeeper.\n ServerConfig java ServerConfig serverConfig = new ServerConfig() .setPort(8803) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;);    ServerConfig represents the container where service runs. The above declares a server using the 8803 port and the bolt protocol.\n ProviderConfig\nProviderConfig\u0026amp;lt;HelloWorldService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloWorldService\u0026amp;gt;() .setInterfaceId(HelloWorldService.class.getName()) .setRef(new HelloWorldServiceImpl()) .setServer(serverConfig) .setRegistry(registryConfig); providerConfig.export();  ProviderConfig represents service publishing. The above declares the interface of the service, implements the server running the service, and eventually publishes the service by the export method.\nService reference Service reference involves two classes, namely RegistryConfig and ConsumerConfig.\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig); HelloService helloService = consumerConfig.refer();   ConsumerConfig represents service reference. The above declares the interface and service registry center of the referenced service interface, and finally references the service by the refer method to get the proxy for the remote call of the service.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-rpc/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"ee6f74a4974c7abf72322cef108d5ef0","permalink":"/en/projects/sofa-rpc/programing-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/programing-rpc/","summary":"SOFARPC Service publishing The process of service publishing involves three classes RegistryConfig, ServerConfig, ProviderConfig.\n RegistryConfig\nRegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026quot;zookeeper\u0026quot;) .setAddress(\u0026quot;127.0.0.1:2181\u0026quot;)  RegistryConfig represents the registry center. As above, the address and port of the service registry center is 127.0.0.1:2181, and the protocol is Zookeeper.\n ServerConfig java ServerConfig serverConfig = new ServerConfig() .setPort(8803) .setProtocol(\u0026quot;bolt\u0026quot;);    ServerConfig represents the container where service runs. The above declares a server using the 8803 port and the bolt protocol.","tags":null,"title":"Use API in non-Spring environment","type":"projects","url":"/en/projects/sofa-rpc/programing-rpc/","wordcount":173},{"author":null,"categories":null,"content":" This article describes how to quickly start installing and configuring Istio by using Docker Compose.\nSOFAMosn can not only support the standard Istio deployment mode, but also support the unilateral Inbound Sidecar or Outbound Sidecar deployment mode to meet the various requirements of users.\nPrerequisites  Docker Docker Compose  Install Istio  Download the latest release package. Unzip the installation file and go to the decompressed path. The installation path contains:  Sample application path samples/. The istioctl client executable file which is in the /bin path. The istioctl can be used to create routing rules and policies. Configuration file istion.VERSION.  Add the Istio\u0026amp;rsquo;s bin path to your system\u0026amp;rsquo;s PATH. For example, execute the following command in the MacOS or Linux operating system:\nexport PATH=$PWD/bin;$PATH   Pull up the Istio control plane container: SHELL docker-compose -f install/zookeeper/istio.yaml up -d   Ensure that all Docker containers are running:\ndocker ps -a  If the Istio pilot container terminates unexpectedly, you can run the istioctl context-create command and re-execute the previous command. 6. Configure istioctl to use the Istio API server:\nistioctl context-create -context istio-local --api-server   Deploy application Now, you can start deploying the SOFABoot demo program. The demo program includes a client and a server, which communicate with each other through Bolt protocol.\ndocker-compose up -f sofa-sample-spec.yaml up -d  Uninstall Istio docker-compose up -f install/zookeeper/istio.yaml down  ","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"1de4868fa0e9c73d932343847864d7fb","permalink":"/en/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","summary":"This article describes how to quickly start installing and configuring Istio by using Docker Compose.\nSOFAMosn can not only support the standard Istio deployment mode, but also support the unilateral Inbound Sidecar or Outbound Sidecar deployment mode to meet the various requirements of users.\nPrerequisites  Docker Docker Compose  Install Istio  Download the latest release package. Unzip the installation file and go to the decompressed path. The installation path contains:  Sample application path samples/.","tags":null,"title":"Use Docker to get started with Istio","type":"projects","url":"/en/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","wordcount":219},{"author":null,"categories":null,"content":" Introduction This section is intended to demonstrate how to use Jarslink 2.0 to dynamically control the life cycle of the Biz package and to complete its installation, uninstallation, and query.\nDemo With reference to How to reform common Spring Boot applications, the reformed spring-boot-transform-sample project has integrated the Jarslink 2.0 component. By executing the Ark package that the application packaged and generated, you can dynamically install or uninstall the application during its running.\n java -jar starts the spring-boot-transform-sample application Ark package. telnet localhost 1234 enters the Jarslink 2.0 command interface, as follows:\n\u0026amp;gt; telnet localhost 1234 s \u0026amp;gt; Trying 127.0.0.1\u0026amp;hellip;\n\u0026amp;gt; Connected to localhost.\n\u0026amp;gt; Escape character is \u0026amp;lsquo;^]\u0026amp;rsquo;.\n\u0026amp;gt; sofa-ark\u0026amp;gt; Execute the check -b query command, and the result is as follows: \u0026amp;gt; sofa-ark\u0026amp;gt;check -b\n\u0026amp;gt; Biz count=1\n\u0026amp;gt; bizName=\u0026amp;lsquo;spring-boot-transform-sample\u0026amp;rsquo;, bizVersion=\u0026amp;lsquo;1.0.0\u0026amp;rsquo;, bizState=\u0026amp;lsquo;activated\u0026amp;rsquo; \u0026amp;gt; \u0026amp;gt; sofa-ark\u0026amp;gt; With reference to How to reform a common Spring Boot application, create any SOFABoot application of non-Web type, package it into a Biz package, and execute the install -b installation command, and the result is as follows: \u0026amp;gt; sofa-ark\u0026amp;gt;install -b file:///Users/qilong.zql/Desktop/test-ark-biz.jar\n\u0026amp;gt; Biz:\u0026amp;lsquo;test-biz:1.0.0\u0026amp;rsquo; is installing. \u0026amp;gt; \u0026amp;gt; sofa-ark\u0026amp;gt;\n Execute the check -b query command again, and the result is as follows: \u0026amp;gt; sofa-ark\u0026amp;gt;check -b\n\u0026amp;gt; Biz count=2\n\u0026amp;gt; bizName=\u0026amp;lsquo;test-biz\u0026amp;rsquo;, bizVersion=\u0026amp;lsquo;1.0.0\u0026amp;rsquo;, bizState=\u0026amp;lsquo;activated\u0026amp;rsquo;\n\u0026amp;gt; bizName=\u0026amp;lsquo;spring-boot-transform-sample\u0026amp;rsquo;, bizVersion=\u0026amp;lsquo;1.0.0\u0026amp;rsquo;, bizState=\u0026amp;lsquo;activated\u0026amp;rsquo; \u0026amp;gt; \u0026amp;gt; sofa-ark\u0026amp;gt;\n Execute the uninstall -b -n -v uninstallation command, and the result is as follows: \u0026amp;gt; sofa-ark\u0026amp;gt;uninstall -b -n test-biz -v 1.0.0\n\u0026amp;gt; Uninstall biz:\u0026amp;lsquo;test-biz:1.0.0\u0026amp;rsquo; success. \u0026amp;gt; \u0026amp;gt; sofa-ark\u0026amp;gt;\n Execute the check -b query command again, and the result is as follows: \u0026amp;gt; sofa-ark\u0026amp;gt;check -b\n\u0026amp;gt; Biz count=1\n\u0026amp;gt; bizName=\u0026amp;lsquo;spring-boot-transform-sample\u0026amp;rsquo;, bizVersion=\u0026amp;lsquo;1.0.0\u0026amp;rsquo;, bizState=\u0026amp;lsquo;activated\u0026amp;rsquo; \u0026amp;gt; \u0026amp;gt; sofa-ark\u0026amp;gt;\n  For use of more commands, refer to Interactive Commands.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-jarslink-deploy-demo/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"749f6debe73b73b4882477779008bb99","permalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy-demo/","summary":"Introduction This section is intended to demonstrate how to use Jarslink 2.0 to dynamically control the life cycle of the Biz package and to complete its installation, uninstallation, and query.\nDemo With reference to How to reform common Spring Boot applications, the reformed spring-boot-transform-sample project has integrated the Jarslink 2.0 component. By executing the Ark package that the application packaged and generated, you can dynamically install or uninstall the application during its running.","tags":null,"title":"Use Jarslink for multi-application dynamic deployment","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-jarslink-deploy-demo/","wordcount":277},{"author":null,"categories":null,"content":" This article introduces how to use MOSN to build the Service Mesh development environment based on SOFAMesh framework, and verify some basic capabilities of MOSN, such as routing and load balancing. This article includes the following content:\n Relationship between MOSN and SOFAMesh Preparations Deploy SOFAMesh with source codes Bookinfo experiment  Relationship between MOSN and SOFAMesh As mentioned in MOSN introduction, MOSN is a Service Mesh data plane agent developed with Golang, and SOFAMesh is a large-scale implementation solution for Service Mesh, which is improved and extended based on Istio. Serving as a critical component of SOFAMesh, MOSN is used to complete data plane forwarding.\nThe following figure shows the workflow chart of MOSN based on the overall SOFAMesh framework.\nNote: Currently, MOSN cannot be directly used in the native Istio.\nPreparations This guide supposes you are using macOS. For other operating systems, you can install the corresponding software.\n1. Install HyperKit Install docker-for-mac, and then install driver.\n1.1 Install Docker Download the Docker software package to install it or run the following command to install it:\n$ brew cask install docker  1.2 Install driver $ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; chmod +x docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \\ \u0026amp;amp;\u0026amp;amp; sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit  2. Install Minikube (or purchase the commercial version of k8s cluster) It is recommended to use Minikube V0.28 or later, see https://github.com/kubernetes/minikube.\n$ brew cask install minikube  3. Start Minikube Note that Pilot requires at least 2G memory, so you can add resources to Minikube by adding parameters at startup. If your machine has insufficient resources, it is recommended to use the commercial version of the k8s cluster.\n$ minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.15.0 --vm-driver=hyperkit  Create Istio namespace\n$ kubectl create namespace istio-system  4. Install kubectl command line tool kubectl is a command line interface used to run commands for k8s cluster. For how to install it, see https://kubernetes.io/docs/tasks/tools/install-kubectl.\n$ brew install kubernetes-cli  5. Install Helm Helm is a package management tool for k8s. For how to install it, see https://docs.helm.sh/using_helm/#installing-helm.\n$ brew install kubernetes-helm  Deploy SOFAMesh with source codes 1. Download SOFAMesh source codes $ git clone git@github.com:sofastack/sofa-mesh.git  2. Use Helm to install SOFAMesh You should change directory to sofa-mesh source code, and then use helm template to install isito crd and istio\n``` $ cd sofa-mesh $ helm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f - $ helm template …","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-run-with-sofamesh/","fuzzywordcount":1100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9c6461e92180417d3a8ec4f3f2c723fe","permalink":"/en/projects/mosn/quick-start-run-with-sofamesh/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/mosn/quick-start-run-with-sofamesh/","summary":"This article introduces how to use MOSN to build the Service Mesh development environment based on SOFAMesh framework, and verify some basic capabilities of MOSN, such as routing and load balancing. This article includes the following content:\n Relationship between MOSN and SOFAMesh Preparations Deploy SOFAMesh with source codes Bookinfo experiment  Relationship between MOSN and SOFAMesh As mentioned in MOSN introduction, MOSN is a Service Mesh data plane agent developed with Golang, and SOFAMesh is a large-scale implementation solution for Service Mesh, which is improved and extended based on Istio.","tags":null,"title":"Use MOSN to build Service Mesh platform","type":"projects","url":"/en/projects/mosn/quick-start-run-with-sofamesh/","wordcount":1078},{"author":null,"categories":null,"content":" This article introduces how to use MOSN to build the Service Mesh development environment based on SOFAMesh framework, and verify some basic capabilities of MOSN, such as routing and load balancing. This article includes the following content:\n Relationship between MOSN and SOFAMesh Preparations Deploy SOFAMesh with source codes Bookinfo experiment  Relationship between MOSN and SOFAMesh As mentioned in MOSN introduction, MOSN is a Service Mesh data plane agent developed with Golang, and SOFAMesh is a large-scale implementation solution for Service Mesh, which is improved and extended based on Istio. Serving as a critical component of SOFAMesh, MOSN is used to complete data plane forwarding.\nThe following figure shows the workflow chart of MOSN based on the overall SOFAMesh framework.\nNote: Currently, MOSN cannot be directly used in the native Istio.\nPreparations This guide supposes you are using macOS. For other operating systems, you can install the corresponding software.\n1. Install HyperKit Install docker-for-mac, and then install driver.\n1.1 Install Docker Download the Docker software package to install it or run the following command to install it:\n$ brew cask install docker  1.2 Install driver $ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; chmod +x docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \\ \u0026amp;amp;\u0026amp;amp; sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit  2. Install Minikube (or purchase the commercial version of k8s cluster) It is recommended to use Minikube V0.28 or later, see https://github.com/kubernetes/minikube.\n$ brew cask install minikube  3. Start Minikube Note that Pilot requires at least 2G memory, so you can add resources to Minikube by adding parameters at startup. If your machine has insufficient resources, it is recommended to use the commercial version of the k8s cluster.\n$ minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.15.0 --vm-driver=hyperkit  Create Istio namespace\n$ kubectl create namespace istio-system  4. Install kubectl command line tool kubectl is a command line interface used to run commands for k8s cluster. For how to install it, see https://kubernetes.io/docs/tasks/tools/install-kubectl.\n$ brew install kubernetes-cli  5. Install Helm Helm is a package management tool for k8s. For how to install it, see https://docs.helm.sh/using_helm/#installing-helm.\n$ brew install kubernetes-helm  Deploy SOFAMesh with source codes 1. Download SOFAMesh source codes $ git clone git@github.com:sofastack/sofa-mesh.git  2. Use Helm to install SOFAMesh You should change directory to sofa-mesh source code, and then use helm template to install isito crd and istio\n``` $ cd sofa-mesh $ helm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f - $ helm template …","date":-62135596800,"description":"","dir":"projects/occlum/quick-start-run-with-sofamesh/","fuzzywordcount":1100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"7353dfd1d668eb3e2a1c8cd26acca372","permalink":"/en/projects/occlum/quick-start-run-with-sofamesh/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/occlum/quick-start-run-with-sofamesh/","summary":"This article introduces how to use MOSN to build the Service Mesh development environment based on SOFAMesh framework, and verify some basic capabilities of MOSN, such as routing and load balancing. This article includes the following content:\n Relationship between MOSN and SOFAMesh Preparations Deploy SOFAMesh with source codes Bookinfo experiment  Relationship between MOSN and SOFAMesh As mentioned in MOSN introduction, MOSN is a Service Mesh data plane agent developed with Golang, and SOFAMesh is a large-scale implementation solution for Service Mesh, which is improved and extended based on Istio.","tags":null,"title":"Use MOSN to build Service Mesh platform","type":"projects","url":"/en/projects/occlum/quick-start-run-with-sofamesh/","wordcount":1078},{"author":null,"categories":null,"content":" Use Registry Different Registry integrations provide different ways to access Metrics.\n1. LookoutRegistry Provides the ability to count metrics by a time window. It is divided into two modes: “active” and “passive”. The passive mode is off currently.\n(1) Active mode\n You can specify the IP address of the remote agent through [Client Configuration], that is, check when start reporting, and regularly report data.\n(2) Passive mode\n This mode can be activated through [Client Configuration], and HTTP service is provided on port 19399.\n2. Connect to Prometheus The data of SOFALookout can be shared with Prometheus. In order to connect to Prometheus, you first need to add dependencies to your project:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-reg-prometheus\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  After adding the dependencies, launch the application, and you can see the data by visiting http://localhost:9494, where 9494 is the default port, you can configure com.alipay.sofa.lookout.prometheus-exporter-server-port in application.properties to change the port.\nOnce you have the URL to access the data, you can edit a prometheus.yml to grab the project information. Assuming that the local IP address is 10.15.232.20, you can configure prometheus.yml as follows:\nscrape_configs: - job_name: \u0026#39;lookout-client\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;10.15.232.20:9494\u0026#39;]  With the above configuration file, you can start Prometheus locally via Docker:\ndocker run -d -p 9090:9090 -v $PWD/prometheus.yml:/etc/prometheus/prometheus.yml --name prom prom/prometheus:master  Then visit http://localhost:9090 through the browser, and you can query the corresponding Metrics through PromQL.\nAn example of connecting to Prometheus is also available in SOFALookout, so you can go and see it as a reference.\n3. Connect to SpringBoot actuator In addition to Prometheus, SOFALookout can be integrated with the Actuator of SpringBoot 1.x by adding the following dependency:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-actuator\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Then, start and visit http://localhost:8080/metrics to see the data of events logged by the SOFALookout API.\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-registry/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3c51ba6519cee542b459a170dabcf32b","permalink":"/en/projects/sofa-lookout/use-guide-registry/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-lookout/use-guide-registry/","summary":"Use Registry Different Registry integrations provide different ways to access Metrics.\n1. LookoutRegistry Provides the ability to count metrics by a time window. It is divided into two modes: “active” and “passive”. The passive mode is off currently.\n(1) Active mode\n You can specify the IP address of the remote agent through [Client Configuration], that is, check when start reporting, and regularly report data.\n(2) Passive mode\n This mode can be activated through [Client Configuration], and HTTP service is provided on port 19399.","tags":null,"title":"Use Registry","type":"projects","url":"/en/projects/sofa-lookout/use-guide-registry/","wordcount":297},{"author":null,"categories":null,"content":" XML mode The way to publish and reference services in xml mode is as follows. sofa:service represents publishing service, and sofa:reference represents referencing service. sofa:binding indicates the protocol for service publishing or reference.\n\u0026amp;lt;bean id=\u0026amp;quot;personServiceImpl\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonServiceImpl\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;personServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  A service can also be published through multiple protocols, as follows:\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;personServiceImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;sofa:binding.dubbo/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Service reference\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  A service can also be referenced through other protocols:\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceRest\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-xml/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"9192a93415bee3070a9be62c0f693949","permalink":"/en/projects/sofa-rpc/programing-sofa-boot-xml/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/programing-sofa-boot-xml/","summary":"XML mode The way to publish and reference services in xml mode is as follows. sofa:service represents publishing service, and sofa:reference represents referencing service. sofa:binding indicates the protocol for service publishing or reference.\n\u0026lt;bean id=\u0026quot;personServiceImpl\u0026quot; class=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonServiceImpl\u0026quot;/\u0026gt; \u0026lt;sofa:service ref=\u0026quot;personServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;/sofa:service\u0026gt;  A service can also be published through multiple protocols, as follows:\n\u0026lt;sofa:service ref=\u0026quot;personServiceImpl\u0026quot; interface=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;sofa:binding.rest/\u0026gt; \u0026lt;sofa:binding.dubbo/\u0026gt; \u0026lt;/sofa:service\u0026gt;  Service reference\n\u0026lt;sofa:reference id=\u0026quot;personReferenceBolt\u0026quot; interface=\u0026quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt/\u0026gt; \u0026lt;/sofa:reference\u0026gt;  A service can also be referenced through other protocols:","tags":null,"title":"Use XML in SOFABoot","type":"projects","url":"/en/projects/sofa-rpc/programing-sofa-boot-xml/","wordcount":80},{"author":null,"categories":null,"content":" Use annotation for service publishing/reference In addition to the regular xml mode, it is also supported to publish and reference services with annotation in the SOFABoot environment. Similar to xml, we provide @SofaService and @SofaReference as well as @SofaServiceBinding and @SofaReferenceBinding annotation for multi-protocol.\nService publishing To publish an RPC service, you only need to add a @SofaService annotation on the bean to specify the interface and protocol type.\n@SofaService(interfaceType = AnnotationService.class, bindings = { @SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;) }) @Component public class AnnotationServiceImpl implements AnnotationService { @Override public String sayAnnotation(String stirng) { return stirng; } }  Service reference For a bean that needs to reference a remote service, you only need to add the Reference annotation on the attribute or method. This supports the bolt, dubbo, rest protocol.\n@Component public class AnnotationClientImpl { @SofaReference(interfaceType = AnnotationService.class, binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;)) private AnnotationService annotationService; public String sayClientAnnotation(String str) { String result = annotationService.sayAnnotation(str); return result; } }  Use the demo You can test in the annotation subproject of the sample project.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-annotation/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2c3afd33cbce4f5aa2473716b3afe5a6","permalink":"/en/projects/sofa-rpc/programing-sofa-boot-annotation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/programing-sofa-boot-annotation/","summary":"Use annotation for service publishing/reference In addition to the regular xml mode, it is also supported to publish and reference services with annotation in the SOFABoot environment. Similar to xml, we provide @SofaService and @SofaReference as well as @SofaServiceBinding and @SofaReferenceBinding annotation for multi-protocol.\nService publishing To publish an RPC service, you only need to add a @SofaService annotation on the bean to specify the interface and protocol type.","tags":null,"title":"Use annotation in SOFABoot","type":"projects","url":"/en/projects/sofa-rpc/programing-sofa-boot-annotation/","wordcount":171},{"author":null,"categories":null,"content":" This topic mainly describes a JRaft-based distributed counter.\nScenario Save a distributed counter in a raft group of multiple nodes (servers). The counter can increment and be called while remaining consistent among all nodes. The counter can normally provide two external services when a minority of nodes fail:\n incrmentAndGet(delta): increments the value of delta and returns the incremented value. get(): gets the latest value.  Remote procedure calls (RPCs) JRaft adopts the Bolt communication framework at the underlayer, and defines two requests:\n IncrementAndGetRequest: used for incrementing the value  public class IncrementAndGetRequest implements Serializable { private static final long serialVersionUID = -5623664785560971849L; private long delta; public long getDelta() { return this.delta; } public void setDelta(long delta) { this.delta = delta; } }   GetValueRequest: used for getting the latest value  public class GetValueRequest implements Serializable { private static final long serialVersionUID = 9218253805003988802L; public GetValueRequest() { super(); } }  ValueResponse responses include:\n success: indicates that the request was successful value: the latest value returned by a successful request errorMsg: the error message of a failed request redirect: indicates that a leader election occurred and the request needs to be sent to the new leader node  public class ValueResponse implements Serializable { private static final long serialVersionUID = -4220017686727146773L; private long value; private boolean success; /** * redirect peer id */ private String redirect; private String errorMsg; public String getErrorMsg() { return this.errorMsg; } public void setErrorMsg(String errorMsg) { this.errorMsg = errorMsg; } ...... }   IncrementAndAddClosure: used for receiving requests at the leader node IncrementAndGetRequest: used for handling callbacks of the request  public class IncrementAndAddClosure implements Closure { private CounterServer counterServer; private IncrementAndGetRequest request; private ValueResponse response; private Closure done; // The network response callback public IncrementAndAddClosure(CounterServer counterServer, IncrementAndGetRequest request, ValueResponse response, Closure done) { super(); this.counterServer = counterServer; this.request = request; this.response = response; this.done = done; } @Override public void run(Status status) { // Return the response to the client if (this.done != null) { done.run(status); } } public IncrementAndGetRequest getRequest() { return this.request; } public void setRequest(IncrementAndGetRequest request) { this.request = request; } public ValueResponse getResponse() { return this.response; } }  Server CounterStateMachine First hold an initial value:\npublic class CounterStateMachine extends StateMachineAdapter { /** * counter value */ private AtomicLong value = new AtomicLong(0);  Implement the core onApply(iterator) method, and apply the user request to the state machine: …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/counter-example/","fuzzywordcount":1900,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"f9c54b9f7883ccb1d7c259b7101f4674","permalink":"/en/projects/sofa-jraft/counter-example/","publishdate":"0001-01-01T00:00:00Z","readingtime":9,"relpermalink":"/en/projects/sofa-jraft/counter-example/","summary":"This topic mainly describes a JRaft-based distributed counter.\nScenario Save a distributed counter in a raft group of multiple nodes (servers). The counter can increment and be called while remaining consistent among all nodes. The counter can normally provide two external services when a minority of nodes fail:\n incrmentAndGet(delta): increments the value of delta and returns the incremented value. get(): gets the latest value.  Remote procedure calls (RPCs) JRaft adopts the Bolt communication framework at the underlayer, and defines two requests:","tags":null,"title":"Use case of a counter","type":"projects","url":"/en/projects/sofa-jraft/counter-example/","wordcount":1859},{"author":null,"categories":null,"content":" SOFABoot provides a class isolation framework SOFAArk, giving Spring Boot a class isolation ability to resolve class or package conflicts in the development. For detailed information, please refer to:SOFAArk\nTo use this feature in SOFABoot projects, we need only two steps: configure the sofa-ark-maven-plugin plugins for packaging and add sofa-ark-springboot-starter dependencies of the class isolation framework.\nConfigure Maven packaging plugins The Maven plugins - sofa-ark-maven-plugin are available on the Central Repository. Through simple configurations, a SpringBoot project can be wrapped into an executable Ark package in the standard format. The coordinate of sofa-ark-maven-plugin is:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;  The configuration template is described as follows:\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--specify destination where executable-ark-jar will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;!-- all class exported by ark plugin would be resolved by ark biz in default, if configure denyImportClasses, then it would prefer to load them by ark biz itself --\u0026amp;gt; \u0026amp;lt;denyImportClasses\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.SampleClass1\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.SampleClass2\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;/denyImportClasses\u0026amp;gt; \u0026amp;lt;!-- Corresponding to denyImportClasses, denyImportPackages is package-level --\u0026amp;gt; \u0026amp;lt;denyImportPackages\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;org.springframework\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;/denyImportPackages\u0026amp;gt; \u0026amp;lt;!-- denyImportResources can prevent resource exported by ark plugin with accurate name to be resolved --\u0026amp;gt; \u0026amp;lt;denyImportResources\u0026amp;gt; \u0026amp;lt;resource\u0026amp;gt;META-INF/spring/test1.xml\u0026amp;lt;/resource\u0026amp;gt; \u0026amp;lt;resource\u0026amp;gt;META-INF/spring/test2.xml\u0026amp;lt;/resource\u0026amp;gt; \u0026amp;lt;/denyImportResources\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  Description of plugin configuration:\n outputDirectory: Execute mvn package and then specify a directory to store the Ark package. The default directory is ${project. Build. Directory}. arkClassifier: Execute mvn docleoy, and then specify the coordinates of Maven repositories to locate the Ark package by setting the classfaulter value (the default is empty). We recommend that you configure this to give a different name from the ordinary Fat jar; denyImportClasses: By default, the application will first load …","date":-62135596800,"description":"","dir":"projects/sofa-boot/classloader-isolation/","fuzzywordcount":1200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"e007416ab008c1dd4b886433dbf8af01","permalink":"/en/projects/sofa-boot/classloader-isolation/","publishdate":"0001-01-01T00:00:00Z","readingtime":6,"relpermalink":"/en/projects/sofa-boot/classloader-isolation/","summary":"SOFABoot provides a class isolation framework SOFAArk, giving Spring Boot a class isolation ability to resolve class or package conflicts in the development. For detailed information, please refer to:SOFAArk\nTo use this feature in SOFABoot projects, we need only two steps: configure the sofa-ark-maven-plugin plugins for packaging and add sofa-ark-springboot-starter dependencies of the class isolation framework.\nConfigure Maven packaging plugins The Maven plugins - sofa-ark-maven-plugin are available on the Central Repository.","tags":null,"title":"Use class isolation in SOFABoot","type":"projects","url":"/en/projects/sofa-boot/classloader-isolation/","wordcount":1129},{"author":null,"categories":null,"content":" Use API SOFABoot provides a set of programming APIs for RPC service publishing and reference. It is convenient to publish and reference RPC services directly in the code. Similar to Spring\u0026amp;rsquo;s ApplicationContextAware, in order to use the programming API, you first need to implement the ClientFactoryAware interface to get the programming component API:\npublic class ClientFactoryBean implements ClientFactoryAware { private ClientFactory clientFactory; @Override public void setClientFactory(ClientFactory clientFactory) { this.clientFactory = clientFactory; } }  With DirectService as an example, see how to use the clientFactory to publish an RPC service through the programming API:\nServiceClient serviceClient = clientFactory.getClient(ServiceClient.class); ServiceParam serviceParam = new ServiceParam(); serviceParam.setInterfaceType(DirectService.class); serviceParam.setInstance(new DirectServiceImpl()); List\u0026amp;lt;BindingParam\u0026amp;gt; params = new ArrayList\u0026amp;lt;BindingParam\u0026amp;gt;(); BindingParam serviceBindingParam = new BoltBindingParam(); params.add(serviceBindingParam); serviceParam.setBindingParams(params); serviceClient.service (serviceParam);  In the code above:\n First, get the ServiceClient object through the clientFactory. Then, construct the ServiceParam object, which contains the parameters required to publish the service, and use the setInstance method to set the object to be published as an RPC service, setInterfaceType to set the interface of the service. Finally, call the service method of ServiceClient to publish an RPC service.  The code that references the RPC service through the programming API is similar:\nReferenceClient referenceClient = clientFactory.getClient(ReferenceClient.class); ReferenceParam\u0026amp;lt;DirectService\u0026amp;gt; referenceParam = new ReferenceParam\u0026amp;lt;DirectService\u0026amp;gt;(); referenceParam.setInterfaceType(DirectService.class); BindingParam refBindingParam = new BoltBindingParam(); referenceParam.setBindingParam(refBindingParam); DirectService proxy = referenceClient.reference(referenceParam); proxy.sayDirect(\u0026amp;quot;hello\u0026amp;quot;);  Likewise, to reference an RPC service, the code simply needs to get a ReferenceClient from the ClientFactory and then construct a ReferenceParam similar to publishing a service, next set up the service interface, and finally call the ReferenceClient\u0026amp;rsquo;s reference method.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-sofa-boot-api/","fuzzywordcount":300,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2679388dc3459714f869d8f8a71739d7","permalink":"/en/projects/sofa-rpc/programing-sofa-boot-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/programing-sofa-boot-api/","summary":"Use API SOFABoot provides a set of programming APIs for RPC service publishing and reference. It is convenient to publish and reference RPC services directly in the code. Similar to Spring\u0026rsquo;s ApplicationContextAware, in order to use the programming API, you first need to implement the ClientFactoryAware interface to get the programming component API:\npublic class ClientFactoryBean implements ClientFactoryAware { private ClientFactory clientFactory; @Override public void setClientFactory(ClientFactory clientFactory) { this.clientFactory = clientFactory; } }  With DirectService as an example, see how to use the clientFactory to publish an RPC service through the programming API:","tags":null,"title":"Use dynamic API in SOFABoot","type":"projects","url":"/en/projects/sofa-rpc/programing-sofa-boot-api/","wordcount":255},{"author":null,"categories":null,"content":" User guide Maven coordinator \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;bolt\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Check release notes for the version information.\n 1. Basic functions 1.1. Implement user request processor (UserProcessor) We provide two types of user request processors: SyncUserProcessor and AsyncUserProcessor. The difference between them is that the former returns the processing result in the form of a return value in the current processor thread, while the latter has an AsyncContext stub and can call the sendResponsemethod in the current thread or an asynchronous thread to return the processing result. For examples, refer to the following two types:\n Synchronous request processor Asynchronous request processor  1.2 Implement connection event processor (ConnectionEventProcessor) We provide two connection event processors: ConnectionEventType.CONNECT and ConnectionEventType.CLOSE. You can create your own event processors and register them with the client or the server. The client side and server side can each monitor both of their connection and disconnection events.\n Process connection event Process disconnection event  1.3 Client side and server side initialization (RpcClient, RpcServer) We have provided an RpcClient and RpcServer. They can be used after going through a simple initialization of necessary functions, or after switching on the functions. The most simple example is as follows:\n Client side initialization example Server side initialization example  1.4 Basic communication model We have provided four types of communication models:\n1. Oneway calls\nThe current thread initiates a call that is not interested in the call result and is not subject to timeout control. As long as the request is sent out, the call is completed. Note: Oneway calls are not guaranteed to succeed, and the initiator of the call has no way of knowing its result. For that reason, these calls are usually used in scenarios that can be retried or that have fixed-time notifications. Network problems or machine malfunctions during the call process may result in failure. This kind of call should only be used in business scenarios that accept such exceptions. For more information, see Example.\n2. Sync calls\nThe current thread initiates a call that only completes if it receives a result within the set timeout time. If a result is not received within the timeout time, it will generate a timeout error. This is the most commonly used call type. Ensure that the timeout time is set reasonably in accordance with the opposing terminal\u0026amp;rsquo;s processing capacity. For more information, see Example.\n3. Future calls\nThe current thread initiates a call and can then move onto executing the next call after getting an RpcResponseFuture object. The get() method of the RpcResponseFuture object can be used at any time to get the result. If the response has already been returned, the result …","date":-62135596800,"description":"","dir":"projects/sofa-bolt/sofa-bolt-handbook/","fuzzywordcount":2000,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"2a0a2e3c7749dbcdceea064f6f850e33","permalink":"/en/projects/sofa-bolt/sofa-bolt-handbook/","publishdate":"0001-01-01T00:00:00Z","readingtime":10,"relpermalink":"/en/projects/sofa-bolt/sofa-bolt-handbook/","summary":"User guide Maven coordinator \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bolt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Check release notes for the version information.\n 1. Basic functions 1.1. Implement user request processor (UserProcessor) We provide two types of user request processors: SyncUserProcessor and AsyncUserProcessor. The difference between them is that the former returns the processing result in the form of a return value in the current processor thread, while the latter has an AsyncContext stub and can call the sendResponsemethod in the current thread or an asynchronous thread to return the processing result.","tags":null,"title":"User guide","type":"projects","url":"/en/projects/sofa-bolt/sofa-bolt-handbook/","wordcount":1988},{"author":null,"categories":null,"content":" ﻿## Version release\nVersion No. Major, minor, and revision version numbers are used. For example 2.0.0.\nRefer to: http://semver.org/lang/zh-CN/.\n Major version number: All versions within a major version number must be compatible with each other. They are not necessarily compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, more features it has. Revision version number: represents the BugFix version. Such versions are only used for bug fixing. The larger the version number, the more stable the application.  Version maintenance At most two versions can be maintained simultaneously.\nFor example, if the current major version is 2.2.0, the BugFix version 2.1.x will be maintained and bugs in version 2.0.x will no longer be fixed and a version upgrade is recommended.\nRelease process  Daily development uses the SNAPSHOT version, such as 2.0.0-SNAPSHOT. When the modified version is officially released, the version number is revised to a formal version, such as 2.0.0. After release, the next version is pulled up, for example, 2.1.0-SNAPSHOT.  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-jarslink-version/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b82f2d74eff3937e10f15b13cb503751","permalink":"/en/projects/sofa-boot/sofa-jarslink-version/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/sofa-jarslink-version/","summary":"﻿## Version release\nVersion No. Major, minor, and revision version numbers are used. For example 2.0.0.\nRefer to: http://semver.org/lang/zh-CN/.\n Major version number: All versions within a major version number must be compatible with each other. They are not necessarily compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, more features it has. Revision version number: represents the BugFix version.","tags":null,"title":"Version release","type":"projects","url":"/en/projects/sofa-boot/sofa-jarslink-version/","wordcount":175},{"author":null,"categories":null,"content":" Version number The system adopts a three-digit versioning scheme. The three digits respectively are major version number, minor version number, and revision number, for example: 5.1.2.\nFor more information, see the http://semver.org/lang/zh-CN/.\n Major version number: All versions in the major version number must be compatible with each other. It is not necessary to be fully compatible with other major version numbers, but it is best to have backward compatibility. Minor version number: Represents new feature enhancements. The larger the version number, the richer the feature. Revision number: Represents the BugFix version. The revision number is only for bug fixes. The larger the version number, the more stable it is.  Version maintenance You can maintain up to two versions at the same time.\nFor example, the current trunk is 5.3.0, then the bugfix branch of 5.2.x will be maintained. When any bugs arise in 5.1.x, users are prompted to upgrade the system.\nRelease process  The daily development branch uses the SNAPSHOT version, for example: 5.3.0-SNAPSHOT. When it comes to official release, you can modify the version to official version, for example: 5.3.0. Pull up the next version after release, for example: 5.3.1-SNAPSHOT.  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/version-release/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"604f113607e6815757f4d1907190c13c","permalink":"/en/projects/sofa-rpc/version-release/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/version-release/","summary":"Version number The system adopts a three-digit versioning scheme. The three digits respectively are major version number, minor version number, and revision number, for example: 5.1.2.\nFor more information, see the http://semver.org/lang/zh-CN/.\n Major version number: All versions in the major version number must be compatible with each other. It is not necessary to be fully compatible with other major version numbers, but it is best to have backward compatibility.","tags":null,"title":"Version release","type":"projects","url":"/en/projects/sofa-rpc/version-release/","wordcount":191},{"author":null,"categories":null,"content":" Version number Major, minor, and revision version numbers are used. For example, 1.0.0.\nFor more information, see https://semver.org/\n Major version number: All versions with the same major version number must be compatible with each other. They are not necessarily fully compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, the more features it has. Revision version number: represents the BugFix version. Such versions are only used for bug fixing. The larger the version number, the more stable the application.  Version maintenance Up to two versions can be maintained simultaneously.\nFor example, if the current version of the master branch code is 1.2.0, the BugFix branch 1.1.x will be maintained, but bugs in branch 1.0.x will no longer be fixed. In this case, a version upgrade is recommended.\nRelease process  The develop branches use SNAPSHOT versions, for example, 1.0.0-SNAPSHOT. Upon formal release, the snapshot version is modified to the formal version, for example 1.0.0. After the formal release, the next version is pulled, for example, 1.0.1-SNAPSHOT.  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/version-rule/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a2093bdf478bdff0e15a2de70e522d03","permalink":"/en/projects/sofa-dashboard/version-rule/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-dashboard/version-rule/","summary":"Version number Major, minor, and revision version numbers are used. For example, 1.0.0.\nFor more information, see https://semver.org/\n Major version number: All versions with the same major version number must be compatible with each other. They are not necessarily fully compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, the more features it has.","tags":null,"title":"Version rules","type":"projects","url":"/en/projects/sofa-dashboard/version-rule/","wordcount":180},{"author":null,"categories":null,"content":" Version number SOFARegistry uses a three-digit version number in the form of major, minor, and patch. For example, 5.2.0.\nFor more information, see https://semver.org/.\n Major version number: All versions with the same major version number must be compatible with each other. They are not necessarily fully compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, the more features it has. Patch number: represents the BugFix version. Such versions are only used for bug fixing. The larger the version number, the more stable the application.  Version maintenance Up to two versions can be maintained simultaneously.\nFor example, if the current version of the master branch code is 5.4.0, the BugFix branch of version 5.3.x will be maintained, but bugs in branch 5.2.x will no longer be fixed. Therefore, a version upgrade for 5.2.x is recommended.\nRelease process  The develop branches use SNAPSHOT versions, for example, 5.3.0-SNAPSHOT. Upon formal release, SNAPSHOT is replaced with a formal version number, for example 5.3.0. After the formal release, the next version is pulled, for example, 5.3.1-SNAPSHOT.  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/release-standard/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"71aad9cbc42aba3d9f875ae9169cf005","permalink":"/en/projects/sofa-registry/release-standard/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-registry/release-standard/","summary":"Version number SOFARegistry uses a three-digit version number in the form of major, minor, and patch. For example, 5.2.0.\nFor more information, see https://semver.org/.\n Major version number: All versions with the same major version number must be compatible with each other. They are not necessarily fully compatible with other major versions. However, it is best to be downward compatible. Minor version number: represents feature enhancement. The larger the version number, the more features it has.","tags":null,"title":"Version rules","type":"projects","url":"/en/projects/sofa-registry/release-standard/","wordcount":186},{"author":null,"categories":null,"content":" ﻿With SOFABoot, we can directly view the version of SOFA middleware and other detailed information in the browser.\nIntroducing SOFABoot Infra Dependency To view the version information of the SOFA middleware directly in the browser in SOFABoot, all you need to do is add the following to the Maven dependency:\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;infra-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Version Information Viewing After an application started successfully, you can visit http://localhost:8080/sofaboot/versions in the browser to view the version information of the SOFA middleware, the response such as:\n[ { GroupId: \u0026amp;quot;com.alipay.sofa\u0026amp;quot;, Doc-Url: \u0026amp;quot;https://github.com/sofastack/sofa-boot\u0026amp;quot;, ArtifactId: \u0026amp;quot;infra-sofa-boot-starter\u0026amp;quot;, Build-Time: \u0026amp;quot;2018-04-05T20:55:22+0800\u0026amp;quot;, Commit-Time: \u0026amp;quot;2018-04-05T20:54:26+0800\u0026amp;quot;, Commit-Id: \u0026amp;quot;049bf890bb468aafe6a3e07b77df45c831076996\u0026amp;quot;, Version: \u0026amp;quot;2.4.0\u0026amp;quot; } ]  ** Note: In SOFABoot 3.x, the endpoint path has been changed from sofaboot/versions to actuator/versions**.\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/view-versions/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"c6b6d22e9038aa1f5e4ce74449ba1cda","permalink":"/en/projects/sofa-boot/view-versions/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-boot/view-versions/","summary":"﻿With SOFABoot, we can directly view the version of SOFA middleware and other detailed information in the browser.\nIntroducing SOFABoot Infra Dependency To view the version information of the SOFA middleware directly in the browser in SOFABoot, all you need to do is add the following to the Maven dependency:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;infra-sofa-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  Version Information Viewing After an application started successfully, you can visit http://localhost:8080/sofaboot/versions in the browser to view the version information of the SOFA middleware, the response such as:","tags":null,"title":"View version","type":"projects","url":"/en/projects/sofa-boot/view-versions/","wordcount":115},{"author":null,"categories":null,"content":"The warm-up weight feature allows the client machine to distribute traffic based on the corresponding weight of the server. This feature is also often used in the scenario where a few machines within a cluster are being started. The server machines can be warmed up in a short time with the traffic weight function, and then continue to receive the normal traffic.\nThe operating mechanism is as follows:  When the server service starts, it pushes its own warm-up duration, weight during warm-up, and normal weight after warm-up to the Service Registry. As shown above, Service B points to Service Registry.\n When referencing service, the client obtains the warm-up weight information of each service instance. As shown above, Service Registry points to client.\n When calling service, the client distributes the traffic according to the warm-up weight of the address where the service is located. As shown above, the client points to Service A and Service B. Service A has completed warm-up, and its weight is 100 by default. Service B is in the warm-up period, and its weight is 10. Therefore, their traffic is 100%110 and 10%110 respectively.\n  This feature is used as follows:\nProviderConfig\u0026amp;lt;HelloWordService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloWordService\u0026amp;gt;() .setWeight(100) .setParameter(ProviderInfoAttrs.ATTR_WARMUP_WEIGHT,\u0026amp;quot;10\u0026amp;quot;) .setParameter(ProviderInfoAttrs.ATTR_WARM_UP_END_TIME, \u0026amp;quot;12000\u0026amp;quot;);  As above, the warm-up duration of the service is 12s, the weight is 10 during warm-up, and the normal weight after warm-up is 100. If the service is published on two machines, such as machine A and B, and the machine A is in the warm-up period with the above configuration, while B has already completed warm-up, and the normal weight is 200, then when the client calls the service, the proportion of traffic distribution is 10:200. After the machine A is warmed up, the traffic distribution ratio is 100:200.\nIn SOFABoot, the warm-up duration and the weight during and after warm-up can be configured as follows:\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;sampleRestFacadeReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.endpoint.facade.SampleFacade\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs weight=\u0026amp;quot;100\u0026amp;quot; warm-up-time=\u0026amp;quot;10000\u0026amp;quot; warm-up-weight=\u0026amp;quot;1000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/provider-warmup-weight/","fuzzywordcount":400,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"b9e320dfaa4f9700ecdca67d76e07d54","permalink":"/en/projects/sofa-rpc/provider-warmup-weight/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/en/projects/sofa-rpc/provider-warmup-weight/","summary":"The warm-up weight feature allows the client machine to distribute traffic based on the corresponding weight of the server. This feature is also often used in the scenario where a few machines within a cluster are being started. The server machines can be warmed up in a short time with the traffic weight function, and then continue to receive the normal traffic.\nThe operating mechanism is as follows:  When the server service starts, it pushes its own warm-up duration, weight during warm-up, and normal weight after warm-up to the Service Registry.","tags":null,"title":"Warm-up weight","type":"projects","url":"/en/projects/sofa-rpc/provider-warmup-weight/","wordcount":319},{"author":null,"categories":null,"content":" X-Protocol X-Protocol is a special common protocol supported by SOFAMesh. It can access different RPC protocols in a unified manner. Because it doesn\u0026amp;rsquo;t require to parse protocols, it can not only provide higher performance, but also reduce the development cost of accessing new protocols.\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-x-protocol/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"220f4a76b277463bb1f7201519950450","permalink":"/en/projects/sofa-mesh/pilot-x-protocol/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-mesh/pilot-x-protocol/","summary":"X-Protocol X-Protocol is a special common protocol supported by SOFAMesh. It can access different RPC protocols in a unified manner. Because it doesn\u0026rsquo;t require to parse protocols, it can not only provide higher performance, but also reduce the development cost of accessing new protocols.","tags":null,"title":"X-Protocol","type":"projects","url":"/en/projects/sofa-mesh/pilot-x-protocol/","wordcount":44},{"author":null,"categories":null,"content":"X-Protocol 协议是 SOFAMesh 支持的特殊通用协议，能够以统一的方式接入不同的 RPC 协议，因为无需进行协议解析，不仅能够提供更高的性能, 更能降低接入新协议的开发成本。\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-x-protocol/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"220f4a76b277463bb1f7201519950450","permalink":"/projects/sofa-mesh/pilot-x-protocol/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-x-protocol/","summary":"X-Protocol 协议是 SOFAMesh 支持的特殊通用协议，能够以统一的方式接入不同的 RPC 协议，因为无需进行协议解析，不仅能够提供更高的性能, 更能降低接入新协议的开发成本。","tags":null,"title":"X-Protocol","type":"projects","url":"/projects/sofa-mesh/pilot-x-protocol/","wordcount":70},{"author":null,"categories":null,"content":" ZooKeeper Adapter ZooKeeper Adapter is an Adapter plug-in developed in accordance with the Istio registry center extension mechanism. It is used for docking all microservices frameworks that use ZooKeeper as a registry center. Currently, ZooKeeper Adapter supports SOFARPC and will be available for Dubbo soon.\nZooKeeper Adapter uses ZooKeeper\u0026amp;rsquo;s watch mechanism to listen to the change events of service registration information, providing better real-time performance than polling.\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-zookeeper-adapter/","fuzzywordcount":100,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"a174a0de8dd47df7c3043f6d49fa1b07","permalink":"/en/projects/sofa-mesh/pilot-zookeeper-adapter/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-mesh/pilot-zookeeper-adapter/","summary":"ZooKeeper Adapter ZooKeeper Adapter is an Adapter plug-in developed in accordance with the Istio registry center extension mechanism. It is used for docking all microservices frameworks that use ZooKeeper as a registry center. Currently, ZooKeeper Adapter supports SOFARPC and will be available for Dubbo soon.\nZooKeeper Adapter uses ZooKeeper\u0026rsquo;s watch mechanism to listen to the change events of service registration information, providing better real-time performance than polling.","tags":null,"title":"ZooKeeper Adapter","type":"projects","url":"/en/projects/sofa-mesh/pilot-zookeeper-adapter/","wordcount":67},{"author":null,"categories":null,"content":" To use Zookeeper as service registry center, you only need to configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=zookeeper://127.0.0.1:2181  Note: Considering the real-time nature of the service, the following features are not supported currently.\nSOFABoot RPC also provides a cache file (not supported currently), which is used for service discovery when ZooKeeper is not available. The way to configure this cache file is as follows:\ncom.alipay.sofa.rpc.registry.address=zookeeper://xxx:2181?file=/home/admin/registry  Zookeeper Auth When users need to auth the providers and consumers, they can use a auth key to write or read the dictionary normally, only when they use the same key, zookeeper server will process these requests.\nSOFARPC API Usage If you use SOFARPC API directly, you can add two parameters to registry config.\nparameters.put(\u0026amp;quot;scheme\u0026amp;quot;, \u0026amp;quot;digest\u0026amp;quot;); //if there was multi auth infos, you need to set the value as user1:passwd1,user2:passwd2 parameters.put(\u0026amp;quot;addAuth\u0026amp;quot;, \u0026amp;quot;sofazk:rpc1\u0026amp;quot;); registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181/authtest\u0026amp;quot;) .setParameters(parameters);  then if another provider or consumer use a different auth info, they will not access these providers or consumers.\nXML Usage You only need to set it in application.properties\ncom.alipay.sofa.rpc.registry.address=zookeeper://xxx:2181?file=/home/admin/registry\u0026amp;amp;scheme=digest\u0026amp;amp;addAuth=sofazk:rpc1  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-zookeeper/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"71d6486c5577cc85d84c56688cdf2af1","permalink":"/en/projects/sofa-rpc/registry-zookeeper/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/sofa-rpc/registry-zookeeper/","summary":"To use Zookeeper as service registry center, you only need to configure it in application.properties as follows:\ncom.alipay.sofa.rpc.registry.address=zookeeper://127.0.0.1:2181  Note: Considering the real-time nature of the service, the following features are not supported currently.\nSOFABoot RPC also provides a cache file (not supported currently), which is used for service discovery when ZooKeeper is not available. The way to configure this cache file is as follows:\ncom.alipay.sofa.rpc.registry.address=zookeeper://xxx:2181?file=/home/admin/registry  Zookeeper Auth When users need to auth the providers and consumers, they can use a auth key to write or read the dictionary normally, only when they use the same key, zookeeper server will process these requests.","tags":null,"title":"Zookeeper","type":"projects","url":"/en/projects/sofa-rpc/registry-zookeeper/","wordcount":174},{"author":null,"categories":null,"content":"Zookeeper Adapter 是按照 Istio 注册中心扩展机制开发的一个 Adapter 插件，用于对接所有使用 Zookeeper 作为注册中心的微服务框架。目前已经支持了 SOFARPC，很快将提供对于 Dubbo 的支持。\nZookeeper Adapter 使用 zk 的 watch 机制监听服务注册信息的变化事件，提供了比轮询机制更好的实时性。\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-zookeeper-adapter/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a174a0de8dd47df7c3043f6d49fa1b07","permalink":"/projects/sofa-mesh/pilot-zookeeper-adapter/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-zookeeper-adapter/","summary":"Zookeeper Adapter 是按照 Istio 注册中心扩展机制开发的一个 Adapter 插件，用于对接所有使用 Zookeeper 作为注册中心的微服务框架。目前已经支持了 SOFARPC，很快将提供对于 Dubbo 的支","tags":null,"title":"Zookeeper Adpater","type":"projects","url":"/projects/sofa-mesh/pilot-zookeeper-adapter/","wordcount":110},{"author":null,"categories":null,"content":" 在介绍 Biz 生命周期 时，我们提到了有三种方式控制 Biz 的生命周期，并且介绍了使用客户端 API 实现 Biz 的安装、卸载、激活。在这一章节我们介绍如何使用 SOFAArk 提供的动态配置插件，通过 Zookeeper 下发指令，控制 Biz 的生命周期。\n引入依赖 SOFAArk 提供了 config-ark-plugin 对接 Zookeeper 配置中心，用于运行时接受配置，达到控制 Biz 生命周期，引入如下依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;config-ark-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置 ZK 地址 参考 SOFAArk 配置，在 SOFAArk 配置文件 conf/ark/bootstrap.properties 增加如下配置：\ncom.alipay.sofa.ark.config.address=zookeeper://ip:port  配置维度 SOFAArk 启动后，会在 ZK 注册两个节点配置，分别是宿主应用维度和 IP 维度：\n sofa-ark/${com.alipay.sofa.ark.master.biz}/ \u0026amp;gt; 宿主应用维度配置，应用启动时，会拉取该维度配置，控制相关 Biz 的部署；应用重启后，配置不会丢失\n sofa-ark/${com.alipay.sofa.ark.master.biz}/ip/ \u0026amp;gt; IP 维度配置，应用重启后丢失，通常用于运行时控制单台机器的 Biz 行为\n  通过写这两个节点的配置，可以控制相关机器和应用的 Biz 运行时状态。\n配置形式 下面介绍配置的形式，动态配置采用状态声明指令，SOFAArk 收到配置后，会根据状态描述解析出具体的指令（包括 install，uninstall, switch），指令格式如下：\nbizName:bizVersion:bizState?k1=v1\u0026amp;amp;k2=v2\n多条指令使用 ; 隔开，单条指令主要由 biz 名称，biz 版本，biz 预期状态及参数组成。简单记住一点，状态配置是描述指令推送之后，所有非宿主 Biz 的状态；\n例如当前 SOFAArk 容器部署了两个应用 A，B，版本均为 1.0，其中 A 应用为宿主应用，因为宿主应用不可卸载，因此不需要考虑宿主应用，可以简单认为当前容器的 Biz 状态声明为：\n B:1.0:Activated\n 如果此时你希望安装 C 应用，版本为 1.0，文件流地址为 urlC，那么推送指令应为：\n B:1.0:Activated;C:1.0:Activated?bizUrl=urlC\n 操作继续，如果你又希望安装安装 B 应用，版本为 2.0，文件流地址为 urlB，且希望 2.0 版本处于激活状态，那么你推送的指令应为：\n B:1.0:Deactivated;B:2.0:Actaivated?bizUrl=urlB;C:1.0:Activated\n解释下为什么是这样配置指令，因为 SOFAArk 只允许应用一个版本处于激活状态，如果存在其他版本，则应处于非激活状态；所以当希望激活 B 应用 2.0 版本时，B 应用 1.0 版本应该声明为非激活状态。另外你可能注意到了 C 应用参数 urlC 不用声明了，原因是目前只有当安装新 Biz 时，才有可能需要配置参数 bizUrl，用于指定 biz 文件流地址，其他场景下，参数的解析没有意义。\n 操作继续，如果你希望卸载 B 应用 2.0 版本，激活 B 应用 1.0 版本，卸载 C 应用，那么推送的指令声明为：\n B:1.0:Activated\n 从上面的操作描述看，在推送动态配置时，只需要声明期望的 Biz 状态即可，SOFAArk 会根据状态声明推断具体的执行指令，并尽可能保持服务的连续性，以上面最后一步操作为例，SOFAArk 推断的执行指令顺序如下： + 执行 switch 指令，激活 B 应用 1.0 版本，钝化 B 应用 2.0 版本，保证服务连续性 + 执行 uninstall 指令，卸载 B 应用 2.0 版本 + 执行 uninstall 指令，卸载 C 应用 1.0 版本\n注意事项 目前只有在安装新 Biz 时才可能使用指令参数 bizUrl，用于指定 Biz 文件流地址。文件流地址字符串是能够直接构建 URL 对象，例如 file://xxx 或者 http://xxx. 安装新 Biz 时，参数 bizUrl 不是必须的，SOFAArk 提供了扩展点：\n@Extensible public interface BizFileGenerator { File createBizFile(String bizName, String bizVersion); }  用于扩展实现，根据 biz 名称和 biz 版本返回 biz 文件。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-zk-config/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c675734b1cb5fa546f96a31d8b9e3533","permalink":"/projects/sofa-boot/sofa-ark-zk-config/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-zk-config/","summary":"在介绍 Biz 生命周期 时，我们提到了有三种方式控制 Biz 的生命周期，并且介绍了使用客户端 API 实现 Biz 的安装、卸载、激活。在这一章节我们介绍如何使用 SOFAArk 提供的","tags":null,"title":"Zookeeper 配置","type":"projects","url":"/projects/sofa-boot/sofa-ark-zk-config/","wordcount":1160},{"author":null,"categories":null,"content":" connection_manager 用于描述 MOSN 的路由配置，通常与 proxy 配合使用。\n{ \u0026amp;quot;router_config_name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;virtual_hosts\u0026amp;quot;: [ ] }   router_config_name，唯一的路由配置标识，与 proxy 中配置的字段对应。 virtual_hosts，描述具体的路由规则细节。  VirtualHost { \u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;domains\u0026amp;quot;:[], \u0026amp;quot;routers\u0026amp;quot;:[] }   name，字符串。用作 virtual host 的唯一标识。 domains，字符串数组。表示一组可以匹配到该 virtual host 的 domain，支持配置通配符。domain 的匹配优先级如下：  首先匹配精确的，如 www.foo.com。 其次匹配最长后缀的通配符，如 *.foo.com、*-bar.foo.com，其中如果一个 domain 是 foo-bar.foo.com，那么会优先匹配 *-bar.foo.com。 最后匹配任意domain的通配符 * 。  routers，一组具体的路由匹配规则。  Router { \u0026amp;quot;match\u0026amp;quot;:{}, \u0026amp;quot;route\u0026amp;quot;:{}, \u0026amp;quot;per_filter_config\u0026amp;quot;:{} }   match，路由的匹配参数。 route，路由行为，描述请求将被路由的 upstream 信息。 per_filter_config，是一个 key: json 格式的 json。 其中 key 需要匹配一个 stream filter 的 type，key 对应的 json 是该 stream filter 的 config。  当配置了该字段时，对于某些 stream filter（依赖具体 filter 的实现），可以使用该字段表示的配置覆盖原有 stream filter 的配置，以此做到路由匹配级别的 stream filter 配置。   match { \u0026amp;quot;prefix\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;path\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;regex\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;headers\u0026amp;quot;: [] }   路径（path）匹配  prefix，表示路由会匹配 path 的前缀，该配置的优先级高于 path 和 regex。 如果 prefix 被配置，那么请求首先要满足 path 的前缀与 prefix 配置相符合。 path，表示路由会匹配精确的 path，该配置的优先级高于 regex。如果 path被配置，那么请求首先要满足 path 与 path 配置相符合。 regex，表示路由会按照正则匹配的方式匹配 path。如果 regex 被配置，那么请求首先要满足 path 与 regex 配置相符合。 路径匹配配置同时存在时，只有高优先级的配置会生效。  Heaer 匹配  headers，表示一组请求需要匹配的 header。请求需要满足配置中所有的 Header 配置条件才算匹配成功。   header { \u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;value\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;regex\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot; }   name，表示 header 的 key。 value，表示 header 对应 key 的 value。 regex，bool 类型，如果为 true，表示 value 支持按照正则表达式的方式进行匹配。  route { \u0026amp;quot;cluster_name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;metadata_match\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;timeout\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;retry_policy\u0026amp;quot;:{} }   cluster_name，表示请求将路由到的 upstream cluster。 metadata_match，metadata，如果配置了该字段，表示该路由会基于该 metadata 去匹配 upstream cluster 的 subset 。 timeout，Duration String，表示默认情况下请求转发的超时时间。如果请求中明确指定了超时时间，那么这个配置会被忽略。 retry_policy，重试配置，表示如果请求在遇到了特定的错误时采取的重试策略，默认没有配置的情况下，表示没有重试。  retry_policy { \u0026amp;quot;retry_on\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;retry_timeout\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;num_retries\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot; }   retry_on，bool 类型，表示是否开启重试。 retry_timeout，Duration String，表示每次重试的超时时间。当 retry_timeout 大于 route 配置的 timeout 或者请求明确指定的 timeout 时，属于无效配置。 num_retries，表示最大的重试次数。  ","date":-62135596800,"description":"","dir":"projects/mosn/configuration/listener/network-filter/connection-manager/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"88e2d06bd6137225eeebf9015b2192a2","permalink":"/projects/mosn/configuration/listener/network-filter/connection-manager/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/mosn/configuration/listener/network-filter/connection-manager/","summary":"connection_manager 用于描述 MOSN 的路由配置，通常与 proxy 配合使用。 { \u0026quot;router_config_name\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;virtual_hosts\u0026quot;: [ ] } router_config_name，唯一的路由配置标识，与 proxy 中配置的字段对应。 vir","tags":null,"title":"connection_manager","type":"projects","url":"/projects/mosn/configuration/listener/network-filter/connection-manager/","wordcount":1188},{"author":null,"categories":null,"content":" Dubbo接口部分  在你的接口项目中引入jar包。  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。\npublic interface HelloService { @Hmily void say(String hello); }  Dubbo实现项目  步骤一 ： 引入依赖hmily的jar包\n 步骤二 ： 新增Hmily配置\n 步骤三 ： 在实现方法上添加注解。TCC模式，则需要完成 confirm，cancel方法的开发\n  引入依赖 Spring-Namespace  Alibaba-Dubbo 用户引入  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Aapche-Dubbo 用户引入\n  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-apache-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在xml中进行如下配置  \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot  Alibaba-Dubbo 用户引入  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Aapche-Dubbo 用户引入  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-apache-dubbo\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入 hmily配置  在项目的 resource 新建文件名为:hmily.yml配置文件\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  实现接口上添加注解 上述我们已经完成了集成，下面将讲述具体的实现。\nTCC模式  对@Hmily 标识的接口方法的具体实现上，加上@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  dubbo注解用户 对于使用 @Reference 注解来注入dubbo服务的用户，请注意：你可以需要做如下配置:\nspring-namespace 用户 在你的xml配置中，需要将 org.dromara.hmily.spring.annotation.RefererAnnotationBeanPostProcessor 注入成spring的bean\n\u0026amp;lt;bean id = \u0026amp;quot;refererAnnotationBeanPostProcessor\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.annotation.RefererAnnotationBeanPostProcessor\u0026amp;quot;/\u0026amp;gt;  spring-boot用户 需要在yml文件里面开启注解支持：\nhmily.support.rpc.annotation = true  或者在项目中显示注入：\n@Bean public BeanPostProcessor …","date":-62135596800,"description":"dubbo用户指南","dir":"projects/hmily/user-dubbo/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c28d182a0aec22568b1dbf4e64014041","permalink":"/projects/hmily/user-dubbo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/hmily/user-dubbo/","summary":"Dubbo接口部分 在你的接口项目中引入jar包。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。 public interface HelloService { @Hmily void say(String hello); } Dubbo","tags":null,"title":"dubbo用户指南","type":"projects","url":"/projects/hmily/user-dubbo/","wordcount":1053},{"author":null,"categories":null,"content":"proxy 是 MOSN 最常用的 network filter，其配置格式如下。\n{ \u0026amp;quot;downstream_protocol\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;upstream_protocol\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;router_config_name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;extend_config\u0026amp;quot;:{} }   downstream_protocol 描述 proxy 期望收到的请求协议，在连接收到数据时，会使用此协议去解析数据包并完成转发，如果收到的数据包协议和配置不符，MOSN 会将连接断开。 upstream_protocol 描述 proxy 将以何种协议转发数据，通常情况下应该和downstream_protocol 保持一致，只有特殊的场景会进行对应协议的转换。 router_config_name 描述 proxy 的路由配置的索引，通常情况下，这个配置会和同 listener 下的 connection_manager 中配置的 router_config_name 保持一致。 extend_config 扩展配置，目前仅在 MOSN 的 XProtocol 协议中使用。  ","date":-62135596800,"description":"","dir":"projects/mosn/configuration/listener/network-filter/proxy/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b04f78179a47a64d7e209b6660bfa80f","permalink":"/projects/mosn/configuration/listener/network-filter/proxy/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/configuration/listener/network-filter/proxy/","summary":"proxy 是 MOSN 最常用的 network filter，其配置格式如下。 { \u0026quot;downstream_protocol\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;upstream_protocol\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;router_config_name\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;extend_config\u0026quot;:{} } downstream_protocol 描述 proxy 期望收到的请求协议，在连接收到数据时，会使用此协议去解析数据包并完成转发，","tags":null,"title":"proxy","type":"projects","url":"/projects/mosn/configuration/listener/network-filter/proxy/","wordcount":221},{"author":null,"categories":null,"content":" Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-sofa Module and Run Build with Maven Configuring（hmily-demo-sofa-account module for instance）  Configure with your business database in application.yml  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   Configure with sofa-rpc registration address(es) in application.yml (can run with local zookeeper instance(s))  com: alipay: sofa: rpc: registry-address: zookeeper://127.0.0.1:2181 bolt-port: 8888   Modify hmily.yml, with mysql persistence backend  repository: database: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026amp;lt;db_host_ip\u0026amp;gt;:\u0026amp;lt;db_host_port\u0026amp;gt;/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 # replace with your db_host_ip and db_host_port username: root # replace with your db username password: your_password # replace with your db user password   run SofaHmilyAccountApplication.java  Run hmily-demo-sofa-inventory(refer to simillar instructions above). Run hmily-demo-sofa-order(refer to simillar instructions above). Access on http://127.0.0.1:8089/swagger-ui.html for more. ","date":-62135596800,"description":"sofa-rpc Quick Start","dir":"projects/hmily/quick-start-rpc/","fuzzywordcount":200,"kind":"page","lang":"en","lastmod":1611451625,"objectID":"3079898dede3a4ceaaf96a0dc0d20328","permalink":"/en/projects/hmily/quick-start-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/en/projects/hmily/quick-start-rpc/","summary":"Prerequisites  JDK 1.8+ Maven 3.2.x Git Zookeeper  Cloning the GitHub Repository and Quick Installation  \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U  Executing SQL(s) in Demo Module sql\nOpen with Your Favourite Editor (IDEA), Locate on hmily-demo-sofa Module and Run Build with Maven Configuring（hmily-demo-sofa-account module for instance）  Configure with your business database in application.yml  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://\u0026lt;db_host_ip\u0026gt;:\u0026lt;db_host_port\u0026gt;/hmily_account?","tags":null,"title":"sofa-rpc Quick Start","type":"projects","url":"/en/projects/hmily/quick-start-rpc/","wordcount":165},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git Zookeeper  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n使用你的工具 idea 打开项目，找到hmily-demo-sofa项目，进行maven构建。 修改项目配置（hmily-demo-sofa-account为列子）  application.yml 下修改业务数据库  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: #改成你的用户名 password: #改成你的密码   application.yml 下修改sofa-rpc的注册中心地址(可以在自己电脑本地启动一个zookeeper服务)  com: alipay: sofa: rpc: registry-address: zookeeper://127.0.0.1:2181 bolt-port: 8888   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   run SofaHmilyAccountApplication.java  启动hmily-demo-sofa-inventory 参考上述。 启动hmily-demo-sofa-order 参考上述。 访问：http://127.0.0.1:8089/swagger-ui.html。 ","date":-62135596800,"description":"sofa-rpc快速体验","dir":"projects/hmily/quick-start-rpc/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3079898dede3a4ceaaf96a0dc0d20328","permalink":"/projects/hmily/quick-start-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/quick-start-rpc/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git Zookeeper 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 使用你的工具 idea 打开项目，找到hmily-dem","tags":null,"title":"sofa-rpc快速体验","type":"projects","url":"/projects/hmily/quick-start-rpc/","wordcount":508},{"author":null,"categories":null,"content":" sofa-rpc接口项目  在你的接口项目中引入jar包。  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-annotation\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。\npublic interface HelloService { @Hmily void say(String hello); }  sofa-rpc实现项目  步骤一 ： 引入依赖hmily的jar包\n 步骤二 ： 新增Hmily配置\n 步骤三 ： 在实现方法上添加注解。TCC模式，则需要完成 confirm，cancel方法的开发\n  引入依赖 Spring-Namespace  引入依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-sofa-rpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  在xml中进行如下配置\n  \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyTransactionAspect\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.aop.SpringHmilyTransactionAspect\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id = \u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt;  Spring-Boot  引入依赖  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-sofa-rpc\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入 hmily配置  在项目的 resource 新建文件名为:hmily.yml配置文件\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  实现接口上添加注解 上述我们已经完成了集成，下面将讲述具体的实现。\nTCC模式  对@Hmily 标识的接口方法的具体实现上，加上@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  TAC模式(在开发，未发布)  对@Hmily 标识的接口方法的具体实现上加上@HmilyTAC   重要注意事项 在调用任何RPC调用之前，当你需要聚合rpc调用成为一次分布式事务的时候，需要在聚合RPC调用的方法上，先行添加 @HmilyTCC 或者 @HmilyTAC 注解,表示开启全局事务。\n负载均衡 \u0026amp;amp;\u0026amp;amp; 设置永不重试  如果服务部署了几个节点， 负载均衡算法最好使用 hmily, 这样 try, confirm, cancel 调用会落在同一个节点 充分利用了缓存，提搞了效率。\n 支持一下几种 hmilyConsistentHash, hmilyRandom, hmilyLocalPref, hmilyRoundRobin, hmilyWeightRoundRobin, hmilyWeightConsistentHash 几种方式均是继承sofa-rpc原生的\n  \u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;accountService\u0026amp;quot; interface=\u0026amp;quot;org.dromara.hmily.demo.common.account.api.AccountService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs retries=\u0026amp;quot;0\u0026amp;quot; timeout=\u0026amp;quot;5000\u0026amp;quot; loadBalancer =\u0026amp;quot;hmilyRandom\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  异常  try, confirm, cancel 方法的所有异常不要自行catch 任何异常都应该抛出给 Hmily框架处理。  ","date":-62135596800,"description":"sofa-rpc用户指南","dir":"projects/hmily/user-rpc/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b0291d256039472c980741163fd918a8","permalink":"/projects/hmily/user-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/user-rpc/","summary":"sofa-rpc接口项目 在你的接口项目中引入jar包。 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dromara\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hmily-annotation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;{last.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在需要进行Hmily分布式事务的接口方法上加上 @Hmily 标识。 public interface HelloService { @Hmily void say(String hello); } so","tags":null,"title":"sofa-rpc用户指南","type":"projects","url":"/projects/hmily/user-rpc/","wordcount":858},{"author":null,"categories":null,"content":" 环境准备  JDK 1.8+ Maven 3.2.x Git Zookeeper  代码拉取  \u0026amp;gt; git clone https://github.com/dromara/hmily.git \u0026amp;gt; cd hmily \u0026amp;gt; mvn -DskipTests clean install -U  执行demo 模块的sql语句。 sql语句\n建立tars节点 根据此文在当前tars平台建立\n- 应用名:TestInventory,服务名称:InventoryApp,Obj名:InventoryObj,端口29740的节点。\n- 应用名:HmilyAccount,服务名称:AccountApp,Obj名:AccountObj,端口10386的节点。\n在完成节点的建立后，分别到hmily-demo-tars-springboot-account和hmily-demo-tars-springboot-inventory目录下执行mvn clean package命令打包并按照此文在两个前面建立的节点上使用打包的成果物进行节点发布。\n使用你的工具 idea 打开项目，找到hmily-demo-tars项目。 修改项目配置（hmily-demo-tars-account为列子）  修改业务数据库(account项目为列子)  spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://改成你的ip+端口/hmily_account?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: #改成你的用户名 password: #改成你的密码   修改 hmily.yml,这里使用mysql来存储  repository: database: driverClassName: com.mysql.jdbc.Driver url : jdbc:mysql://改成你的ip+端口/hmily?useUnicode=true\u0026amp;amp;characterEncoding=utf8 username: root #改成你的用户名 password: #改成你的密码   将rescouces目录下的config.conf后缀文件里的192.168.41.102全局替换成tars平台ip,并在启动参数中添加-Dconfig=该文件的路径\n run TarsHmilyAccountApplication.java\n  启动hmily-demo-tars-springboot-inventory 参考上述。 启动hmily-demo-tars-springboot-order 参考上述。 访问：http://127.0.0.1:18087/swagger-ui.html。 ","date":-62135596800,"description":"tars快速体验","dir":"projects/hmily/quick-start-tars/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"76bbac5c6c6575e871bc59f8373e9890","permalink":"/projects/hmily/quick-start-tars/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/quick-start-tars/","summary":"环境准备 JDK 1.8+ Maven 3.2.x Git Zookeeper 代码拉取 \u0026gt; git clone https://github.com/dromara/hmily.git \u0026gt; cd hmily \u0026gt; mvn -DskipTests clean install -U 执行demo 模块的sql语句。 sql语句 建立tars节点 根据此文在当前tars平台建立","tags":null,"title":"tars快速体验","type":"projects","url":"/projects/hmily/quick-start-tars/","wordcount":848},{"author":null,"categories":null,"content":" Tars用户指南  引入jar包\n 引入hmily配置\n 在需要进行Hmily分布式事务的自动生成的Servant接口方法上加上 @Hmily 标识。\n 在具体的实现方法上（服务提供端），加上@HmilyTCC or HmilyTAC 注解\n  引入依赖 Spring-Namespace\n 引入依赖\n  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-tars\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   在xml中进行如下配置\n\u0026amp;lt;!--配置扫码hmily框架的包--\u0026amp;gt; \u0026amp;lt;context:component-scan base-package=\u0026amp;quot;org.dromara.hmily.*\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--设置开启aspectj-autoproxy--\u0026amp;gt; \u0026amp;lt;aop:aspectj-autoproxy expose-proxy=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!--配置Hmily启动的bean参数--\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyApplicationContextAware\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.spring.HmilyApplicationContextAware\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;hmilyCommunicatorBeanPostProcessor\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.tars.spring.TarsHmilyCommunicatorBeanPostProcessor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;tarsHmilyStartupBean\u0026amp;quot; class=\u0026amp;quot;org.dromara.hmily.tars.spring.TarsHmilyFilterStartupBean\u0026amp;quot;/\u0026amp;gt;  Spring-Boot\n 引入依赖\nxml \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.dromara\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;hmily-spring-boot-starter-tars\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;{last.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;    引入Hmily配置  在项目的 resource 新建文件名为: hmily.ym 配置文件。\n 具体的参数配置可以参考配置详解,本地配置模式, zookeeper配置模式, nacos配置模式,apollo配置模式\n  实现接口上添加注解 上述我们已经完成了集成，下面将讲述具体的实现。\nTCC模式  对@Hmily 标识的接口方法的具体实现上，加上@HmilyTCC(confirmMethod = \u0026amp;quot;confirm\u0026amp;quot;, cancelMethod = \u0026amp;quot;cancel\u0026amp;quot;)\n confirmMethod : 确认方法名称，该方法参数列表与返回类型应与标识方法一致。\n cancelMethod : 回滚方法名称，该方法参数列表与返回类型应与标识方法一致。\n TCC模式应该保证 confirm 和 cancel 方法的幂等性，用户需要自行去开发这个2个方法，所有的事务的确认与回滚，完全由用户决定。Hmily框架只是负责来进行调用\n  public class HelloServiceImpl implements HelloService { @HmilyTCC(confirmMethod = \u0026amp;quot;sayConfrim\u0026amp;quot;, cancelMethod = \u0026amp;quot;sayCancel\u0026amp;quot;) public void say(String hello) { System.out.println(\u0026amp;quot;hello world\u0026amp;quot;); } public void sayConfrim(String hello) { System.out.println(\u0026amp;quot; confirm hello world\u0026amp;quot;); } public void sayCancel(String hello) { System.out.println(\u0026amp;quot; cancel hello world\u0026amp;quot;); } }  重要注意事项 异常  try, confirm, cancel 方法的所有异常不要自行catch 任何异常都应该抛出给 Hmily框架处理。  ","date":-62135596800,"description":"tars用户指南","dir":"projects/hmily/user-tars/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6ab504fb449aa59762cf68f052d17f16","permalink":"/projects/hmily/user-tars/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/hmily/user-tars/","summary":"Tars用户指南 引入jar包 引入hmily配置 在需要进行Hmily分布式事务的自动生成的Servant接口方法上加上 @Hmily 标识。 在具体的实现方法","tags":null,"title":"tars用户指南","type":"projects","url":"/projects/hmily/user-tars/","wordcount":623},{"author":null,"categories":null,"content":" 打开 ACTS IDE 在 Package 视图下，右键含 @Test 注解的函数名，ACTS 功能 -\u0026amp;gt; 修改测试用例，如下图：\n编写测试数据 准备入参 根据被测的接口方法的入参（类型、顺序、数量）正确准备入参数据，简单类型包括 String、Date、Integer、Float、Double、Long、Short、Byte（包含其对应的基本类型，即 int、float 等）；复杂类型为 List、Map、Set、自定义类、Java 定义的类以及前面五者的嵌套等。\n简单入参 入参设置上右键 -\u0026amp;gt; 模版选择 -\u0026amp;gt; 简单入参选择：\n导入简单入参后，值直接在这里填写； 自上而下表示被测接口方法的第1个、第2个和第3个参数，右键可以调节顺序。\n复杂入参 如图27所示，AccountTransRequest 和 BusinessActionContext 类需要生成入参模板，一般情况下，在一键生成测试脚本时会自动生成方法的入参和返回结果的类模板，打开 ACTS IDE 可对其进行编辑，如图28。\n图28\n如果生成测试脚本时没有识别出方法的入参和返回结果模版，可先生成复杂入参和结果模版（具体操作参考[对象模型生成](../usage-model/#对象模型生成)），然后打开 ACTS IDE 编辑器，在入参设置上右键 - 模版选择 - 复杂类型，添加后可以看到复杂对象，直接进行编辑。 ![复杂类型](complex-type.png) ### list ![List 示例](list-example.png) ![编辑值](edit-value.png) ### map 以示例2为例（Set 与此类似） 图32中，演示示例2的方法入参为 `Map` 类型。由于 Object 不是具体类型，如果要设置 Object 为复杂对象，则需要去编辑 YAML。例如设置 Object 为 AccountTransResult 类型，则按照如下编辑： ![Map 示例](map-example.png) 图32\nenum 代码样例：\n 在 ACTS IDE 中编辑如下：   如果枚举嵌套在其他类中，则在该类的 CSV 模版中设置枚举的值为 DEBIT；\n 用例数据 YAML 中，如图37：\n  interestRecoverTypeEnum: !!com.alipay.fc.loancore.common.util.enums.InterestRecoverTypeEnum \u0026#39;ALL\u0026#39;  图37\n### 编码方式准备入参 覆盖 prepare 方法，通过 ActsRuntimeContext 的方法，快速获取和设置用例入参，如图38所示： 1. 获取所有入参：`List getInputParams()` 2. 按位置获取：`Object getInputParamByPos(int i)` 3. 新增用例参数：`void addInputParam(Object obj)` ![ActsRuntimeContext 方法](ActsRuntimeContext-method.png) 图 38\n## 准备 DB 数据 ### 准备 DB 数据-单列场景{#db-single} 如图39，在数据库准备设置位置右键，选择好要插入的 DB 模板（请先确保该DB模板已经生成），图中1、2、3步骤之后点击 OK 即插入 DB 准备模板，如图41，可对要插入 DB 的数据进行编辑： ![选择模板](select-module.png) 图39\n图40\n图41\n### 准备 DB 数据-多列场景{#db-multi} 选中一列数据，点击复制，按此方法可复制多列数据，然后进行编辑即可： ![复制数据](copy-value.jpeg) 图42\n### 准备 DB 数据-flag说明 数据依赖标记： ```plain Y: 插入 N：不插入 C：以此为where条件对插入后的数据进行清理 F：数据库函数 L: 大字段换行准备，准备方式为A=B;C=D ``` ![数据依赖标记](mark.png) 图43\n## 准备期望结果数据 生成期望结果的对象模型后，在 ACTS IDE 界面中，期望结果设置右键 - 模版选择，见下图。 ![期望结果设置](expected-result.png) 图44\n### 期望结果的 flag 说明 ```plain Y: 校验 N：不校验 D：时间偏移值比较，如 D200 ME：map 默认全 key 校验，ME则以期望 key 为准，实际值多余期望值的 key 不予校验 ``` 对于返回结果的时间 Date 类型字段校验说明： 1. Y | null - 代表期望为 null 2. Y | 2015-01-01 00:00:00 - 代表期望为 2015-01-01 00:00:00 3. N | null - 代表不校验 4. D200 | 2015-01-01 00:00:00/null - 代表与 2015-01-01 00:00:00/new Date() 相差 200 秒 ### 编码方式准备期望结果 覆盖 prepare 方法，通过 ActsRuntimeContext 的如下方法，快速获取和设置期望结果。 1. 获取期望结果：`Object getExpectResult()` 2. 设置期望结果：`Boolean setExpectResult(Object objToSet)` ## 准备期望 DB 数据 ### 准备期望 DB 数据-单列场景 在数据库期望设置里配置，操作参考[准备 DB 数据-单列场景](#db-single) ### 准备期望 DB 数据-多列场景 在数据库期望设置里配置，操作参考[准备 DB 数据-多列场景](#db-multi) ### 期望 DB 数据的 flag 说明 数据校验标记： ```plain Y: 校验 N：不校验 C：以此为条件 select 然后比较，如果结果有多个，则返回的结果所有记录都要和当前需要校验的数据进行校验 CN： 这个 flag 表示当前这张表中以 C 和 CN 为条件查询出的结果为空 D200：表示对比时间的时候误差 200s 之内都算通过，日期类型的格式为：today L： 数据库大字段换行数据校验，准备方式为 A=B;C=D P：DB 大字段校验，以期望结果的 kv 为基准，对 DB 大字段里的 kv 进行校验，要求 DB 里的 kv 之间是换行分隔 R：正则匹配校验 ``` ## 准备期望异常数据 ### 编码方式准备期望异常数据 部分系统封装的异常类没有默认构造函数，这样通过模版添加的异常结果在加载 YAML 时会有问题（无默认构造函数无法构造当前类），需要通过代码方式，结合自定义参数编写异常脚本，如下图： ![编写异常脚本](exception-script.png) 图45\n## 准备自定义数据 ### 自定义数据-用途 用户自定义的各类型数据，用于测试过程中自由使用。 ### 自定义数据-数据类型 数据类型可参考 [入参](#准备入参) 部分 ### 编码方式准备自定义数据 快速获取 …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-ide/","fuzzywordcount":3300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"697e7e6d35a2e058f3ca8b0a72032690","permalink":"/projects/sofa-acts/usage-ide/","publishdate":"0001-01-01T00:00:00Z","readingtime":7,"relpermalink":"/projects/sofa-acts/usage-ide/","summary":"打开 ACTS IDE 在 Package 视图下，右键含 @Test 注解的函数名，ACTS 功能 -\u0026gt; 修改测试用例，如下图： 编写测试数据 准备入参 根据被测的接口方法的入参（类型、顺序、数量","tags":null,"title":"一站式编辑","type":"projects","url":"/projects/sofa-acts/usage-ide/","wordcount":3272},{"author":null,"categories":null,"content":" 快速理解 ACTS 的模型 在写测试用例的过程中，需要预先准备一些 DB 表、方法入参的数据，或者需要校验一些 DB 表、返回结果的数据，这些数据可以以模版的形式保存下来，在编辑用例时，可以方便的导入这些数据到准备数据或者校验数据，实现数据复用。目前 ACTS 模型可以分为 DB 模型和类模型。\n常规的测试用例编写，DB 、方法入参、返回结果等领域模型的数据准备是通过测试代码组织的，随着业务复杂度，领域模型复杂度也在不断增加，尤其在金融级业务用，往往一个类或者数据表有数十个属性或者字段，类与类的嵌套也是随处可见，代码构造复杂对象变得十分困难且容易疏漏，问题频现： * 表太多容易遗漏，排查时间太长； * 表的字段名记不住，时不时写错； * 接口入参数量多类型复杂，看见就头疼； * 类的属性太多，容易遗漏重要属性； * 嵌套构造对象，不断的 new 和 set 赋值； * 继承和实现关系复杂，遗漏重要属性；\nACTS 的模版有可以有效应对上述问题，通过将类和表固化为 CSV，类的结构一目了然，通过类、数据表的模版可以快速的模版化地创建对象，并序列化到 YAML 文件中，使用 ACTS IDE 可以方便的管理用例数据。\n模型存储位置 在 test 模块的 resource/model 目录可以查看已经存在的模型。\n图4\n数据表模型生成 数据表模型样例 图5\n1. 校验 flag 说明\n```plain Y: 插入 N：不插入 C：以此为 where 条件对插入后的数据进行清理 F：数据库函数 L: 大字段换行准备，准备方式为 A=B;C=D ```  2. 用例编辑使用模型快速导入数据\n使用 ACTS IDE 编辑 DB 表数据（包括准备表数据、期望表数据）时，可右键新增指定表的模型，用于直接从表模型的 CSV 中导入表的全部字段和值，以便快速编辑。 DB 模版的使用可参考准备 DB 数据。\n生成表模型 图6\n图7\n图8\n点击 OK 后生成模板，如图9：\n图9\n同时支持不配置直连获取表结构的方式生成表模型，即在 DO 类上右键根据类生成表模型： DO 类上右击 -\u0026amp;gt; ACTS 功能 -\u0026amp;gt; 生成 DO 模型：\n图10\n![生产的模型](generated-do-model.png) 图11\n对象模型生成 对象模型样例 图12\n图13\n一个复杂对象是一个闭包，不但包含其自身模型还包含其嵌套对象的模型。\nACTS 使用模型快速导入数据、编辑复杂对象（包括入参、返回结果和异常等），在 ACTS IDE 中可右键选择类模型，用于构建该类的对象并赋值以便快速编辑。\n生成方法 有两种方式： 1.待构建模型的类定义的任意方法上点击； 1.接口定义的方法上点击，详细操作看下图示例。\n使用 IDEA 的同学请注意：请先确保代码已编译，IDEA 不会自动编译而需要手动 mvn clean install 或者打开自动编译 File -\u0026amp;gt; Settings -\u0026amp;gt; Build,Execution,Deployment -\u0026amp;gt; Compiler -\u0026amp;gt; Make project automatically。\nACTS IDE 生成对象模型 （1）待构建模型的类定义的任意方法上点击，生成当前类的模型\n图14\n（2）接口定义任意方法上点击，生成当前接口中，所有方法的复杂入参、复杂返回结果的模型\n图15\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-model/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"65aaf62462b3b0ea142ca75a5b61eb0d","permalink":"/projects/sofa-acts/usage-model/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-acts/usage-model/","summary":"快速理解 ACTS 的模型 在写测试用例的过程中，需要预先准备一些 DB 表、方法入参的数据，或者需要校验一些 DB 表、返回结果的数据，这些数据可以以模版的形式保","tags":null,"title":"一键模型化","type":"projects","url":"/projects/sofa-acts/usage-model/","wordcount":1096},{"author":null,"categories":null,"content":" 快速理解 ACTS 中的脚本 如果你是一个经常编写测试用例的同学，是不是经常苦于这样的问题： * 不断的 assertEquals 写得快吐了，重复性编码毫无创意； * 少一个 assert 容易假绿，错一个败坏心情； * 场景一旦复杂，测试代码比业务代码还要长，写起来痛不欲生； * 每换一个应用，之前写的工具类就要搬一次；\n左图为 TestNG 用例，右图为 ACTS 用例，重复性代码一去不回，代码体积明显缩小。区别于普通测试脚本，ACTS 脚本继承自 ActsTestBase 类，封装了数据加载、驱动、执行引擎和校验规则，无需用户来组织清理数据、准备数据、执行用例和校验结果，对于简单业务可以做到零编码，极大释放代码编写和后期维护成本。\n测试脚本生成 前提条件：务必 mvn 编译工程和生成对象模型，否则会造成 ACTS IDE 不可预料的错误，如无法编辑、数据不正确等。\n接口定义的方法上点击，选择 ACTS 功能 -\u0026amp;gt; 生成测试用例。\n测试脚本运行 方法：右键 ACTS 脚本中的被测方法，选择 TestNG 来执行测试脚本，如下图：\n指定测试脚本运行  在 src/test/resource/config/acts-config.properties 中配置 test_only＝^T，表示只跑用例名称以 T 开头的用例，^T 也可以换成其他正则表达式；\n 修改要测试的用例名称，在用例名前面加 T，ACTS 运行时时仅执行用例名称以 T 开头的用例。\n  脚本用例拆分功能 默认每个测试脚本的所有用例数据保存在同一个 YAML 中，ACTS 支持用例数据根据开关 spilt_yaml_by_case 来决定同一测试脚本的所有用例数据存储在一个 YAML 中还是每个用例存储为一个 YAML。 开关默认为关闭，即同一测试脚本的所有测试数据存储在一个 YAML 文件中。\n在 acts-config.properities 中设置 spilt_yaml_by_case=true 即可打开开关，之后新生成测试脚本时每个用例对应一个单独的以 caseId 命名的 YAML文件，拆分的方式可以降低多人研发同一接口带来的文件冲突问题。\n此外，为了支持将老的 YAML 文件按用例拆分，ACTS 提供了工具类，如下，支持将指定脚本下，指定路径的 YAML 文件按用例拆分。\n BaseDataUtil.saveYamlDataToCaseByCase\n 注意：拆分后，建议先给原有 YAML 重命名做备份，然后打开用例编辑器检查拆分后的文件内容是否正确，确认无误后可删除原有 YAML 文件，两者不能并存。  编码方式准备数据 ACTS 提供了数据自定义 API 接口，封装于 ActsRuntimeContext 类中，如下： + 快速获取和设置自定义参数\n获取全部自定义参数：`getParamMap getParamMap()` 按 key 获取：`Object getParamByName(String paraName)` 新增自定义参数：`void addOneParam(String paraName, Object paraObj)` 替换自定义参数：`void setParamMap(Map\u0026amp;lt;String, Object\u0026amp;gt; paramMap)` 泛型方式获取自定义参数：`T getParamByNameWithGeneric(String paraName)`   快速获取和设置用例入参\n获取所有入参：List getInputParams() 按位置获取：Object getInputParamByPos(int i) 新增用例参数：void addInputParam(Object obj)\n 快速获取和设置期望结果\n获取期望结果：Object getExpectResult() 设置期望结果：Boolean setExpectResult(Object objToSet)\n  Mock 功能使用 Mock 功能目前是采用 Mockito 的方案，具体资料见 Mockito 英文文档和 Mockito 中文文档\n增加依赖 在 test 模块增加如下依赖（如果已经引入 SOFABoot 的测试 starter 则无需重复引入）\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-test\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;scope\u0026amp;gt;test\u0026amp;lt;/scope\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  默认 spring test 依赖的 mockito 版本是 1.x，想要升级的可以排除后再引入相应的版本\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.mockito\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;mockito-core\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.18.3\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Mockito 用于测试时进行打桩处理，通过它可以指定某个类的某个方法在什么情况下返回什么样的值。Mockito 库能够 Mock 对象、验证结果以及打桩，示例如下：\n@SpringBootTest(classes = SOFABootApplication.class) @TestExecutionListeners(listeners = MockitoTestExecutionListener.class) public class RegisterUserActsTest extends ActsTestBase { @TestBean @Autowired // 这是测试类 public UserService userService; @MockBean // 这是要mock的bean public AccountManageFacadeClient accountManageFacadeClient; @Test(dataProvider = \u0026amp;quot;ActsDataProvider\u0026amp;quot;) public void registerUser (String caseId, String desc, PrepareData prepareData) { runTest(caseId, prepareData); } @Override public void beforeActsTest(ActsRuntimeContext actsRuntimeContext) { super.beforeActsTest(actsRuntimeContext); AccountManageResult accountManageResult = new AccountManageResult(); …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-script/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0d20739dedad1f11277bd02ed65329c3","permalink":"/projects/sofa-acts/usage-script/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-acts/usage-script/","summary":"快速理解 ACTS 中的脚本 如果你是一个经常编写测试用例的同学，是不是经常苦于这样的问题： * 不断的 assertEquals 写得快吐了，重复性编码毫无创意； * 少一个 assert 容易假绿","tags":null,"title":"一键脚本化","type":"projects","url":"/projects/sofa-acts/usage-script/","wordcount":1276},{"author":null,"categories":null,"content":" 在本文档将演示如何使用 SOFATracer 集成 Zipkin 进行数据上报展示。\n假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作：\n 下面的示例中将分别演示在 SOFABoot/SpringBoot 工程中 以及 非 SOFABoot/SpringBoot 工程中如何使用。\n 依赖引入 添加 SOFATracer 依赖 工程中添加 SOFATracer 依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;tracer-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置 Zipkin 依赖 考虑到 Zipkin 的数据上报能力不是 SOFATracer 默认开启的能力，所以期望使用 SOFATracer 做数据上报时，需要添加如下的 Zipkin 数据汇报的依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.zipkin.zipkin2\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;zipkin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.11.12\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.zipkin.reporter2\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;zipkin-reporter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.7.13\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置文件 在工程的 application.properties 文件下添加一个 SOFATracer 要使用的参数，包括spring.application.name 用于标示当前应用的名称；logging.path 用于指定日志的输出目录。\n# Application Name spring.application.name=SOFATracerReportZipkin # logging path logging.path=./logs com.alipay.sofa.tracer.zipkin.enabled=true com.alipay.sofa.tracer.zipkin.baseUrl=http://localhost:9411  启动 Zipkin 服务端 启动 Zipkin 服务端用于接收 SOFATracer 汇报的链路数据，并做展示。Zipkin Server 的搭建可以参考此文档进行配置和服务端的搭建。\n运行 可以将工程导入到 IDE 中运行生成的工程里面中的 main 方法启动应用，也可以直接在该工程的根目录下运行 mvn spring-boot:run，将会在控制台中看到启动日志：\n2018-05-12 13:12:05.868 INFO 76572 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: \u0026#39;SpringMvcSofaTracerFilter\u0026#39; to urls: [/*] 2018-05-12 13:12:06.543 INFO 76572 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \u0026amp;quot;{[/helloZipkin]}\u0026amp;quot; onto public java.util.Map\u0026amp;lt;java.lang.String, java.lang.Object\u0026amp;gt; com.alipay.sofa.tracer.examples.zipkin.controller.SampleRestController.helloZipkin(java.lang.String) 2018-05-12 13:12:07.164 INFO 76572 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)  可以通过在浏览器中输入 http://localhost:8080/helloZipkin 来访问 REST 服务，结果类似如下：\n{ content: \u0026amp;quot;Hello, SOFATracer Zipkin Remote Report!\u0026amp;quot;, id: 1, success: true }  查看 Zipkin 服务端展示 打开 Zipkin 服务端界面，假设我们部署的 Zipkin 服务端的地址是 http://localhost:9411，打开 URL 并搜索 helloZipkin(由于我们本地访问的地址是 localhost:8080/helloZipkin)，可以看到展示的链路图。\nSpring 工程运行 对于一般的 Spring 工程，我们通常使用 tomcat/jetty 作为 servlet 容器来启动应用。具体工程参考 在 Spring 工程中使用 SOFATracer\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/report-to-zipkin/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d28d192386829452262116de9c32b570","permalink":"/projects/sofa-tracer/report-to-zipkin/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/report-to-zipkin/","summary":"在本文档将演示如何使用 SOFATracer 集成 Zipkin 进行数据上报展示。 假设你已经基于 SOFABoot 构建了一个简单的 Spring Web 工程，那么可以通过如下步骤进行操作： 下面的示例中将分别演","tags":null,"title":"上报数据至 Zipkin","type":"projects","url":"/projects/sofa-tracer/report-to-zipkin/","wordcount":661},{"author":null,"categories":null,"content":" 优雅关闭，包括两部分，一个是 RPC 框架作为客户端，一个是 RPC 框架作为服务端。\n作为服务端 作为服务端的时候，RPC 框架在关闭时，不应该直接暴力关闭。在 RPC 框架中\ncom.alipay.sofa.rpc.context.RpcRuntimeContext  在静态初始化块中，添加了一个 ShutdownHook\n// 增加jvm关闭事件 if (RpcConfigs.getOrDefaultValue(RpcOptions.JVM_SHUTDOWN_HOOK, true)) { Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() { @Override public void run() { if (LOGGER.isWarnEnabled()) { LOGGER.warn(\u0026amp;quot;SOFA RPC Framework catch JVM shutdown event, Run shutdown hook now.\u0026amp;quot;); } destroy(false); } }, \u0026amp;quot;SOFA-RPC-ShutdownHook\u0026amp;quot;)); }  这个 ShutdownHook 的作用是当发布平台/用户执行 kill pid 的时候，会先执行 ShutdownHook 中的逻辑。在销毁操作中，RPC 框架会先执行向注册中心取消服务注册、关闭服务端口等动作。\nprivate static void destroy(boolean active) { RpcRunningState.setShuttingDown(true); for (Destroyable.DestroyHook destroyHook : DESTROY_HOOKS) { destroyHook.preDestroy(); } List\u0026amp;lt;ProviderConfig\u0026amp;gt; providerConfigs = new ArrayList\u0026amp;lt;ProviderConfig\u0026amp;gt;(); for (ProviderBootstrap bootstrap : EXPORTED_PROVIDER_CONFIGS) { providerConfigs.add(bootstrap.getProviderConfig()); } // 先反注册服务端 List\u0026amp;lt;Registry\u0026amp;gt; registries = RegistryFactory.getRegistries(); if (CommonUtils.isNotEmpty(registries) \u0026amp;amp;\u0026amp;amp; CommonUtils.isNotEmpty(providerConfigs)) { for (Registry registry : registries) { registry.batchUnRegister(providerConfigs); } } // 关闭启动的端口 ServerFactory.destroyAll(); // 关闭发布的服务 for (ProviderBootstrap bootstrap : EXPORTED_PROVIDER_CONFIGS) { bootstrap.unExport(); } // 关闭调用的服务 for (ConsumerBootstrap bootstrap : REFERRED_CONSUMER_CONFIGS) { ConsumerConfig config = bootstrap.getConsumerConfig(); if (!CommonUtils.isFalse(config.getParameter(RpcConstants.HIDDEN_KEY_DESTROY))) { // 除非不让主动unrefer bootstrap.unRefer(); } } // 关闭注册中心 RegistryFactory.destroyAll(); // 关闭客户端的一些公共资源 ClientTransportFactory.closeAll(); // 卸载模块 if (!RpcRunningState.isUnitTestMode()) { ModuleFactory.uninstallModules(); } // 卸载钩子 for (Destroyable.DestroyHook destroyHook : DESTROY_HOOKS) { destroyHook.postDestroy(); } // 清理缓存 RpcCacheManager.clearAll(); RpcRunningState.setShuttingDown(false); if (LOGGER.isWarnEnabled()) { LOGGER.warn(\u0026amp;quot;SOFA RPC Framework has been release all resources {}...\u0026amp;quot;, active ? \u0026amp;quot;actively \u0026amp;quot; : \u0026amp;quot;\u0026amp;quot;); } }  其中以 bolt 为例，关闭端口并不是一个立刻执行的动作\n@Override public void destroy() { if (!started) { return; } int stopTimeout = serverConfig.getStopTimeout(); if (stopTimeout \u0026amp;gt; 0) { // 需要等待结束时间 AtomicInteger count = boltServerProcessor.processingCount; // 有正在执行的请求 或者 队列里有请求 if (count.get() \u0026amp;gt; 0 || bizThreadPool.getQueue().size() \u0026amp;gt; 0) { long start = RpcRuntimeContext.now(); if (LOGGER.isInfoEnabled()) { LOGGER.info(\u0026amp;quot;There are {} call in processing and {} call in queue, wait {} ms to end\u0026amp;quot;, count, bizThreadPool.getQueue().size(), stopTimeout); } while ((count.get() \u0026amp;gt; 0 || bizThreadPool.getQueue().size() \u0026amp;gt; 0) \u0026amp;amp;\u0026amp;amp; RpcRuntimeContext.now() - start \u0026amp;lt; stopTimeout) { // 等待返回结果 try { Thread.sleep(10); } catch (InterruptedException ignore) { } } } // 关闭前检查已有请求？ } // 关闭线程池 bizThreadPool.shutdown(); stop(); }  而是 …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/graceful-shutdown/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"53af179e23ba184b01eb8234c055b15d","permalink":"/projects/sofa-rpc/graceful-shutdown/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/graceful-shutdown/","summary":"优雅关闭，包括两部分，一个是 RPC 框架作为客户端，一个是 RPC 框架作为服务端。 作为服务端 作为服务端的时候，RPC 框架在关闭时，不应该直接暴力关闭。在","tags":null,"title":"优雅关闭","type":"projects","url":"/projects/sofa-rpc/graceful-shutdown/","wordcount":866},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"使用该指南您可以快速部署应用到 CloudMesh ，对服务进行访问，通过监控查看流量，体验服务治理、Sidecar管理和对服务的新版本进行灰度发布等实用功能。","dir":"guides/kc-cloud-mesh-demo/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e389a65e6736e909718275cd76505525","permalink":"/guides/kc-cloud-mesh-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/kc-cloud-mesh-demo/","summary":"","tags":null,"title":"使用 CloudMesh 轻松实践 Service Mesh","type":"guides","url":"/guides/kc-cloud-mesh-demo/","wordcount":0},{"author":null,"categories":null,"content":"使用 Consul 作为服务注册中心需要添加如下依赖\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.ecwid.consul\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;consul-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  然后在 application.properties 中如下配置：\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500  其中后面的值为 consul 的连接地址，如果需要设置一些其他参数，也可以通过\ncom.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500?a=1\u0026amp;amp;b=2  进行设置\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-consul/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e6b0aa843ea0ad401c3184f6ce87649b","permalink":"/projects/sofa-rpc/registry-consul/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-consul/","summary":"使用 Consul 作为服务注册中心需要添加如下依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.ecwid.consul\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;consul-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.4.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 然后在 application.properties 中如下配置： com.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500 其中后面的值为 consul 的连接地址，如果需要设置一些其他参数，也可以通过 com.alipay.sofa.rpc.registry.address=consul://127.0.0.1:8500?a=1\u0026amp;b=2 进行","tags":null,"title":"使用 Consul 作为注册中心","type":"projects","url":"/projects/sofa-rpc/registry-consul/","wordcount":72},{"author":null,"categories":null,"content":" 本文将介绍如何使用 MOSN 在 SOFAMesh 框架下搭建 Service Mesh 的开发环境，并验证 MOSN 的一些基础路由能力、负载均衡能力等。本文介绍的内容将包括 :\n MOSN 与 SOFAMesh 的关系 准备工作 源码方式部署 SOFAMesh Bookinfo 实验  MOSN 与 SOFAMesh 的关系 我们曾在 MOSN 介绍中介绍过，MOSN 是一款采用 Go 语言开发的 Service Mesh 数据平面代理。而 SOFAMesh 则是基于 Istio 改进和扩展而来的 Service Mesh 大规模落地实践方案，MOSN 作为 SOFAMesh 的关键组件用来完成数据面的转发。\n下图是 SOFAMesh 整体框架下，MOSN 的工作示意图。\n注意：当前 MOSN 不支持在原生的 Istio 中直接使用。\n 准备工作 本文以 macOS 为例 ，其他环境可以安装对应版本的软件。\n1. 安装 hyperkit 先安装 docker-for-mac，之后安装驱动\n1.1 安装 docker 下载软件包安装，或者使用如下的命令安装。\n$ brew cask install docker  1.2 安装驱动 $ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; chmod +x docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo mv docker-machine-driver-hyperkit /usr/local/bin/ \\ \u0026amp;amp;\u0026amp;amp; sudo chown root:wheel /usr/local/bin/docker-machine-driver-hyperkit \\ \u0026amp;amp;\u0026amp;amp; sudo chmod u+s /usr/local/bin/docker-machine-driver-hyperkit  2. 安装 Minikube(也可以购买商业 k8s 集群) 推荐使用 Minikube v0.28 以上来体验，请参考 https://github.com/kubernetes/minikube\n$ brew cask install minikube  3. 启动 Minikube 注意，pilot 至少需要 2G 内存，所以在启动的时候，可以通过加参数的方法给 minikube 添加分配的资源，如果你机器的资源不够，推荐使用商业版本的 k8s 集群。\n$ minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.15.0 --vm-driver=hyperkit  创建istio 命名空间\n$ kubectl create namespace istio-system  4. 安装 kubectl 命令行工具 kubectl 是用于针对 k8s 集群运行命令的命令行接口，安装参考 https://kubernetes.io/docs/tasks/tools/install-kubectl。\n$ brew install kubernetes-cli  5. 安装 Helm Helm 是一个 k8s 的包管理工具，安装参考 https://docs.helm.sh/using_helm/#installing-helm\n$ brew install kubernetes-helm  源码方式部署 SOFAMesh 1. 下载 SOFAMesh 源码 $ git clone https://github.com/sofastack/sofa-mesh.git  2. 通过 Helm 安装 SOFAMesh 使用 helm template 安装\n首先需要切换到SOFAMesh源码所在目录，然后使用Helm安装istio CRD以及各个组件\n$ cd sofa-mesh $ helm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f - $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl apply -f -  3. 验证安装 istio-system 命名空间下的 pod 状态都是 Running 时，说明已经部署成功。 如果仅仅是为了运行bookinfo，只需要pilot,injector,citadel这三个pods运行成功就可以满足最低要求\n$ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-citadel-6579c78cd9-w57lr 1/1 Running 0 5m istio-egressgateway-7649f76df4-zs8kw 1/1 Running 0 5m istio-galley-c77876cb6-nhczq 1/1 Running 0 5m istio-ingressgateway-5c9c8565d9-d972t 1/1 Running 0 5m istio-pilot-7485f9fb4b-xsvtm 1/1 Running 0 5m istio-policy-5766bc84b9-p2wfj 1/1 Running 0 5m istio-sidecar-injector-7f5f586bc7-2sdx6 1/1 Running 0 5m istio-statsd-prom-bridge-7f44bb5ddb-stcf6 1/1 Running 0 5m istio-telemetry-55ff8c77f4-q8d8q 1/1 Running 0 5m prometheus-84bd4b9796-nq8lg 1/1 Running 0 5m  4. 卸载安装 卸载SOFAMesh\n$ helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl delete -f - $ kubectl delete namespace istio-system  BookInfo 实验 BookInfo 是一个类似豆瓣的图书应用，它包含四个基础服务：\n Product Page：主页，由 python 开发，展示所有图书信息，它会调用 Reviews 和 Details 服务 Reviews：评论，由 java 开发，展示图书评论，会调用 Ratings 服务 Ratings：评分服务，由 nodejs …","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-run-with-sofamesh/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9c6461e92180417d3a8ec4f3f2c723fe","permalink":"/projects/mosn/quick-start-run-with-sofamesh/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/mosn/quick-start-run-with-sofamesh/","summary":"本文将介绍如何使用 MOSN 在 SOFAMesh 框架下搭建 Service Mesh 的开发环境，并验证 MOSN 的一些基础路由能力、负载均衡能力等。本文介绍的内容将包括 : MOSN 与 SOFAMesh 的关系 准备工作 源码","tags":null,"title":"使用 MOSN 搭建 Service Mesh 平台","type":"projects","url":"/projects/mosn/quick-start-run-with-sofamesh/","wordcount":1654},{"author":null,"categories":null,"content":"SOFARPC 已支持使用 Nacos 作为服务注册中心。假设你已经根据 Nacos 的快速开始在本地部署好 Nacos Server，服务发现的端口默认设置在 8848。\n在 SOFARPC 中使用 Nacos 作为服务注册中心只需要在 application.properties 中加入如下配置即可：\ncom.alipay.sofa.rpc.registry.address=nacos://127.0.0.1:8848  如果你直接使用了 SOFARPC，而不是 SOFABoot，需要手动添加 nacos 的依赖，其中 version 为用户想使用的 version。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alibaba.nacos\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;nacos-client\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  当前支持 Nacos 的版本： - SOFARPC：5.5.0 支持 Nacos 服务端版本 0.6.0，SOFABoot: 2.5.3。 - SOFARPC：5.6.0 支持 Nacos 服务端版本 1.0.0。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-nacos/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cc161f22cd2145fe309e63087581adc1","permalink":"/projects/sofa-rpc/registry-nacos/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-nacos/","summary":"SOFARPC 已支持使用 Nacos 作为服务注册中心。假设你已经根据 Nacos 的快速开始在本地部署好 Nacos Server，服务发现的端口默认设置在 8848。 在 SOFARPC 中使用 Nacos 作为服务","tags":null,"title":"使用 Nacos 作为注册中心","type":"projects","url":"/projects/sofa-rpc/registry-nacos/","wordcount":232},{"author":null,"categories":null,"content":"SOFARPC 已支持使用 SOFARegistry 作为服务注册中心。假设你已经根据 SOFARegistry 的快速开始在本地部署好 SOFARegistry Server，服务发现的端口默认设置在 9603。\n在 SOFARPC 中使用 SOFARegistry 作为服务注册中心首先要添加如下的依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;5.2.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  然后在 application.properties 中加入如下配置即可：\ncom.alipay.sofa.rpc.registry.address=sofa://127.0.0.1:9603  当前支持 SOFARegistry 的版本：\nSOFARPC: 5.5.2, SOFABoot: 2.6.3。\n由于本次发布的时间问题，暂时需要用户指定SOFARPC Starter的版本\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;5.5.2\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  SOFARPC 集成验证 SOFARegistry 服务端版本：5.2.0\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-sofa/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"65085018ce2b2b2ef452993bb79a69de","permalink":"/projects/sofa-rpc/registry-sofa/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-sofa/","summary":"SOFARPC 已支持使用 SOFARegistry 作为服务注册中心。假设你已经根据 SOFARegistry 的快速开始在本地部署好 SOFARegistry Server，服务发现的端口默认设置在 9603。 在 SOFARPC 中使用 SOFARegistry 作为服务","tags":null,"title":"使用 SOFARegistry 作为注册中心","type":"projects","url":"/projects/sofa-rpc/registry-sofa/","wordcount":182},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"本指南将基于 SOFAStack 快速构建一个微服务。","dir":"guides/sofastack-quick-start/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"78bfd4806a86dc15ac86eee16fb85c82","permalink":"/guides/sofastack-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/sofastack-quick-start/","summary":"","tags":null,"title":"使用 SOFAStack 快速构建微服务","type":"guides","url":"/guides/sofastack-quick-start/","wordcount":0},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"该指南将向您展示如何使用开源分布式事务框架 Seata 的 AT 模式、TCC 模式解决业务数据的最终一致性问题。 ","dir":"guides/kc-seata-demo/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"60071a0eb44bf0901fb187eefd63ccdb","permalink":"/guides/kc-seata-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/kc-seata-demo/","summary":"","tags":null,"title":"使用 Seata 保障支付一致性","type":"guides","url":"/guides/kc-seata-demo/","wordcount":0},{"author":null,"categories":null,"content":" 使用 Zookeeper 作为服务注册中心只需要在 application.properties 中如下配置即可：\ncom.alipay.sofa.rpc.registry.address=zookeeper://127.0.0.1:2181  注意：考虑掉服务的实时性，以下特性暂不支持\nSOFABoot RPC 也提供一个缓存文件(目前暂不支持)，当 Zookeeper 不可用时，使用该缓存文件进行服务发现。配置该缓存文件的方式如下：\ncom.alipay.sofa.rpc.registry.address=zookeeper://xxx:2181?file=/home/admin/registry  Zookeeper Auth 支持 当用户需要对发布和消费服务，进行权限认证的时候，可以通过在操作 zookeeper 时，指定对应的目录和账号密码来进行读写。这样只有使用了相同密码的 服务方或者消费方才能进行读写。\nSOFARPC API 支持 在构造注册中心的时候，将Auth添加上\nparameters.put(\u0026amp;quot;scheme\u0026amp;quot;, \u0026amp;quot;digest\u0026amp;quot;); //如果存在多个认证信息，则在参数形式为为user1:passwd1,user2:passwd2 parameters.put(\u0026amp;quot;addAuth\u0026amp;quot;, \u0026amp;quot;sofazk:rpc1\u0026amp;quot;); registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181/authtest\u0026amp;quot;) .setParameters(parameters);  之后其他没有使用正确auth的，将无法访问authtest目录\nXML 方式支持 如下使用即可\ncom.alipay.sofa.rpc.registry.address=zookeeper://xxx:2181?file=/home/admin/registry\u0026amp;amp;scheme=digest\u0026amp;amp;addAuth=sofazk:rpc1  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-zookeeper/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"71d6486c5577cc85d84c56688cdf2af1","permalink":"/projects/sofa-rpc/registry-zookeeper/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-zookeeper/","summary":"使用 Zookeeper 作为服务注册中心只需要在 application.properties 中如下配置即可： com.alipay.sofa.rpc.registry.address=zookeeper://127.0.0.1:2181 注意：考虑掉服务的实时性，以下特性暂不支持 SOFABoot RPC 也提供一个缓存文件(目前暂不支持)，当 Zookeeper 不可","tags":null,"title":"使用 Zookeeper 作为注册中心","type":"projects","url":"/projects/sofa-rpc/registry-zookeeper/","wordcount":309},{"author":null,"categories":null,"content":"使用本地文件作为服务注册中心在 application.properties 中如下配置即可：\ncom.alipay.sofa.rpc.registry.address=local:///home/admin/registry/localRegistry.reg  其中 /home/admin/registry/localRegistry.reg 就是使用的本地文件的目录。\n对于 windows 用户，则以上地址类似：\ncom.alipay.sofa.rpc.registry.address=local://c://users/localRegistry.reg  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-local/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"33bc89393392e21b3917f090313c0df5","permalink":"/projects/sofa-rpc/registry-local/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-local/","summary":"使用本地文件作为服务注册中心在 application.properties 中如下配置即可： com.alipay.sofa.rpc.registry.address=local:///home/admin/registry/localRegistry.reg 其中 /home/admin/registry/localRegistry.reg 就是使用的本地文件的目录。 对于 windows 用户，则以上地址类似： com.alipay.sofa.rpc.registry.address=local://c://users/localRegistry.reg","tags":null,"title":"使用本地文件作为注册中心","type":"projects","url":"/projects/sofa-rpc/registry-local/","wordcount":56},{"author":null,"categories":null,"content":" SOFABoot 是在 Spring Boot 的基础上提供的功能扩展。基于 Spring Boot 的机制，SOFABoot 管理了 SOFA 中间件的依赖，并且提供了 Spring Boot 的 Starter，方便用户在 Spring Boot 中使用 SOFA 中间件。\nSOFABoot 依赖管理 \u0026amp;ndash; Maven 在使用 SOFA 中间件之前，需要引入 SOFABoot 依赖管理。类似 Spring Boot 引入方式，在工程中增加如下 \u0026amp;lt;parent/\u0026amp;gt; 标签配置的方式:\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  其中 ${sofa.boot.version} 为具体的 SOFABoot 版本，参考发布历史。\nSOFABoot 依赖管理 \u0026amp;ndash; Gradle 从 SOFABoot 3.1.1 版本开始，SOFABoot 开始支持使用 Gradle 来进行依赖管理，如果要使用 Gradle 来进行依赖管理，需要按照如下的形式来配置 build.gradle：\nbuildscript { ext { sofaBootVersion = \u0026#39;3.1.1\u0026#39; } repositories { mavenLocal() mavenCentral() } dependencies { classpath(\u0026amp;quot;com.alipay.sofa:sofa-boot-gradle-plugin:${sofaBootVersion}\u0026amp;quot;) } } apply plugin: \u0026#39;java\u0026#39; apply plugin: \u0026#39;eclipse\u0026#39; apply plugin: \u0026#39;com.alipay.sofa.boot\u0026#39; apply plugin: \u0026#39;io.spring.dependency-management\u0026#39; group = \u0026#39;com.example\u0026#39; version = \u0026#39;0.0.1-SNAPSHOT\u0026#39; sourceCompatibility = 1.8 repositories { mavenLocal() mavenCentral() } dependencies { implementation(\u0026#39;com.alipay.sofa:rpc-sofa-boot-starter\u0026#39;) implementation(\u0026#39;org.springframework.boot:spring-boot-starter\u0026#39;) testImplementation(\u0026#39;org.springframework.boot:spring-boot-starter-test\u0026#39;) }  主要有几个步骤：\n 添加 buildScript，增加 sofa-boot-gradle-plugin 的依赖，其中版本号为你使用的 SOFABoot 的版本。 添加两个 plugin，分别是 com.alipay.sofa.boot 和 io.spring.dependency-management。  这样，在 dependencies 里面，就可以直接添加 SOFABoot 管理的各种中间件和依赖了，而不用声明版本号。\n引入 SOFA 中间件 SOFABoot 使用一系列后缀为 -sofa-boot-starter 来标示一个中间件组件，如果想要使用某个中间件，直接添加对应的依赖即可。例如，如果期望使用 SOFARPC，只需增加下面的 Maven 依赖即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;rpc-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  注意上面的 Maven 依赖中并没有声明版本，这个是因为版本已经在 sofaboot-dependencies 里面声明好。这样做的好处是对于 SOFA 中间件，用户统一进行升级即可，不需要单独升级一个中间件的版本，防止出现依赖冲突以及兼容性的问题。目前管控的 SOFABoot 中间件列表如下:\n   中间件 starter     SOFARPC rpc-sofa-boot-starter   SOFATracer tracer-sofa-boot-starter   SOFALookout lookout-sofa-boot-starter    引入 SOFABoot 扩展组件 SOFABoot 基于 Spring Boot 提供了健康检查，模块隔离，类隔离等扩展能力。遵循 Spring Boot 依赖即服务的理念，添加相关组件依赖之后，扩展能力即可生效。目前提供的扩展组件如下：\n   扩展组件 starter     健康检查 healthcheck-sofa-boot-starter   模块化隔离 isle-sofa-boot-starter   类隔离 sofa-ark-springboot-starter   测试扩展 test-sofa-boot-starter    引入 SOFA 中间件 ark 插件 SOFABoot 提供了类隔离组件 SOFAArk，借助 SOFAArk 容器，用户可以将依赖冲突的三方包打包成 ark 插件。运行时，ark 插件使用单独的类加载器加载，可以和其他 ark 插件以及业务依赖隔离，解决类冲突问题。SOFABoot 官方提供了 SOFARPC 和 SOFATracer 的 ark 插件，例如在应用中引入 SOFARPC ark 插件依赖替代 SOFARPC starter，从而隔离应用和 SOFARPC 及其间接依赖。目前管控的 ark 插件列表如下:\n   Ark插件 plugin     SOFARPC rpc-sofa-boot-plugin   SOFATracer tracer-sofa-boot-plugin    引入 SOFABoot 命名空间 使用 SOFA 中间件时，需要在 XML 中根据中间件的具体使用方式添加相应的配置，这个时候需要引入 SOFABoot 的命名空间 xmlns:sofa=\u0026amp;quot;http://sofastack.io/schema/sofaboot\u0026amp;quot; 以能够正确解析相应的配置标签，示例：\n\u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/dependency-management/","fuzzywordcount":1000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dabdbd425f20dee4d7ab580d43574456","permalink":"/projects/sofa-boot/dependency-management/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/dependency-management/","summary":"SOFABoot 是在 Spring Boot 的基础上提供的功能扩展。基于 Spring Boot 的机制，SOFABoot 管理了 SOFA 中间件的依赖，并且提供了 Spring Boot 的 Starter，方便用户在 Spring Boot 中使用","tags":null,"title":"依赖管理","type":"projects","url":"/projects/sofa-boot/dependency-management/","wordcount":981},{"author":null,"categories":null,"content":"SOFARPC 使用了一些三方开源组件，他们分别是：\n一些主要依赖：\n Netty under Apache License 2.0 SLF4j under the MIT License SOFA Bolt under Apache License 2.0 Javassist under Apache License 2.0 Resteasy under Apache License 2.0 SOFA Hessian under Apache License 2.0  一些扩展依赖：\n protobuf under New BSD License Snappy under Apache License 2.0 dubbo under Apache License 2.0  \u0026amp;hellip; 其它整理中。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/notice/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b6c87388d5c1462f13d92012639a08b2","permalink":"/projects/sofa-rpc/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/notice/","summary":"SOFARPC 使用了一些三方开源组件，他们分别是： 一些主要依赖： Netty under Apache License 2.0 SLF4j under the MIT License SOFA Bolt under Apache License 2.0 Javassist under Apache License 2.0 Resteasy under Apache License 2.0 SOFA Hessian under Apache License 2.0 一些扩展依赖： protobuf under New BSD License","tags":null,"title":"依赖组件版权说明","type":"projects","url":"/projects/sofa-rpc/notice/","wordcount":87},{"author":null,"categories":null,"content":" SOFABoot 为 Spring Boot 的健康检查能力增加了 Readiness Check 的能力。如果你需要使用 SOFA 中间件，那么建议使用 SOFABoot 的健康检查能力的扩展，来更优雅的上线应用实例\n引入健康检查扩展 要引入 SOFABoot 的健康检查能力的扩展，只需要引入以下的 Starter 即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;healthcheck-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  如果不引入 SOFABoot 的健康检查扩展，用户依然可以直接依赖 HealthIndicator 接口进行原生的 Spring Boot Actuator 的 Liveness Check。\n安全提醒 从 SOFABoot 2.3.0 开始，由于健康检查能力依赖于 SpringBoot 1.4.x 里的 Actuator 组件，而 Actuator 会默认开启很多 EndPoint，例如 /dump，/trace 等等，可能存在安全风险，可以参照官方文档里的安全建议进行设置。\n SpringBoot 1.5.x 和 SpringBoot 2.x 已修复了部分安全行为，SOFABoot 将通过升级 SpringBoot 内核进行支持。\n 查看健康检查结果 加入健康检查扩展之后，我们可以直接在浏览器中输入 http://localhost:8080/health/readiness 来查看 Readiness Check 的结果。如果要查看 Liveness Check 的结果，可以直接查看 Spring Boot 的健康检查的 URL http://localhost:8080/health。\n除了通过 URL 来查看健康检查的结果之外，在 SOFABoot 中，还可以通过查看具体的日志来确定健康检查的结果，日志的目录位于 health-check 目录下，日志的内容大概如下：\n2018-04-06 23:29:50,240 INFO main - Readiness check result: success  目前 SOFA 中间件已经通过 SOFABoot 的 Readiness Check 的能力来控制了上游流量的进入，但是一个应用的流量可能并不是全部都是从中间件进入的，比较常见的还有从负载均衡器进入的，为了控制从负载均衡器进入的流量，建议使用者通过 PAAS 来访问 Readiness Check 的结果，根据结果来控制是否要在负载均衡器中上线对应的节点。\n注: 自 SOFABoot 2.x 之后，不再间接引入 spring-boot-starter-web 依赖，如果需要在浏览器中查看健康检查结果，需要额外在工程中引入 web 容器依赖。\n注: 在 SOFABoot 3.x 中调整了 endpoint 路径，health/readiness 更改为 actuator/readiness\n扩展 Readiness Check 能力 在 Readiness Check 的各个阶段，SOFABoot 都提供了扩展的能力，应用可以根据自己的需要进行扩展，在 2.x 版本中，可供扩展的点如下：\n   回调接口 说明     org.springframework.context.ApplicationListener 如果想要在 Readiness Check 之前做一些事情，那么监听这个 Listener 的 SofaBootBeforeHealthCheckEvent 事件。   org.springframework.boot.actuate.health.HealthIndicator 如果想要在 SOFABoot 的 Readiness Check 里面增加一个检查项，那么可以直接扩展 Spring Boot 的这个接口。   com.alipay.sofa.healthcheck.startup.SofaBootAfterReadinessCheckCallback 如果想要在 Readiness Check 之后做一些事情，那么可以扩展 SOFABoot 的这个接口。    在 3.x 版本中，可供扩展点如下：\n   回调接口 说明     com.alipay.sofa.healthcheck.core.HealthChecker 如果想要在 SOFABoot 的 Readiness Check 里面增加一个检查项，可以直接扩展该接口。相较于 Spring Boot 本身的 HealthIndicator 接口，该接口提供了一些额外的参数配置，比如检查失败重试次数等。   org.springframework.boot.actuate.health.HealthIndicator 如果想要在 SOFABoot 的 Readiness Check 里面增加一个检查项，那么可以直接扩展 Spring Boot 的这个接口。   org.springframework.boot.actuate.health.ReactiveHealthIndicator 在 WebFlux 中，如果想要在 SOFABoot 的 Readiness Check 里面增加一个检查项，那么可以直接扩展 Spring Boot 的这个接口。   com.alipay.sofa.healthcheck.startup.ReadinessCheckCallback 如果想要在 Readiness Check 之后做一些事情，那么可以扩展 SOFABoot 的这个接口。    需要指出的是，上述四个扩展接口均可以通过 Spring Boot 标准的 Ordered, PriorityOrdered 和注解 @Order 实现执行顺序的设置。\nReadiness Check 配置项 应用在引入 SOFABoot 的健康检查扩展之后，可以在 Spring Boot 的配置文件 application.properties 中添加相关配置项来定制 Readiness Check 的相关行为。\n   Readiness Check 配置项 说明 默认值 开始支持版本     com.alipay.sofa.healthcheck.skip.all 是否跳过整个 Readiness Check 阶段 false 2.4.0   com.alipay.sofa.healthcheck.skip.component 是否跳过 SOFA 中间件的 Readiness Check false 2.4.0   com.alipay.sofa.healthcheck.skip.indicator 是否跳过 HealthIndicator 的 Readiness Check false 2.4.0 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/health-check/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a366b25125fa4aedb08a9cef572db1c8","permalink":"/projects/sofa-boot/health-check/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/health-check/","summary":"SOFABoot 为 Spring Boot 的健康检查能力增加了 Readiness Check 的能力。如果你需要使用 SOFA 中间件，那么建议使用 SOFABoot 的健康检查能力的扩展，来更优雅的上线应用实例 引入健康检查扩展 要","tags":null,"title":"健康检查","type":"projects","url":"/projects/sofa-boot/health-check/","wordcount":1269},{"author":null,"categories":null,"content":"  目前默认生效的的扩展模块是：lookout-ext-jvm，lookout-ext-os(from v1.5.0)。\n JVM 线程    metric name metric tags specification     jvm.threads.totalStarted  \u0026amp;mdash;   jvm.threads.active  \u0026amp;mdash;   jvm.threads.peak  \u0026amp;mdash;   jvm.threads.daemon  \u0026amp;mdash;    JVM 类加载    metric name metric tags specification     jvm.classes.unloaded  \u0026amp;mdash;   jvm.classes.loaded  \u0026amp;mdash;   jvm.classes.total  \u0026amp;mdash;    JVM 内存    metric name metric tags specification     jvm.memory.heap.init  \u0026amp;mdash;   jvm.memory.heap.used  \u0026amp;mdash;   jvm.memory.heap.max  \u0026amp;mdash;   jvm.memory.heap.committed  \u0026amp;mdash;    JVM 垃圾回收    metric name metric tags specification     jvm.gc.young.time  \u0026amp;mdash;   jvm.gc.young.count  \u0026amp;mdash;   jvm.gc.old.time  \u0026amp;mdash;   jvm.gc.old.count  \u0026amp;mdash;    机器文件系统信息    metric name metric tags specification     instance.file.system.free.space root（文件系统根目录名） \u0026amp;mdash;   instance.file.system.total.space root \u0026amp;mdash;   instance.file.system.usabe.space root \u0026amp;mdash;    机器信息    metric name metric tags specification     instance.mem.free  \u0026amp;mdash;   instance.mem.total  \u0026amp;mdash;   instance.processors  \u0026amp;mdash;   instance.uptime  \u0026amp;mdash;   instance.systemload.average  \u0026amp;mdash;    Linux 操作系统信息 （1.5.0版本之后默认启用）    metric name metric tags specification     os.systemload.average.1min  \u0026amp;mdash;   os.systemload.average.5min  \u0026amp;mdash;   os.systemload.average.15min  \u0026amp;mdash;   os.cpu.idle  \u0026amp;mdash;   os.cpu.iowait  \u0026amp;mdash;   os.cpu.irq  \u0026amp;mdash;   os.cpu.nice  \u0026amp;mdash;   os.cpu.softirq  \u0026amp;mdash;   os.cpu.system  \u0026amp;mdash;   os.cpu.user  \u0026amp;mdash;   os.disk.usage.percent.used device,root,type \u0026amp;mdash;   os.disk.usage.total.bytes device,root,type \u0026amp;mdash;   os.disk.usage.used.bytes device,root,type \u0026amp;mdash;   os.net.stats.in.bytes intfc \u0026amp;mdash;   os.net.stats.in.compressed intfc \u0026amp;mdash;   os.net.stats.in.dropped intfc \u0026amp;mdash;   os.net.stats.in.errs intfc \u0026amp;mdash;   os.net.stats.in.fifo.errs intfc \u0026amp;mdash;   os.net.stats.in.frame.errs intfc \u0026amp;mdash;   os.net.stats.in.multicast intfc \u0026amp;mdash;   os.net.stats.in.packets intfc \u0026amp;mdash;   os.net.stats.out.bytes intfc \u0026amp;mdash;   os.net.stats.out.carrier.errs intfc \u0026amp;mdash;   os.net.stats.out.collisions intfc \u0026amp;mdash;   os.net.stats.out.compressed intfc \u0026amp;mdash;   os.net.stats.out.dropped intfc \u0026amp;mdash;   os.net.stats.out.errs intfc \u0026amp;mdash;   os.net.stats.out.fifo.errs intfc \u0026amp;mdash;   os.net.stats.out.packets intfc \u0026amp;mdash;   os.memory.stats.buffers.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.cached.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.free.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3   os.memory.stats.total.bytes \u0026amp;mdash; \u0026amp;gt;= 1.5.3    ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/client-ext-metrics/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c8a4fb3d904e359e99db9d4e81e60812","permalink":"/projects/sofa-lookout/client-ext-metrics/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/client-ext-metrics/","summary":"目前默认生效的的扩展模块是：lookout-ext-jvm，lookout-ext-os(from v1.5.0)。 JVM 线程 metric name metric tags specification jvm.threads.totalStarted \u0026mdash; jvm.threads.active \u0026mdash; jvm.threads.peak","tags":null,"title":"内置扩展 Metrics 指标","type":"projects","url":"/projects/sofa-lookout/client-ext-metrics/","wordcount":296},{"author":null,"categories":null,"content":" 分布式共识算法 (Consensus Algorithm) 如何理解分布式共识?  多个参与者 针对 某一件事 达成完全 一致 ：一件事，一个结论 已达成一致的结论，不可推翻  有哪些分布式共识算法?  Paxos：被认为是分布式共识算法的根本，其他都是其变种，但是 paxos 论文中只给出了单个提案的过程，并没有给出复制状态机中需要的 multi-paxos 的相关细节的描述，实现 paxos 具有很高的工程复杂度（如多点可写，允许日志空洞等） Zab：被应用在 zookeeper 中，业界使用广泛，但没有抽象成通用的 library Raft：以容易理解著称，业界也涌现出很多 raft 实现，比如大名鼎鼎的 etcd, braft, tikv 等  什么是 Raft？ Raft 是一种更易于理解的分布式共识算法，核心协议本质上还是师承 paxos 的精髓，不同的是依靠 raft 模块化的拆分以及更加简化的设计，raft 协议相对更容易实现。\n模块化的拆分主要体现在：Raft 把一致性协议划分为 Leader 选举、MemberShip 变更、日志复制、Snapshot 等几个几乎完全解耦的模块\n更加简化的设计则体现在：Raft 不允许类似 paxos 中的乱序提交、简化系统中的角色状态（只有 Leader、Follower、Candidate三种角色）、限制仅 Leader 可写入、使用随机化的超时时间来设计 Leader Election 等等\n特点：Strong Leader  系统中必须存在且同一时刻只能有一个 leader，只有 leader 可以接受 clients 发过来的请求 Leader 负责主动与所有 followers 通信，负责将\u0026amp;rsquo;提案\u0026amp;rsquo;发送给所有 followers，同时收集多数派的 followers 应答 Leader 还需向所有 followers 主动发送心跳维持领导地位(保持存在感)  一句话总结 Strong Leader: \u0026amp;ldquo;你们不要 BB! 按我说的做，做完了向我汇报!\u0026amp;rdquo; 另外，身为 leader 必须保持一直 BB(heartbeat) 的状态，否则就会有别人跳出来想要 BB\n复制状态机 对于一个无限增长的序列 a[1, 2, 3…]，如果对于任意整数 i，a[i] 的值满足分布式一致性，这个系统就满足一致性状态机的要求 基本上所有的真实系统都会有源源不断的操作，这时候单独对某个特定的值达成一致显然是不够的。为了让真实系统保证所有的副本的一致性，通常会把操作转化为 write-ahead-log(WAL)。然后让系统中所有副本对 WAL 保持一致，这样每个副本按照顺序执行 WAL 里的操作，就能保证最终的状态是一致的\n Client 向 leader 发送写请求 Leader 把\u0026amp;rsquo;操作\u0026amp;rsquo;转化为 WAL 写本地 log 的同时也将 log 复制到所有 followers Leader 收到多数派应答, 将 log 对应的\u0026amp;rsquo;操作\u0026amp;rsquo; 应用到状态机 回复 client 处理结果  Raft 中的基本概念 Raft-node 的 3 种角色/状态  Follower：完全被动，不能发送任何请求，只接受并响应来自 leader 和 candidate 的 message，每个节点启动后的初始状态一定是 follower Leader：处理所有来自客户端的请求，以及复制 log 到所有 followers Candidate：用来竞选一个新 leader （candidate 由 follower 触发超时而来）  Message 的 3 种类型  RequestVote RPC：由 candidate 发出，用于发送投票请求 AppendEntries (Heartbeat) RPC：由 leader 发出，用于 leader 向 followers 复制日志条目，也会用作 Heartbeat （日志条目为空即为 Heartbeat） InstallSnapshot RPC：由 leader 发出，用于快照传输，虽然多数情况都是每个服务器独立创建快照，但是leader 有时候必须发送快照给一些落后太多的 follower，这通常发生在 leader 已经丢弃了下一条要发给该follower 的日志条目(Log Compaction 时清除掉了) 的情况下  任期逻辑时钟  时间被划分为一个个任期 (term)，term id 按时间轴单调递增 每一个任期的开始都是 leader 选举，选举成功之后，leader 在任期内管理整个集群，也就是 \u0026amp;lsquo;选举 + 常规操作\u0026amp;rsquo; 每个任期最多一个 leader，可能没有 leader (spilt-vote 导致)  Raft 功能分解 Leader 选举  超时驱动：Heartbeat/Election timeout 随机的超时时间：降低选举碰撞导致选票被瓜分的概率 选举流程：  Follower \u0026amp;ndash;\u0026amp;gt; Candidate (选举超时触发)  赢得选举：Candidate \u0026amp;ndash;\u0026amp;gt; Leader 另一个节点赢得选举：Candidate \u0026amp;ndash;\u0026amp;gt; Follower 一段时间内没有任何节点器赢得选举：Candidate \u0026amp;ndash;\u0026amp;gt; Candidate   选举动作：  Current term++ 发送 RequestVote RPC  New Leader 选取原则 (最大提交原则)  Candidates include log info in RequestVote RPCs(index \u0026amp;amp; term of last log entry) During elections, choose candidate with log most likely to contain all committed entries Voting server V denies vote if its log is “more complete”: (lastTermV \u0026amp;gt; lastTermC) || ((lastTermV == lastTermC) \u0026amp;amp;\u0026amp;amp; (lastIndexV \u0026amp;gt; lastIndexC)) Leader will have “most complete” log among electing majority  安全性：一个 term，最多选出一个 leader，可以没 leader，下一个 term 再选   影响 raft 选举成功率的几个时间参数：  RTT(Round Trip Time)：网络延时 Heartbeat timeout：心跳间隔，通常应该比 election timeout 小一个数量级，目的是让 leader 能够持续发送心跳来阻止 followers 触发选举 Election timeout：Leader 与 followers …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/consistency-raft-jraft/","fuzzywordcount":8000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a0e98df1bec305cca7db6fc34fc97771","permalink":"/projects/sofa-jraft/consistency-raft-jraft/","publishdate":"0001-01-01T00:00:00Z","readingtime":16,"relpermalink":"/projects/sofa-jraft/consistency-raft-jraft/","summary":"分布式共识算法 (Consensus Algorithm) 如何理解分布式共识? 多个参与者 针对 某一件事 达成完全 一致 ：一件事，一个结论 已达成一致的结论，不可推翻 有哪些分布式共识算法? P","tags":null,"title":"分布式一致性 Raft 与 JRaft","type":"projects","url":"/projects/sofa-jraft/consistency-raft-jraft/","wordcount":7950},{"author":null,"categories":null,"content":" 单元测试 单元测试例子放到自己开发的模块下。\n如果依赖了第三方服务端（例如Zookeeper），请手动加入 profile。参考 registry-zookeeper 模块代码。\n如果依赖了其它模块要集成测试，请放到 test/test-intergrated 模块中。\n如果还依赖了第三方服务端（例如Zookeeper），请放到 test-intergrated-3rd 模块中。\n性能测试 关闭了以下默认开启项目：\n-Dcontext.attachment.enable=false -Dserialize.blacklist.enable=false -Ddefault.tracer= -Dlogger.impl=com.alipay.sofa.rpc.log.SLF4JLoggerImpl -Dmultiple.classloader.enable=false -Devent.bus.enable=false\n我们对 BOLT+hessian 进行了压测。\n服务端：4C8G 虚拟机，千M网络，jdk1.8.0_111；\n客户端：50个客户端并发请求。\n   协议 请求 响应 服务端 TPS 平均RT(ms)     bolt+hessian 1K String 1K String 直接返回 10000 1.93   bolt+hessian 1K String 1K String 直接返回 20000 4.13   bolt+hessian 1K String 1K String 直接返回 30000 7.32   bolt+hessian 1K String 1K String 直接返回 40000 15.78   bolt+hessian 1K String 1K String 直接返回 50000(接近极限,错误率0.3%） 26.51    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/test/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ccda7c2372a7f55d61f682b72d3b1dc2","permalink":"/projects/sofa-rpc/test/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/test/","summary":"单元测试 单元测试例子放到自己开发的模块下。 如果依赖了第三方服务端（例如Zookeeper），请手动加入 profile。参考 registry-zookeeper 模块代码。 如果依","tags":null,"title":"单元测试与性能测试","type":"projects","url":"/projects/sofa-rpc/test/","wordcount":292},{"author":null,"categories":null,"content":" 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。\n git 工具用法可以查看git 官方书籍,需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章Git 协作流程  GitHub 贡献代码流程 提交 issue 不论您是修复 ACTS 的 bug 还是新增 ACTS 的功能，在您提交代码之前，在 ACTS 的 GitHub 上提交一个 issue，描述您要修复的问题或者要增加的功能。这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作. ACTS 的维护人员会对您提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少 pull request 被拒绝的情况。  获取源码 要修改或新增功能，在提 issue 后，点击左上角的 fork 按钮，复制一份 ACTS 主干代码到您的代码仓库。\n拉分支 ACTS 所有修改都在分支上进行，修改完后提交 pull request， 在 Code Review 后由项目维护人员 Merge 到主干。 因此，在获取源码步骤介绍后，您需要：\n 下载代码到本地,这一步您可以选择 git/https 方式.\ngit clone https://github.com/您的账号名/acts.git  拉分支准备修改代码\ngit branch add_xxx_feature  执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：\ngit branch -a  如果您想切换回主干，执行下面命令:\ngit checkout -b master  如果您想切换回分支，执行下面命令：\ngit checkout -b \u0026amp;quot;branchName\u0026amp;quot;   修改代码提交到本地 拉完分支后，就可以修改代码了。\n 修改完代码后，执行如下命令提交所有修改到本地\ngit commit -am \u0026#39;添加xx功能\u0026#39;   修改代码注意事项  代码风格保持一致 ACTS 通过 Maven 插件来保持代码格式一致.在提交代码前,务必本地执行\nmvn clean compile  补充单元测试代码\n 新有修改应该通过已有的单元测试.\n 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug 您可以用如下命令运行所有测试\nmvn clean test  也可以通过 IDE 来辅助运行。\n  其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等. 对于无用的注释，请直接删除 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。 * 执行如下命令提交本地修改到 github 上\n``` git push origin \u0026amp;quot;branchname\u0026amp;quot; ```  如果前面您是通过 fork 来做的,那么这里的 origin 是 push 到您的代码仓库，而不是 ACTS 的代码仓库.\n提交合并代码到主干的请求 在的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 ACTS 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request按钮。选择目标分支,一般就是 master，系统会通知 ACTS 的人员， ACTS 人员会 Review 您的代码，符合要求后就会合入主干，成为 ACTS 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员.\n对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 ACTS 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/contributing/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cd68baede6258921f83665ef0a446f1f","permalink":"/projects/sofa-acts/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-acts/contributing/","summary":"准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 git 工具用法可以查看git 官方书籍,需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章G","tags":null,"title":"参与贡献","type":"projects","url":"/projects/sofa-acts/contributing/","wordcount":1224},{"author":null,"categories":null,"content":"  可以先去 发展路线 内了解下开发任务及未来规划。\n 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。\n git 工具用法可以查看 git官方书籍，需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章 Git协作流程。  GitHub 贡献代码流程 提交issue 不论您是修复 SOFAArk 的 bug 还是新增 SOFAArk 的功能，在您提交代码之前，在 SOFAArk 的 GitHub 地址上提交一个 issue，描述您要修复的问题或者要增加的功能。这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作. SOFAArk 的维护人员会对您提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少pull request被拒绝的情况。  获取源码 要修改或新增功能，在提 issue 后，点击左上角的fork按钮，复制一份 SOFAArk 主干代码到您的代码仓库。\n拉分支 SOFAArk 所有修改都在分支上进行，修改完后提交 pull request， 在 Code Review 后由项目维护人员 Merge 到主干。\n因此，在获取源码步骤介绍后，您需要：\n 下载代码到本地,这一步您可以选择git/https方式.\ngit clone https://github.com/您的账号名/sofa-ark.git  拉分支准备修改代码\ngit branch add_xxx_feature   执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：\n git branch -a  如果您想切换回主干，执行下面命令:\n git checkout -b master  如果您想切换回分支，执行下面命令：\n git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致  SOFAArk 通过 Maven 插件来保持代码格式一致.在提交代码前,务必本地执行\nmvn clean compile   补充单元测试代码 新有修改应该通过已有的单元测试. 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug  您可以用如下命令运行所有测试\n mvn clean test  也可以通过IDE来辅助运行。\n其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等. 对于无用的注释，请直接删除 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  修改完代码后，执行如下命令提交所有修改到本地:\n git commit -am \u0026#39;添加xx功能\u0026#39;  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到 github 上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面您是通过 fork 来做的,那么这里的 origin 是 push 到您的代码仓库，而不是 SOFAArk 的代码仓库.\n提交合并代码到主干的请求 在的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 SOFAArk 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request按钮。选择目标分支,一般就是 master，系统会通知 SOFAArk 的人员， SOFAArk 人员会 Review 您的代码，符合要求后就会合入主干，成为 SOFAArk 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员.\n对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 SOFAArk 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-contribution/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dbf77f98884a71c5c7a3fbb4dd189cfe","permalink":"/projects/sofa-boot/sofa-ark-contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-contribution/","summary":"可以先去 发展路线 内了解下开发任务及未来规划。 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 git 工具用法可以查看 git官方书籍，需要阅","tags":null,"title":"参与贡献","type":"projects","url":"/projects/sofa-boot/sofa-ark-contribution/","wordcount":1278},{"author":null,"categories":null,"content":"  可以先去 RoadMap 内了解下开发任务及未来规划。\n 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。\n git 工具用法可以查看 git官方书籍，需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章 Git协作流程  GitHub 贡献代码流程 提交issue 不论您是修复 SOFADashboard 的 bug 还是新增 SOFADashboard 的功能，在您提交代码之前，在 SOFADashboard 的 GitHub 上提交一个 issue，描述您要修复的问题或者要增加的功能。\n这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作. SOFADashboard 的维护人员会对您提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少 pull request 被拒绝的情况。  获取源码 要修改或新增功能，在提 issue 后，点击左上角的 fork 按钮，复制一份 SOFADashboard 主干代码到您的代码仓库。\n拉分支 SOFADashboard 所有修改都在分支上进行，修改完后提交 pull request， 在 Code Review 后由项目维护人员 Merge 到主干。 因此，在获取源码步骤介绍后，您需要：\n 下载代码到本地,这一步您可以选择 git/https 方式.\ngit clone https://github.com/您的账号名/sofa-dashboard.git   拉分支准备修改代码 bash git branch add_xxx_feature   执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：\ngit branch -a   如果您想切换回主干，执行下面命令: bash git checkout -b master   如果您想切换回分支，执行下面命令：\ngit checkout -b \u0026amp;quot;branchName\u0026amp;quot;  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致  SOFADashboard 通过 Maven插件来保持代码格式一致。在提交代码前，务必本地执行\nmvn clean compile  补充单元测试代码\n 新有修改应该通过已有的单元测试.\n 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug\n您可以用如下命令运行所有测试\nmvn clean test   也可以通过 IDE 来辅助运行。  其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等. 对于无用的注释，请直接删除 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  修改完代码后，执行如下命令提交所有修改到本地：\ngit commit -am \u0026#39;添加xx功能\u0026#39;   提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到 github 上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面您是通过 fork 来做的,那么这里的 origin 是 push 到您的代码仓库，而不是 SOFADashboard 的代码仓库。\n提交合并代码到主干的请求 在的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 SOFADashboard 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request按钮。选择目标分支,一般就是 master，系统会通知 SOFADashboard 的人员， SOFADashboard 人员会 Review 您的代码，符合要求后就会合入主干，成为 SOFADashboard 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员。 对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 SOFADashboard 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/contribution/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"584584be9c13f2d36c85890dd192368a","permalink":"/projects/sofa-dashboard/contribution/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-dashboard/contribution/","summary":"可以先去 RoadMap 内了解下开发任务及未来规划。 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 git 工具用法可以查看 git官方书籍，需要阅读前几","tags":null,"title":"参与贡献","type":"projects","url":"/projects/sofa-dashboard/contribution/","wordcount":1262},{"author":null,"categories":null,"content":"  可以先去 发展路线 \u0026amp;amp; 任务认领 内了解下开发任务及未来规划。\n 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。\n git 工具用法可以查看git官方书籍,需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章 Git协作流程  GitHub 贡献代码流程 提交issue 不论您是修复 SOFARegistry 的 bug 还是新增 SOFARegistry 的功能，在您提交代码之前，在 SOFARegistry 的 GitHub 上提交一个 issue，描述您要修复的问题或者要增加的功能。这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作. SOFARegistry 的维护人员会对您提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少 pull request 被拒绝的情况。  获取源码 要修改或新增功能，在提 issue 后，点击左上角的 fork 按钮，复制一份 SOFARegistry 主干代码到您的代码仓库。\n拉分支 SOFARegistry 所有修改都在分支上进行，修改完后提交 pull request， 在 Code Review 后由项目维护人员 Merge 到主干。\n因此，在获取源码步骤介绍后，您需要：\n 下载代码到本地,这一步您可以选择 git/https 方式.  git clone https://github.com/您的账号名/sofa-registry.git   拉分支准备修改代码  git branch add_xxx_feature  执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：\ngit branch -a  如果您想切换回主干，执行下面命令:\ngit checkout -b master  如果您想切换回分支，执行下面命令：\ngit checkout -b \u0026amp;quot;branchName\u0026amp;quot;  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致  SOFARegistry 通过 Maven插件来保持代码格式一致.在提交代码前,务必本地执行\nmvn clean compile   补充单元测试代码 新有修改应该通过已有的单元测试. 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug您可以用如下命令运行所有测试  mvn clean test  也可以通过 IDE 来辅助运行。\n其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等. 对于无用的注释，请直接删除 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  修改完代码后，执行如下命令提交所有修改到本地:\ngit commit -am \u0026#39;添加xx功能\u0026#39;  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到 github 上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面您是通过 fork 来做的,那么这里的 origin 是 push 到您的代码仓库，而不是 SOFARegistry 的代码仓库.\n提交合并代码到主干的请求 在您的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 SOFARegistry 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request 按钮。选择目标分支,一般就是 master，系统会通知 SOFARegistry 的人员， SOFARegistry 人员会 Review 您的代码，符合要求后就会合入主干，成为 SOFARegistry 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员。\n对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 SOFARegistry 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/contributing/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c08b5945719137833634c111c43a8d9e","permalink":"/projects/sofa-registry/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-registry/contributing/","summary":"可以先去 发展路线 \u0026amp; 任务认领 内了解下开发任务及未来规划。 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 git 工具用法可以查看git官方书","tags":null,"title":"参与贡献","type":"projects","url":"/projects/sofa-registry/contributing/","wordcount":1264},{"author":null,"categories":null,"content":"  可以先去 发展路线 内了解下开发任务及未来规划。\n 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。\n git 工具用法可以查看git官方书籍,需要阅读前几章来熟悉。 git 协作流程可以查看这篇文章Git协作流程  GitHub 贡献代码流程 提交issue 不论您是修复 SOFARPC 的 bug 还是新增 SOFARPC 的功能，在您提交代码之前，在 SOFARPC 的GitHub上提交一个 issue，描述您要修复的问题或者要增加的功能。这么做有几个好处:\n 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作. SOFARPC 的维护人员会对您提的bug或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 在达成一致后再开发,并提交代码，减少双方沟通成本，也减少pull request被拒绝的情况。  获取源码 要修改或新增功能，在提 issue 后，点击左上角的fork按钮，复制一份 SOFARPC 主干代码到您的代码仓库。\n拉分支 SOFARPC 所有修改都在分支上进行，修改完后提交 pull request， 在 Code Review 后由项目维护人员 Merge 到主干。\n因此，在获取源码步骤介绍后，您需要：\n 下载代码到本地,这一步您可以选择git/https方式.\ngit clone https://github.com/您的账号名/sofa-rpc.git  拉分支准备修改代码\ngit branch add_xxx_feature   执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：\n git branch -a  如果您想切换回主干，执行下面命令:\n git checkout -b master  如果您想切换回分支，执行下面命令：\n git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致  SOFARPC 通过 Maven插件来保持代码格式一致.在提交代码前,务必本地执行\nmvn clean compile   补充单元测试代码 新有修改应该通过已有的单元测试. 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug  您可以用如下命令运行所有测试\n mvn clean test  也可以通过IDE来辅助运行。\n其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等. 对于无用的注释，请直接删除 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  修改完代码后，执行如下命令提交所有修改到本地:\n git commit -am \u0026#39;添加xx功能\u0026#39;  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到 github 上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面您是通过 fork 来做的,那么这里的 origin 是 push 到您的代码仓库，而不是 SOFARPC 的代码仓库.\n提交合并代码到主干的请求 在的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 SOFARPC 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request按钮。选择目标分支,一般就是 master，系统会通知 SOFARPC 的人员， SOFARPC 人员会 Review 您的代码，符合要求后就会合入主干，成为 SOFARPC 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员.\n对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 SOFARPC 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/contributing/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"448a7b9a949bd2d9e2e71ac6c237f9df","permalink":"/projects/sofa-rpc/contributing/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-rpc/contributing/","summary":"可以先去 发展路线 内了解下开发任务及未来规划。 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 git 工具用法可以查看git官方书籍,需要阅","tags":null,"title":"参与贡献","type":"projects","url":"/projects/sofa-rpc/contributing/","wordcount":1284},{"author":null,"categories":null,"content":" 客户端发展规划  v2.0 ，在基于 Java 8 进行重构，v1.0 为支持 Java 6 做了些设计与性能妥协； 集成更多的开源产品；  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/plan/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6153fc1f5e000f195d96dfdb03c5b381","permalink":"/projects/sofa-lookout/plan/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/plan/","summary":"客户端发展规划 v2.0 ，在基于 Java 8 进行重构，v1.0 为支持 Java 6 做了些设计与性能妥协； 集成更多的开源产品；","tags":null,"title":"发展规划","type":"projects","url":"/projects/sofa-lookout/plan/","wordcount":49},{"author":null,"categories":null,"content":" 任务列表 下面表格记录了还没有实现的功能特性，欢迎大家认领任务，参与贡献。\n   类型 任务 困难度 认领人及时间 计划完成时间 进度 相关 Issue     代码 支持多个 Web 应用合并部署，采用多 Host/单 Host 两种模式 难       代码 支持 telnet 指令查看 ark plugin 简单       代码 支持 telnet 指令查看 jvm/rpc 服务 中        版本迭代计划 v0.5.0  支持多个 Web 应用合并部署，采用多 Host/单 Host 两种模式  v0.6.0  支持 telnet 指令查看 ark plugin； 支持 telnet 指令查看 jvm/rpc 服务；  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-roadmap/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c4532d11cef15d8fe3ff5e04c7b08f90","permalink":"/projects/sofa-boot/sofa-ark-roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/sofa-ark-roadmap/","summary":"任务列表 下面表格记录了还没有实现的功能特性，欢迎大家认领任务，参与贡献。 类型 任务 困难度 认领人及时间 计划完成时间 进度 相关 Issue 代码 支持多个 Web 应用合","tags":null,"title":"发展路线","type":"projects","url":"/projects/sofa-boot/sofa-ark-roadmap/","wordcount":175},{"author":null,"categories":null,"content":" 任务列表 部分内部已有的功能特性，待内部整理完毕后随各个迭代放出。\n如果还没有实现的功能特性会列在下面的表格中，欢迎大家认领任务，参与贡献。\n   类型 任务 困难度 认领人及时间 计划完成时间 进度 相关Issue     文档 文档翻译 低       代码 弹性长连接管理方式 低    #56   代码 etcd注册中心实现 中 @wynn5a\n2018-6   #153   代码 eureka注册中心实现 中 @liufeiit\n2018-4   #52   代码 gRPC 支持 高    #57   代码 CXF 协议 高    #58   代码 TLS 支持 高        版本迭代计划 v5.5.0  JSON 序列化支持 H2的TLS安全支持 弹性连接池 hystrix集成 Consul注册中心支持  v5.6.0  grpc 通讯层支持 etcd注册中心支持 SofaMesh支持 BOLT 版本协商与 CRC 校验  v5.7.0  Telnet 内置指令支持 SpringBoot 2.0 支持 Mock功能支持 加密功能支持  v5.8.0  授权支持 SofaRegistry 支持 Reactive 支持  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/roadmap/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6064fc180911f520f6d1590b88595693","permalink":"/projects/sofa-rpc/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/roadmap/","summary":"任务列表 部分内部已有的功能特性，待内部整理完毕后随各个迭代放出。 如果还没有实现的功能特性会列在下面的表格中，欢迎大家认领任务，参与贡献。 类型","tags":null,"title":"发展路线","type":"projects","url":"/projects/sofa-rpc/roadmap/","wordcount":292},{"author":null,"categories":null,"content":" 任务列表 欢迎大家领取任务参与贡献。\n   类型 任务 困难度 认领人及时间 计划发布时间 计划完成时间 进度 相关 issue     代码 SOFATracer 性能优化专题 高     issue 18和 issue 11   代码 SOFATracer 支持 HttpClient 中    已经完成，见 HttpClient 接入文档    代码 SOFATracer 数据汇报能力提供在非 SOFABoot 框架下的运行和配置能力 中    已经完成，见 Spring 工程中使用 SOFATracer issue 32   代码 SOFATracer 提供采样能力 中    已经完成，见使用 SOFATracer 的采样能力 issue 10   代码 SOFATracer 支持 Zipkin 2.X.X 版本 中    已经完成，见使用 SOFATracer 远程汇报数据到 Zipkin issue 23   代码 SOFATracer 支持 Druid 中    已经完成，见 DataSource 接入文档    代码 SOFATracer 支持 c3p0 中    已经完成    代码 SOFATracer 支持 Tomcat-JDBC 中    已经完成    代码 SOFATracer 支持 HikariCP 中    已经完成    代码 SOFATracer 支持 dbcp 中    已经完成，见 DataSource 接入文档    代码 SOFATracer 支持 dbcp2 中    已经完成    代码 SOFATracer 支持 Sharding-JDBC 中        代码 SOFATracer 支持 Mysql JDBC Driver 中        代码 SOFATracer 支持 Oracle JDBC Driver 中        代码 SOFATracer 支持 Dubbo 中        代码 SOFATracer 支持 RestTemplate 和 AsyncRestTemplate 中    已经完成    代码 SOFATracer 支持标准Servlet 中    已经完成，见对于标准 servlet 容器的支持（ tomcat/jetty 等）    代码 SOFATracer 支持单机版链路分析并给用户通过注解使用的埋点方式，数据汇报到 Zipkin 中        代码 SOFATracer 支持 Kafka 中        代码 SOFATracer 支持 Redis 中        代码 SOFATracer 支持 hystrix 中        文档 文档翻译 中         版本迭代计划 2.2.0  SOFATracer 支持 JDBC 数据源  SOFATracer 支持 Mysql Driver SOFATracer 支持 Sharding-JDBC SOFATracer 支持 Mysql-JDBC SOFATracer 支持 Druid SOFATracer 支持 c3p0 SOFATracer 支持 Tomcat-JDBC SOFATracer 支持 HikariCP  SOFATracer 支持 HttpClient SOFATracer 支持 Zipkin 2.X.X 版本，开发验证并测试  2.3.0  SOFATracer 支持RestTemplate 和 AsyncRestTemplate SOFATracer 支持提供采样能力 SOFATracer 支持标准 servlet 容器 SOFATracer 支持 Zipkin UI 中文界面 SOFATracer 支持数据汇报能力提供在非 SOFABoot 框架下的运行和配置能力  2.4.0  SOFATracer 支持 Dubbo SOFATracer 支持 Kafka SOFATracer 性能优化专题  2.5.0  SOFATracer 支持单机版链路分析并给用户通过注解使用的埋点方式，数据汇报到 Zipkin 展示 SOFATracer 支持 manual report SOFATracer 支持 基于 Opentracing API 埋点方式 SOFATracer 支持 Opentracing 0.30.0+ 版本  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/roadmap/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8b0a6fbe5f6ea5ae789f5186271073c3","permalink":"/projects/sofa-tracer/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/roadmap/","summary":"任务列表 欢迎大家领取任务参与贡献。 类型 任务 困难度 认领人及时间 计划发布时间 计划完成时间 进度 相关 issue 代码 SOFATracer 性能优化专题 高 issue 18和 issue 11 代码 SOFATracer 支持 HttpClient 中","tags":null,"title":"发展路线","type":"projects","url":"/projects/sofa-tracer/roadmap/","wordcount":605},{"author":null,"categories":null,"content":" 发展路线 任务列表 部分内部已有的功能特性，待内部整理完毕后随各个迭代放出。\n如果还没有实现的功能特性会列在下面的表格中，欢迎大家认领任务，参与贡献。\n   类型 任务 困难度 认领人及时间 计划完成时间 进度 相关issue     文档 文档翻译 低       代码 支持Spring Cloud 中       代码 数据自检 高       代码 黑名单过滤 中       代码 SOFARegistry Dashboard 高       代码 支持其他微服务框架 中       代码 支持 Docker \u0026amp;amp; Kubernetes 高       代码 多语言客户端支持 高        版本迭代计划 v5.3.0  支持 Spring Cloud 数据自检 黑名单过滤  v5.4.0  SOFARegistry Dashboard 支持其他微服务框架  v5.5.0  支持 Docker \u0026amp;amp; Kubernetes 多语言客户端支持  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/roadmap/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b0ab45d52ba3eb7db590a4f5e4197c9e","permalink":"/projects/sofa-registry/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-registry/roadmap/","summary":"发展路线 任务列表 部分内部已有的功能特性，待内部整理完毕后随各个迭代放出。 如果还没有实现的功能特性会列在下面的表格中，欢迎大家认领任务，参与贡","tags":null,"title":"发展路线 \u0026 任务认领","type":"projects","url":"/projects/sofa-registry/roadmap/","wordcount":216},{"author":null,"categories":null,"content":"更多参见：https://github.com/mosn/mosn/blob/master/CHANGELOG_ZH.md\n","date":-62135596800,"description":"","dir":"projects/mosn/release-notes/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"62efb8e40401ab4612bcccaa6e942c97","permalink":"/projects/mosn/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/release-notes/","summary":"更多参见：https://github.com/mosn/mosn/blob/master/CHANGELOG_ZH.md","tags":null,"title":"发布历史","type":"projects","url":"/projects/mosn/release-notes/","wordcount":61},{"author":null,"categories":null,"content":"更多参见：https://github.com/sofastack/sofa-rpc/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/release-notes/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ab7d46caa6906863103b77b742ec7e84","permalink":"/projects/sofa-rpc/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/release-notes/","summary":"更多参见：https://github.com/sofastack/sofa-rpc/releases","tags":null,"title":"发布历史","type":"projects","url":"/projects/sofa-rpc/release-notes/","wordcount":51},{"author":null,"categories":null,"content":"更多参见：https://github.com/sofastack/sofa-ark/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-release/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"994c3569ea416ee5b0dea253f08af6be","permalink":"/projects/sofa-boot/sofa-ark-release/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/sofa-ark-release/","summary":"更多参见：https://github.com/sofastack/sofa-ark/releases","tags":null,"title":"发布说明","type":"projects","url":"/projects/sofa-boot/sofa-ark-release/","wordcount":51},{"author":null,"categories":null,"content":"更多参见：https://github.com/sofastack/sofa-dashboard/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/release-node/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3c8e6985123810c9692f47cc56b50081","permalink":"/projects/sofa-dashboard/release-node/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/release-node/","summary":"更多参见：https://github.com/sofastack/sofa-dashboard/releases","tags":null,"title":"发布说明","type":"projects","url":"/projects/sofa-dashboard/release-node/","wordcount":57},{"author":null,"categories":null,"content":"更多参见：https://github.com/sofastack/sofa-registry/releases\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/release-notes/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d92dddf77bbbd6078f3f96ba2224a53d","permalink":"/projects/sofa-registry/release-notes/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-registry/release-notes/","summary":"更多参见：https://github.com/sofastack/sofa-registry/releases","tags":null,"title":"发布说明","type":"projects","url":"/projects/sofa-registry/release-notes/","wordcount":56},{"author":null,"categories":null,"content":" SOFABoot 提供了模块并行加载以及 Spring Bean 异步初始化能力，用于加快应用启动速度。模块并行加载参考相应文档，下面介绍如何使用 SOFABoot 异步初始化 Spring Bean 能力来提高应用启动速度。\n引入依赖 SOFABoot 在 v2.6.0 开始提供异步初始化 Spring Bean 能力，引入如下 Starter 即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;runtime-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  使用场景 在实际使用 Spring/Spring Boot 开发中，会有一些 Bean 在初始化过程中执行准备操作，如拉取远程配置、初始化数据源等等；在应用启动期间，这类 Bean 会增加 Spring 上下文刷新时间，导致应用启动耗时变长。为了加速应用启动，SOFABoot 通过配置可选项，将 Bean 的初始化方法(init-method) 使用单独线程异步执行，加快 Spring 上下文加载过程，提高应用启动速度。\n使用方法 异步初始化 Bean 的原理是开启单独线程负责执行 Bean 的初始化方法(init-method)，因此在使用过程中，除了引入上述依赖管理，还需要在 Bean 的 xml 定义中配置 sofa:async-init=\u0026amp;quot;true\u0026amp;quot; 属性，用于指定是否异步执行该 Bean 的初始化方法，例如：\n\u0026amp;lt;?xml version=\u0026amp;quot;1.0\u0026amp;quot; encoding=\u0026amp;quot;UTF-8\u0026amp;quot;?\u0026amp;gt; \u0026amp;lt;beans xmlns=\u0026amp;quot;http://www.springframework.org/schema/beans\u0026amp;quot; xmlns:xsi=\u0026amp;quot;http://www.w3.org/2001/XMLSchema-instance\u0026amp;quot; xmlns:sofa=\u0026amp;quot;http://sofastack.io/schema/sofaboot\u0026amp;quot; xsi:schemaLocation=\u0026amp;quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://sofastack.io/schema/sofaboot http://sofastack.io/schema/sofaboot.xsd\u0026amp;quot; default-autowire=\u0026amp;quot;byName\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;!-- async init test --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;testBean\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.runtime.beans.TimeWasteBean\u0026amp;quot; init-method=\u0026amp;quot;init\u0026amp;quot; sofa:async-init=\u0026amp;quot;true\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/beans\u0026amp;gt;  配置 SOFABoot 异步初始化能力提供两个属性配置，用于指定负责异步执行 Bean 初始化方法(init-method)的线程池大小： + com.alipay.sofa.boot.asyncInitBeanCoreSize \u0026amp;gt; 线程池基本大小，默认值为 CPU 核数加一 + com.alipay.sofa.boot.asyncInitBeanMaxSize \u0026amp;gt; 线程池中允许的最大线程数大小，默认值为 CPU 核数加一\n配置可以通过 VM -D 参数或者 Spring Boot 配置文件 application.yml 设置。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/speed-up-startup/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ebd933894c7828948b87610d1d0ca020","permalink":"/projects/sofa-boot/speed-up-startup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-boot/speed-up-startup/","summary":"SOFABoot 提供了模块并行加载以及 Spring Bean 异步初始化能力，用于加快应用启动速度。模块并行加载参考相应文档，下面介绍如何使用 SOFABoot 异步初始化 Spring Bean 能力来提高应用启","tags":null,"title":"启动加速","type":"projects","url":"/projects/sofa-boot/speed-up-startup/","wordcount":519},{"author":null,"categories":null,"content":" 团队成员（排名不分先后）    名字 github 角色 所在公司     肖宇 yu199195 VP 京东   张永伦 tuohai666 committer 京东   赵俊 cherrylzhao committer 联通   陈斌 prFor committer 某创业公司   李浪 cysy-lli committer 携程   汤煜冬 tydhot committer perfma    ","date":-62135596800,"description":"团队介绍","dir":"projects/hmily/team/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7d27bab44ea88b422afcda3ff9b66b36","permalink":"/projects/hmily/team/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/team/","summary":"团队成员（排名不分先后） 名字 github 角色 所在公司 肖宇 yu199195 VP 京东 张永伦 tuohai666 committer 京东 赵俊 cherrylzhao committer 联通 陈斌 prFor committer 某创业公司 李浪 cysy-lli committer 携程 汤煜冬 tydhot committer perfma","tags":null,"title":"团队介绍","type":"projects","url":"/projects/hmily/team/","wordcount":61},{"author":null,"categories":null,"content":" 本文旨在描述如何在 Kubernetes 快速开始安装和配置 Istio。 SOFA Mosn 不仅可以支持 Istio 标准的部署模式，也能支持单方面的 Inbound Sidecar，Outbound Sidecar的部署模式，满足用户的各种需求。\n前置要求  Docker Docker Compose  安装步骤  下载最新的 release 包 解压安装文件，并且进入解压后的路径，安装路径包含： 示例应用路径 samples/ /bin 路径下应该能找到 istioctl 客户端可执行文件，istioctl 可用于创建路由规则和策略 配置文件 istion.VERSION 把 Istio 的 bin 路径添加到系统的 PATH。比如，在 MacOS 或者 Linux 系统下执行如下命令：\nexport PATH=$PWD/bin;$PATH   安装helm 创建命名空间 SHELL kubectl create namespace istio-system   使用helm安装istio CRD\nhelm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f -   使用helm安装各个组件 SHELL helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl apply -f -   确认所有 pod 都在运行中：\nkubectl get pod -n istio-system  如果 Istio pilot 容器意外终止，确保运行 istioctl context-create 命令，并且重新执行上一个命令。\n部署应用程序 现在开始部署 Bookinfo 示例程序 为 default 命名空间打上标签 istio-injection=enabled，实现 Sidecar 自动注入\nkubectl label namespace default istio-injection=enabled   使用 kubectl 部署Bookinfo的服务\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml  确认所有的服务和 Pod 都已经正确的定义和启动\nkubectl get services kubectl get pods  卸载 Istio helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl delete -f - kubectl delete namespace istio-system  ","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"1de4868fa0e9c73d932343847864d7fb","permalink":"/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","summary":"本文旨在描述如何在 Kubernetes 快速开始安装和配置 Istio。 SOFA Mosn 不仅可以支持 Istio 标准的部署模式，也能支持单方面的 Inbound Sidecar，Outbound Sid","tags":null,"title":"在 Kubernetes 中快速开始","type":"projects","url":"/projects/sofa-mesh/pilot-setup-zookeeper-quick-start-docker/","wordcount":465},{"author":null,"categories":null,"content":" 本文旨在描述如何在 Kubernetes 快速开始安装和配置 Istio。\nSOFA Mosn 不仅可以支持 Istio 标准的部署模式，也能支持单方面的 Inbound Sidecar，Outbound Sidecar的部署模式，满足用户的各种需求。\n前置要求  Kubernetes 安装 Helm  安装步骤 Step 1. 下载最新的 release 包 Step 2. 把 Istio 的 bin 路径添加到系统的 PATH。比如，在 Linux 系统下执行如下命令：\nexport PATH=$PWD/bin;$PATH  Step 3. 创建命名空间\nkubectl create namespace istio-system  Step 4. 使用helm安装istio CRD\nhelm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f -  Step 5. 使用helm安装各个组件\nhelm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl apply -f -  Step 6. 确认所有 pod 都在运行中\nkubectl get pod -n istio-system  部署应用程序 现在开始部署 Bookinfo 示例程序。\n为 default 命名空间打上标签 istio-injection=enabled，实现 Sidecar 自动注入：\nkubectl label namespace default istio-injection=enabled  使用 kubectl 部署Bookinfo的服务：\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml  确认所有的服务和 Pod 都已经正确的定义和启动：\nkubectl get services kubectl get pods  卸载 Istio helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl delete -f - kubectl delete namespace istio-system  ","date":-62135596800,"description":"","dir":"projects/sofa-mesh/sofa-mesh-setup/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"940d012b4883e5c91bf777916cd3c6b3","permalink":"/projects/sofa-mesh/sofa-mesh-setup/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/sofa-mesh-setup/","summary":"本文旨在描述如何在 Kubernetes 快速开始安装和配置 Istio。 SOFA Mosn 不仅可以支持 Istio 标准的部署模式，也能支持单方面的 Inbound Sidecar，Outbound Sid","tags":null,"title":"在 Kubernetes 中快速开始","type":"projects","url":"/projects/sofa-mesh/sofa-mesh-setup/","wordcount":362},{"author":null,"categories":null,"content":"","date":-62135596800,"description":"使用该指南您可以体验到快速创建 Serveless 应用、根据业务请求秒级 0-1-N 自动伸缩、通过日志查看器快速排错、按时间触发应用等产品新功能。","dir":"guides/kc-serverless-demo/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f355d1b598fed47b730bd74ad25f3683","permalink":"/guides/kc-serverless-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":0,"relpermalink":"/guides/kc-serverless-demo/","summary":"","tags":null,"title":"基于 Serverless 轻松构建云上应用","type":"guides","url":"/guides/kc-serverless-demo/","wordcount":0},{"author":null,"categories":null,"content":" Ark 包 SOFAArk 定义特殊格式的可执行 Jar 包，使用官方提供的 Maven 插件 sofa-ark-maven-plugin 可以将工程应用打包成一个标准格式的 Ark 包；使用命令 java -jar 即可在 SOFAArk 容器之上启动应用；Ark 包 通常包含 Ark Container 、Ark Plugin 依赖（如果有）、合并部署的 Ark Biz （如果有）以及应用自身的 Ark Biz；详情可参考 Ark 包；\nArk Container Ark 容器，Ark Plugin 和 Ark Biz 运行在 SOFAArk 容器之上；容器具备管理多插件、多应用的功能；容器启动成功后，会解析 Ark Plugin 和 Ark Biz 配置，完成隔离加载并按优先级依次启动之；Ark Container 一般不会被用户直接感知，由打包插件 sofa-ark-maven-plugin 自动打入。详情可参考 SOFAArk 容器启动；\nArk Plugin Ark 插件，SOFAArk 定义特殊格式的 Fat Jar，使用官方提供的 Maven 插件 sofa-ark-plugin-maven-plugin 可以将一个或多个普通的 Java Jar 包打包成一个标准格式的 Ark Plugin； Ark Plugin 会包含一份配置文件，通常包括插件类和资源的导入导出配置、插件启动优先级等；运行时，Ark 容器会使用独立的 PluginClassLoader 加载插件，并根据插件配置构建类加载索引表，从而使插件与插件、插件与应用之间相互隔离；详情可参考 Ark Plugin；\nArk Biz Ark 模块，SOFAArk 定义特殊格式的 Fat Jar ，使用官方提供的 Maven 插件 sofa-ark-maven-plugin 可以将工程应用打包成一个标准格式的 Ark Biz 包；作用有二点，一、在 Ark 包 中，作为工程应用模块及其依赖包的组织单元；二、可以被其他应用当成普通 Jar 包依赖，用于在同一个 SOFAArk 容器启动多个 Ark Biz；多个 Ark Biz 共享 Ark Container 和 Ark Plugin ；详情可参考 Ark Biz；\n合并部署 SOFAArk 允许将多个应用（Biz 包）合并打入到 Ark 包中，当启动 Ark 包时，会启动所有应用；也支持在运行时通过 API 或者配置中心（例如 Zookeeper）动态的部署和卸载应用，这些应用同时运行在同一个 JVM 中，由独立的 BizClassLoader 加载，各应用之间通过 SofaService/SofaReference 实现交互，称之为多应用的合并部署。\n宿主应用 宿主应用是相对合并部署而言，在打包 Ark 包时，至少有一个 Biz 包被打入，如果应用引入了其他 Biz 包，则 Ark 包中会存在多个 Biz 包。当只有一个 Biz 包时，默认将其设置为宿主应用；如果存在多个 Biz 包，则需要配置指定宿主应用。宿主应用相对其他 Biz 包最大的不同，即不允许被卸载。\n简单总结下，在 SOFAArk 框架中，应用(配置、源码、依赖)被打包成 Biz 包组织在一起，但是特殊的依赖（Ark Plugin 和其他应用 Biz 包）不会被打入 Biz 包中，Biz 包是不可执行的 Fat Jar; Ark Plugin 是特殊的二方包，可以将多个二方依赖打包成 Plugin，运行时由独立的 PluginClassLoader 加载，根据打包时配置的导出导入资源、类，构建运行时类加载模型；Ark 包是可执行 Fat Jar，一般由 Ark Container、Ark Plugin(0个或多个)、Ark Biz(至少一个)。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-terminology/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b6d0ed10afe9d04bc00307017ffba7c5","permalink":"/projects/sofa-boot/sofa-ark-terminology/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-terminology/","summary":"Ark 包 SOFAArk 定义特殊格式的可执行 Jar 包，使用官方提供的 Maven 插件 sofa-ark-maven-plugin 可以将工程应用打包成一个标准格式的 Ark 包；使用命令 java -jar 即可在 SOFAArk 容器之上启动应用；Ark 包","tags":null,"title":"基础术语","type":"projects","url":"/projects/sofa-boot/sofa-ark-terminology/","wordcount":1015},{"author":null,"categories":null,"content":" 业界通用术语    术语 说明     服务（Service） 通过网络提供的、具有特定业务逻辑处理能力的软件功能。   服务提供者（Service Provider） 通过网络提供服务的计算机节点。   服务消费者（Service Consumer） 通过网络调用服务的计算机节点。一个计算机节点可以既作为一些服务的提供者，又作为一些服务的消费者。   服务发现（Service Discovery） 服务消费者获取服务提供者的网络地址的过程。   服务注册中心（Service Registry） 一种提供服务发现功能的软件系统，帮助服务消费者获取服务提供者的网络地址。   数据中心（Data Center） 物理位置、供电、网络具备一定独立性的物理区域，通常作为高可用设计的重要考量粒度。一般可认为：同一数据中心内，网络质量较高、网络传输延时较低、同时遇到灾难的概率较大；不同数据中心间，网络质量较低、网络延时较高、同时遇到灾难的概率较小。    SOFARegistry 约定术语    术语 说明     SOFARegistry 蚂蚁金服开源的一款服务注册中心产品，基于“发布-订阅”模式实现服务发现功能。同时它并不假定总是用于服务发现，也可用于其他更一般的“发布-订阅”场景。   数据（Data） 在服务发现场景下，特指服务提供者的网络地址及其它附加信息。其他场景下，也可以表示任意发布到 SOFARegistry 的信息。   单元（Zone） 单元化架构关键概念，在服务发现场景下，单元是一组发布与订阅的集合，发布及订阅服务时需指定单元名，更多内容可参考异地多活单元化架构解决方案。   发布者（Publisher） 发布数据到 SOFARegistry 的节点。在服务发现场景下，服务提供者就是“服务提供者的网络地址及其它附加信息”的发布者。   订阅者（Subscriber） 从 SOFARegistry 订阅数据的节点。在服务发现场景下，服务消费者就是“服务提供者的网络地址及其它附加信息”的订阅者。   数据标识（DataId） 用来标识数据的字符串。在服务发现场景下，通常由服务接口名、协议、版本号等信息组成，作为服务的标识。   分组标识（GroupId） 用于为数据归类的字符串，可以作为数据标识的命名空间，即只有 DataId、GroupId、InstanceId 都相同的服务，才属于同一服务。   实例 ID（InstanceId） 实例 ID，可以作为数据标识的命名空间，即只有DataId、GroupId、InstanceId都相同的服务，才属于同一服务。   会话服务器（SessionServer） SOFARegistry 内部负责跟客户端建立 TCP 长连接、进行数据交互的一种服务器角色。   数据服务器（DataServer） SOFARegistry 内部负责数据存储的一种服务器角色。   元信息服务器（MetaServer） SOFARegistry 内部基于 Raft 协议，负责集群内一致性协调的一种服务器角色。    ","date":-62135596800,"description":"","dir":"projects/sofa-registry/terminology/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b678a49547c55f2a70e2d94dbce5b4a2","permalink":"/projects/sofa-registry/terminology/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-registry/terminology/","summary":"业界通用术语 术语 说明 服务（Service） 通过网络提供的、具有特定业务逻辑处理能力的软件功能。 服务提供者（Service Provider） 通","tags":null,"title":"基础术语","type":"projects","url":"/projects/sofa-registry/terminology/","wordcount":1089},{"author":null,"categories":null,"content":"   名词 说明     TraceId TraceId 指的是 SOFATracer 中代表唯一一次请求的 ID，此 ID 一般由集群中第一个处理请求的系统产生，并在分布式调用下通过网络传递到下一个被请求系统。   SpanId SpanId 代表了本次请求在整个调用链路中的位置或者说层次，比如 A 系统在处理一个请求的过程中依次调用了 B，C，D 三个系统，那么这三次调用的的 SpanId 分别是：0.1，0.2，0.3。如果 B 系统继续调用了 E，F 两个系统，那么这两次调用的 SpanId 分别是：0.1.1，0.1.2。    其他相关的名词解释可以参考 OpenTracing 规范。\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/explanation/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8ba307b0679e918f7ac68c7efb7e53f7","permalink":"/projects/sofa-tracer/explanation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/explanation/","summary":"名词 说明 TraceId TraceId 指的是 SOFATracer 中代表唯一一次请求的 ID，此 ID 一般由集群中第一个处理请求的系统产生，并在分布式调用下通过网络传递到下一个被请求系统。 SpanId SpanId","tags":null,"title":"基础术语","type":"projects","url":"/projects/sofa-tracer/explanation/","wordcount":211},{"author":null,"categories":null,"content":" 消息 内部全部使用 SofaRequest 和 SofaResponse 进行传递。\n如果需要转换为其它协议，那么在真正调用和收到请求的时候，转换为实际要传输的对象。\n可以对 SofaRequest 和 SofaResponse 进行写操作的模块： - Invoker - Filter - ServerHandler - Serialization\n对消息体是只读的模块： - Cluster - Router - LoadBalance\n日志 日志的初始化也是基于扩展机制。虽然是扩展，但是由于日志的加载应该是最早的，所以在 rpc-config.json 里有一个单独的 Key。\n{ // 日志实现，日志早于配置加载，所以不能适应Extension机制 \u0026amp;quot;logger.impl\u0026amp;quot;: \u0026amp;quot;com.alipay.sofa.rpc.log.MiddlewareLoggerImpl\u0026amp;quot; }  配置项 使用者的 RPC 配置 用户的配置，例如端口配置（虽然已经开放对象中设置端口的字段，但是sofa默认是从配置文件里取的），线程池大小配置等。 - 通过 SofaConfigs 加载配置，调用 ExternalConfigLoader 读取外部属性。 - 通过 SofaConfigs 提供的 API 进行获取。 - 所有内部配置的Key都在 SofaOptions 类 - 优先级： System.property \u0026amp;gt; sofa-config.properties(每个应用一个) \u0026amp;gt; rpc-config.properties\nRPC 框架配置 框架自身的配置，例如默认序列化，默认超时等。 未来要一个ClassLoader一个。 - 通过 RpcConfigs 加载配置文件。 - 通过 RpcConfigs 其提供的API进行获取和监听数据变化 - 所有内部配置的Key都在 RpcOptions 类 - 优先级： System.property \u0026amp;gt; custom rpc-config.json（可能存在多个自定义，会排序） \u0026amp;gt; rpc-config-default.json\n常量  全局的基本常量在 RpcConstants 中。例如：  调用方式 sync oneway 协议 bolt/grpc、 序列化 hessian/java/protobuf\n 上下文的key 等等。  如果扩展实现自身的常量，请自行维护。  例如 BOLT 协议的常量。  SERIALIZE_CODE_HESSIAN = 1 PROTOCOL_TR = 13  例如 DSR 配置中心相关的常量。  _WEIGHT、_CONNECTTIMEOUT 这种 配置中心特有的key    地址  地址信息放到 ProviderInfo 类中 ProviderInfo 的值主要分为三部分  字段，一般是一些必须项目。 例如IP，端口，状态等； 静态字段：例如应用名； 动态字段：例如预热权重等；  字段枚举维护在 ProviderInfoAttrs 类中。  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/common-model/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b2cc3f7ed134408d6adc25e418e1978b","permalink":"/projects/sofa-rpc/common-model/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/common-model/","summary":"消息 内部全部使用 SofaRequest 和 SofaResponse 进行传递。 如果需要转换为其它协议，那么在真正调用和收到请求的时候，转换为实际要传输的对象。 可以对 SofaRequest 和 SofaResponse 进行写操作的模块","tags":null,"title":"基础模型","type":"projects","url":"/projects/sofa-rpc/common-model/","wordcount":684},{"author":null,"categories":null,"content":" 准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 - git 工具用法可以查看 git 官方书籍,需要阅读前几章来熟悉。 - git 协作流程可以查看这篇文章 git 协作流程\nGitHub 贡献代码流程 提交issue 不论您是修复 SOFAJRaft 的 bug 还是新增 SOFAJRaft 的功能，在您提交代码之前，在 SOFAJRaft 的 GitHub 上提交一个 issue，描述您要修复的问题或者要增加的功能。这么做有几个好处: - 不会与其它开发者或是他们对这个项目的计划发生冲突，产生重复工作。 - SOFAJRaft 的维护人员会对您提的 bug 或者新增功能进行相关讨论，确定该修改是不是必要，有没有提升的空间或更好的办法。 - 在达成一致后再开发，并提交代码，减少双方沟通成本，也减少 pull request 被拒绝的情况。\n获取源码 要修改或新增功能，在提 issue 后，点击左上角的 fork 按钮，复制一份 SOFAJRaft 主干代码到您的代码仓库。\n拉分支 在这之前，可以先看看 SOFAJRaft 的 分支管理策略\nSOFAJRaft 所有修改都在分支上进行，修改完后提交 pull request，在 Code Review 后由项目维护人员 Merge 到主干。 因此，在获取源码步骤介绍后，您需要： - 下载代码到本地，这一步您可以选择git/https方式。\ngit clone https://github.com/您的账号名/sofa-jraft   拉分支准备修改代码  git branch add_xxx_feature   执行完上述命令后，您的代码仓库就切换到相应分支了。执行如下命令可以看到您当前分支：  git branch -a   如果您想切换回主干，执行下面命令：  git checkout -b master   如果您想切换回分支，执行下面命令：  git checkout -b \u0026amp;quot;branchName\u0026amp;quot;  修改代码提交到本地 拉完分支后，就可以修改代码了。\n修改代码注意事项  代码风格保持一致 SOFAJRaft 通过 Maven 插件来保持代码格式一致。在提交代码前，务必本地执行  mvn clean compile   补充单元测试代码 新有修改应该通过已有的单元测试 应该提供新的单元测试来证明以前的代码存在 bug，而新的代码已经解决了这些 bug 您可以用如下命令运行所有测试：  mvn clean test  也可以通过IDE来辅助运行。\n其它注意事项  请保持您编辑的代码的原有风格，尤其是空格换行等。 对于无用的注释，请直接删除。 对逻辑和功能不容易被理解的地方添加注释。 及时更新文档  修改完代码后，请按照如下格式执行命令提交所有修改到本地:\ngit commit -am \u0026#39;(feat) 添加xx功能\u0026#39; git commit -am \u0026#39;(fix) 修复xx功能\u0026#39;  提交代码到远程仓库 在代码提交到本地后，就是与远程仓库同步代码了。执行如下命令提交本地修改到 github 上：\ngit push origin \u0026amp;quot;branchname\u0026amp;quot;  如果前面您是通过 fork 来做的，那么这里的 origin 是 push 到您的代码仓库，而不是 SOFAJRaft 的代码仓库。\n提交合并代码到主干的请求 在的代码提交到 GitHub 后，您就可以发送请求来把您改好的代码合入 SOFAJRaft 主干代码了。此时您需要进入您的 GitHub 上的对应仓库，按右上角的 pull request 按钮。选择目标分支，一般就是 master，系统会通知 SOFAJRaft 的人员， SOFAJRaft 人员会 Review 您的代码，符合要求后就会合入主干，成为 SOFAJRaft 的一部分。\n代码 Review 在您提交代码后，您的代码会被指派给维护人员 Review，请耐心等待。如果在数天后，仍然没有人对您的提交给予任何回复，可以在 PR 下面留言，并 @ 对应的人员。\n对于代码 Review 的意见会直接备注到到对应 PR 或者 Issue。如果觉得建议是合理的，也请您把这些建议更新到您的补丁中。\n合并代码到主干 在代码 Review 通过后，就由 SOFAJRaft 维护人员操作合入主干了。这一步不用参与，代码合并之后，您会收到合并成功的提示。\n","date":-62135596800,"description":"","dir":"projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","fuzzywordcount":1300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"99034715298f73cd835672b872141609","permalink":"/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","summary":"准备工作 贡献代码前需要先了解 git 工具的使用和 GitHub 网站的使用。 - git 工具用法可以查看 git 官方书籍,需要阅读前几章来熟悉。 - git 协作流程可以查看这篇文章 git","tags":null,"title":"如何参与 SOFAJRaft 代码贡献","type":"projects","url":"/projects/sofa-jraft/how-to-contribute-code-to-sofajraft/","wordcount":1268},{"author":null,"categories":null,"content":" 简介 该样例工程演示了如何借助 maven 插件将一个普通的 Java 工程打包成标准格式规范的 Ark Plugin\n背景 现实开发中，常常会遇到依赖包冲突的情况；假设我们开发了一个类库 sample-lib , 业务应用在引入使用时，可能存在跟已有的依赖发生冲突的情况；通常这个时候，我们会希望自己的类库能够和业务其他依赖进行隔离，互不协商双方依赖包版本。 Ark Plugin 正是基于这种需求背景下的实践产物； Ark Plugin 运行在 Ark Container 之上，由容器负责加载启动，任何一个 Ark Plugin 由独立的 ClassLoader 加载，从而做到相互隔离。Ark Plugin 存在四个概念： * 导入类：插件启动时，优先委托给导出该类的插件负责加载，如果加载不到，才会尝试从本插件内部加载；\n 导出类：其他插件如果导入了该类，优先从本插件加载；\n 导入资源：插件在查找资源时，优先委托给导出该资源的插件负责加载，如果加载不到，才会尝试从本插件内部加载；\n 导出资源：其他插件如果导入了该资源，优先从本插件加载；\n   详细请参考插件规范\n 工具 官方提供了 Maven 插件 - sofa-ark-plugin-maven-plugin ，只需要简单的配置项，即可将普通的 Java 工程打包成标准格式规范的 Ark Plugin ，插件坐标为:\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;   详细请参考插件配置文档\n 入门 基于该用例工程，我们一步步描述如何构建一个 Ark Plugin\n创建标准 Maven 工程 该用例工程是一个标准的 Maven 工程，一共包含两个模块： * common 模块：包含了插件导出类\n plugin 模块：包含了 com.alipay.sofa.ark.spi.service.PluginActivator 接口实现类和一个插件服务类，插件打包工具 sofa-ark-plugin-maven-plugin 即配置在该模块的 pom.xml 中；  配置打包插件 在 plugin 模块的 pom.xml 中按如下配置打包插件：\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-plugin-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${project.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;ark-plugin\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--can only configure no more than one activator--\u0026amp;gt; \u0026amp;lt;activator\u0026amp;gt;com.alipay.sofa.ark.sample.activator.SamplePluginActivator\u0026amp;lt;/activator\u0026amp;gt; \u0026amp;lt;!-- configure exported class --\u0026amp;gt; \u0026amp;lt;exported\u0026amp;gt; \u0026amp;lt;!-- configure package-level exported class--\u0026amp;gt; \u0026amp;lt;packages\u0026amp;gt; \u0026amp;lt;package\u0026amp;gt;com.alipay.sofa.ark.sample.common\u0026amp;lt;/package\u0026amp;gt; \u0026amp;lt;/packages\u0026amp;gt; \u0026amp;lt;!-- configure class-level exported class --\u0026amp;gt; \u0026amp;lt;classes\u0026amp;gt; \u0026amp;lt;class\u0026amp;gt;com.alipay.sofa.ark.sample.facade.SamplePluginService\u0026amp;lt;/class\u0026amp;gt; \u0026amp;lt;/classes\u0026amp;gt; \u0026amp;lt;/exported\u0026amp;gt; \u0026amp;lt;!--specify destination where ark-plugin will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;../target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  在用例工程中，我们只配置了一部分配置项，这已经足够生成一个可用的 Ark Plugin，各配置项含义如下： * activator: Ark 容器启动插件的入口类，最多只能配置一个；通常来说，在插件的 activator 会执行一些初始化操作，比如发布插件服务；在本样例工程中，即发布了插件服务。\n 导出包：包级别的导出类配置，插件中所有以导出包名为前缀的类，包括插件的三方依赖包，都会被导出；\n 导出类：精确类名的导出类配置，导出具体的类；\n outputDirectory： mvn package 打包后，输出的 ark plugin 文件存放目录；\n  需要指出的是，在用例工程中，我们只导出了工程创建的类；实际在使用时，也可以把工程依赖的三方包也导出去。\n打包、安装、发布、引入 和普通的工程操作类似，使用 mvn package , mvn install , mvn deploy 即可完成插件包的安装和发布；需要注意的是，默认发布的 Ark Plugin 其 Maven 坐标会增加 classifier=ark-plugin ；例如在该样例工程中，如果需要使用该 ark plugin，必须如下配置依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sample-ark-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-plugin-demo/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d8125843ced13352dd228299f222c74d","permalink":"/projects/sofa-boot/sofa-ark-ark-plugin-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-plugin-demo/","summary":"简介 该样例工程演示了如何借助 maven 插件将一个普通的 Java 工程打包成标准格式规范的 Ark Plugin 背景 现实开发中，常常会遇到依赖包冲突的情况；假设我们开发了一个类","tags":null,"title":"如何打包 Ark Plugin","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-plugin-demo/","wordcount":1172},{"author":null,"categories":null,"content":" 简介 该样例工程演示了如何借助 Maven 插件将一个 Spring Boot Web 工程打包成标准格式规范的可执行 Ark 包；\n准备 因该样例工程依赖 sample-ark-plugin，因此需要提前在本地安装该 Ark Plugin\n工具 官方提供了 Maven 插件 - sofa-ark-maven-plugin ，只需要简单的配置项，即可将 Spring Boot Web 工程打包成标准格式规范的可执行 Ark 包，插件坐标为：\n\u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt;   详细请参考插件使用文档\n 入门 基于该样例工程，我们一步步描述如何将一个 Spring Boot Web 工程打包成可运行 Ark 包\n创建 SpringBoot Web 工程 在官网 https://start.spring.io/ 下载一个标准的 Spring Boot Web 工程\n引入 sample-ark-plugin 在工程主 pom.xml 中如下配置，添加另一个样例工程打包生成的 Ark Plugin 依赖，参考文档\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sample-ark-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;classifier\u0026amp;gt;ark-plugin\u0026amp;lt;/classifier\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置打包插件 在工程主 pom.xml 中如下配置 Maven 插件 sofa-ark-maven-plugin :\n\u0026amp;lt;build\u0026amp;gt; \u0026amp;lt;plugins\u0026amp;gt; \u0026amp;lt;plugin\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-maven-plugin\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;executions\u0026amp;gt; \u0026amp;lt;execution\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default-cli\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;!--goal executed to generate executable-ark-jar --\u0026amp;gt; \u0026amp;lt;goals\u0026amp;gt; \u0026amp;lt;goal\u0026amp;gt;repackage\u0026amp;lt;/goal\u0026amp;gt; \u0026amp;lt;/goals\u0026amp;gt; \u0026amp;lt;configuration\u0026amp;gt; \u0026amp;lt;!--specify destination where executable-ark-jar will be saved, default saved to ${project.build.directory}--\u0026amp;gt; \u0026amp;lt;outputDirectory\u0026amp;gt;./target\u0026amp;lt;/outputDirectory\u0026amp;gt; \u0026amp;lt;!--default none--\u0026amp;gt; \u0026amp;lt;arkClassifier\u0026amp;gt;executable-ark\u0026amp;lt;/arkClassifier\u0026amp;gt; \u0026amp;lt;/configuration\u0026amp;gt; \u0026amp;lt;/execution\u0026amp;gt; \u0026amp;lt;/executions\u0026amp;gt; \u0026amp;lt;/plugin\u0026amp;gt; \u0026amp;lt;/plugins\u0026amp;gt; \u0026amp;lt;/build\u0026amp;gt;  在该样例工程中，我们只配置了一部分配置项，这已经足够生成一个可用的可执行 Ark 包，各配置项含义如下： * outputDirectory: mvn package 打包后，输出的 Ark 包文件存放目录；\n arkClassifier: 指定发布的 Ark 包其 Maven 坐标包含的 classifier 值，默认为空；  关于 arkClassifier 配置项需要特别注意下，默认值为空；如果不指定 classifier ，上传到仓库的 Jar 包其实是一个可运行的 Ark 包；如果需要和普通的打包加以区分，需要配置该项值。\n打包、安装、发布 和普通的工程操作类似，使用 mvn package , mvn install , mvn deploy 即可完成插件包的安装和发布；\n运行 我们提供了两种方式在 Ark 容器上启动工程应用，通过命令行启动或者在 IDE 启动；在 IDE 启动时，需要额外添加依赖；使用命令行启动非常简便，直接使用 java -jar 即可启动应用；下面我们说下如何在 IDE 启动 Ark 应用；\n Spring Boot 工程：Spring Boot 工程需要添加如下依赖即可：  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-springboot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   普通 Java 工程： 相较于 SpringBoot 工程，普通的 Java 工程需要添加另一个依赖：  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofa-ark-support-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.ark.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  除此之外，还需要在工程 main 方法最开始处，执行容器启动，如下：\npublic class Application{ public static void main(String[] args) { SofaArkBootstrap.launch(args); ... } }  运行测试用例 SOFAArk 提供了 org.junit.runner.Runner 的两个实现类，ArkJUnit4Runner 和 ArkBootRunner，分别用于集成 JUnit4 测试框架和 Spring Test；对于 TestNG 测试框架，提供了注解 @TestNGOnArk，对于任何 TestNG 测试用例，只有打有 @TestNGOnArk 的测试用例才会跑在 Ark Container 之上，否则普通用例一样。 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofa-ark-ark-demo/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2c97c409788f41051c79836d277997be","permalink":"/projects/sofa-boot/sofa-ark-ark-demo/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofa-ark-ark-demo/","summary":"简介 该样例工程演示了如何借助 Maven 插件将一个 Spring Boot Web 工程打包成标准格式规范的可执行 Ark 包； 准备 因该样例工程依赖 sample-ark-plugin，因","tags":null,"title":"如何打包 Ark 包","type":"projects","url":"/projects/sofa-boot/sofa-ark-ark-demo/","wordcount":1166},{"author":null,"categories":null,"content":" 如何编译  安装 JDK7 及以上，Maven 3.2.5 及以上。\n 直接下载代码，然后执行如下命令：\ncd sofa-rpc mvn clean install  注意：不能在子目录（即子模块）下进行编译。因为 SOFARPC 模块太多，如果每个子模块都会install 和 deploy，仓库内会有较多无用记录。 所以在设计 SOFARPC 工程结构的时候，我们决定各个子模块组件是不需要 install 和 deploy 到仓库里的，我们只会install 和 deploy 一个sofa-rpc-all(all) 模块。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/how-to-build/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"52ad3debb35be8743c97bb4b6b77f22b","permalink":"/projects/sofa-rpc/how-to-build/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/how-to-build/","summary":"如何编译 安装 JDK7 及以上，Maven 3.2.5 及以上。 直接下载代码，然后执行如下命令： cd sofa-rpc mvn clean install 注意：不能在子目录（即子模块）下进行编译。因为 SOFARPC 模块太多","tags":null,"title":"如何编译 SOFARPC 工程","type":"projects","url":"/projects/sofa-rpc/how-to-build/","wordcount":180},{"author":null,"categories":null,"content":" 在非 Kubernetes 环境下使用 Istio 需要达成以下的关键任务：\n 为 Istio 控制平面配置 Istio API server，也可以通过 memostore 的方式启动 Pilot 用作演示用途。 给所有微服务实例手工添加 SOFA MOSN，并以 Sidecar 模式启动。 确保请求都通过 SOFA MOSN 进行路由。  设定控制平面 Istio 控制平面由四个主要的服务组成：Pilot、Mixter、Citadel 以及 API server。\nAPI server Istio\u0026amp;rsquo;s API server (基于 kubernetes API server) 提供了配置管理和基于角色的访问控制。API server 需要 etcd 集群作为底层的持久化存储。\n本地安装 使用如下的 docker compose file 安装一个用于 POC 目的的 API server：\nversion: \u0026#39;2\u0026#39; services: etcd: image: quay.io/coreos/etcd:latest networks: default: aliases: - etcd ports: - \u0026amp;quot;4001:4001\u0026amp;quot; - \u0026amp;quot;2380:2380\u0026amp;quot; - \u0026amp;quot;2379:2379\u0026amp;quot; environment: - SERVICE_IGNORE=1 command: [ \u0026amp;quot;/usr/local/bin/etcd\u0026amp;quot;, \u0026amp;quot;-advertise-client-urls=http://0.0.0.0:2379\u0026amp;quot;, \u0026amp;quot;-listen-client-urls=http://0.0.0.0:2379\u0026amp;quot; ] istio-apiserver: image: gcr.io/google_containers/kube-apiserver-amd64:v1.7.3 networks: default: aliases: - apiserver ports: - \u0026amp;quot;8080:8080\u0026amp;quot; privileged: true environment: - SERVICE_IGNORE=1 command: [ \u0026amp;quot;kube-apiserver\u0026amp;quot;, \u0026amp;quot;--etcd-servers\u0026amp;quot;, \u0026amp;quot;http://etcd:2379\u0026amp;quot;, \u0026amp;quot;--service-cluster-ip-range\u0026amp;quot;, \u0026amp;quot;10.99.0.0/16\u0026amp;quot;, \u0026amp;quot;--insecure-port\u0026amp;quot;, \u0026amp;quot;8080\u0026amp;quot;, \u0026amp;quot;-v\u0026amp;quot;, \u0026amp;quot;2\u0026amp;quot;, \u0026amp;quot;--insecure-bind-address\u0026amp;quot;, \u0026amp;quot;0.0.0.0\u0026amp;quot; ]  其他控制平面组件 目前 SOFA MOSN 还没有集成 Pilot 之外的其他组件，因此我们暂时无需安装 Mixer、Citadel 等组件。\n为微服务实例添加 SOFA MOSN Sidecar 微服务应用的每个实例都必须有个伴生的 SOFA MOSN 实例。\n","date":-62135596800,"description":"","dir":"projects/sofa-mesh/pilot-setup-zookeeper-installation/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"4c0bd56673dc8aebef9011a22496392d","permalink":"/projects/sofa-mesh/pilot-setup-zookeeper-installation/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-mesh/pilot-setup-zookeeper-installation/","summary":"在非 Kubernetes 环境下使用 Istio 需要达成以下的关键任务： 为 Istio 控制平面配置 Istio API server，也可以通过 memostore 的方式启动 Pilot 用作演示用途。 给所有微服务实例手工添加 SOFA","tags":null,"title":"安装指南","type":"projects","url":"/projects/sofa-mesh/pilot-setup-zookeeper-installation/","wordcount":372},{"author":null,"categories":null,"content":"提供可以允许配置的所有参数。 * 发布订阅配置 * 预热转发配置 * 自动故障剔除配置\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b1a8a8c426beab292165716f1dff1ae4","permalink":"/projects/sofa-rpc/configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/configuration/","summary":"提供可以允许配置的所有参数。 * 发布订阅配置 * 预热转发配置 * 自动故障剔除配置","tags":null,"title":"完整配置参数","type":"projects","url":"/projects/sofa-rpc/configuration/","wordcount":37},{"author":null,"categories":null,"content":" 客户端 API 使用说明 SOFALookout 客户端设计上保持了 API 与实现解耦。如果我们只需要基于 SOFALookout API 进行埋点，那么只需要依赖 API 包即可。在没有依赖具体实现模块依赖时（比如 client 依赖 或 SOFABoot（Spring Boot）Start 依赖），API 包会自动使用 NoopRegistry，使得所有埋点的地方都已空实现替代。\n1.API 依赖引入 \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.关于 ID Lookout metrics 相比传统的 metrics 库单一维度的信息描述，提供了支持多维度描述的 tags 能力。 Lookout metrics 的唯一标识 Id 类，由 name 和 tags 构成。\nId basicId = registry.createId(\u0026amp;quot;rpc.provider.service.stats\u0026amp;quot;); id = basicId.withTag(\u0026amp;quot;service\u0026amp;quot;, \u0026amp;quot;com.alipay.demo.demoService\u0026amp;quot;) .withTag(\u0026amp;quot;method\u0026amp;quot;, \u0026amp;quot;sayHi\u0026amp;quot;) .withTag(\u0026amp;quot;protocol\u0026amp;quot;, \u0026amp;quot;tr\u0026amp;quot;) .withTag(\u0026amp;quot;alias\u0026amp;quot;, \u0026amp;quot;group1\u0026amp;quot;);  上面是 Id 的简单示例了，如何创建 Id，如何打 tag，（每打一次 tag，都会生成并返回一个新的 Id 对象引用）切记！使用返回新的 Id 对象了哦。\n 不要主动缓存 Id 或具体的 Metric 对象,Lookout 的 Registry 已经登记记录了。相等的 Id （ name 和 tags 一样）会复用已有的Id 对应 Metric 对象。  2.1 Priority tag (不是必须) PRIORITY 枚举级别: HIGH, NORMAL, LOW;\nid.withTag(LookoutConstants.LOW_PRIORITY_TAG);  如果不打该 Tag (建议)，默认是 NORMAL 级别。级别代表了采集间隔周期（HIGH：2s, NORMAL: 30s, LOW: 1min）\n2.2 关于 tags  通用的 tags，比如：本机 ip，机房等详细会统一附上，不需要单独指定。 普通 Java 项目直接 lookout-client 时，tags 需要自己指定，特别记住：appName 别忘啦！tag:app=xx key 必须小写，尽量是字母，数字，下划线； （尤其是运行期的 Counter, Timer, DistributeSummary 这些 metrics）某个类型的 tag 的 values 尽量是稳定不变有限集合，而且 tags 尽量少，防止 metrics 数量超过最大限制！ 比如：rpc 场合 service, method 两个 tag 对应的值是有限较小的； 反例是每次 rpc 调用都有是个独立的 tag-value。 因此，总体原则自定义打 tags 时要尽量少，对应 values 的集合数量尽量小； 专门用途的 TAG 名称: \u0026amp;ldquo;priority\u0026amp;rdquo;: 表示优先级。 系统保留的 tag key 统配格式_*_,以下划线开始，以下划线结束（比如: \u0026amp;ldquo;type\u0026amp;rdquo; ）。 请不要使用这种格式的 key，可能会被系统覆盖或丢弃  3.可接入的统计( Metric )类型API Counter 「计数器」  场景：方法调用次数； 主动汇报的数据包括： count, rate (也就是 qps)； 使用方式  Counter counter=registry.counter(id); counter.inc();  Timer 「耗时统计器」  场景:统计任务，方法耗时，支持分桶统计 主动汇报的数据包括：elapPerExec (单次执行耗时), total 耗时，Max 耗时,（上报单位：秒）； 使用方式  Timer timer=registry.timer(id); timer.record(2, TimeUnit.SECONDS);  DistributionSummary 「值分布情况统计器」  场景：比如 io 流量，支持分桶统计 主动汇报的数据包括: count, total(size), max(size)； 使用方式:  DistributionSummary distributionSummary=registry.distributionSummary(id); distributionSummary.record(1024);  Gauge 「即时数据观察」  场景：比如线程池，内存值等即时值； 主动汇报的数据包括: value； 往注册表中登记新 gauge 时，ID 值相等，注册表继续使用已有的(忽略新的)；  注意，推荐 gauge 观察的对象尽量是单例的（复用），并且建议在运行时一直活着（而不是作为一个临时的统计）！，如果不是单例或者只存活一段时间，那么一定要从 Registry 中 remove 掉，否则影响 GC（主要是它持有的外部对象引用无法释放，浪费空间）!!\n 使用方式:  registry.gauge(id,new Gauge\u0026amp;lt;Double\u0026amp;gt;() { @Override public Double value() { return 0.1; } });  MixinMetric 「上述基本统计 metrics 的混合管理体」 MixinMetric 是 Lookout 特有的，表示多个基本 metrics 的混合体。引入该 Mixin 目的是优化对「同一度量目标」（即测量目标一致，tags 一致）的多测量指标传输和存储效率，比如：同一线程池的各种指标(线程总数，活跃总数，等待队列大小\u0026amp;hellip;)。\n 使用方式（比如，对一次服务调用，加入多个测量指标：调用耗时，输入字节，调用次数，输出字节等等）  //1. getOrAdd MixinMetric MixinMetric rpcServiceMetric=registry.minxinMetric(id); //2. getOrAdd basic component metric to use Timer rpcTimer = rpcServiceMetric.timer(\u0026amp;quot;perf\u0026amp;quot;); DistributionSummary …","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-api/","fuzzywordcount":1600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"76574f2435a3565fe1fc50831ff9ab0c","permalink":"/projects/sofa-lookout/use-guide-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-lookout/use-guide-api/","summary":"客户端 API 使用说明 SOFALookout 客户端设计上保持了 API 与实现解耦。如果我们只需要基于 SOFALookout API 进行埋点，那么只需要依赖 API 包即可。在没有依赖具体实现模块依赖时（比如","tags":null,"title":"客户端 API 使用指南","type":"projects","url":"/projects/sofa-lookout/use-guide-api/","wordcount":1562},{"author":null,"categories":null,"content":" Registry 的使用 不同的 Registry 的集成提供了不同的访问 Metrics 的方式。\n1. LookoutRegistry 提供按照一定时间窗口统计 metrics 的能力。它又分为“主动推”和“被动拉”两种模式，暂时被动拉取模式处于关闭状态。\n（1）主动推模式\n可以通过【客户端配置】指定远程 Agent 的IP地址，即开始上报检查，和定时上报数据。  （2）被动拉模式\n可以通过【客户端配置】启动该模式，则在 19399 端口提供 HTTP 服务。更多交互细节请参考（待补充）  2. 对接到 Prometheus SOFALookout 的数据可以对接到 Prometheus 上面。为了将数据对接到 Prometheus 上面，首先需要在工程中加入依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-reg-prometheus\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  添加好依赖之后，启动应用，既可以访问 http://localhost:9494 来看到数据，其中 9494 为默认的端口，可以通过在 application.properties 里面配置 com.alipay.sofa.lookout.prometheus-exporter-server-port 来改变端口。\n有了访问数据的 URL 之后，可以编辑一个 prometheus.yml 来抓取该项目信息，假设本机 IP 地址为 10.15.232.20，那么可以配置如下的 prometheus.yml：\nscrape_configs: - job_name: \u0026#39;lookout-client\u0026#39; scrape_interval: 5s static_configs: - targets: [\u0026#39;10.15.232.20:9494\u0026#39;]  有了上面的配置文件之后，可以再到本地通过 Docker 来启动 Prometheus：\ndocker run -d -p 9090:9090 -v $PWD/prometheus.yml:/etc/prometheus/prometheus.yml --name prom prom/prometheus:master  然后通过浏览器访问: http://localhost:9090，再通过 PromQL 查询即可查询到对应的 Metrics。\nSOFALookout 中也提供了一个对接 Prometheus 的样例，大家可以前往自行查看。\n3. 对接 SpringBoot actuator 除了 Prometheus 之外，SOFALookout 可以与 SpringBoot 1.x 的 Actuator 的相集成，只需依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-actuator\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  然后启动后访问 http://localhost:8080/metrics 既可以看到通过 SOFALookout API 埋点的数据。\nSOFALookout 也提供了集成的样例工程，大家可以前往自行查看。\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-registry/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3c51ba6519cee542b459a170dabcf32b","permalink":"/projects/sofa-lookout/use-guide-registry/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-lookout/use-guide-registry/","summary":"Registry 的使用 不同的 Registry 的集成提供了不同的访问 Metrics 的方式。 1. LookoutRegistry 提供按照一定时间窗口统计 metrics 的能力。它又分为“主动推”和“被动拉”两种模式，暂时被动拉取模","tags":null,"title":"客户端 Registry 使用指南","type":"projects","url":"/projects/sofa-lookout/use-guide-registry/","wordcount":575},{"author":null,"categories":null,"content":" 1. 创建 Maven 工程 服务端部署完毕后，我们可以新建一个 Maven 工程使用 SOFARegistry 提供的服务。首先新建一个 Maven 工程，然后引入如下依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;registry-client-all\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${registry.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2. 发布数据 // 构建客户端实例 RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); // 构造发布者注册表 String dataId = \u0026amp;quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026amp;quot;; PublisherRegistration registration = new PublisherRegistration(dataId); // 将注册表注册进客户端并发布数据 registryClient.register(registration, \u0026amp;quot;10.10.1.1:12200?xx=yy\u0026amp;quot;);  使用 SOFARegistry 发布数据一共包含三个步骤：\n 构建客户端实例; 构造发布者注册表; 将注册表注册进客户端并发布数据。  2.1 构建客户端实例 构建客户端实例的关键是创建 RegistryClientConfig 对象，创建 RegistryClientConfig 对象时需要指定 RegistryEndpoint 和 RegistryEndpointPort:\n RegistryEndpoint：注册中心任一 Session 节点地址； RegistryEndpointPort: Session 节点配置的 session.server.httpServerPort 端口值。  2.2 构造发布者注册表 构造发布注册表只需要创建 PublisherRegistration 并指定 dataId，dataId 是发布服务的唯一标识。\n2.3 发布数据 调用 RegistryClient 的 register 方法可以进行数据发布，该方法需要两个参数，第一个参数是发布注册表，指定了服务的 dataId，第二个参数是数据值，一般是一个字符串类型。\n3. 订阅数据 // 构建客户端实例 RegistryClientConfig config = DefaultRegistryClientConfigBuilder.start().setRegistryEndpoint(\u0026amp;quot;127.0.0.1\u0026amp;quot;).setRegistryEndpointPort(9603).build(); DefaultRegistryClient registryClient = new DefaultRegistryClient(config); registryClient.init(); // 创建 SubscriberDataObserver SubscriberDataObserver subscriberDataObserver = new SubscriberDataObserver() { public void handleData(String dataId, UserData userData) { System.out.println(\u0026amp;quot;receive data success, dataId: \u0026amp;quot; + dataId + \u0026amp;quot;, data: \u0026amp;quot; + userData); } }; // 构造订阅者注册表，设置订阅维度，ScopeEnum 共有三种级别 zone, dataCenter, global String dataId = \u0026amp;quot;com.alipay.test.demo.service:1.0@DEFAULT\u0026amp;quot;; SubscriberRegistration registration = new SubscriberRegistration(dataId, subscriberDataObserver); registration.setScopeEnum(ScopeEnum.global); // 将注册表注册进客户端并订阅数据，订阅到的数据会以回调的方式通知 SubscriberDataObserver registryClient.register(registration);  使用 SOFARegistry 发布数据一共包含三个步骤：\n 构建客户端实例; 创建 SubscriberDataObserver； 构造订阅者注册表； 将注册表注册进客户端并订阅数据。  其中创建客户端实例方式与上文发布数据时创建客户端实例的方法一致。\n3.1 创建 SubscriberDataObserver SubscriberDataObserver 是一个回调接口，该接口定义了 handleData 方法，该方法包含两个参数，分别是 dataId 及最终数据，当客户端收到服务端订阅的数据时会调用该方法。在 SOFARegistry 中，服务端返回数据用 UserData 表示，该类包含以下两个方法：\npublic interface UserData { Map\u0026amp;lt;String, List\u0026amp;lt;String\u0026amp;gt;\u0026amp;gt; getZoneData(); String getLocalZone(); }   getLocalZone: 返回当前zone； getZoneData: 返回以 zone 为 key，每个 zone 的数据为 value 的数据。  3.2 构造订阅者注册表 构造订阅者注册表需要创建 SubscriberRegistration 对象，创建该对象需要指定 dataId 及 SubscriberDataObserver。\n3.3 订阅数据 调用 RegistryClient 的 register 方法可以进行数据订阅，该方法包含一个参数，只需传入 SubscriberRegistration 对象即可。\n如果先运行发布数据的程序，然后再运行订阅数据的程序，那么我们将在控制端看到如下输出：\nreceive data success, dataId: …","date":-62135596800,"description":"","dir":"projects/sofa-registry/client-quick-start/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"66e300d44b2f2a903d976bf83eb7c16e","permalink":"/projects/sofa-registry/client-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-registry/client-quick-start/","summary":"1. 创建 Maven 工程 服务端部署完毕后，我们可以新建一个 Maven 工程使用 SOFARegistry 提供的服务。首先新建一个 Maven 工程，然后引入如下依赖： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;registry-client-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${registry.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. 发布数据 // 构建客户端","tags":null,"title":"客户端使用","type":"projects","url":"/projects/sofa-registry/client-quick-start/","wordcount":897},{"author":null,"categories":null,"content":" 该项目演示了如何在 SOFABoot 中使用 SOFALookout 并且对接到 Spring Boot 的 Actuator 中。如果想要对接到 Prometheus 上或者其他的 Registry 中，请参考 Registry 一节。\n新建 SpringBoot（或 SofaBoot ）项目 新建一个 Spring Boot 的应用（如果是 SOFABoot 工程按照 SOFABoot 文档 - 依赖管理中的方式引入 SOFABoot 即可）。\n引入 Lookout 的 Starter 依赖 在 pom.xml 中引入以下依赖即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  如果 Spring Boot 项目需指定版本。\n新建一个 Metrics 指标 在完成依赖的引入之后，然后可以在 Spring Boot 中的启动类中，加入如下的方法：\n@Autowired private Registry registry; @PostConstruct public void init() { Counter counter = registry.counter(registry.createId(\u0026amp;quot;http_requests_total\u0026amp;quot;).withTag(\u0026amp;quot;instant\u0026amp;quot;, NetworkUtil.getLocalAddress().getHostName())); counter.inc(); }  上面的代码中直接通过 @Autowired 注入了一个 Registry 的字段，通过这个 Registry 的字段，我们就可以创建对应的 Counter，然后通过修改这个 Counter 的数据来生成 SOFALookout 的 Metrics 的指标。\n添加配置项 在 SOFABoot 项目中，需要增加一个应用名的配置项：spring.application.name=xxx。\n与 Spring Boot Actuator 对接 新增了一个指标之后，我们可以选择对接到 Spring Boot Actuator 上，要对接到 Spring Boot Actuator 上面，需要添加如下的依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-actuator\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  添加如上的依赖之后，我们在本地启动应用，访问 http://localhost:8080/metrics，就可以看到前面添加的指标，如下：\n\u0026amp;quot;http_requests_total.instant-MacBook-Pro-4.local\u0026amp;quot;: 1,  以上的 QuickStart 的代码在: lookout-client-samples-boot，大家可以下载作为参考。\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/quick-start-client-boot/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"27e057f8a8a4ac97f42ea66ca6a17fdd","permalink":"/projects/sofa-lookout/quick-start-client-boot/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/quick-start-client-boot/","summary":"该项目演示了如何在 SOFABoot 中使用 SOFALookout 并且对接到 Spring Boot 的 Actuator 中。如果想要对接到 Prometheus 上或者其他的 Registry 中，请参考 Registry 一节。 新建 SpringBoot（或 SofaBoot ）项目 新建一","tags":null,"title":"客户端快速开始 - SOFABoot 项目","type":"projects","url":"/projects/sofa-lookout/quick-start-client-boot/","wordcount":490},{"author":null,"categories":null,"content":" 普通 Java 项目 在应用中加入 client 的 Maven 依赖\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.lookout\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;lookout-client\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${lookout.client.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  lookout-client 默认依赖了 lookout-reg-server 模块（支持向 lookout server 上报 metrics 数据），如果希望使用其他类型注册表(比如 lookout-reg-prometheus)，那么再加上对应依赖即可。\n开始使用 SOFALookout 的 Client 之前，首先需要构建一个全局的客户端实例（ com.alipay.lookout.client.DefaultLookoutClient ）\nLookoutConfig lookoutConfig = new LookoutConfig(); DefaultLookoutClient client = new DefaultLookoutClient(\u0026amp;quot;appName\u0026amp;quot;); //选择构建需要使用的 Registry(如果多个注册表类型，建议使用同一 lookoutConfig 实例，便于集中管理) LookoutRegistry lookoutRegistry = new LookoutRegistry(lookoutConfig); //客户端可以后置添加 registry 实例(至少要加一个) client.addRegistry(lookoutRegistry); //(可选)对已加入或后续加入的客户端的 registry 实例，统一注册扩展模块的 metrics client.registerExtendedMetrics();  然后通过客户端拿取 Registry 实例，进行使用：\n//该注册表是个“组合”型的注册表 Registry registry = client.getRegistry(); //demo Id id = registry.createId(\u0026amp;quot;http_requests_total\u0026amp;quot;); Counter counter = registry.counter(id); counter.inc();  客户端的使用，可以详细参考样例工程。\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/quick-start-client-java/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5dc476aa21ece4789859f1af598d4445","permalink":"/projects/sofa-lookout/quick-start-client-java/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/quick-start-client-java/","summary":"普通 Java 项目 在应用中加入 client 的 Maven 依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alipay.sofa.lookout\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lookout-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lookout.client.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; lookout-client 默认依赖了 lookout-reg-server 模块（支持向 lookout server 上报 metrics 数据），如果希望使用其他类型注册表(比如 lookout-reg","tags":null,"title":"客户端快速开始 - 普通 Java 项目","type":"projects","url":"/projects/sofa-lookout/quick-start-client-java/","wordcount":311},{"author":null,"categories":null,"content":"客户端模块是一个较复杂的模块，这里包含了集群管理、路由、地址管理器、连接管理器、负载均衡器，还与代理、注册中心等模块交互。\n参见：\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/client-invoke-flow/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"310d99d64b808a3b526563e92c699952","permalink":"/projects/sofa-rpc/client-invoke-flow/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/client-invoke-flow/","summary":"客户端模块是一个较复杂的模块，这里包含了集群管理、路由、地址管理器、连接管理器、负载均衡器，还与代理、注册中心等模块交互。 参见：","tags":null,"title":"客户端调用流程","type":"projects","url":"/projects/sofa-rpc/client-invoke-flow/","wordcount":64},{"author":null,"categories":null,"content":" 客户端配置项设置示例 lookoutConfig.setProperty(LookoutConfig.LOOKOUT_AGENT_HOST_ADDRESS,\u0026amp;quot;127.0.0.1\u0026amp;quot;);  客户端配置项说明    配置项 对应 SpringBoot 配置项 默认配置值 说明     lookout.enable com.alipay.sofa.lookout.enable true 功能开关，默认是 true。如果改为 false，那么所有 metrics 就几乎没有内存与计算消耗(空对象与空方法)   lookout.max.metrics.num com.alipay.sofa.lookout.max-metrics-num 5000 metrics 最大数目限制，超过会自动忽略   lookout.prometheus.exporter.server.port com.alipay.sofa.lookout.prometheus-exporter-server-port 9494 prometheus 抓取的端口   lookout.exporter.enable com.alipay.sofa.lookout.exporter-enable false 是否开启支持被动采集的服务   lookout.agent.host.address com.alipay.sofa.lookout.agent-host-address - 主动上报 Agent 服务器的注解地址，支持多个地址以逗号分隔    客户端日志配置说明    系统属性配置项 对应 SpringBoot 配置项 默认配置值 说明     -Dlogging.level.com.alipay.lookout=? logging.level.com.alipay.lookout warn lookout 客户端的日志级别，debug 可以看见汇报数据的详情   -Dlogging.path=? logging.path 当前用户目录 SpringBoot V1的日志目录调整，包括 \u0026amp;ldquo;lookout/\u0026amp;rdquo; 日志子目录    客户端配置自定义(适用于 SpringBoot 技术栈模式) 使用配置定制扩展: MetricConfigCustomizerConfig\n@Configuration public class MetricConfigCustomizerConfig { @Bean public MetricConfigCustomizer metricConfigCustomizer() { return new MetricConfigCustomizer() { @Override public void customize(MetricConfig metricConfig) { metricConfig.addProperty(\u0026amp;quot;testaa\u0026amp;quot;, \u0026amp;quot;testbb\u0026amp;quot;); } }; } }  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/client-configuration/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5fd84950d4d565d3fb20781337792bf1","permalink":"/projects/sofa-lookout/client-configuration/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/client-configuration/","summary":"客户端配置项设置示例 lookoutConfig.setProperty(LookoutConfig.LOOKOUT_AGENT_HOST_ADDRESS,\u0026quot;127.0.0.1\u0026quot;); 客户端配置项说明 配置项 对应 SpringBoot 配置项 默认配置值 说明 lookout.enable com.alipay.sofa.lookout.enable true 功能开关，默认是 true。如果改为 false，那么所有 metrics 就几乎没","tags":null,"title":"客户端配置","type":"projects","url":"/projects/sofa-lookout/client-configuration/","wordcount":298},{"author":null,"categories":null,"content":"包含单机故障剔除和 Hystrix 熔断。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e567dc5e291867e92c8dd1c4f953b768","permalink":"/projects/sofa-rpc/fault/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/fault/","summary":"包含单机故障剔除和 Hystrix 熔断。","tags":null,"title":"容灾恢复","type":"projects","url":"/projects/sofa-rpc/fault/","wordcount":13},{"author":null,"categories":null,"content":" 本文档中提供了 MOSN 的示例工程。\n使用 MOSN 作为 HTTP 代理 请参考 MOSN 转发 HTTP 的示例工程 http-sample。\n使用 MOSN 作为 SOFARPC 代理 请参考 MOSN 转发 SOFARPC 的示例工程 sofarpc-sample。\n使用 MOSN 作为TCP 代理 请参考 MOSN 作为 TCP Proxy 的示例工程 tcpproxy-sample 。\n","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-run-samples/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"600c182fdee786a59e14899ba0fce8a1","permalink":"/projects/mosn/quick-start-run-samples/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/mosn/quick-start-run-samples/","summary":"本文档中提供了 MOSN 的示例工程。 使用 MOSN 作为 HTTP 代理 请参考 MOSN 转发 HTTP 的示例工程 http-sample。 使用 MOSN 作为 SOFARPC 代理 请参考 MOSN 转发 SOFARPC 的示例工程 sofa","tags":null,"title":"工程示例","type":"projects","url":"/projects/mosn/quick-start-run-samples/","wordcount":106},{"author":null,"categories":null,"content":" 源码工程中提供了一些样例工程，辅助说明项目的使用。样例工程的 readme 有使用补充说明，另外需要将这些 sample 工程单独的导入 IDE。\n客户端样例工程  lookout-client-samples-java  该样例工程展示了，在普通 Java 项目中,如何以代码形式使用和配置客户端。\n lookout-client-samples-boot  该样例工程展示了，在 SpringBoot(或SofaBoot) 项目中,如何使用和配置客户端。\n lookout-client-samples-prometheus  该样例工程展示了，在 SpringBoot(或SofaBoot) 项目中,如何使用和配置客户端使用 prometheus。\n lookout-samples-prom-push  该样例工程展示了，在 Java 项目中,使用 prometheus 客户端并以push方式（PushGateway）上报数据。\n服务器端样例工程 ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-samples/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a8a0fcd3f99ce2fb46e4d543e30797c9","permalink":"/projects/sofa-lookout/use-guide-samples/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/use-guide-samples/","summary":"源码工程中提供了一些样例工程，辅助说明项目的使用。样例工程的 readme 有使用补充说明，另外需要将这些 sample 工程单独的导入 IDE。 客户端样例工程 lookout-client-samples-java 该样例工","tags":null,"title":"工程示例","type":"projects","url":"/projects/sofa-lookout/use-guide-samples/","wordcount":261},{"author":null,"categories":null,"content":" Q：报错误 NoSuchMethodError 一般情况下该类错误由依赖冲突导致。已知的依赖冲突列举如下，遇到时选择性排除它们。\n日志冲突 commons-logging 冲突 \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;commons-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;commons-logging\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt;  logback-classic 冲突 在冲突位置将 logback-classic 排除，如 spring-boot-starter-logging 和 spring-test 为存在冲突的应用依赖。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;ch.qos.logback\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;logback-classic\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-test\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;4.3.4.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;ch.qos.logback\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;logback-classic\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  snakeyaml 冲突 java.lang.NoSuchMethodError: org.yaml.snakeyaml.Yaml.\u0026amp;lt;init\u0026amp;gt;(Lorg/yaml/snakeyaml/constructor/BaseConstructor;)V  spring-boot-starter-test 与 org.testng 中引用的 org.yaml 存在冲突。这里以排除 spring-boot-starter-test 中的 org.yaml 为例（也可在 org.testng 等冲突位置排除）\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-test\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;scope\u0026amp;gt;test\u0026amp;lt;/scope\u0026amp;gt; \u0026amp;lt;exclusions\u0026amp;gt; \u0026amp;lt;exclusion\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.yaml\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;snakeyaml\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/exclusion\u0026amp;gt; \u0026amp;lt;/exclusions\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  Q：报错 NoClassDefFoundError 一般情况下依赖缺失或者依赖冲突会导致该类问题。\nMockito 报错找不到类 SOFABoot 使用 Mockito 时，如果已经存在 spring-boot-starter-test 则无需重复引入 Mockito。\nQ：报错 No bean dataAccessConfigManager available ACTS 测试脚本指定的 Application 启动类中缺少 acts-core.xml，如图添加即可。\nQ：No runnable methods 一般是由于选择 Junit 运行 ACTS 测试脚本导致的，ACTS 测试脚本可使用 TestNG 方式运行。\nQ：生成模版异常 有较多情况会导致这一现象，常见的是新编写的类或者对类进行变更后，没有进行 mvn 编译。先执行 mvn clean install -Dmaven.test.skip=true，再进行模版生成。\nQ：编辑器设置入参错误 使用 ACTS IDE 操作入参时，出现无法选中或者设置数值出错等情况，一般是生成测试脚本操作有误，没有生成入参模版而直接生成测试脚本，导致初始生成的 YAML 中入参不正确。\n 解法一：删除测试脚本对应的 YAML 文件，然后打开 ACTS IDE 并右键入参设置 -\u0026amp;gt; 模版选择，编辑后保存则 YAML 文件会自动重建。 解法二：删除生成的测试脚本和 YAML 文件，首先生成入参的模版，再重新生成测试脚本即可，YAML 中会默认带入参设置；  Q：报错 argument type mismatch 该问题一般是被测接口有多个同名重载方法导致的，从而引发反射时参数不匹配错误。\n解决方法 可以在脚本中重写 ACTS 测试基类的 findMethod 方法，返回真正被测的方法对象。下面的方法也适用于获取被测方法失败的情况。\n@Override public void beforeActsTest(ActsRuntimeContext actsRuntimeContext) { Method method =null; try { method = VirtualAssetQueryService.class.getDeclaredMethod (\u0026amp;quot;query\u0026amp;quot;, QueryVirtualAssetListParam.class); } catch (NoSuchMethodException e) { e.printStackTrace(); } catch (SecurityException e) { e.printStackTrace(); } actsRuntimeContext.setTestedMethod(method); }  使用 ACTS IDE 编辑类的属性后保存取值失效 ACTS IDE 默认类是标准的 JavaBean 形式，会调用属性的 set 方法为其赋值，如果不存在 set 方法则无法保存取值。\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/faq/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5f89d1f5695cbe6b669a8738741529bd","permalink":"/projects/sofa-acts/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-acts/faq/","summary":"Q：报错误 NoSuchMethodError 一般情况下该类错误由依赖冲突导致。已知的依赖冲突列举如下，遇到时选择性排除它们。 日志冲突 commons-logging 冲突 \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;commons-logging\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;commons-logging\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; logback-classic 冲突 在冲突位置将 logback-classic 排除，","tags":null,"title":"常见问题","type":"projects","url":"/projects/sofa-acts/faq/","wordcount":776},{"author":null,"categories":null,"content":" Q: Readiness Check 有啥应用场景？ Liveness Check 和 Readiness Check 概念来自于 Kuberentes，分别代表运行时检查和启动时检查。Spring Boot 提供了 Liveness Check，但是没有提供 Readiness Check。利用 Readiness Check 的能力，SOFA 中间件中的各个组件只有在 Readiness Check 通过之后，才将流量引入到应用的实例中，比如 RPC，只有在 Readiness Check 通过之后，才会向服务注册中心注册，后面来自上游应用的流量才会进入。除了中间件可以利用 Readiness Check 的事件来控制流量，PAAS 系统也可以通过访问 http://localhost:8080/health/readiness 来获取应用的 Readiness Check 的状况，从而控制例如负载均衡设备等等的流量。\nQ: 是否可以在 SOFABoot 模块中定义 Controller 组件？ SOFABoot 模块一般用于封装对外发布服务接口的具体实现，属于业务层，Controller 属于展现层内容，我们不建议也不支持在 SOFABoot 模块中定义 Controller 组件，Controller 组件相关定义建议直接放在 Root Application Context。\nQ: 类隔离在蚂蚁内部使用是否广泛？ 类隔离在蚂蚁内部使用非常广泛，绝大部分业务应用都是运行在蚂蚁中间件自研的类隔离框架之上。主要是为了解决依赖冲突的问题，像蚂蚁金服这种体量的公司，业务线繁杂、基础服务组件众多，很难做到对所有 JAR 包做统一管控。特别涉及到跨团队模块组件相互依赖时，因为各自技术栈历史包袱的存在，难以有效统一冲突包版本。使用类隔离技术解决了实际开发中的很多痛点，业务开发者不需要担心自身依赖冲突的问题，在多团队协作开发中，也有很大的优势。\nQ: SOFABoot类隔离框架（SOFAArk）和 OSGI 容器有哪些差异？ 作为开源界早负盛名的动态模块系统，基于 OSGi 规范的 Equinox、Felix 等同样具备类隔离能力，然而他们更多强调的是一种编程模型，面向模块化开发，有一整套模块生命周期的管理，定义模块通信机制以及复杂的类加载模型。作为专注于解决依赖冲突的隔离框架，SOFAArk 专注于类隔离，简化了类加载模型，因此显得更加轻量。其次在 OSGi 规范中，所有的模块定义成 Bundle 形式，作为应用开发者，他需要了解 OSGi 背后的工作原理，对开发者要求比较高。在 SOFAArk 中，定义了两层模块类型，Ark Plugin 和 Ark Biz，应用开发者只需要添加隔离的 Ark Plugin 依赖，底层的类加载模型对应用开发者俩说是透明的，基本不会带来额外的学习成本。\nQ: SOFAArk 和 Java9 模块化有哪些差异？ Jigsaw 作为 Java9 模块化方案，抛开内部实现细节，在使用规范上和 OSGi 特别相似：模块的依赖、包导入导出、动态导出、可读性传递、模块服务注册与消费、开放模块、可选模块等等若干概念，相对于 SOFAArk 简单的包导入导出显然过于复杂。在实现细节上，考虑到 JDK 代码的兼容性，Jigsaw 没有采用类加载器隔离的方式，不同模块之间仍然可能是同一个类加载器加载。严格上来讲，Jigsaw 并没有解决同一个类多版本的问题，但是因为模块显示的依赖声明，使用纯 Jigsaw 模块化编程，不同版本类冲突的问题在编译期就能被检查或者启动失败，因为不允许不同模块含有相同类名的包。对于在实际开发中遇到的一类情况，例如两个组件依赖不同版本 hessian 包，即使这两个组件定义成了两个模块，运行时也只有一个hessian版本被加载，依然解决不了不同版本类共存的问题。另外，Jigsaw 相对 Ark 或者 OSGi 有一个明显的缺点，Jigsaw 不允许运行时动态发布模块服务，模块间的通信依赖在 module-info.java 中使用 provides 和 uses 静态注册和引用模块服务。当然，Jigsaw 有很多自己的优点，通过引入module-path，在 module 中显示声明模块依赖关系，避免了传统 maven/gradle 中因为间接依赖导致运行时加载类不确定的缺点；其次通过设置模块包的导入导出配置，可以完全做到接口和实现的分离，提升安全性；另外 Java9 本身借助模块化改造，使用jlink工具，开发者可以将自身应用必须的模块聚合，打包一个自定义的jre镜像。\nQ: 为什么使用 SNAPSHOT 版本拉取不到依赖？ 如果需要使用处于研发状态的 SNAPSHOT 版本，有两种方式：\n 拉取 sofa-ark 仓库代码，本地执行 mvn install。 在本地 maven setting.xml 文件增加如下 profile 配置:  \u0026amp;lt;profile\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;default\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;activation\u0026amp;gt; \u0026amp;lt;activeByDefault\u0026amp;gt;true\u0026amp;lt;/activeByDefault\u0026amp;gt; \u0026amp;lt;/activation\u0026amp;gt; \u0026amp;lt;repositories\u0026amp;gt; \u0026amp;lt;repository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/repository\u0026amp;gt; \u0026amp;lt;/repositories\u0026amp;gt; \u0026amp;lt;pluginRepositories\u0026amp;gt; \u0026amp;lt;pluginRepository\u0026amp;gt; \u0026amp;lt;snapshots\u0026amp;gt; \u0026amp;lt;enabled\u0026amp;gt;true\u0026amp;lt;/enabled\u0026amp;gt; \u0026amp;lt;/snapshots\u0026amp;gt; \u0026amp;lt;id\u0026amp;gt;maven-snapshot\u0026amp;lt;/id\u0026amp;gt; \u0026amp;lt;url\u0026amp;gt;https://oss.sonatype.org/content/repositories/snapshots\u0026amp;lt;/url\u0026amp;gt; \u0026amp;lt;/pluginRepository\u0026amp;gt; \u0026amp;lt;/pluginRepositories\u0026amp;gt; \u0026amp;lt;/profile\u0026amp;gt;  Q: 为什么使用 java -jar 启动 Spring Boot/SOFABoot 应用 Ark 包时，应用自动退出？ 因为 SOFAArk 容器不会开启任何非 Daemon 线程，如果是非 Web 应用或者应用启动时不会创建非 Daemon 线程，则应用在执行完 main 方法时，会正常退出。判断 Ark 包是否正常启动，可以观察是否有如下日志出现：\nArk container started in …","date":-62135596800,"description":"","dir":"projects/sofa-boot/faq/","fuzzywordcount":2100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"56f06d32d37d8a5947d7c7ee43d6d955","permalink":"/projects/sofa-boot/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/projects/sofa-boot/faq/","summary":"Q: Readiness Check 有啥应用场景？ Liveness Check 和 Readiness Check 概念来自于 Kuberentes，分别代表运行时检查和启动时检查。Spring Boot 提供了 Liveness Check，但是没有提供","tags":null,"title":"常见问题","type":"projects","url":"/projects/sofa-boot/faq/","wordcount":2071},{"author":null,"categories":null,"content":" 与 Prometheus的差异 答：主要包括:（1）Lookout metrics server 支持适配更多的协议接入；（2）聚焦在围绕 ES 生态提供易使用和运维的最佳实践；（3）支持计算能力下推；（4）除了 Metrics 后期会有 tracing，eventing等方案； （5）对聚合函数和 REST API 都做了兼容性的扩展和增强；（6）支持分布式集群部署具备高可用能力。\nLookout客户端会提供多语言(c,go,python\u0026amp;hellip;)支持吗 答：暂时不会。因为 Lookout Gateway已经支持很多主流协议的数据上报，同时也支持自定义扩展。 如果非Java技术栈，我们推荐大家使用其他开源主流的sdk库，比如: Prometheus sdk，Metricbeat等\n\u0026amp;ldquo;In order to improve query performance, you need to add tag filtering! realQuery:jvm.classes.loaded\u0026amp;rdquo; 答：只输入metric name查询会影响查询性能，所以我们强制每个查询至少有一个 tag（label）的筛选能力，比如:\u0026amp;ldquo;jvm.classes.loaded{app=\u0026amp;ldquo;lookout-gateway\u0026amp;rdquo;}\u0026amp;ldquo;。\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/faq/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"f05d40c9d503ad466634f0473a5fac40","permalink":"/projects/sofa-lookout/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/faq/","summary":"与 Prometheus的差异 答：主要包括:（1）Lookout metrics server 支持适配更多的协议接入；（2）聚焦在围绕 ES 生态提供易使用和运维的最佳实践；","tags":null,"title":"常见问题","type":"projects","url":"/projects/sofa-lookout/faq/","wordcount":430},{"author":null,"categories":null,"content":" 咨询 Q: SOFARPC 是蚂蚁金服内部使用的版本吗？ 是的，SOFARPC 有良好的扩展接口，内部使用的版本就是在开源的版本多一些扩展实现。例如我们云上的商业版本集成了蚂蚁金融云的共享版注册中心、链路跟踪等产品；蚂蚁内部的版本集成了蚂蚁内部的注册中心、LDC路由等特性扩展。\nQ: SOFARPC 内部是用 Zookeeper 作为注册中心的吗？可以集成其它 etcd 等注册中心吗？ 在蚂蚁内部使用的是蚂蚁自研的注册中心产品。SOFARPC 的注册中心模块是可扩展的，对内对外使用的都是一套核心接口。目前开源的版本中集成了 Zookeeper，其它的注册中心实现社区已经在集成中。\nQ: 与Dubbo对比？ Dubbo 是阿里集团开源的一款非常优秀的RPC框架，高性能，具有良好的扩展性。Dubbo在国内开源界起步较早，使用者较多，开源生态更加丰富，目前已进入Apache基金会进行孵化。Dubbo最早在阿里巴巴B2B部门广泛使用。更多信息这里就不多介绍了。\nSOFARPC 最早起源于阿里集团内部的 HSF，但是经过了蚂蚁金服集团内部多年的独立发展，目前脱离为一个独立的产品。SOFARPC 在协议，网络，路由，可扩展性等层面都进行了大量的改造和优化的工作，以满足蚂蚁金服大规模金融级的业务场景。在蚂蚁金服内部，SOFARPC 在蚂蚁中间件（SOFAStack）的生态下，有完善的微服务技术栈支持，包括微服务研发框架，RPC 框架，服务注册中心，分布式定时任务，限流/熔断框架，动态配置推送，分布式链路追踪，Metrics监控度量等等。截止 2019 年双十一，SOFARPC 已经被蚂蚁几千个系统所使用，生产环境发布的接口数量超过了十几万。\n但是在开源领域，SOFARPC 目前还是一个起步阶段，开源生态还在建设当中，随着开源计划的推进，我们会在后续的版本里增加各个周边组件，完善微服务技术栈。同时也欢迎大家来贡献，共同打造 SOFAStack。\n对于性能的对比，类似协议下使用的技术点都是差不多的，所以基本上可比性不高。 对于扩展性的对比，两者都具有良好的扩展性。 对于其它功能差异点的话，这里列一些已经开源或者即将开源的功能点供参考：SOFARPC 协议上将支持 HTTP/2、GRPC，能力上如服务预热权重、自动故障降级、协商机制、CRC数据校验等，结合 SOFABoot 可以实现 RPC 框架与业务的类隔离防止类冲突等等，另外 SOFARPC 在跨单元机房的路由，包括配合服务注册体系实现的对异地多活的支撑也是非常有特色的，期望后面能逐步跟大家分享讨论，甚至形成行业标准。而 SOFARPC 结合内部微服务下做一致性的框架实现的「微交易」架构也是蚂蚁在金融领域非常有价值的沉淀，也是跟 dubbo 体系不一样的地方。\nQ: 对比其他 RPC 框架有何优势？ 作为RPC框架，最基本的能力就是 RPC 调用，其它都是些性能、扩展性、功能性的差异，可能各家的侧重点不一样。\nSOFARPC 在蚂蚁金服内部大规模应用足以证明 SOFARPC 是一款可靠的生产级的 RPC 框架。而蚂蚁金服的金融的属性决定了 SOFARPC 在金融场景下的功能侧重点。\nQ: 和Spring Cloud 的对比？ SOFARPC 定位在 RPC 框架，和 Spring Cloud 的比较不在一个对比维度上面。 Spring Cloud 可对比的是 SOFAStack，SOFAStack 是蚂蚁金服自主研发的金融级分布式中间件，包含了构建金融级云原生架构所需的各个组件，包括微服务研发框架，RPC 框架，服务注册中心，分布式定时任务，限流/熔断框架，动态配置推送，分布式链路追踪，Metrics监控度量，以及分布式高可用消息队列，分布式事务框架，分布式数据库代理层等组件，是一套分布式架构的完整的解决方案。SOFAStack 的各个组件会在未来逐渐开源。\n另外，SOFARPC 的 Starter 是基于 Spring Boot 开发的，Spring Cloud 的各个组件也是基于 Spring Boot 开发的，所以两者并不冲突。\n研发类 Q: 为什么不使用 JDK8 SOFARPC 在蚂蚁金服内部还有JDK6的使用场景，所以编译选择JDK7，而编译级别选择JDK6。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/faq/","fuzzywordcount":1600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a6ec77ce5a423c5345394f42c64a416b","permalink":"/projects/sofa-rpc/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-rpc/faq/","summary":"咨询 Q: SOFARPC 是蚂蚁金服内部使用的版本吗？ 是的，SOFARPC 有良好的扩展接口，内部使用的版本就是在开源的版本多一些扩展实现。例如我们云上的商业版","tags":null,"title":"常见问题","type":"projects","url":"/projects/sofa-rpc/faq/","wordcount":1533},{"author":null,"categories":null,"content":"SOFARPC 可以在使用 Bolt 通信协议的情况下，可以选择不同的序列化协议，目前支持 hessian2 和 protobuf。\n默认的情况下，SOFARPC 使用 hessian2 作为序列化协议，如果需要将序列化协议设置成 protobuf，在发布服务的时候，需要做如下的设置：\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleService\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofarpc.demo.SampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs serialize-type=\u0026amp;quot;protobuf\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  即在 \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; 标签内增加 \u0026amp;lt;sofa:global-attrs\u0026amp;gt; 标签，并且设置 serialize-type 属性为 protobuf。\n对应的，在引用服务的时候，也需要将序列化协议改成 protobuf，设置方式和发布服务的时候类似：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sofarpc.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleServiceRef\u0026amp;quot; jvm-first=\u0026amp;quot;false\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs serialize-type=\u0026amp;quot;protobuf\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  目前，使用注解的方式尚不能支持设置序列化协议，这个将在后续的版本中支持，详见 ISSUE：https://github.com/sofastack/sofa-boot/issues/278\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/serialization/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"87e2faa84c2c7a7605243dc096bc4e17","permalink":"/projects/sofa-rpc/serialization/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/serialization/","summary":"SOFARPC 可以在使用 Bolt 通信协议的情况下，可以选择不同的序列化协议，目前支持 hessian2 和 protobuf。 默认的情况下，SOFARPC 使用 hessian2 作为序列化协议，如","tags":null,"title":"序列化协议","type":"projects","url":"/projects/sofa-rpc/serialization/","wordcount":296},{"author":null,"categories":null,"content":" SLF4J 提供了 MDC （Mapped Diagnostic Contexts）功能，可以支持用户定义和修改日志的输出格式以及内容。本文将介绍 SOFATracer 集成的 SLF4J MDC功能，方便用户在只简单修改日志配置文件的前提下输出当前 SOFATracer 上下文 TraceId 以及 SpanId 。\n使用前提 为了在应用中的日志正确打印 TraceId 和 SpanId 参数，我们的日志编程接口需要面向 SLF4J 进行编程，即打印日志的编程接口不要依赖具体的日志实现。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.slf4j\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;slf4j-api\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  引入依赖 如果是 SOFABoot 或者 Spring Boot 的应用具体的日志实现需要大家去引入，我们推荐的日志打印实现是 Logback 和 Log4j2，不推荐 Log4j，同时日志实现建议只使用一个而不要使用多个实现。\n Logback 实现引入：  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-logging\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;   Log4j2 实现引入：  \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-log4j2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;!--SOFABoot 没有管控 log4j2 版本 --\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;1.4.2.RELEASE\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  配置方法 我们基于 SLF4J MDC 的原理打印对应的 TraceId 和 SpanId，首先我们的应用中的日志编程接口应该面向 SLF4J，如通过如下的方式：\n//引入接口 import org.slf4j.Logger; import org.slf4j.LoggerFactory; //构造日志打印实例 private static final Logger logger = LoggerFactory.getLogger(XXX.class);  其次，我们为了正确打印 TraceId 和 SpanId 参数，我们还需要在日志的配置文件中配置 PatternLayout 的额外参数，这两个参数是 %X{SOFA-TraceId} 和 %X{SOFA-SpanId}，参数值我们均是从 MDC 中获取的值。\n以 Logback 为例配置的 pattern 参数：\n\u0026amp;lt;pattern\u0026amp;gt;%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%X{SOFA-TraceId}, %X{SOFA-SpanId}] ---- %m%n\u0026amp;lt;/pattern\u0026amp;gt;   关键配置项目：[%X{SOFA-TraceId},%X{SOFA-SpanId}] 作为 Logback pattern 的一部分，在对应的 appender 被调用的时候，会根据 pattern 中的占位符替换为当前线程上下文中 TraceId 和 SpanId 的具体值，当前线程中没有对应的 TraceId 和 SpanId 值时，会用“空字符串”替代。  Log4j2 配置 PatternLayout 样例：\n\u0026amp;lt;PatternLayout pattern=\u0026amp;quot;%d{yyyy-MM-dd HH:mm:ss.SSS} %5p [%X{SOFA-TraceId},%X{SOFA-SpanId}] ---- %m%n \u0026amp;quot; /\u0026amp;gt;  Log4j 配置 PatternLayout 样例：\n\u0026amp;lt;layout class=\u0026amp;quot;org.apache.log4j.PatternLayout\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;param name=\u0026amp;quot;ConversionPattern\u0026amp;quot; value=\u0026amp;quot;%d %-5p %-32t [%X{SOFA-TraceId},%X{SOFA-SpanId}] - %m%n\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/layout\u0026amp;gt;   需要注意的是：[%X{SOFA-TraceId},%X{SOFA-SpanId}] 使我们推荐的打印格式，用户可以根据自己的实际需求场景进行定制\n 附:基于 Log4j2 示例工程的源代码地址。\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/print-traceid-spanid/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0d8cc680f811d1db2cffddbba269571c","permalink":"/projects/sofa-tracer/print-traceid-spanid/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/print-traceid-spanid/","summary":"SLF4J 提供了 MDC （Mapped Diagnostic Contexts）功能，可以支持用户定义和修改日志的输出格式以及内容。本文将介绍 SOFATracer 集成的 SLF4J MDC功能，方便用户在只","tags":null,"title":"应用日志打印 traceId 和 spanId","type":"projects","url":"/projects/sofa-tracer/print-traceid-spanid/","wordcount":701},{"author":null,"categories":null,"content":" SOFADashboard 支持查看应用的IP、端口、健康检查状态等基本信息。此功能依赖 SOFADashboard client ，如果一个应用需要将应用信息展示到 SOFADashboard 管控端，可以通过引入客户端依赖即可：\n\u0026amp;lt;denpendency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;dashboard-client-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/denpendency\u0026amp;gt;  除此之外，SOFADashboard 解耦了类似于 SpringBoot Admin 客户端和服务端直连的模式，引入了第三方的储存，目前默认是 redis，因此如果希望能够监控 更多 actuator 信息，可以添加如下依赖：\n\u0026amp;lt;denpendency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;dashboard-ext-redis-store\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/denpendency\u0026amp;gt;  功能展示 相关数据的展示采用了 react-json-view 组件，是的可以直观的看到原始数据集。\n应用维度展示 应用实例 基础信息 健康检查详细数据 环境变量 loggins mappings 配置 client , prefix : com.alipay.sofa.dashboard.client\n   属性 名称 默认值 备注     enable 是否可用 true 当开启时，dashboard client 的相应功能才会作用   instanceIp 指定当前实例的IP 地址 \u0026amp;rdquo;\u0026amp;rdquo; 一般用于测试或者需要指定 IP 的场景   storeInitDelayExp 初始上报延迟 30s Dashboard度量数据存储上报延迟期望(s)   storeUploadPeriodExp 上报周期 60s Dashboard度量数据存储上报周期(s)   virtualHost 虚拟地址 \u0026amp;rdquo;\u0026amp;rdquo; 服务发布虚拟host（同SofaRpc中相同定义），可使用-Dcom.alipay.sofa.rpc.virtual.host引入   virutalPort 虚拟端口 \u0026amp;rdquo;\u0026amp;rdquo; 服务发布虚拟port（同SofaRpc中相同定义），可使用-Dcom.alipay.sofa.rpc.virtual.port引入   internalHost 内部地址 \u0026amp;rdquo;\u0026amp;rdquo; 容器内部地址（例如podIp等)，可使用-Dcom.alipay.sofa.rpc.virtual.internal.host引入   arkEnable 是否启用ark管理 true 当开启时，dashboard client的相应功能才会作用    注：virtualHost，virutalPort 如果通过com.alipay.sofa.rpc指定了相应参数，则不需要通过dashborad再次指定\nzookeeper , prefix : com.alipay.sofa.dashboard.zookeeper\n   属性 名称 默认值 备注     address 地址 true    baseSleepTimeMs 客户端错误重试间隔(ms). 1000    maxRetries 客户端最大重试次数 3    sessionTimeoutMs 客户端会话超时时间(ms) 6000    connectionTimeoutMs 客户端超时时间(ms) 6000     redis , prefix : com.alipay.sofa.dashboard.redis\n   属性 名称 默认值 备注     enble 是否可用 true 当开启时，dashboard会使用redis作为存储   recordTtl 上报周期(ms). 3600    url redis对应url  例如：redis://user:password@example.com:6379   host redis对应host（单实例模式）     port redis对应port（单实例模式）     password redis密码     Sentinel.master Sentinel模式master  master节点名，需参阅集群搭建设置   Sentinel.nodes Sentinel模式节点地址  例如host1:port1;host2:port2;host3:port3   Cluster.nodes Cluster模式节点地址  例如host1:port1;host2:port2;host3:port3   Cluster.maxRedirects Cluster模式重定向次数 0 建议给值，例如10    ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/dashboard-client/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"60586c6dfee1f2afcdac88cbe7a36b83","permalink":"/projects/sofa-dashboard/dashboard-client/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-dashboard/dashboard-client/","summary":"SOFADashboard 支持查看应用的IP、端口、健康检查状态等基本信息。此功能依赖 SOFADashboard client ，如果一个应用需要将应用信息展示到 SOFADashboard 管控端，可以通过引入客户端依赖即可： \u0026lt;denpendency\u0026gt;","tags":null,"title":"应用面板","type":"projects","url":"/projects/sofa-dashboard/dashboard-client/","wordcount":1078},{"author":null,"categories":null,"content":" 首先参考基本代码贡献需知  注意测试用例覆盖率； 代码格式；  验证 Samples  单独导入 Samples 的 Maven 项目； 修改对应 Pom 文件中依赖版本； 验证 Samples 也能正确工作；  ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/development-use-guide/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"423a54ec3f5fbfc9c0e150eb853738ae","permalink":"/projects/sofa-lookout/development-use-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/development-use-guide/","summary":"首先参考基本代码贡献需知 注意测试用例覆盖率； 代码格式； 验证 Samples 单独导入 Samples 的 Maven 项目； 修改对应 Pom 文件中依赖版本； 验证 Samples 也能正确工作；","tags":null,"title":"开发指南","type":"projects","url":"/projects/sofa-lookout/development-use-guide/","wordcount":63},{"author":null,"categories":null,"content":" 1.如何编译  安装 JDK7 及以上，Maven 3.2.5 及以上。 直接下载代码，然后在代码目录下执行如下命令：\n mvn clean install  2.版本发布 版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号，例如 1.0.1。\n参见: http://semver.org/lang/zh-CN/。\n 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本号不一定完全兼容，尽量向下兼容。 次版本号：代表新特性增强。版本号越大特性越丰富。 修订版本号：代表 BugFix 版本。只做 bug 修复使用，版本号越大越稳定。  版本维护 最多同时维护两个版本。\n例如当前主干为 1.3.0，那么将会维护 1.2.x 的 bugfix 分支，而 1.1.x 遇到 bug 将不再修复，建议升级。\n发布流程  日常开发分支采用 SNAPSHOT 版本，例如 1.3.0-SNAPSHOT。 正式发布时修改版本为正式版本，例如 1.3.0。 发布后拉起下一个版本，例如 1.3.1-SNAPSHOT。  3.测试 单元测试 单元测试例子放到自己开发的模块下，测试类的包名与被测试类所在包相同。\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/developer-guide/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"fcacc7e89b979f3aec8dc3333a7a3c37","permalink":"/projects/sofa-acts/developer-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-acts/developer-guide/","summary":"1.如何编译 安装 JDK7 及以上，Maven 3.2.5 及以上。 直接下载代码，然后在代码目录下执行如下命令： mvn clean install 2.版本发布 版本号 采用三位版本号，分别是主版","tags":null,"title":"开发者手册","type":"projects","url":"/projects/sofa-acts/developer-guide/","wordcount":404},{"author":null,"categories":null,"content":"介绍实现架构和相关的细节介绍： * 如何编译 * 架构介绍 * 调用流程 * 基础模型 * 扩展点设计 * 版本发布 * 测试\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/developer-guide/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"903b9f3a5372a75d654f8eeaaf750eeb","permalink":"/projects/sofa-rpc/developer-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/developer-guide/","summary":"介绍实现架构和相关的细节介绍： * 如何编译 * 架构介绍 * 调用流程 * 基础模型 * 扩展点设计 * 版本发布 * 测试","tags":null,"title":"开发者手册","type":"projects","url":"/projects/sofa-rpc/developer-guide/","wordcount":49},{"author":null,"categories":null,"content":" 线程中使用 java.lang.Runnable 如果用户在代码中通过 java.lang.Runnable 新启动了线程或者采用了线程池去异步地处理一些业务，那么需要将 SOFATracer 日志上下文从父线程传递到子线程中去，SOFATracer 提供的 com.alipay.common.tracer.core.async.SofaTracerRunnable 默认完成了此操作，大家可以按照如下的方式使用：\nThread thread = new Thread(new SofaTracerRunnable(new Runnable() { @Override public void run() { //do something your business code } })); thread.start();  线程中使用 java.util.concurrent.Callable 如果用户在代码中通过 java.util.concurrent.Callable 新启动线程或者采用了线程池去异步地处理一些业务，那么需要将 SOFATracer 日志上下文从父线程传递到子线程中去，SOFATracer 提供的 com.alipay.common.tracer.core.async.SofaTracerCallable 默认完成了此操作，大家可以按照如下的方式使用：\nExecutorService executor = Executors.newCachedThreadPool(); SofaTracerCallable\u0026amp;lt;Object\u0026amp;gt; sofaTracerSpanSofaTracerCallable = new SofaTracerCallable\u0026amp;lt;Object\u0026amp;gt;(new Callable\u0026amp;lt;Object\u0026amp;gt;() { @Override public Object call() throws Exception { return new Object(); } }); Future\u0026amp;lt;Object\u0026amp;gt; futureResult = executor.submit(sofaTracerSpanSofaTracerCallable); //do something in current thread Thread.sleep(1000); //another thread execute success and get result Object objectReturn = futureResult.get();  这个实例中，假设 java.util.concurrent.Callable 返回结果的对象类型是 java.lang.Object，实际使用时可以根据情况替换为期望的类型。\nSOFATracer 对线程池、异步调用场景下的支持 异步场景  异步调用，以 rpc 调用为例，每次 rpc 调用请求出去之后不会等待到结果返回之后才去发起下一次处理，这里有个时间差，在前一个 rpc 调用的 callback 回来之前，又一个新的 rpc 请求发起，此时当前线程中的 TracerContext 没有被清理，则 spanId 会自增，tracerId 相同。\n 对于上面这种情况，SOFATracer 在对于异步情况处理时，不会等到 callback 回来之后，调用 cr 阶段才会清理，而是提前就会清理当前线程的 tracerContext 上下文，从而来保证链路的正确性。\n线程池 目前来说，不管是 SOFARPC 还是 Dubbo 的埋点实现，在使用单线程或者线程池时，情况是一样的：\n 同步调用，线程池中分配一个线程用于处理 rpc 请求，在请求结束之前会一直占用线程；此种情况下不会造成下一个 rpc 请求错拿上一个请求的 tracerContext 数据问题 异步调用，由于异步回调并非是在 callback 中来清理上下文，而是提前清理的，所以也不会存在数据串用问题。 callback 异步回调，这个本质上就是异步调用，所以处理情况和异步调用相同。  附：案例工程\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/async/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e755346c441115663c101638667fe4c0","permalink":"/projects/sofa-tracer/async/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/async/","summary":"线程中使用 java.lang.Runnable 如果用户在代码中通过 java.lang.Runnable 新启动了线程或者采用了线程池去异步地处理一些业务，那么需要将 SOFATracer 日志上下文从父线程传递到子线程中去，SOFA","tags":null,"title":"异步线程处理","type":"projects","url":"/projects/sofa-tracer/async/","wordcount":726},{"author":null,"categories":null,"content":" 本文用于帮助初次接触 MOSN 项目的开发人员，快速搭建开发环境，完成构建，测试，打包和示例代码的运行。\n注：MOSN 基于 Go 1.12.7 开发，使用 dep 进行依赖管理。\n准备运行环境  如果您使用容器运行 MOSN，请先 安装 docker 如果您使用本地机器，请使用类 Unix 环境 安装 Go 的编译环境 安装 dep : 参考官方安装文档  获取代码 MOSN 项目的代码托管在 Github，获取方式如下：\ngo get -u mosn.io/mosn  如果您的 go get 下载存在问题，请手动创建项目工程\n# 进入 GOPATH 下的 src 目录 cd $GOPATH/src # 创建 mosn.io 目录 mkdir -p mosn.io cd mosn.io # 克隆 MOSN 代码 git clone git@github.com:mosn/mosn.git cd mosn  最终 MOSN 的源代码代码路径为 $GOPATH/src/mosn.io/mosn\n导入IDE 使用您喜爱的 Go IDE 导入 $GOPATH/src/mosn.io/mosn 项目，推荐 Goland。\n编译代码 在项目根目录下，根据自己机器的类型以及欲执行二进制的环境，选择以下命令编译 MOSN 的二进制文件。\n使用 docker 镜像编译 make build // 编译出 linux 64bit 可运行二进制文件  本地编译 使用下面的命令编译本地可运行二进制文件。\nmake build-local  在非 Linux 机器交叉编译 Linux 64bit 可运行二进制文件。\nmake build-linux64  在非 Linux 机器交叉编译 Linux 32bit 可运行二进制文件。\nmake build-linux32  完成后可以在 build/bundles/${version}/binary 目录下找到编译好的二进制文件。\n打包 在项目根目录下执行如下命令进行打包。\nmake rpm  完成后可以在 build/bundles/${version}/rpm 目录下找到打包好的文件。\n创建镜像 执行如下命令进行镜像创建。\nmake image  运行测试 在项目根目录下执行如下命令运行单元测试：\nmake unit-test  在项目根目录下执行如下命令运行集成测试（较慢）。\nmake integrate  从配置文件启动 MOSN 运行下面的命令使用配置文件启动 MOSN。\n./mosn start -c \u0026#39;$CONFIG_FILE\u0026#39;  开启 MOSN 转发示例程序 参考 examples 目录下的示例工程运行 Samples。\n使用 MOSN 搭建 Service Mesh 平台 请参考与 Istio 集成。\n","date":-62135596800,"description":"","dir":"projects/mosn/quick-start-setup/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d41615315adb522aa4b84762f113a574","permalink":"/projects/mosn/quick-start-setup/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/mosn/quick-start-setup/","summary":"本文用于帮助初次接触 MOSN 项目的开发人员，快速搭建开发环境，完成构建，测试，打包和示例代码的运行。 注：MOSN 基于 Go 1.12.7 开发，使用 dep 进行依赖管理。","tags":null,"title":"快速开始","type":"projects","url":"/projects/mosn/quick-start-setup/","wordcount":614},{"author":null,"categories":null,"content":" 本文档共分为四部分：\n 第一部分：在 Intellij IDEA 上安装 ACTS IDE 可视化编辑器； 第二部分：向您介绍如何在多模块工程中引入 ACTS 依赖； 第三部分：测试模块下一键搭建 ACTS 框架以管理后续 ACTS 用例； 第四部分：一键生成 ACTS 测试脚本；  1.安装 ACTS IDE 推荐使用 Intellij IDEA 2017，为了您的安全，请仅从该下载源获取 ACTS IDE 安装包： 点击下载 ACTS IDE，\n本地磁盘安装：Preference -\u0026amp;gt; Plugins -\u0026amp;gt; Install plugin from disk -\u0026amp;gt; Restart IDEA 即可。 2.引入 ACTS 依赖 在引入依赖之前，需要您的应用是一个多模块工程（包含 test 模块），后续 ACTS 会将全部的测试代码放置在 test 模块下以便管理 ACTS 用例。\n您可以依据应用的具体情况，选择性阅读以下内容：\n应用已经是完整的多模块工程，可参考文档 2.1 部分，帮助您引入 ACTS 依赖； 应用是多模块工程但无 test 模块，可参考文档 2.2 部分，帮助您快速添加 test 模块； 应用不是一个多模块工程，可参考文档 2.3 部分，帮助您快速构建多模块工程。 如果还没有创建工程，可参考 SOFABoot 快速开始搭建应用。\n2.1多模块应用-包含 test 模块 只需在 test 模块的 pom.xml 中引入 acts-bom 即可。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.acts\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;acts-bom\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${acts.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;type\u0026amp;gt;pom\u0026amp;lt;/type\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.2多模块应用-无 test 模块 这里是使用 Intellij IDEA 来创建子模块。\n对着父工程右键 -\u0026amp;gt; New -\u0026amp;gt; Module -\u0026amp;gt; 输入 test 模块名字（一般是 appname-test），分步示例图如下：\n第一步：新建 test 模块 第二步：管理 test 模块 在父工程的 pom.xml 中管理刚刚新建的 test 模块。\n第三步：依赖 ACTS 引入 最后，找到刚刚新建的 test 模块，并在其 pom.xml 中引入 acts-bom 即可。\n\u0026amp;lt;!-- 引入包含 SOFABootApplication 的 pom --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.example\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;example-service\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;!-- 引入 ACTS 依赖 --\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.acts\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;acts-bom\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${acts.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;type\u0026amp;gt;pom\u0026amp;lt;/type\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  2.3非多模块应用 如果你已经有了一个不错的 SOFABoot 应用，但它不是一个多模块应用，下面的内容将帮助你快速地将现有工程构建为多模块工程。\n第一步：新建父工程 创建好一个 SOFABoot 工程，然后删除无关的文件，只需保留 pom.xml 文件。\n第二步：新建子模块 新建子工程模块，将原有应用作为子工程并入父工程下，相关依赖管理提到父工程中。以新建 service 模块和 test 模块为例。\n第三步：管理子模块 第四步：依赖引入 最后，在 test 模块的 pom.xml 文件中引入 acts-bom 即可。\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa.acts\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;acts-bom\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${acts.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;type\u0026amp;gt;pom\u0026amp;lt;/type\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  3.一键初始化 ACTS 测试框架 下面只需要你轻轻动动手指即可完成初始化工作。在图3.2中，您需要正确填写应用名称并选择适合应用的编码格式。\n有关一键初始化生成的文件有何作用，可以参考 ACTS 使用手册的框架准备部分。\n4.一键生成测试脚本 4.1启动类 将 service 模块中的启动类，如 SOFABootApplication 拷贝到 test 模块，并增加需要加载的配置文件：classpath*:META-INF/spring/acts-core.xml\n4.2测试脚本 前提条件：务必 mvn 编译工程和生成对象模型，否则会造成 ACTS IDE 不可预料的错误，如无法编辑、数据不正确等。\n接口定义的方法上点击，选择 ACTS 功能 -\u0026amp;gt; 生成测试用例，并在刚生成的测试脚本中矫正 SOFABoot 启动类的 import 位置。\n","date":-62135596800,"description":"","dir":"projects/sofa-acts/getting-started/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"dfc5fb9b394ea14c280568dcb881a8b0","permalink":"/projects/sofa-acts/getting-started/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-acts/getting-started/","summary":"本文档共分为四部分： 第一部分：在 Intellij IDEA 上安装 ACTS IDE 可视化编辑器； 第二部分：向您介绍如何在多模块工程中引入 ACTS 依赖； 第三部分：测试模块下一键搭建 ACTS 框","tags":null,"title":"快速开始","type":"projects","url":"/projects/sofa-acts/getting-started/","wordcount":1074},{"author":null,"categories":null,"content":" 在本文档中，将创建一个 Spring Boot 的工程，引入 SOFABoot 基础依赖，并且引入 SOFABoot 的健康检查扩展能力，演示如何快速上手 SOFABoot。\n环境准备 要使用 SOFABoot，需要先准备好基础环境，SOFABoot 依赖以下环境： - JDK7 或 JDK8 - 需要采用 Apache Maven 3.2.5 或者以上的版本来编译\n创建工程 SOFABoot 是直接构建在 Spring Boot 之上，因此可以使用 Spring Boot 的工程生成工具 来生成，在本文档中，我们需要添加一个 Web 的依赖，以便最后在浏览器中查看效果。\n引入 SOFABoot 在创建好一个 Spring Boot 的工程之后，接下来就需要引入 SOFABoot 的依赖，首先，需要将上文中生成的 Spring Boot 工程的 zip 包解压后，修改 maven 项目的配置文件 pom.xml，将\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-parent\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${spring.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;relativePath/\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  替换为：\n\u0026amp;lt;parent\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;sofaboot-dependencies\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;${sofa.boot.version}\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/parent\u0026amp;gt;  这里的 ${sofa.boot.version} 指定具体的 SOFABoot 版本，参考发布历史。 然后，添加 SOFABoot 健康检查扩展能力的依赖及 Web 依赖(方便查看健康检查结果)：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;healthcheck-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;org.springframework.boot\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;spring-boot-starter-web\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  最后，在工程的 application.properties 文件下添加 SOFABoot 工程常用的参数配置，其中 spring.application.name 是必需的参数，用于标示当前应用的名称；logging path 用于指定日志的输出目录。\n# Application Name spring.application.name=SOFABoot Demo # logging path logging.path=./logs  运行 可以将工程导入到 IDE 中运行生成的工程里面中的 main 方法（一般上在 XXXApplication 这个类中）启动应用，也可以直接在该工程的根目录下运行 mvn spring-boot:run，将会在控制台中看到启动打印的日志：\n2018-04-05 21:36:26.572 INFO ---- Initializing ProtocolHandler [\u0026amp;quot;http-nio-8080\u0026amp;quot;] 2018-04-05 21:36:26.587 INFO ---- Starting ProtocolHandler [http-nio-8080] 2018-04-05 21:36:26.608 INFO ---- Using a shared selector for servlet write/read 2018-04-05 21:36:26.659 INFO ---- Tomcat started on port(s): 8080 (http)  可以通过在浏览器中输入 http://localhost:8080/sofaboot/versions 来查看当前 SOFABoot 中使用 Maven 插件生成的版本信息汇总，结果类似如下：\n[ { GroupId: \u0026amp;quot;com.alipay.sofa\u0026amp;quot;, Doc-Url: \u0026amp;quot;https://github.com/sofastack/sofa-boot\u0026amp;quot;, ArtifactId: \u0026amp;quot;infra-sofa-boot-starter\u0026amp;quot;, Built-Time: \u0026amp;quot;2018-04-05T20:55:26+0800\u0026amp;quot;, Commit-Time: \u0026amp;quot;2018-04-05T20:54:26+0800\u0026amp;quot;, Commit-Id: \u0026amp;quot;049bf890bb468aafe6a3e07b77df45c831076996\u0026amp;quot;, Version: \u0026amp;quot;2.4.4\u0026amp;quot; } ]  注: 在 SOFABoot 3.x 中调整了 endpoint 路径，sofaboot/versions 更改为 actuator/versions\n可以通过在浏览器中输入 http://localhost:8080/health/readiness 查看应用 Readiness Check 的状况，类似如下：\n{ status: \u0026amp;quot;UP\u0026amp;quot;, sofaBootComponentHealthCheckInfo: { status: \u0026amp;quot;UP\u0026amp;quot; }, springContextHealthCheckInfo: { status: \u0026amp;quot;UP\u0026amp;quot; }, DiskSpace: { status: \u0026amp;quot;UP\u0026amp;quot;, total: 250140434432, free: 22845308928, threshold: 10485760 } }  注: 在 SOFABoot 3.x 中调整了 endpoint 路径，health/readiness 更改为 actuator/readiness\nstatus: \u0026amp;quot;UP\u0026amp;quot; 表示应用 Readiness Check 健康的。可以通过在浏览器中输入 http://localhost:8080/health 来查看应用的运行时健康状态（可能会随着时间发生变化）。\n注: 在 SOFABOOT 3.X 中调整了 endpoint 路径，/health 更改 …","date":-62135596800,"description":"","dir":"projects/sofa-boot/quick-start/","fuzzywordcount":2200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7f582b905fde4a56791c03d4dd6b5a57","permalink":"/projects/sofa-boot/quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/projects/sofa-boot/quick-start/","summary":"在本文档中，将创建一个 Spring Boot 的工程，引入 SOFABoot 基础依赖，并且引入 SOFABoot 的健康检查扩展能力，演示如何快速上手 SOFABoot。 环境准备 要使用 SOFABo","tags":null,"title":"快速开始","type":"projects","url":"/projects/sofa-boot/quick-start/","wordcount":2154},{"author":null,"categories":null,"content":" 这个快速开始可以帮您快速在您的电脑上，下载、安装并使用 SOFADashboard。\n环境准备 sofa-dashboard-backend 依赖 Java 环境来运行。请确保是在以下运行环境可以正常使用:\n JDK 1.8+；下载 \u0026amp;amp; 配置。 Maven 3.2.5+；下载 \u0026amp;amp; 配置。  sofa-dashboard-frontend 使用了 Ant Design Pro 脚手架，前端环境请参考 Ant Design\n数据库初始化  Mysql 版本：5.6+\n SOFAArk 管控需要依赖 MySQL 进行资源数据存储，工程目录下有一个 SofaDashboardDB.sql 脚本文件，可以通过执行这个脚本文件进行数据库表的初始化。\nZookeeper   ZooKeeper 3.4.x and ZooKeeper 3.5.x\n SOFADashboard 中的服务治理、SOFAArk 管控依赖于 Zookeeper，需要本地启动 Zookeeper 服务： ZooKeeper Document。\n后端运行 \u0026amp;gt; git clone https://github.com/sofastack/sofa-dashboard.git \u0026amp;gt; cd sofa-dashboard \u0026amp;gt; mvn clean package -DskipTests \u0026amp;gt; cd sofa-dashboard-backend/sofa-dashboard-web/target/ \u0026amp;gt; java -jar sofa-dashboard-web-1.0.0-SNAPSHOT.jar  前端运行 sofa-dashboard-front 是 SOFADashboard 的前端代码工程，基于蚂蚁金服开源的前端框架 antd 开发。\n\u0026amp;gt; cd sofa-dashboard-front \u0026amp;gt; npm i \u0026amp;gt; npm run dev  案例工程  sofastack-dashboard-guides  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/quick-start/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"fa4c5f48810727f71d675255f19617a3","permalink":"/projects/sofa-dashboard/quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/quick-start/","summary":"这个快速开始可以帮您快速在您的电脑上，下载、安装并使用 SOFADashboard。 环境准备 sofa-dashboard-backend 依赖 Java 环境来运行。请确保是在以下运行环境可以正常","tags":null,"title":"快速开始","type":"projects","url":"/projects/sofa-dashboard/quick-start/","wordcount":313},{"author":null,"categories":null,"content":"SOFARPC 有多种编程界面，下面会对各种界面进行举例： - SOFARPC 方式 - SOFABoot 方式\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/getting-started/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"990bb2211b02b04c3ab6e03f3ba1f74b","permalink":"/projects/sofa-rpc/getting-started/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/getting-started/","summary":"SOFARPC 有多种编程界面，下面会对各种界面进行举例： - SOFARPC 方式 - SOFABoot 方式","tags":null,"title":"快速开始","type":"projects","url":"/projects/sofa-rpc/getting-started/","wordcount":30},{"author":null,"categories":null,"content":" SOFATracer 接入的组件列表参考：SOFATracer 介绍，在使用时请注意不同组件对应的SOFATracer 版本和 JDK 版本。\n环境准备 要使用 SOFABoot，需要先准备好基础环境，SOFABoot 依赖以下环境： - JDK7 或 JDK8 - 需要采用 Apache Maven 3.2.5 或者以上的版本来编译\n示例列表 下面所有 Samples 工程均为 SOFABoot 工程(同时支持 SpringBoot 工程中使用)，关于如何创建 SOFABoot 工程请参考 SOFABoot 快速开始。\n 组件接入  Spring MVC 埋点接入 HttpClient 埋点接入 DataSource 埋点接入 RestTemplate 埋点接入 OkHttp 埋点接入 Dubbo 埋点接入  采样 上报数据到 Zipkin  ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/component-access/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"143f2b9022161ae5a7b10d261752ae5f","permalink":"/projects/sofa-tracer/component-access/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/component-access/","summary":"SOFATracer 接入的组件列表参考：SOFATracer 介绍，在使用时请注意不同组件对应的SOFATracer 版本和 JDK 版本。 环境准备 要使用 SOFABoot","tags":null,"title":"快速开始指南","type":"projects","url":"/projects/sofa-tracer/component-access/","wordcount":209},{"author":null,"categories":null,"content":" SOFATracer 此前的埋点均是基于组件维度的埋点，用户很难在自己业务代码中进行埋点操作，或者增加自定义 tag 值来监控一些链路信息。基于此，SOFATracer 从 2.4.1\u0026amp;frasl;3.0.6 版本开始支持手动埋点和基于注解的埋点方式，帮助用户解决自定义埋点问题。\n使用方式 自定义埋点提供了两种方式，一种是手动埋点，一种是基于注解方式埋点。\n手动埋点 手动埋点的方式遵循 opentracing 规范，SOFATracer 中通过 beforeInvoke 和 afterInvoke 两个函数封装了 span 的周期，如下：\n// 注入 tracer @Autowired Tracer tracer; private void testManual(){ try { // beforeInvoke 开始 SofaTracerSpan sofaTracerSpan = ((FlexibleTracer) tracer).beforeInvoke(\u0026amp;quot;testManual\u0026amp;quot;); sofaTracerSpan.setTag(\u0026amp;quot;manualKey\u0026amp;quot;,\u0026amp;quot;glmapper\u0026amp;quot;); // do your biz } catch (Throwable t){ // 异常结束 ((FlexibleTracer) tracer).afterInvoke(t.getMessage()); } finally { // 正常结束 ((FlexibleTracer) tracer).afterInvoke(); } }  这种方式在使用上没有直接使用注解方便，但是可以直观的了解到 span 的生命周期，另外手动埋点也是对基于注解方式埋点的一种补充，下面介绍。\n基于注解方式 SOFATracer 中提供了 @Tracer 注解，其作用域是 method 级别。\n// 在 hello 方法上使用 @Tracer 注解进行埋点 @Tracer public String hello(String word){ // 自定义 tag 数据 SpanTags.putTags(\u0026amp;quot;author\u0026amp;quot;,\u0026amp;quot;glmapper\u0026amp;quot;); // 失效 helloInner(word); return \u0026amp;quot;glmapper : hello \u0026amp;quot; + word; } // 在 hello 方法上使用 @Tracer 注解进行埋点 @Tracer private String helloInner(String word){ return \u0026amp;quot;glmapper : hello \u0026amp;quot; + word; }  @Tracer 是基于 Spring Aop 实现，因此一定程度上依赖 Spring 中的代理机制。如代码片段中所示，helloInner 方法由于执行过程中不会使用代理对象，而是 this，所以会导致 helloInner 的注解埋点失效。那么对于此种情况，就可以使用手动埋点的方式来弥补。\nSpanTags 是 SOFATracer 中提供的工具类，在使用注解或者手动埋点的情况下，可以通过此类提供的静态方法来设置 tag 。\n日志格式  json 格式  {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2019-09-05 10:23:53.549\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;flexible-sample\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe9291567650233504100130712\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.2\u0026amp;quot;,\u0026amp;quot;span.kind\u0026amp;quot;:\u0026amp;quot;client\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-1\u0026amp;quot;,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:\u0026amp;quot;4ms\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;hello\u0026amp;quot;,\u0026amp;quot;param.types\u0026amp;quot;:\u0026amp;quot;java.lang.String\u0026amp;quot;,\u0026amp;quot;author\u0026amp;quot;:\u0026amp;quot;glmapper\u0026amp;quot;,\u0026amp;quot;sys.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;,\u0026amp;quot;biz.baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;}   非 json 格式   2019-09-05 10:25:50.992,flexible-sample,0a0fe9291567650350953100130778,0.2,client,,http-nio-8080-exec-1,4ms,hello,param.types=java.lang.String\u0026amp;amp;author=glmapper\u0026amp;amp;,,\n ","date":-62135596800,"description":"","dir":"projects/sofa-tracer/flexible/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5aaadb77e734e58428a0852d14888e92","permalink":"/projects/sofa-tracer/flexible/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/flexible/","summary":"SOFATracer 此前的埋点均是基于组件维度的埋点，用户很难在自己业务代码中进行埋点操作，或者增加自定义 tag 值来监控一些链路信息。基于此，SOFATracer","tags":null,"title":"手动埋点","type":"projects","url":"/projects/sofa-tracer/flexible/","wordcount":566},{"author":null,"categories":null,"content":" 1. 集成部署模式 1.1 扩容 registry-integration 假设目前已经部署了 3 台 registry-integration，分别是 node1/node2/node3，扩容的新节点是 node4。\n操作步骤：\n第一步. 部署新的 registry-integration 节点\n首先参考部署文档，将 registry-integration.tgz 在新节点 node4 上部署起来，值得注意的是，node4 需要将 nodes.metaNode 配置项指定为4台机器的地址列表：\nnodes.metaNode=DefaultDataCenter:\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;,\u0026amp;lt;node3\u0026amp;gt;,\u0026amp;lt;node4\u0026amp;gt;  在这一步中，node4启动完成后，访问 curl http://\u0026amp;lt;node4\u0026amp;gt;:9615/health/check 状态显示是不健康，因为 node4 尚未加入集群，要加入集群，需要做第二步。\n第二步. 调用 changePeer 使新节点加入集群\n对已经存在的 node1/node2/node3 的任意一台，执行“修改节点列表”的运维命令，将原有由 node1/node2/node3 构成的集群，改为 node1/node2/node3/node4 集群：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;,\u0026amp;lt;node3\u0026amp;gt;,\u0026amp;lt;node4\u0026amp;gt;\u0026amp;quot;  做完这一步之后，访问 curl http://\u0026amp;lt;node4\u0026amp;gt;:9615/health/check 状态显示应当是健康。\n1.2 缩容 registry-integration 假设集群目前有3台机器 node1/node2/node3，需要缩容 node3。\n1.2.1 平滑缩容 操作步骤：\n第一步. 调用 changePeer 移除节点\n对 node1/node2 的任意一台，执行“修改节点列表”的运维命令，将集群列表由“node1/node2/node3”改为“node1/node2”，即把node3移除出地址列表：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;\u0026amp;quot;  做完这一步之后，访问 curl http://\u0026amp;lt;node3\u0026amp;gt;:9615/health/check 状态显示应当是不健康的，因为 node3 已经被踢出了集群。\n**第二步.关闭 node3 **\n这一步可选，因为 node3 已经被移除集群了，所以即便 node3 还在运行，也对原集群不影响。\n1.2.2 宕机处理 假设 node3 已经宕机，也需要将 node3 移除出集群\n操作步骤：\n第一步. 调用 changePeer 移除节点\n对 node1/node2 的任意一台，执行“修改节点列表”的运维命令，将集群列表由“node1/node2/node3”改为“node1/node2”，即把 node3 移除出地址列表：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;node1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;node1\u0026amp;gt;,\u0026amp;lt;node2\u0026amp;gt;\u0026amp;quot;  2. 独立部署模式 2.1 扩容 registry-meta 假设目前已经部署了3台 registry-meta ，分别是 metaNode1/metaNode2/metaNode3，扩容的新节点是 metaNode4.\n操作步骤：\n第一步. 部署新的 registry-meta 节点\n首先参考部署文档，将 registry-meta.tgz 在新节点 metaNode4 上部署起来，值得注意的是，metaNode4 需要将 nodes.metaNode 配置项指定为4台机器的地址列表：\nnodes.metaNode=DefaultDataCenter:\u0026amp;lt;metaNode1\u0026amp;gt;,\u0026amp;lt;metaNode2\u0026amp;gt;,\u0026amp;lt;metaNode3\u0026amp;gt;,\u0026amp;lt;metaNode4\u0026amp;gt;  在这一步中，metaNode4 启动完成后，访问 curl http://localhost:9615/health/check 状态显示是不健康，因为 metaNode4 尚未加入集群，要加入集群，需要做第二步。\n第二步. 调用 changePeer 使新节点加入集群\n对已经存在的 metaNode1/metaNode2/metaNode3 的任意一台，执行“修改节点列表”的运维命令，将原有由 metaNode1/metaNode2/metaNode3 构成的集群，改为 metaNode1/metaNode2/metaNode3/metaNode4 集群：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;metaNode1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;metaNode1\u0026amp;gt;,\u0026amp;lt;metaNode2\u0026amp;gt;,\u0026amp;lt;metaNode3\u0026amp;gt;,\u0026amp;lt;metaNode4\u0026amp;gt;\u0026amp;quot;  做完这一步之后，访问 curl http://localhost:9615/health/check 状态显示应当是健康。\n2.2 缩容 registry-meta 假设集群目前有3台机器 metaNode1/metaNode2/metaNode3，需要缩容 metaNode3。\n2.2.1 平滑缩容 操作步骤：\n第一步. 调用 changePeer 移除节点\n对 metaNode1/metaNode2 的任意一台，执行“修改节点列表”的运维命令，将集群列表由“metaNode1/metaNode2/metaNode3”改为“metaNode1/metaNode2”，即把 metaNode3 移除出地址列表：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;metaNode1\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;metaNode1\u0026amp;gt;,\u0026amp;lt;metaNode2\u0026amp;gt;\u0026amp;quot;  做完这一步之后，访问 curl http://\u0026amp;lt;metaNode3\u0026amp;gt;:9615/health/check 状态显示应当是不健康的，因为metaNode3已经被踢出了集群。\n第二步. 关闭 metaNode3\n这一步可选，因为 metaNode3 已经被移除集群了，所以即便 metaNode3 还在运行，也对 …","date":-62135596800,"description":"","dir":"projects/sofa-registry/scale/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"57de6dc4da1292063ff25ecea9ffbd08","permalink":"/projects/sofa-registry/scale/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-registry/scale/","summary":"1. 集成部署模式 1.1 扩容 registry-integration 假设目前已经部署了 3 台 registry-integration，分别是 node1/node2/node3，扩容的新节点","tags":null,"title":"扩容与缩容","type":"projects","url":"/projects/sofa-registry/scale/","wordcount":1695},{"author":null,"categories":null,"content":" 自定义引擎各个阶段 可以在测试脚本中或者基类中重写 ActsTestBase 提供的 API。\n 重写 prepare，execute，check，clear 等。可以通过在 super.prepare() 之前或者之后进行某些操作。 重写 process 方法，在 super.process() 之前或之后进行操作。可将整个脚本重新编排，例如在现有的清理 -\u0026amp;gt; 准备 -\u0026amp;gt; 执行 -\u0026amp;gt; 校验流程中增加一些个性化的步骤。 重写 beforeActsTest、afterActsTest，可以在每一个用例运行前后做一些个性化的操作，如准备上下文、缓存刷新等。  参数化 在结果期望和数据库期望中可以使用 $变量名 来标识某个值是变量，测试脚本中可以把值设置进去； 支持范围：入参、返回结果、数据库表字段，支持类型：目前仅支持 String 的参数化。\n使用方法：\n（1）界面以 $ 开头定义变量\n（2）代码中给变量赋值\n@Override public void beforeActsTest(ActsRuntimeContext actsRuntimeContext) { actsRuntimeContext.paramMap.put(\u0026amp;quot;roleId\u0026amp;quot;, \u0026amp;quot;123\u0026amp;quot;); actsRuntimeContext.refreshDataParam(); }  在写 DB 数据期望的时候，也可以通过 = 符号来进行赋值，表示这个值来自于查询结果，后面的表就可以使用这个变量作为值。\n假设接口会向 2 张表插入数据。\n   id_A value_A     123 abc       id_B value_B     abc efg    查询的时候要先通过接口返回的 A 表的 id_A 查到 value_A, 然后把 value_A 作为 B 表的查询条件，在插件上面可以这样写：\n   字段 flag 值     id_A C $param1   value_A Y =param2       字段 flag 值     id_B C $param2   value_B Y efg    上面操作说明：\n =param2 和 $param2 的操作，表示框架会先从 A 表查出 value_A 然后 select from B where id_B = value_A，进而得到全部 B 表的属性值； $param1 表示可以在代码中对 id_A 赋值，代码形如：  actsRuntimeContext.paramMap.put(\u0026amp;quot;param1\u0026amp;quot;,\u0026amp;quot;123\u0026amp;quot;);  表示对变量 param1 赋值 123，上述代码可以写到脚本的 beforeActsTest 里面，这样在查询 A 表之前，框架就会将 123 赋值给 id_A。\n参数组件化 目前仅支持 String 的组件化\n如果属性是需要动态生成的字符串，例如某些 ID，可以通过 @ 符号来调用一个组件生成这个属性，组件要放在跟 test 同级的 component 包下，即：com.corpname.appname.acts.component (这里appname是系统名，corpname是公司名，如alipay)。\npublic class MyComponent { @TestComponent(id = \u0026amp;quot;test\u0026amp;quot;) public String test(String param) { return param+\u0026amp;quot;123\u0026amp;quot;; } }  并通过 acts-config.properties 配置指明参数化组件使其生效，多个组件使用英文逗号 , 分隔，末尾注意不必要的空格。\nparam_components=IdGenerateComponent,NoGenerateComponent  如上图 alis_value 值为 @test?param=123 则在用例运行时会自动替换 alis_value 的取值。\n组件的 ID 要保证唯一，否则默认调用第一个，如果声明了一个无参组件方法，调用方式为 @test 即可，同时支持组件化参数通过变量传入：@test?param=$id，实际执行时会替换 $id 的值为实际值。\n脚本中也可以通过代码调用：\nActsComponentUtil.run(\u0026amp;quot;@test?param=123\u0026amp;quot;);  自定义组件多个参数的场景使用 \u0026amp;amp; 分割参数，如 @test?param1=xxx\u0026amp;amp;param2=yyy\nDB 工具类 1. 指定数据源进行 DB 表访问 框架 ActsDBUtils 中提供了 DB 的指定数据源访问，用于个性化 DB 操作。例如某张表的某条纪录不是准备数据也不是校验数据，但是需要在运行后删掉或更新，此时就需要用到该工具操作 DB 数据。\n使用前配置\n使用指定数据源方式需要在 acts-config.properties 文件中首先将要指定的数据源进行配置，配置例子如下：\ndatasource_bean_name_exampleDataSource=com.alipay.example.dal;exampleDataSource #整体配置的格式为：datasource_bean_name_xxx(数据源名字)=yyy(数据源所在 Module);xxx(数据源名字)  指定数据源方法\nActsDBUtils 工具类中指定数据源方法说明：\npublic static int getUpdateResultMap(String sql,String tableName,String dbConfigKey);  该方法用于指定数据源进行表的增、删和改的操作。sql 为标准 sql 语句，tableName 为逻辑表名，dbConfigKey 为该 表所在的逻辑数据源配置，与 acts-config.properties 配置的 xxx（数据源名字）相同。\npublic static List\u0026amp;lt;Map\u0026amp;lt;String, Object\u0026amp;gt;\u0026amp;gt; getQueryResultMap(String sql, String tableName,String dbConfigKey);  该方法用于指定数据源进行表的查询操作，以上两个方法都是原子化的 DB 表的操作，如果有其他的 DB 需求可以在上面进行 封装。\n2. 不指定数据源进行 DB 表访问 该方式下 ACTS 框架默认根据表名搜索数据源，工具方法的使用步骤如下：\n使用前配置\ndatasource_bundle_name=com.alipay.example.common.dal ds_exampleDataSource=table1,tabal2 #整体配置的格式为 #datasource_bundle_name=数据源所在模块名 #ds_数据源名字=该数据源下的逻辑表名  不指定数据源方法\npublic static int getUpdateResultMap(String …","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-api/","fuzzywordcount":2000,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ce7e264713a6f7a3f0672e2432489f59","permalink":"/projects/sofa-acts/usage-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-acts/usage-api/","summary":"自定义引擎各个阶段 可以在测试脚本中或者基类中重写 ActsTestBase 提供的 API。 重写 prepare，execute，check，clear 等。可以通过在 super.prepare() 之","tags":null,"title":"扩展功能","type":"projects","url":"/projects/sofa-acts/usage-api/","wordcount":1955},{"author":null,"categories":null,"content":" ExtensionLoader 为了对 SOFARPC 各个环节的都有充足的可扩展性，SOFA-RPC定义了一套十分灵活的扩展机制，所有扩展实现都是平等的。\n这套机制不管是对SOFA-RPC本身的开发者其使用者而言都是非常有用的。SOFA-RPC将其自身抽象为了多个模块，各个模块之间无显示依赖，通过SPI的方式进行交互。\n这套扩展机制抽象了这一SPI的交互方式。如果你读了上面文档讲到的 Filter 和 Router，应该已经有所体会。\n这里讲一下如何使方式进行扩展的。\nSOFARPC 提供了 ExtensionLoader 的能力。\n扩展点设计 SOFARPC 定义了一个注解 @Extensible，该注解标识在接口或者抽象类上，标识该类是一个扩展点。即告诉 SOFARPC 该类是可扩展的，需要寻找该扩展点的实现，同时也定义了寻找实现类的文件名称，是否单例。\n@Documented @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.TYPE }) public @interface Extensible { /** * 指定自定义扩展文件名称，默认就是全类名 * * @return 自定义扩展文件名称 */ String file() default \u0026amp;quot;\u0026amp;quot;; /** * 扩展类是否使用单例，默认使用 * * @return 是否使用单例 */ boolean singleton() default true; /** * 扩展类是否需要编码，默认不需要 * * @return 是否需要编码 */ boolean coded() default false; }  SOFARPC 同时定义了 @Extension 注解，标识该类是一个扩展实现类。也定义了扩展点在文件中寻找扩展实现时使用的名字。\n@Documented @Retention(RetentionPolicy.RUNTIME) @Target({ ElementType.TYPE }) public @interface Extension { /** * 扩展点名字 * * @return 扩展点名字 */ String value(); /** * 扩展点编码，默认不需要，当接口需要编码的时候需要 * * @return 扩展点编码 * @see Extensible#coded() */ byte code() default -1; /** * 优先级排序，默认不需要，大的优先级高 * * @return 排序 */ int order() default 0; /** * 是否覆盖其它低{@link #order()}的同名扩展 * * @return 是否覆盖其它低排序的同名扩展 * @since 5.2.0 */ boolean override() default false; /** * 排斥其它扩展，可以排斥掉其它低{@link #order()}的扩展 * * @return 排斥其它扩展 * @since 5.2.0 */ String[] rejection() default {}; }  新增扩展点 1.定义扩展点。\n@Extensible public interface Person { void getName(); }  2.定义扩展实现\n@Extension(\u0026amp;quot;A\u0026amp;quot;) public class PersonA implements Person{ @Override public void getName() { System.out.println(\u0026amp;quot;li wei\u0026amp;quot;); } }  3.编写扩展描述文件：META-INF/services/sofa-rpc/com.alipay.sofa.rpc.extension.Person。文件内容如下：\nA=com.alipay.sofa.rpc.extension.PersonA  4.加载扩展点，获取到扩展实现类使用。\nPerson person = ExtensionLoaderFactory.getExtensionLoader(Person.class).getExtension(\u0026amp;quot;A\u0026amp;quot;);  已有扩展点 如果想对 SOFARPC 的各个内置扩展点进行功能扩展，可直接实现已有扩展，配置扩展模式文件即可。\n目前已有的扩展点如下：\n   接口名 中文名 备注 内置实现     com.alipay.sofa.rpc.client.Client 客户端  Failover、Failfast   com.alipay.sofa.rpc.client.ConnectionHolder 连接管理器  AllConnect（全部连接）   com.alipay.sofa.rpc.client.AddressHolder 地址管理器  单组、多组   com.alipay.sofa.rpc.client.LoadBalancer 负载均衡  随机、轮询、最少并发、一致性hash、本机优先   com.alipay.sofa.rpc.client.Router 路由器     com.alipay.sofa.rpc.codec.Compressor 压缩  snappy、quicklz   com.alipay.sofa.rpc.codec.Serializer 序列化器  java、hessian、pb   com.alipay.sofa.rpc.filter.Filter 拦截器     com.alipay.sofa.rpc.protocol.Protocol 协议  bolt、dubbo、rest   com.alipay.sofa.rpc.protocol.ProtocolDecoder 协议解码  bolt   com.alipay.sofa.rpc.protocol.ProtocolEncoder 协议编码  bolt   com.alipay.sofa.rpc.protocol.TelnetHandler telnet的响应  version、help、ls   com.alipay.sofa.rpc.proxy.Proxy 代理类  java、javassist   com.alipay.sofa.rpc.registry.Registry 注册中心  zookeeper   com.alipay.sofa.rpc.server.Server 服务端实现  bolt、rest   com.alipay.sofa.rpc.transport.ClientTransport 客户端长连接实现  netty   com.alipay.sofa.rpc.transport.ServerTransport 服务端长连接实现  netty    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/extension-loader/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"acc5628da3a7ea2df5eb68bd8ec17159","permalink":"/projects/sofa-rpc/extension-loader/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-rpc/extension-loader/","summary":"ExtensionLoader 为了对 SOFARPC 各个环节的都有充足的可扩展性，SOFA-RPC定义了一套十分灵活的扩展机制，所有扩展实现都是平等的。 这套机制不管是对SOFA-RP","tags":null,"title":"扩展点设计","type":"projects","url":"/projects/sofa-rpc/extension-loader/","wordcount":1131},{"author":null,"categories":null,"content":" SOFARPC 的服务发布和引用的基本配置已经在「编程界面」章节中说明，这里主要介绍服务发布和引用的一些特性。\n同一服务发布多种协议 在 SOFARPC 中，可以将同一个服务发布成多个协议，让调用端可以使用不同的协议调用服务提供方。\n如果使用 Java API，可以按照如下的代码构建多个 ServerConfig，不同的 ServerConfig 设置不同的协议，然后将这些 ServerConfig 设置给 ProviderConfig：\nList\u0026amp;lt;ServerConfig\u0026amp;gt; serverConfigs = new ArrayList\u0026amp;lt;ServerConfig\u0026amp;gt;(); serverConfigs.add(serverConfigA); serverConfigs.add(serverConfigB); providerConfig.setServer(serverConfigs);  如果使用 XML 的方式，直接在 \u0026amp;lt;sofa:service\u0026amp;gt; 标签中增加多个 binding 即可：\n\u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleFacadeImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.bean.SampleFacade\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt/\u0026amp;gt; \u0026amp;lt;sofa:binding.rest/\u0026amp;gt; \u0026amp;lt;sofa:binding.dubbo/\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  如果使用 Annotation 的方式，在 @SofaService 中增加多个 binding 即可：\n@SofaService( interfaceType = SampleService.class, bindings = { @SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;), @SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;) } ) public class SampleServiceImpl implements SampleService { // ... }  同一服务注册多个注册中心 如果使用 API 的方式，构建多个 RegistryConfig 设置给 ProviderConfig 即可：\nList\u0026amp;lt;RegistryConfig\u0026amp;gt; registryConfigs = new ArrayList\u0026amp;lt;RegistryConfig\u0026amp;gt;(); registryConfigs.add(registryA); registryConfigs.add(registryB); providerConfig.setRegistry(registryConfigs);  如果是使用 XML 的方式\n如果使用 Annotation 的方式\n方法级参数设置 在 Java API 方式中，调用 MethodConfig 对象相应的 set 方法即可设置对应的参数，如下所示：\nMethodConfig methodConfigA = new MethodConfig(); MethodConfig methodConfigB = new MethodConfig(); List\u0026amp;lt;MethodConfig\u0026amp;gt; methodConfigs = new ArrayList\u0026amp;lt;MethodConfig\u0026amp;gt;(); methodConfigs.add(methodConfigA); methodConfigs.add(methodConfigB); providerConfig.setMethods(methodConfigs); //服务端设置 consumerConfig.setMethods(methodConfigs); //客户端设置  使用 XML 的方式，在对应的 binding 里面使用 \u0026amp;lt;sofa:method\u0026amp;gt; 标签即可设置对应的参数：\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;personReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.boot.examples.demo.rpc.bean.PersonService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs timeout=\u0026amp;quot;3000\u0026amp;quot; address-wait-time=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- 调用超时；地址等待时间。 --\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;127.0.0.1:22000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- 直连地址 --\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;sayName\u0026amp;quot; timeout=\u0026amp;quot;3000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- 方法级别配置 --\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;sampleFacadeImpl\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.bean.SampleFacade\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs timeout=\u0026amp;quot;3000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;sofa:method name=\u0026amp;quot;sayName\u0026amp;quot; timeout=\u0026amp;quot;2000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  目前 Annotation 的方式暂不支持设置方法级别的参数，将在后续版本中支持。\n配置覆盖 SOFARPC 里面的某些配置在服务提供方可以设置，在服务调用方也可以设置，比如调用的超时的 timeout 属性，这些配置的优先级为：\n线程调用级别设置 \u0026amp;gt;\u0026amp;gt; 服务调用方方法级别设置 \u0026amp;gt;\u0026amp;gt; 服务调用方 Reference 级别设置 \u0026amp;gt;\u0026amp;gt; 服务提供方方法级别设置 \u0026amp;gt;\u0026amp;gt; 服务提供方 Service 级别设置\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/publish-and-reference/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"6a78b8b84b226eaf1e6d2b1ff1d15fee","permalink":"/projects/sofa-rpc/publish-and-reference/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/publish-and-reference/","summary":"SOFARPC 的服务发布和引用的基本配置已经在「编程界面」章节中说明，这里主要介绍服务发布和引用的一些特性。 同一服务发布多种协议 在 SOFARPC 中，可以将同一个服务","tags":null,"title":"服务发布与引用","type":"projects","url":"/projects/sofa-rpc/publish-and-reference/","wordcount":607},{"author":null,"categories":null,"content":" 自动化 推荐版本: ES 5\n自动初始化库 Lookout 服务器端启动时，会自动检查（默认开启，可关闭）所连接的ES机器(或集群)，检查 Metrics 数据存储的 Index和 Mapping 是否已经建立， 如果未初始化则进行初始化工作。默认初始化并产生索引alias: \u0026amp;ldquo;lookout-active-metrics，lookout-search-metrics\u0026amp;rdquo;。\n 看下 Alias 和 Indices\nhttp://localhost:9200/_cat/aliases lookout-active-metrics metrics-2019.05.30-1 - - - lookout-search-metrics metrics-2019.05.30-1 - - -  看下存储 Mapping：\nhttp://localhost:9200/lookout-active-metrics/_mapping { \u0026amp;quot;metrics-2019.05.30-1\u0026amp;quot;: { \u0026amp;quot;mappings\u0026amp;quot;: { \u0026amp;quot;metrics\u0026amp;quot;: { \u0026amp;quot;properties\u0026amp;quot;: { \u0026amp;quot;id\u0026amp;quot;: { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;keyword\u0026amp;quot; }, \u0026amp;quot;tags\u0026amp;quot;: { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;keyword\u0026amp;quot; }, \u0026amp;quot;time\u0026amp;quot;: { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;date\u0026amp;quot; }, \u0026amp;quot;value\u0026amp;quot;: { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;float\u0026amp;quot; } } } } } }   自动运维  自动 Indices Rollover\n 如果超过1天，则切换新索引 如果单个索引的 docs 数目超过: 100000000，则切换新索引；   自动删除过期 Indices\n  默认最多只保留 7 天的数据，过期的 Index 会自动检查并被删除；\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-es/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"016a397aa24e885b5aaa32cf1cac3f35","permalink":"/projects/sofa-lookout/use-guide-es/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/use-guide-es/","summary":"自动化 推荐版本: ES 5 自动初始化库 Lookout 服务器端启动时，会自动检查（默认开启，可关闭）所连接的ES机器(或集群)，检查 Metrics 数据存储的 Index和 Mapping 是","tags":null,"title":"服务器端 ES 存储使用指南","type":"projects","url":"/projects/sofa-lookout/use-guide-es/","wordcount":315},{"author":null,"categories":null,"content":"由于 SOFALookout Metrics Server 兼容 Prometheus API,所以 Grafana 集成 Lookout 很简单，只需要选择 Prometheus 作为数据源协议即可 （注意 Lookout Server 的默认查询端口也是: 9090）。\n下图展示 Grafana 新增数据源配置:\n使用 PromQL 查询展示数据:\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-grafana/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"45b8a5084ac2a151af28ff11413b13cb","permalink":"/projects/sofa-lookout/use-guide-grafana/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-lookout/use-guide-grafana/","summary":"由于 SOFALookout Metrics Server 兼容 Prometheus API,所以 Grafana 集成 Lookout 很简单，只需要选择 Prometheus 作为数据源协议即可 （注意 Lookout Server 的默认查询端口也是: 9090）。 下图展示 Grafana 新增数据源配置","tags":null,"title":"服务器端 Grafana 使用指南","type":"projects","url":"/projects/sofa-lookout/use-guide-grafana/","wordcount":81},{"author":null,"categories":null,"content":" 如果需要扩展支持适配一个新的数据存储，可能需要下面的步骤:\n1.写入适配  需要在 gateway/metrics/exporter/ 下面添加新的 exporter;  参考已有的 \u0026amp;ldquo;gateway/metrics/exporter/elasticsearch\u0026amp;rdquo; 模块；\n 提供个新存储的 MetricExporter  功能是写入数据到存储中，参考\u0026amp;rdquo;com.alipay.sofa.lookout.gateway.metrics.exporter.es.ESMetricExporter\u0026amp;rdquo;，提供个新存储的 MetricExporter；\n 提供个该模块的spring配置类  参考 \u0026amp;ldquo;com.alipay.sofa.lookout.gateway.metrics.exporter.es.spring.bean.config.EsExporterConfiguration\u0026amp;rdquo;，它包括 ESProperties 的配置描述映射。尤其重要的是带有注解 @ConditionalOnExporterComponent方便该功能开关；\n 在 \u0026amp;ldquo;com.alipay.sofa.lookout.gateway.metrics.starter.MetricPipelineConfiguration\u0026amp;rdquo; 中 @import 上述存储spring配置类；   2.查询数据适配  需要在 server/metrics 目录下，添加新 storage 扩展；  参考 “server/metrics/storage-ext-es” 模块，比如新增“storage-ext-**”\n 提供个新的存储 Storage 实现；  参考已有 “com.alipay.sofa.lookout.server.storage.ext.es.ElasticSearchStorage”，实现Storage接口。这里也需要ES实现对应的 “QueryStmt”，”LabelValuesStmt“，”LabelNamesStmt“.\n 提供个该模块的spring配置类  参考\u0026amp;rdquo;com.alipay.sofa.lookout.server.storage.ext.es.spring.bean.config.ElasticSearchServerConfig\u0026amp;rdquo;,提供 Storage的实例。 另外参考支持”@ConditionalOnProperty“的功能开关配置；\n 在 \u0026amp;ldquo;com.alipay.sofa.lookout.server.starter.ServerAutoConfiguration\u0026amp;rdquo; 中 @import 上述存储spring配置类；  3.最后贡献建议  提issue说明需求，并可以介绍下方案； 保证测试覆盖； fork 代码，编译通过后，提交 PR；   ","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-storage-ext/","fuzzywordcount":900,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b990dad82668bc22c24d4ad0468f0535","permalink":"/projects/sofa-lookout/use-guide-storage-ext/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-lookout/use-guide-storage-ext/","summary":"如果需要扩展支持适配一个新的数据存储，可能需要下面的步骤: 1.写入适配 需要在 gateway/metrics/exporter/ 下面添加新的 exporter; 参考已有的 \u0026ldquo;gateway/metrics/exporter/elasticsearch\u0026rdquo; 模块； 提供个新存储的 MetricExporter 功能是写入数据","tags":null,"title":"服务器端 Metrics 存储扩展机制","type":"projects","url":"/projects/sofa-lookout/use-guide-storage-ext/","wordcount":864},{"author":null,"categories":null,"content":" 1. Tag选择器的“in”筛选 =~| 将tag符合表达式的提供的值选择出来，类似于SQL中的in语义\n示例\n将app为 foo 或 foo2的应用时序数据查询出来\njvm.memory.heap.used{app=~|\u0026amp;quot;foo|foo2\u0026amp;quot;,instance_id=\u0026amp;quot;xxx\u0026amp;quot;}  2. Tag选择器的\u0026amp;rdquo;not in\u0026amp;rdquo;筛选 !~| 将tag不符合表达式提供的值选择出来,类似SQL中的not in语义\n示例\njvm.memory.heap.used{app!~|\u0026amp;quot;foo|foo2\u0026amp;quot;,instance_id=\u0026amp;quot;xxx\u0026amp;quot;}  3. Increase2函数 （类似Increase），适用于Promethues的Counter型指标 increase 会根据根据查询步长，做个时间点上的函数估值（根据既有增长斜率）。所以可能是非整型。 如果你就想返回真实时间点的整数差值，不愿估算目标时刻的近视值。那么推荐使用 Increase2。\n需要组合聚合函数时，记住“Rate then sum, never sum then rate”（这里说的rate 与 increase 函数类似）\nsum by (job)(Increase2(http_requests_total{job=\u0026amp;quot;node\u0026amp;quot;}[5m])) # This is okay  4. \u0026amp;ldquo;histogram_quantile\u0026amp;rdquo; 与(Lookout)自定义 \u0026amp;ldquo;zhistogram_quantile\u0026amp;rdquo; histogram_quantile ，是对使用prometheus client得到的 metrics buckets进行分析；\nzhistogram_quantile，是对使用 lookout sdk的得到的 metrics buckets 进行分析;\n5. Promethues 一些最佳实践总结 rate，increase 函数用于计算指标的速率，在使用时要根据指标数据的采集或上报时间间隔来进行over_time时间的控制。比如metrics数据是1分钟上报一次,如果想获取某个metric指标的1分钟的速率，应该按如下方式写promql语句\nrate(http_requestl{token=\u0026amp;quot;mobile\u0026amp;quot;}[2m]  step 表示最终显示的采样（步长），如果和range （比如2m）保持一致，表示原样输出，不采样。 如果想进行采样，step \u0026amp;gt; range 就行了！ 所以总的来说，step \u0026amp;gt;= range.\nrange, 从实际使角度，推荐 range 值要尽量明显大于数据实际的上报时间间隔才有意义，比如上报单位是30秒，那range尽量\u0026amp;gt;= 1分钟。\n","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-promql-feature-enhancement/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3bd5cd1f9d3ce3f9ba5b503ef0ba9da1","permalink":"/projects/sofa-lookout/use-guide-promql-feature-enhancement/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-lookout/use-guide-promql-feature-enhancement/","summary":"1. Tag选择器的“in”筛选 =~| 将tag符合表达式的提供的值选择出来，类似于SQL中的in语义 示例 将app为 foo 或 foo2的应用时序数据查询出来","tags":null,"title":"服务器端 PromQL 语法特性增强","type":"projects","url":"/projects/sofa-lookout/use-guide-promql-feature-enhancement/","wordcount":691},{"author":null,"categories":null,"content":" 使用 Lookout sdk是推荐方式，当然 Lookout gateway 还支持其他协议上报。（但由于属于非标接入，细节可联系我们）\n注意如果使用 非lookout sdk ，自己一定注意控制客户端metrics数量！ [Don\u0026amp;rsquo;t over use labels(tags)]\n1.Promethues Push协议写入支持  Lookout-gateway这里扮演的是一个 prometheus-pushgateway 角色：\necho \u0026amp;quot;some_metric{k1=\u0026amp;quot;v1\u0026amp;quot;} 3.14\u0026amp;quot; | curl --data-binary \\ @- http://localhost:7200/prom/metrics/job/{job}/app/{app}/step/{step}  区别在于：\u0026amp;rdquo;http://localhost:7200/prom/\u0026amp;ldquo;，端口为7200，加了级主路径为/prom.\n 【必选】URL路径变量 {app} {job} 和{step}，必须要指定哦。step 单位秒，表示您定时上报的时间间隔（假如10s 上报一次数据，那么 step=10）\n 【可选】如果和lookout gateway间有网络代理，建议URL 里也附带上客户端真实 ip （如 \u0026amp;ldquo;/ip/{ip}\u0026amp;ldquo;）。\n 上报格式样式: 【 http_requests_total{method=\u0026amp;ldquo;post\u0026amp;rdquo;,code=\u0026amp;ldquo;200\u0026amp;rdquo;} 1027 】，多个以换行符【\u0026amp;rsquo;\\n\u0026amp;rsquo;】分割；\n 更多细节可以参考：prometheus-pushgateway ，你可以选择官方对应编程语言的SDKs\n  2. Lookout 自有协议写入支持 默认的收集服务和数据协议标准(即Lookout自有的协议支持标准)\n localhost:7200/lookout/metrics/app/{app}/step/{step}\ncurl -H \u0026amp;quot;Content-type:text/plain\u0026amp;quot; -X POST -d \u0026#39;xx\u0026#39; \\ localhost:7200/lookout/metrics/app/{app}/step/{step}  请求体是一种批量复合形式。内容是多条 metrics 数据以 \u0026amp;ldquo;\\t\u0026amp;rdquo; 进行连接；\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;1970-01-01T08:00:00+08:00\u0026amp;quot;,\u0026amp;quot;tags\u0026amp;quot;:{\u0026amp;quot;k1\u0026amp;quot;:\u0026amp;quot;v1\u0026amp;quot;},\u0026amp;quot;m_name\u0026amp;quot;:{\u0026amp;quot;count\u0026amp;quot;:0,\u0026amp;quot;rate\u0026amp;quot;:0.0}} \\t{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;1970-01-01T08:00:00+08:00\u0026amp;quot;,\u0026amp;quot;tags\u0026amp;quot;:{\u0026amp;quot;k1\u0026amp;quot;:\u0026amp;quot;v1\u0026amp;quot;},\u0026amp;quot;m_name\u0026amp;quot;:{\u0026amp;quot;value\u0026amp;quot;:99.0}} \\t{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;1970-01-01T08:00:00+08:00\u0026amp;quot;,\u0026amp;quot;tags\u0026amp;quot;:{\u0026amp;quot;k1\u0026amp;quot;:\u0026amp;quot;v1\u0026amp;quot;},\u0026amp;quot;m_name\u0026amp;quot;:{\u0026amp;quot;elapPerExec\u0026amp;quot;:0.0,\u0026amp;quot;totalTime\u0026amp;quot;:0.0,\u0026amp;quot;max\u0026amp;quot;:0.0}} \\t{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;1970-01-01T08:00:00+08:00\u0026amp;quot;,\u0026amp;quot;tags\u0026amp;quot;:{\u0026amp;quot;k1\u0026amp;quot;:\u0026amp;quot;v1\u0026amp;quot;},\u0026amp;quot;m_name\u0026amp;quot;:{\u0026amp;quot;totalAmount\u0026amp;quot;:0.0,\u0026amp;quot;rate\u0026amp;quot;:0.0,\u0026amp;quot;max\u0026amp;quot;:0}}   上面内容中组成部分分别是：counter型,gauge型,Timer型\n 其中单条数据结构\n{ \u0026amp;quot;time\u0026amp;quot;: \u0026amp;quot;1970-01-01T08:00:00+08:00\u0026amp;quot;, \u0026amp;quot;tags\u0026amp;quot;: { \u0026amp;quot;k1\u0026amp;quot;: \u0026amp;quot;v1\u0026amp;quot; }, \u0026amp;quot;m_name\u0026amp;quot;: { \u0026amp;quot;count\u0026amp;quot;: 0, \u0026amp;quot;rate\u0026amp;quot;: 0 } }  tag 的 value 需要转义;\n 如果内容由进行了 snappy 压缩，需添加请求头 \u0026amp;ldquo;Content-Encoding:snappy\u0026amp;rdquo;,且\u0026amp;rdquo;Content-type: application/octet-stream\u0026amp;rdquo;;\n  3.OPEN TSDB 协议写入支持  请求demo\ncurl -X POST \\ http://localhost:7200/opentsdb/api/put \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;step: 10000\u0026#39; \\ -H \u0026#39;app: xx\u0026#39; \\ -H \u0026#39;X-Lookout-Token: xx\u0026#39; \\ -d \u0026#39;[{ \u0026amp;quot;metric\u0026amp;quot;: \u0026amp;quot;xzc.cpu\u0026amp;quot;, \u0026amp;quot;timestamp\u0026amp;quot;: 1530624430, \u0026amp;quot;value\u0026amp;quot;: 30, \u0026amp;quot;tags\u0026amp;quot;: { \u0026amp;quot;host\u0026amp;quot;: \u0026amp;quot;web02\u0026amp;quot;, \u0026amp;quot;dc\u0026amp;quot;: \u0026amp;quot;lga\u0026amp;quot; } }]\u0026#39;  注意timestamp的单位是秒(而且尽量是当前时间附近哦，否则不太好查询)\n post的内容可以是一个json对象或json数组(批量模式)\n 更多细节可以参考 OpenTSDB的 /api/put 接口 http://opentsdb.net/docs/build/html/api_http/put.html\n  4.Metricbeat 写入协议支持 （1）.metricbeat的配置 配置文件 metricbeat.yml\noutput.elasticsearch: hosts: [\u0026#39;10.15.232.67:7200\u0026#39;] path: /beat  host 是 lookout-gateway 的地址,端口是7200. 另外加了级主路径/beat;\n(2).为了符合metrics2.0标准，gateway会对数据进行转换 这块后续去时序库查询，你需要关注：\n FROM:\n{ \u0026amp;quot;@timestamp\u0026amp;quot;: \u0026amp;quot;2018-03-29T08:27:21.200Z\u0026amp;quot;, …","date":-62135596800,"description":"","dir":"projects/sofa-lookout/use-guide-other-metrics-protocol-support/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"df745d82f3f681cfd94b8187934a8477","permalink":"/projects/sofa-lookout/use-guide-other-metrics-protocol-support/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-lookout/use-guide-other-metrics-protocol-support/","summary":"使用 Lookout sdk是推荐方式，当然 Lookout gateway 还支持其他协议上报。（但由于属于非标接入，细节可联系我们） 注意如果使用 非lookout sdk ，自己一定注意控制客","tags":null,"title":"服务器端常见数据采集协议支持","type":"projects","url":"/projects/sofa-lookout/use-guide-other-metrics-protocol-support/","wordcount":1013},{"author":null,"categories":null,"content":" SOFADashboard 服务治理主要是对 SOFARpc 的服务进行管理。 目前已经支持基于 ZK 和 SofaRegistry 两个注册中心。\n功能展示 1、基于服务维度  服务列表   服务提供者详情：  2、基于应用维度  应用列表   应用服务详情  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/governance/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"e547baf489fd5d125be9e67a366854b6","permalink":"/projects/sofa-dashboard/governance/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/governance/","summary":"SOFADashboard 服务治理主要是对 SOFARpc 的服务进行管理。 目前已经支持基于 ZK 和 SofaRegistry 两个注册中心。 功能展示 1、基于服务维度 服务列表 服务提供者详情： 2、基于应用维度 应用","tags":null,"title":"服务治理","type":"projects","url":"/projects/sofa-dashboard/governance/","wordcount":78},{"author":null,"categories":null,"content":" 部署模式 SOFARegistry 支持两种部署模式，分别是集成部署模式及独立部署模式，本文将介绍最简单的单节点集成部署模式，更多更详细的部署模式介绍可以查看 部署文档。\n部署步骤 1. 下载源码或者安装包 下载源码方式 git clone https://github.com/sofastack/sofa-registry.git cd sofa-registry mvn clean package -DskipTests cp server/distribution/integration/target/registry-integration.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-integration tar -zxvf registry-integration.tgz -C registry-integration cd registry-integration  下载安装包方式 您可以从 release 页面 下载最新的 registry-integration-$version.tar.gz 包。\nmkdir registry-integration tar -zxvf registry-integration-$version.tar.gz -C registry-integration cd registry-integration  2. 启动 registry-integration Linux/Unix/Mac 启动命令：sh bin/startup.sh\nWindows 双击 bin 目录下的 startup.bat 运行文件。 3. 确认运行状态 可访问三个角色提供的健康监测 API，或查看日志 logs/registry-startup.log：\n# 查看meta角色的健康检测接口： $ curl http://localhost:9615/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... raftStatus:Leader\u0026amp;quot;} # 查看data角色的健康检测接口： $ curl http://localhost:9622/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... status:WORKING\u0026amp;quot;} # 查看session角色的健康检测接口： $ curl http://localhost:9603/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;...\u0026amp;quot;}  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/server-quick-start/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b620900b56ba04f4668838846a97698a","permalink":"/projects/sofa-registry/server-quick-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-registry/server-quick-start/","summary":"部署模式 SOFARegistry 支持两种部署模式，分别是集成部署模式及独立部署模式，本文将介绍最简单的单节点集成部署模式，更多更详细的部署模式介绍可以查看 部署文档","tags":null,"title":"服务端部署","type":"projects","url":"/projects/sofa-registry/server-quick-start/","wordcount":297},{"author":null,"categories":null,"content":" 术语  发起者：全局事务的发起者，在一个请求链路资源方法里面，最先需要对分布式资源进行事务处理的地方，在Hmily框架里面 可以表示为：一个请求最先遇到 @HmilyTCC or @HmilyTAC 注解的方法，该所属方法应用被称为发起者。\n 参与者：分布式服务或者资源，需要与其他服务一起参与到一次分布式事务场景下。在Hmily框架里面，表现为一个RPC框架的接口被加上@Hmily注解。\n 协调者：用来协调分布式事务到底是commit，还是 rollback的角色，他可以是远程的，也可以是本地的，可以是中心化的，也可以是去中心化的。在Hmily框架里面的协调者是本地去中心化的角色。\n TCC ：Try, Confirm, Cancel 3个阶段的简称。\n TAC ：Try Auto Cancel的简称。Try阶段预留资源后，会由框架自动生成反向的操作资源的行为。\n  ","date":-62135596800,"description":"术语","dir":"projects/hmily/term/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"8a22c263ed5f16b93fa1175383094d9d","permalink":"/projects/hmily/term/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/term/","summary":"术语 发起者：全局事务的发起者，在一个请求链路资源方法里面，最先需要对分布式资源进行事务处理的地方，在Hmily框架里面 可以表示为：一个请求最","tags":null,"title":"术语","type":"projects","url":"/projects/hmily/term/","wordcount":327},{"author":null,"categories":null,"content":" 如图: Node Raft 分组中的一个节点，连接封装底层的所有服务，用户看到的主要服务接口，特别是 apply(task) 用于向 raft group 组成的复制状态机集群提交新任务应用到业务状态机。\n存储  Log 存储，记录 raft 配置变更和用户提交任务的日志，将从 Leader 复制到其他节点上。LogStorage 是存储实现， LogManager 负责对底层存储的调用，对调用做缓存、批量提交、必要的检查和优化。 Meta 存储，元信息存储,记录 raft 实现的内部状态，比如当前 term,、投票给哪个节点等信息。 Snapshot 存储,，用于存放用户的状态机 snapshot 及元信息，可选。 SnapshotStorage 用于 snapshot 存储实现， SnapshotExecutor 用于 snapshot 实际存储、远程安装、复制的管理。  状态机  StateMachine： 用户核心逻辑的实现,核心是 onApply(Iterator) 方法，应用通过 Node#apply(task) 提交的日志到业务状态机。 FSMCaller： 封装对业务 StateMachine 的状态转换的调用以及日志的写入等，一个有限状态机的实现，做必要的检查、请求合并提交和并发处理等。  复制  Replicator： 用于 leader 向 follower 复制日志，也就是 raft 中的 appendEntries 调用，包括心跳存活检查等。 ReplicatorGroup: 用于单个 RAFT Group 管理所有的 replicator，必要的权限检查和派发。  RPC RPC 模块用于节点之间的网络通讯: 1. RPC Server: 内置于 Node 内的 RPC 服务器，接收其他节点或者客户端发过来的请求，转交给对应服务处理。 2. RPC Client: 用于向其他节点发起请求，例如投票、复制日志、心跳等。\n","date":-62135596800,"description":"","dir":"projects/sofa-jraft/engine-architecture/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d2cc9de133aed20695229d0cde5b6ff9","permalink":"/projects/sofa-jraft/engine-architecture/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-jraft/engine-architecture/","summary":"如图: Node Raft 分组中的一个节点，连接封装底层的所有服务，用户看到的主要服务接口，特别是 apply(task) 用于向 raft group 组成的复制状态机集群提交新任务应用到业务状态机","tags":null,"title":"核心引擎设计","type":"projects","url":"/projects/sofa-jraft/engine-architecture/","wordcount":529},{"author":null,"categories":null,"content":" MOSN 主要划分为如下模块，包括了网络代理具备的基础能力，也包含了 xDS 等云原生能力。\nxDS（UDPA）支持 MOSN 支持云原生统一数据面 API（UDPA），支持全动态配置更新。\nxDS 是 Envoy 创建的一个关键概念，它是一类发现服务的统称，其包括如下几类：\n CDS：Cluster Discovery Service EDS：Endpoint Discovery Service SDS：Secret Discovery Service RDS：Route Discovery Service LDS：Listener Discovery Service  正是通过对 xDS 的请求来动态更新 Envoy 配置，另外还有个 ADS（Aggregated Discovery Service）通过聚合的方式解决以上 xDS 的更新顺序问题。\n业务支持 MOSN 作为底层的高性能安全网络代理，支撑了 RPC、消息（Messaging）、网关（Gateway）等业务场景。\nIO 模型 MOSN 支持以下两种 IO 模型：\n Golang 经典 netpoll 模型：goroutine-per-connection，适用于在连接数不是瓶颈的情况。\n RawEpoll 模型：也就是 Reactor 模式，I/O 多路复用（I/O multiplexing）+ 非阻塞 I/O（non-blocking I/O）的模式。对于接入层和网关有大量长链接的场景，更加适合于 RawEpoll 模型。\n  netpoll 模型 MOSN 的 netpoll 模型如上图所示，协程数量与链接数量成正比，大量链接场景下，协程数量过多，存在以下开销：\n Stack 内存开销 Read buffer 开销 Runtime 调度开销  RawEpoll 模型 RawEpoll 模型如上图所示，使用 epoll 感知到可读事件之后，再从协程池中为其分配协程进行处理，步骤如下：\n 链接建立后，向 Epoll 注册 oneshot 可读事件监听；并且此时不允许有协程调用 conn.read，避免与 runtime netpoll 冲突。 可读事件到达，从 goroutine pool 挑选一个协程进行读事件处理；由于使用的是 oneshot 模式，该 fd 后续可读事件不会再触发。 请求处理过程中，协程调度与经典 netpoll 模式一致。 请求处理完成，将协程归还给协程池；同时将 fd 重现添加到 RawEpoll 中。  协程模型 MOSN 的协程模型如下图所示。\n 一条 TCP 连接对应一个 Read 协程，执行收包、协议解析； 一个请求对应一个 worker 协程，执行业务处理，proxy 和 Write 逻辑；  常规模型一个 TCP 连接将有 Read/Write 两个协程，我们取消了单独的 Write 协程，让 workerpool 工作协程代替，减少了调度延迟和内存占用。\n能力扩展 协议扩展 MOSN 通过使用统一的编解码引擎以及编/解码器核心接口，提供协议的 plugin 机制，包括支持：\n SOFARPC HTTP1.x/HTTP2.0 Dubbo  NetworkFilter 扩展 MOSN 通过提供 network filter 注册机制以及统一的 packet read/write filter 接口，实现了 Network filter 扩展机制，当前支持：\n TCP proxy Fault injection  StreamFilter 扩展 MOSN 通过提供 stream filter 注册机制以及统一的 stream send/receive filter 接口，实现了 Stream filter 扩展机制，包括支持：\n 流量镜像 RBAC 鉴权  TLS 安全链路 通过测试，原生的 Go 的 TLS 经过了大量的汇编优化，在性能上是 Nginx（OpenSSL）的80%，Boring 版本的 Go（使用 cgo 调用 BoringSSL）因为 cgo 的性能问题， 并不占优势，所以我们最后选择使用原生 Go 的 TLS，相信 Go Runtime 团队后续会有更多的优化，我们也会有一些优化计划。\nGo vs Nginx 测试结果如下图所示：\n Go 在 RSA 上没有太多优化，go-boring（CGO）的能力是 Go 的两倍。 p256 在 Go 上有汇编优化，ECDSA 优于go-boring。 在 AES-GCM 对称加密上，Go 的能力是 go-boring 的 20 倍。 在 SHA、MD 等 HASH 算法也有对应的汇编优化。  为了满足金融场景的安全合规，我们同时也对国产密码进行了开发支持，这个是 Go Runtime 所没有的。虽然目前的性能相比国际标准 AES-GCM 还是有一些差距，大概是 50%，但是我们已经有了后续的一些优化计划，敬请期待。\n支持国密的性能测试结果如下图所示：\n","date":-62135596800,"description":"","dir":"projects/mosn/concept/core-concept/","fuzzywordcount":1400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5fd84bf5ceb2a4ab800cd0e2db774731","permalink":"/projects/mosn/concept/core-concept/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/mosn/concept/core-concept/","summary":"MOSN 主要划分为如下模块，包括了网络代理具备的基础能力，也包含了 xDS 等云原生能力。 xDS（UDPA）支持 MOSN 支持云原生统一数据面 API（UDPA），","tags":null,"title":"核心概念","type":"projects","url":"/projects/mosn/concept/core-concept/","wordcount":1332},{"author":null,"categories":null,"content":" 框架准备 在阅读前，您可以参考快速开始下载并安装 ACTS IDE 和引入 ACTS 框架.\n本部分主要包含编码说明、数据源配置和一键配置说明，以帮助您使用 ACTS 框架。\n编码说明 请确保 ACTS 的编码与系统代码的编码一致，即确定以下的编码保持一致：生成脚本选择的编码、workspace 的编码应该都与应用代码编码保持一致，不一致时会出现乱码问题。\n生成脚本选择的编码，如下图设置：\nIDEA workspace 的编码：\n数据源配置 ACTS 配置数据源的目的，是为了在数据准备、数据清理、数据校验阶段，能够使用系统的数据源正确的进行 DB 增删改查。\n数据源配置 在 src/test/resource/config/acts-config.properties 中配置 dal 层的 ModuleName、数据源以及表的对应关系，以 ds_ 开头，如下：\ndatasource_bundle_name =com.alipay.testapp.common.dal ds_bean1=table1,table2 ds_bean2=table3,table4 #配置格式 #ds_数据源bean=逻辑表名1,逻辑表名2  其中数据源 bean1、数据源 bean2 是应用代码中 dal 层的数据源 bean 的名称，支持多个数据源。表名支持正则表达式，无需带分库分表后缀，若有多个数据源时请注意，某张表只能属于一个数据源，如下图：\n数据库直连 数据库直连，用于 DB 数据模型的生成。在 src/test/resource/config/dbConf/ 下的 devdb.conf 或 testdb.conf 中配置如下：\nxxx_url = jdbc:oracle:thin:@localhost:1521:cifdb xxx_username = myname xxx_password = mypswd  一键配置的说明 一键配置测试框架主要生成包含两部分，一部分是基础 Java 类，另一类是必须的配置文件，具体生成内容如下：\nJava 类  AppNameActsBaseUtils.java\n测试脚本编写过程中常用的从框架中获取各种数据的工具类，初始化搭建只提供了常用的方法，可自行添加。\n AppNameActsTestBase.java\n封装后的应用测试基类，业务系统如有特殊需求可在其上自行封装，如果没有则可以忽略此文件。\n  配置文件 ","date":-62135596800,"description":"","dir":"projects/sofa-acts/usage-ready/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c3a89cbf42d55c98206a08e94d05ffde","permalink":"/projects/sofa-acts/usage-ready/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-acts/usage-ready/","summary":"框架准备 在阅读前，您可以参考快速开始下载并安装 ACTS IDE 和引入 ACTS 框架. 本部分主要包含编码说明、数据源配置和一键配置说明，以帮助您使用 ACTS 框架。 编码说","tags":null,"title":"框架准备","type":"projects","url":"/projects/sofa-acts/usage-ready/","wordcount":593},{"author":null,"categories":null,"content":" SOFABoot 从 2.4.0 版本开始支持基于 Spring 上下文隔离的模块化开发能力。为了更好的理解 SOFABoot 模块化开发的概念，我们来区分几个常见的模块化形式：\n 基于代码组织上的模块化：这是最常见的形式，在开发期，将不同功能的代码放在不同 Java 工程下，在编译期被打进不同 jar 包，在运行期，所有 Java 类都在一个 classpath 下，没做任何隔离； 基于 Spring 上下文隔离的模块化：借用 Spring 上下文来做不同功能模块的隔离，在开发期和编译期，代码和配置也会分在不同 Java 工程中，但在运行期，不同模块间的 Spring Bean 相互不可见，DI 只在同一个上下文内部发生，但是所有的 Java 类还是在同一个 ClassLoader 下； 基于 ClassLoader 隔离的模块化：借用 ClassLoader 来做隔离，每个模块都有独立的 ClassLoader，模块与模块之间的 classpath 不同，SOFAArk 就是这种模块化的实践方式。  SOFABoot 模块化开发属于第二种模块化形式 —— 基于 Spring 上下文隔离的模块化。每个 SOFABoot 模块使用独立的 Spring 上下文，避免不同 SOFABoot 模块间的 BeanId 冲突，有效降低企业级多模块开发时团队间的沟通成本。\n关于 SOFABoot 模块化产生的背景，可参考文章《蚂蚁金服的业务系统模块化 \u0026amp;mdash;- 模块化隔离方案》\n功能简介 依赖引入 使用 SOFABoot 模块化开发方案，需要引入如下依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;isle-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  SOFABoot 模块 SOFABoot 框架定义了 SOFABoot 模块的概念，一个 SOFABoot 模块是一个包括 Java 代码、Spring 配置文件、SOFABoot 模块标识等信息的普通 Jar 包，一个 SOFABoot 应用可以包含多个 SOFABoot 模块，每个 SOFABoot 模块都含有独立的 Spring 上下文。\n以 SOFABoot 模块为单元的模块化方式为开发者提供了以下功能：\n 运行时，每个 SOFABoot 模块的 Spring 上下文是隔离的，模块间定义的 Bean 不会相互影响； 每个 SOFABoot 模块是功能完备且自包含的，可以很容易在不同的 SOFABoot 应用中进行模块迁移和复用，只需将 SOFABoot 模块整个拷贝过去，调整 Maven 依赖，即可运行。  SOFABoot 模块的格式定义见: 模块配置。\nSOFABoot 模块间通信 上下文隔离后，模块与模块间的 Bean 无法直接注入，模块间需要通过 SOFA 服务进行通信，目前SOFABoot 提供了两种形式的服务发布和引用，用于解决不同级别的模块间调用的问题：\n JVM 服务发布和引用：解决一个 SOFABoot 应用内部各个 SOFABoot 模块之间的调用问题， JVM 服务发布与引用 RPC 服务发布和引用：解决多个 SOFABoot 应用之间的远程调用问题，RPC 服务发布与引用。  模块并行化启动 每个 SOFABoot 模块都是独立的 Spring 上下文，多个 SOFABoot 模块支持并行化启动，与 Spring Boot 的单 Spring 上下文模式相比，模块并行化启动能够加快应用的启动速度。\nRoot Application Context SOFABoot 应用运行时，本身会产生一个 Spring Context，我们把它叫做 Root Application Context，它是每个 SOFABoot 模块创建的 Spring Context 的 Parent。这样设计的目的是为了保证每个 SOFABoot 模块的 Spring Context 都能发现 Root Application Context 中创建的 Bean，这样当应用新增 Starter 时，不仅 Root Application Context 能够使用 Starter 中新增的 Bean，每个 SOFABoot 模块的 Spring Context 也能使用这些 Bean。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/modular-development/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"95bc080787c3614bfa485d2f3cd0de4c","permalink":"/projects/sofa-boot/modular-development/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/modular-development/","summary":"SOFABoot 从 2.4.0 版本开始支持基于 Spring 上下文隔离的模块化开发能力。为了更好的理解 SOFABoot 模块化开发的概念，我们来区分几个常见的模块化形式： 基于代码组织上的模块化","tags":null,"title":"模块化开发概述","type":"projects","url":"/projects/sofa-boot/modular-development/","wordcount":1073},{"author":null,"categories":null,"content":"SOFABoot 会根据 Require-Module 计算模块依赖树，例如以下依赖树表示模块B 和模块C 依赖模块A，模块E 依赖模块D，模块F 依赖模块E：\n该依赖树会保证模块A 必定在模块B 和模块C 之前启动，模块D 在模块E 之前启动，模块E 在模块F 之前启动，但是依赖树没有定义模块B 与模块C，模块B、C与模块D、E、F之间的启动顺序，这几个模块之间可以串行启动，也可以并行启动。\nSOFABoot 默认会并行启动模块，在使用过程中，如果希望关闭并行启动，可以在 application.properties 中增加以下参数:\ncom.alipay.sofa.boot.module-start-up-parallel=false  ","date":-62135596800,"description":"","dir":"projects/sofa-boot/parallel-start/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a6ef51b78d2a4f9af0debbc25ea45e8a","permalink":"/projects/sofa-boot/parallel-start/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/parallel-start/","summary":"SOFABoot 会根据 Require-Module 计算模块依赖树，例如以下依赖树表示模块B 和模块C 依赖模块A，模块E 依赖模块D，模块F 依赖模块E： 该依赖树会保证模块A 必定在模块B 和","tags":null,"title":"模块并行化启动","type":"projects","url":"/projects/sofa-boot/parallel-start/","wordcount":204},{"author":null,"categories":null,"content":" SOFABoot 模块是一个普通的 Jar 包加上一些 SOFABoot 特有的配置，这些 SOFABoot 特有的配置，让一个 Jar 包能够被 SOFABoot 识别，使之具备模块化的能力。\n一个完整的 SOFABoot 模块和一个普通的 Jar 包有两点区别:\n SOFABoot 模块包含一份 sofa-module.properties 文件，这份文件里面定义了 SOFABoot 模块的名称以及模块之间的依赖关系。 SOFABoot 模块的 META-INF/spring 目录下，可以放置任意多的 Spring 配置文件，SOFABoot 会自动把它们作为本模块的 Spring 配置加载起来。  sofa-module.properties 文件详解 先来看一份完整的 sofa-module.properties 文件（src/main/resources 目录下）：\nModule-Name=com.alipay.test.biz.service.impl Spring-Parent=com.alipay.test.common.dal Require-Module=com.alipay.test.biz.shared Module-Profile=dev  Module-Name Module-Name 是 SOFABoot 模块的名称，也是 SOFABoot 模块的唯一标示符。在一个 SOFABoot 应用中，一个 SOFABoot 模块的 Module-Name 必须和其他的 SOFABoot 模块的 Module-Name 不一样。需要注意的一点是，一个 SOFABoot 应用运行时的 SOFABoot 模块，不仅仅只包含本应用的模块，还包括依赖了其他应用的 SOFABoot 模块，确定是否唯一的时候需要把这些 SOFABoot 模块也考虑进去。\nRequire-Module Require-Module 用于定义模块之间的依赖顺序，值是以逗号分隔的 SOFABoot 模块名列表，比如上面的配置中，就表示本模块依赖于 com.alipay.test.biz.shared 模块。对于这种依赖关系的处理，SOFABoot 会将 com.alipay.test.biz.shared 模块在本模块之前启动，即com.alipay.test.biz.shared 模块将先启动 Spring 上下文。\n一般情况下，是不需要为模块定义 Require-Module 的，只有当模块的 Spring 上下文的启动依赖于另一个模块的 Spring 上下文的启动时，才需要定义 Require-Module。举一个例子，如果你在 A 模块中发布了一个 SOFA JVM Service。在 B 模块的某一个 Bean 的 init 方法里面，需要使用 SOFA Reference 调用这个 JVM Service。假设 B 模块在 A 模块之前启动了，那么 B 模块的 Bean 就会因为 A 模块的 JVM Service 没有发布而 init 失败，导致 Spring 上下文启动失败。这个时候，我们就可以使用 Require-Module 来强制 A 模块在 B 模块之前启动。\nSpring-Parent 在 SOFABoot 应用中，每一个 SOFABoot 模块都是一个独立的 Spring 上下文，并且这些 Spring 上下文之间是相互隔离的。虽然这样的模块化方式可以带来诸多好处，但是，在某些场景下还是会有一些不便，这个时候，你可以通过 Spring-Parent 来打通两个 SOFABoot 模块的 Spring 上下文。Spring-Parent 属性可以配置一个模块的名称，比如上面的配置中，就将 com.alipay.test.common.dal 的 Spring 上下文设置为当前模块的 Spring 上下文的父 Spring 上下文。\n由于 Spring 的限制，一个模块的 Spring-Parent 只能有一个模块\n关于 Spring 的父上下文的作用可以看 Spring 的 BeanFactory 的说明：http://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/beans/factory/BeanFactory.html\nModule-Profile 支持 SOFABoot Profile 能力： SOFABoot Profile\nSpring 配置文件 SOFABoot 模块可以包含 Spring 配置文件，配置文件需要放置在 META-INF/spring 目录下，SOFABoot 启动时会自动扫描该目录，并把目录下所有 XML 文件作为本模块的 Spring 配置加载起来。在 Spring 配置文件中，我们可以定义 Bean、发布服务等等。\nSOFABoot 模块一般用于封装对外发布服务接口的具体实现，属于业务层，Controller 属于展现层内容，我们不建议也不支持在 SOFABoot 模块中定义 Controller 组件，Controller 组件相关定义请直接放在 Root Application Context。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/sofaboot-module/","fuzzywordcount":1200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2dbb8a536237f21afbee1e3f320b8193","permalink":"/projects/sofa-boot/sofaboot-module/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/sofa-boot/sofaboot-module/","summary":"SOFABoot 模块是一个普通的 Jar 包加上一些 SOFABoot 特有的配置，这些 SOFABoot 特有的配置，让一个 Jar 包能够被 SOFABoot 识别，使之具备模块化的能力。 一个完整的 SOFABoot 模块和一个普通的 Jar 包","tags":null,"title":"模块配置","type":"projects","url":"/projects/sofa-boot/sofaboot-module/","wordcount":1194},{"author":null,"categories":null,"content":"如果你要扩展一个注册中心，我们先看下注册中心的抽象类。\npackage com.alipay.sofa.rpc.registry; @Extensible(singleton = false) public abstract class Registry implements Initializable, Destroyable { public abstract boolean start(); public abstract void register(ProviderConfig config); public abstract void unRegister(ProviderConfig config); public abstract void batchUnRegister(List\u0026amp;lt;ProviderConfig\u0026amp;gt; configs); public abstract List\u0026amp;lt;ProviderGroup\u0026amp;gt; subscribe(ConsumerConfig config); public abstract void unSubscribe(ConsumerConfig config); public abstract void batchUnSubscribe(List\u0026amp;lt;ConsumerConfig\u0026amp;gt; configs); }  可以看到我们需要的主要接口。\n启动注册中心客户端、维持连接 销毁注册中心客户端、释放资源 发布服务、缓存发布信息 取消发布服务、删除缓存 订阅服务列表、同步或者异步返回数据，有变化接收通知 取消订阅服务列表、删除缓存  其它 - [ ] 注册中心节点断连后，不影响本地调用 - [ ] 和一个注册中心节点断连后，可自己切换到其它注册中心节点 - [ ] 注册中心节点切换后，自动恢复注册和订阅信息 - [ ] 注册中心数据缓存到本地文件，就算连不上任何注册中心，服务提供者和服务调用者也能重启并正常调用\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-extension-guide/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c952ecbea16f7ae68ad095ab8baf0583","permalink":"/projects/sofa-rpc/registry-extension-guide/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-extension-guide/","summary":"如果你要扩展一个注册中心，我们先看下注册中心的抽象类。 package com.alipay.sofa.rpc.registry; @Extensible(singleton = false) public abstract class Registry implements Initializable, Destroyable { public abstract boolean start(); public abstract void register(ProviderConfig config); public abstract void unRegister(ProviderConfig config); public abstract void batchUnRegister(List\u0026lt;ProviderConfig\u0026gt; configs); public abstract List\u0026lt;ProviderGroup\u0026gt; subscribe(ConsumerConfig config); public abstract void unSubscribe(ConsumerConfig config); public","tags":null,"title":"注册中心扩展指南","type":"projects","url":"/projects/sofa-rpc/registry-extension-guide/","wordcount":302},{"author":null,"categories":null,"content":"SOFABoot RPC Starter 为用户提供多种注册中心选择和方便的配置。 目前 bolt ， rest ， dubbo 都支持 Zookeeper 作为注册中心。另外 bolt ， rest 支持本地文件系统作为注册中心，该种模式一般用于测试。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/registry-usage/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5a1a4619c8ac4a9fc27b8576472aed9f","permalink":"/projects/sofa-rpc/registry-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/registry-usage/","summary":"SOFABoot RPC Starter 为用户提供多种注册中心选择和方便的配置。 目前 bolt ， rest ， dubbo 都支持 Zookeeper 作为注册中心。另外 bolt ， rest 支持本地文件系统作为注册中心，该种模式一般用于测","tags":null,"title":"注册中心选择","type":"projects","url":"/projects/sofa-rpc/registry-usage/","wordcount":72},{"author":null,"categories":null,"content":" 本文描述的是 MOSN 作为 Sidecar 使用时的流量劫持方案。\nMOSN 作为 Sidecar 和业务容器部署在同一个 Pod 中时，需要使得业务应用的 Inbound 和 Outbound 服务请求都能够经过 Sidecar 处理。区别于 Istio 社区使用 iptables 做流量透明劫持，MOSN 目前使用的是流量接管方案，并在积极探索适用于大规模流量下的透明劫持方案。\n流量接管 区别于 Istio 社区的 iptables 流量劫持方案，MOSN 使用的流量接管的方案如下：\n 假设服务端运行在 1.2.3.4 这台机器上，监听 20880 端口，首先服务端会向自己的 Sidecar 发起服务注册请求，告知 Sidecar 需要注册的服务以及 IP + 端口（1.2.3.4:20880） 服务端的 Sidecar 会向服务注册中心（如 SOFA Registry）发起服务注册请求，告知需要注册的服务以及 IP + 端口，不过这里需要注意的是注册上去的并不是业务应用的端口（20880），而是 Sidecar 自己监听的一个端口（例如：20881） 调用端向自己的 Sidecar 发起服务订阅请求，告知需要订阅的服务信息 调用端的 Sidecar 向调用端推送服务地址，这里需要注意的是推送的 IP 是本机，端口是调用端的 Sidecar 监听的端口（例如 20882） 调用端的 Sidecar 会向服务注册中心（如 SOFA Registry）发起服务订阅请求，告知需要订阅的服务信息； 服务注册中心（如 SOFA Registry）向调用端的 Sidecar 推送服务地址（1.2.3.4:20881）  服务调用过程 经过上述的服务发现过程，流量转发过程就显得非常自然了：\n 调用端拿到的服务端地址是 127.0.0.1:20882，所以就会向这个地址发起服务调用 调用端的 Sidecar 接收到请求后，通过解析请求头，可以得知具体要调用的服务信息，然后获取之前从服务注册中心返回的地址后就可以发起真实的调用（1.2.3.4:20881） 服务端的 Sidecar 接收到请求后，经过一系列处理，最终会把请求发送给服务端（127.0.0.1:20880）  透明劫持 上文通过在服务注册过程中把服务端地址替换成本机监听端口实现了轻量级的“流量劫持”，在存在注册中心，且调用端和服务端同时使用特定SDK的场景中可以很好的工作，如果不满足这两个条件，则无法流量劫持。为了降低对于应用程序的要求，需要引入透明劫持。\n使用 iptables 做流量劫持 iptables 通过 NAT 表的 redirect 动作执行流量重定向，通过 syn 包触发新建 nefilter 层的连接，后续报文到来时查找连接转换目的地址与端口。新建连接时同时会记录下原始目的地址，应用程序可以通过(SOL_IP、SO_ORIGINAL_DST)获取到真实的目的地址。\niptables 劫持原理如下图所示：\n使用 iptables 做流量劫持时存在的问题 目前 Istio 使用 iptables 实现透明劫持，主要存在以下三个问题： 1. 需要借助于 conntrack 模块实现连接跟踪，在连接数较多的情况下，会造成较大的消耗，同时可能会造成 track 表满的情况，为了避免这个问题，业内有关闭 conntrack 的做法，比如阿里巴巴就关闭了这个模块。 1. iptables 属于常用模块，全局生效，不能显式的禁止相关联的修改，可管控性比较差。 1. iptables 重定向流量本质上是通过 loopback 交换数据，outbond 流量将两次穿越协议栈，在大并发场景下会损失转发性能。\n上述几个问题并非在所有场景中都存在，比方说某些场景下，连接数并不多，且 NAT 表未被使用到的情况下，iptables 是一个满足要求的简单方案。为了适配更加广泛的场景，透明劫持需要解决上述三个问题。\n透明劫持方案优化 使用 tproxy 处理 inbound 流量\ntproxy 可以用于 inbound 流量的重定向，且无需改变报文中的目的 IP/端口，不需要执行连接跟踪，不会出现 conntrack 模块创建大量连接的问题。受限于内核版本，tproxy 应用于 outbound 存在一定缺陷。目前 Istio 支持通过 tproxy 处理 inbound 流量。\n使用 hook connect 处理 outbound 流量\n为了适配更多应用场景，outbound 方向通过 hook connect 来实现，实现原理如下：\n无论采用哪种透明劫持方案，均需要解决获取真实目的 IP/端口的问题，使用 iptables 方案通过 getsockopt 方式获取，tproxy 可以直接读取目的地址，通过修改调用接口，hook connect 方案读取方式类似于tproxy。\n实现透明劫持后，在内核版本满足要求（4.16以上）的前提下，通过 sockmap 可以缩短报文穿越路径，进而改善 outbound 方向的转发性能。\n总结 总结来看，如果应用程序通过注册中心发布/订阅服务时，可以结合注册中心劫持流量；在需要用到透明劫持的场景，如果性能压力不大，使用 iptables redirect 即可，大并发压力下使用 tproxy 与hook connect 结合的方案。\n","date":-62135596800,"description":"","dir":"projects/mosn/concept/traffic-hijack/","fuzzywordcount":1700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5845d8478a48fcddc74f0b9d28ede2c2","permalink":"/projects/mosn/concept/traffic-hijack/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/mosn/concept/traffic-hijack/","summary":"本文描述的是 MOSN 作为 Sidecar 使用时的流量劫持方案。 MOSN 作为 Sidecar 和业务容器部署在同一个 Pod 中时，需要使得业务应用的 Inbound 和 Outbound 服务请求都能够经过 Sidecar 处理。区别于 Istio 社","tags":null,"title":"流量劫持","type":"projects","url":"/projects/mosn/concept/traffic-hijack/","wordcount":1671},{"author":null,"categories":null,"content":" 版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 5.1.2。\n参见: http://semver.org/lang/zh-CN/。\n 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本号不一定完全兼容，尽量向下兼容。 次版本号：代表新特性增强。版本号越大特性越丰富。 修订版本号：代表BugFix版本。只做bug修复使用，版本号越大越稳定。  版本维护 最多同时维护两个版本。\n例如当前主干为 5.3.0，那么将会维护 5.2.x 的 bugfix 分支，而 5.1.x 遇到 bug 将不再修复，建议升级。\n发布流程  日常开发分支采用 SNAPSHOT 版本，例如 5.3.0-SNAPSHOT。 正式发布时修改版本为正式版本，例如 5.3.0。 发布后拉起下一个版本，例如 5.3.1-SNAPSHOT。  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/version-release/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"604f113607e6815757f4d1907190c13c","permalink":"/projects/sofa-rpc/version-release/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/version-release/","summary":"版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 5.1.2。 参见: http://semver.org/lang/zh-CN/","tags":null,"title":"版本发布","type":"projects","url":"/projects/sofa-rpc/version-release/","wordcount":315},{"author":null,"categories":null,"content":" 1.3.2 2020-06-19\n Bug Fixes  移除对 bolt address parser 的扩展，避免 check connection 返回结果不符合预期 SPI 组件 JRaftServiceLoader 改为延迟加载策略规避多余对象的创建 几个 corner case 修复，比如 replicate logs 如果比 appliedIndex（follower）更小，那么可以认为是成功的 关闭 Recyclers 时的 IndexOutOfBoundsException 问题修复  Features  抽象出网络通信层，增加 GRPC 实现并支持 Replication Pipeline，用户亦可自行对通信层进行其他实现的扩展 RheaKV 增加 reverseScan API 提供 Replicator 与 RPC 的线程池隔离，避免相互影响 read-index 线性一致读请求提供请求超时（timeout）配置  Breaking Changes  无  致谢（排名不分先后）  @shibd @SteNicholas @killme2008 @zongtanghu   1.3.1 2020-04-17\n Bug Fixes  修复 bolt rpc callback 默认的任务饱和丢弃策略，改为抛出异常 修复 learner 启动晚于 leader 选举成功时无法复制日志的 bug  Features  multi raft group 之间共享 timer 和 scheduler 等较重的线程资源，优化 multi group 场景中的多余资源占用 提供 RPC adapter，用户可基于 SPI 扩展不同的 RPC 实现 正式提供稳定的 RocksDBSegmentLogStorage，适合 value 较大的数据存储 sofa-bolt 升级到 1.6.1，支持 SSL 以及具有更好的小数据包传输能力 引入一个新的数据结构 segment list 来解决 LogManager 中过多的 log memory copy 采纳 nacos 的建议，对 raft Task 增加 join API  Breaking Changes  无  致谢（排名不分先后）  @jovany-wang @SteNicholas @zongtanghu @OpenOpened   1.3.0 2019-11-29\n Bug Fixes  删除数据并重启且期间没有新的 task 提交的情况下 prev log index 紊乱的修复 修复一些选举和线性一致读相关的 corner case Recyclers 多个线程 recycle 资源时的 NPE 修复  Features  新增 Read-only member(learner) 角色，支持 learner 节点上的线性一致读 实现优先级选举 在 multi raft group 的场景中，随机打散每个 group 的第一次 snapshot timeout 时间，避免一个进程内多个 group 同时 snapshot RheaKV 新增 containsKey API RheaKV 实现 snapshot checksum 以及异步 snapshot 新增 replicator 的 state 监听器： ReplicatorStateListener RepeatedTimer 的默认实现替换为 HashedWheelTimer 修复 windows 上定时器 CPU 消耗偏高的问题 kill -s SIGUSR2 pid 中增加打印 rocksdb stats 和所有 ThreadPool 指标统计信息 升级 rocksdb 版本到 5.18.3 新增实验性质的 RocksDBSegmentLogStorage，适合 value 较大的数据存储 Counter 例子改进，演示 ReadIndex 线性一致读 当优化 checksum 中多余的 mem copy  Breaking Changes  无  致谢（排名不分先后）  @zongtanghu @devYun @masaimu @SteNicholas @yetingsky   1.2.6 2019-08-15\n Bug Fixes  修复 ReadIndex 并发情况下可能出现的读超时 保存 raft meta 失败后终止状态机 修复 windows 环境中无法原子 move 文件的问题 当 RheaKV apply 失败时终止状态机避免出现数据不一致情况  Features  增加 LogEntry checksum validation 优化 log replication 线程模型减少锁竞争 优化 RheaKV multi group snapshot 对于 multi-raft-group 场景，提供 manual rebalance API 在无 PD 模式手动平衡各节点 leader 数量 CliService 提供获取存活 follower 节点的 API 引入 SPI 扩展机制，LogStorage、SnapshotStorage、RaftMetaStorage、LogEntryCodec 均可基于 SPI 扩展 Linux 平台 SIGUSR2 信号输出节点状态以及 metric 信息 RheaKV 增加 CompareAndPut 原子更新 API 新增 pooled buf allocator 解决 log replication 时大量分配 byte[] 频繁触发 fullgc 默认关闭 RheaKV rocksdb 的 fsync 和 WAL，依靠 raft log 和 snapshot 确保数据一致性 当 raft node 过载时拒绝新的请求  Breaking Changes  无  致谢（排名不分先后）  @SteNicholas @zongtanghu   1.2.5 2019-04-01\n Bug Fixes  修复 jmh 与 unit test 代码冲突问题 修复 snapshot 过大引起的安装失败 bug，会影响新增节点的加入  Features  LogManagerImpl 中耗费 cpu 部分的代码优化 修正一些单词拼写错误  Breaking Changes  无   此版本强烈推荐升级\n1.2.4 2019-03-20\n Bug Fixes  修复一种情况下 lease read 的 stale read 部分 timestamp 修改为 monotonic time 修复一种情况下 replicator 被 block 住的问题 解决 windows 平台下某些单测无法创建目录 解决 windows 平台下某些 rocksdb options 设置不当导致进程 crash  Features  开放 RocksDB options 的设置给用户层 Pre-vote 优化，启用 lease 机制来规避网络分区+集群长时间无写入的情况下，游离节点回归后打断当前 term，提升系统可用性 升级 bolt …","date":-62135596800,"description":"","dir":"projects/sofa-jraft/release-log/","fuzzywordcount":2300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9e24fb74a3cda6a600252b01f8a85db9","permalink":"/projects/sofa-jraft/release-log/","publishdate":"0001-01-01T00:00:00Z","readingtime":5,"relpermalink":"/projects/sofa-jraft/release-log/","summary":"1.3.2 2020-06-19 Bug Fixes 移除对 bolt address parser 的扩展，避免 check connection 返回结果不符合预期 SPI 组件 JRaftServiceLoader 改为延迟加载策略规避多余对象的创建 几个 corner case 修复，比如 replicate logs 如果比 appliedI","tags":null,"title":"版本发行日志","type":"projects","url":"/projects/sofa-jraft/release-log/","wordcount":2298},{"author":null,"categories":null,"content":" 通过 SOFABoot，我们可以直接在浏览器中就可以查看 SOFA 中间件的版本等详细信息。\n引入 SOFABoot Infra 依赖 要在 SOFABoot 中直接通过浏览器查看 SOFA 中间件的版本信息，只需要在 Maven 依赖中增加如下的内容即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.alipay.sofa\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;infra-sofa-boot-starter\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  版本信息查看 在应用启动成功后，可以在浏览器中输入 http://localhost:8080/sofaboot/versions 查看 SOFA 中间件的版本信息，如：\n[ { GroupId: \u0026amp;quot;com.alipay.sofa\u0026amp;quot;, Doc-Url: \u0026amp;quot;https://github.com/sofastack/sofa-boot\u0026amp;quot;, ArtifactId: \u0026amp;quot;infra-sofa-boot-starter\u0026amp;quot;, Built-Time: \u0026amp;quot;2018-04-05T20:55:22+0800\u0026amp;quot;, Commit-Time: \u0026amp;quot;2018-04-05T20:54:26+0800\u0026amp;quot;, Commit-Id: \u0026amp;quot;049bf890bb468aafe6a3e07b77df45c831076996\u0026amp;quot;, Version: \u0026amp;quot;2.4.0\u0026amp;quot; } ]  注: 在 SOFABoot 3.x 中调整了 endpoint 路径，sofaboot/versions 更改为 actuator/versions\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/view-versions/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c6b6d22e9038aa1f5e4ce74449ba1cda","permalink":"/projects/sofa-boot/view-versions/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/view-versions/","summary":"通过 SOFABoot，我们可以直接在浏览器中就可以查看 SOFA 中间件的版本等详细信息。 引入 SOFABoot Infra 依赖 要在 SOFABoot 中直接通过浏览器查看 SOFA 中间件的版本信息，只","tags":null,"title":"版本查看","type":"projects","url":"/projects/sofa-boot/view-versions/","wordcount":182},{"author":null,"categories":null,"content":" 版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 1.0.0。\n参见: http://semver.org/lang/zh-CN/\n 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本号不一定完全兼容，尽量向下兼容。 次版本号：代表新特性增强。版本号越大特性越丰富。 修订版本号：代表BugFix版本。只做bug修复使用，版本号越大越稳定。  版本维护 最多同时维护两个版本。\n例如当前主干为 1.2.0，那么将会维护 1.1.x 的 bugfix 分支，而 1.0.x 遇到 bug 将不再修复，建议升级。\n发布流程  日常开发分支采用 SNAPSHOT 版本，例如 1.0.0-SNAPSHOT。 正式发布时修改版本为正式版本，例如 1.0.0。 发布后拉起下一个版本，例如 1.0.1-SNAPSHOT。  ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/version-rule/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a2093bdf478bdff0e15a2de70e522d03","permalink":"/projects/sofa-dashboard/version-rule/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/version-rule/","summary":"版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 1.0.0。 参见: http://semver.org/lang/zh-CN/ 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本","tags":null,"title":"版本规范","type":"projects","url":"/projects/sofa-dashboard/version-rule/","wordcount":286},{"author":null,"categories":null,"content":" 版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 5.2.0。\n参见: http://semver.org/lang/zh-CN/\n 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本号不一定完全兼容，尽量向下兼容。 次版本号：代表新特性增强。版本号越大特性越丰富。 修订版本号：代表BugFix版本。只做bug修复使用，版本号越大越稳定。  版本维护 最多同时维护两个版本。\n例如当前主干为 5.4.0，那么将会维护 5.3.x 的 bugfix 分支，而 5.2.x 遇到 bug 将不再修复，建议升级。\n发布流程  日常开发分支采用 SNAPSHOT 版本，例如 5.3.0-SNAPSHOT。 正式发布时修改版本为正式版本，例如 5.3.0。 发布后拉起下一个版本，例如 5.3.1-SNAPSHOT。  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/release-standard/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"71aad9cbc42aba3d9f875ae9169cf005","permalink":"/projects/sofa-registry/release-standard/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-registry/release-standard/","summary":"版本号 采用三位版本号，分别是主版本号、次版本号、修订版本号。例如 5.2.0。 参见: http://semver.org/lang/zh-CN/ 主版本号：主版本号内的所有版本必须相互兼容；与其它主版本","tags":null,"title":"版本规范","type":"projects","url":"/projects/sofa-registry/release-standard/","wordcount":286},{"author":null,"categories":null,"content":"SOFABoot 使用了一些三方开源组件，他们分别是：\n一些主要依赖：\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License sofa-common-tools under Apache 2.0 license  一些扩展依赖：\n nuxeo under Apache License, Version 2.0  \u0026amp;hellip; 其它整理中。\n","date":-62135596800,"description":"","dir":"projects/sofa-boot/notice/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"11f073a7a9965ab5690ed166fe319bbd","permalink":"/projects/sofa-boot/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-boot/notice/","summary":"SOFABoot 使用了一些三方开源组件，他们分别是： 一些主要依赖： Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License sofa-common-tools under Apache 2.0 license 一些扩展依赖： nuxeo under Apache License, Version 2.0 \u0026hellip; 其它整理中。","tags":null,"title":"版权声明","type":"projects","url":"/projects/sofa-boot/notice/","wordcount":67},{"author":null,"categories":null,"content":" 依赖组件版权说明 SOFADashboard 使用了一些三方开源组件，他们分别是：\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License SOFA Bolt under Apache License 2.0 SOFA Bolt under Apache License 2.0 Curator under Apache License 2.0  \u0026amp;hellip; 其它整理中。\n","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/notice/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a9ebe38d245302f94ab7bfa793329926","permalink":"/projects/sofa-dashboard/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/notice/","summary":"依赖组件版权说明 SOFADashboard 使用了一些三方开源组件，他们分别是： Spring under Apache 2.0 license Spring Boot under Apache 2.0 license SLF4j under the MIT License SOFA Bolt under Apache License 2.0 SOFA Bolt under Apache License 2.0 Curator under Apache License 2.0 \u0026hellip; 其它整理中。","tags":null,"title":"版权声明","type":"projects","url":"/projects/sofa-dashboard/notice/","wordcount":67},{"author":null,"categories":null,"content":" 依赖组件版权说明 SOFARegistry 使用了一些三方开源组件，他们分别是：\n Spring under Apache 2.0 license Spring Boot under Apache 2.0 license Netty under Apache License 2.0 SLF4j under the MIT License jersey under CDDL Version 1.1  SOFAJRaft under Apache License 2.0 SOFABolt under Apache License 2.0 SOFAHessian under Apache License 2.0  \u0026amp;hellip; 其它整理中，如若发现遗漏，还请主动告知。\n","date":-62135596800,"description":"","dir":"projects/sofa-registry/notice/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"c40263ffd56a2f1292756c9fafea55e2","permalink":"/projects/sofa-registry/notice/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-registry/notice/","summary":"依赖组件版权说明 SOFARegistry 使用了一些三方开源组件，他们分别是： Spring under Apache 2.0 license Spring Boot under Apache 2.0 license Netty under Apache License 2.0 SLF4j under the MIT License jersey under CDDL Version 1.1 SOFAJRaft under Apache License 2.0 SOFABolt under Apache License 2.0 SOFAHessian under Apache License 2.0 \u0026hellip; 其","tags":null,"title":"版权声明","type":"projects","url":"/projects/sofa-registry/notice/","wordcount":89},{"author":null,"categories":null,"content":" RheaKV：基于 JRaft 和 RocksDB 实现的嵌入式、分布式、高可用、强一致的 KV 存储类库。 AntQ Streams QCoordinator： 使用 JRaft 在 coordinator 集群内做选举、元信息存储等功能。 SOFA 服务注册中心元信息管理模块：IP 数据信息注册，要求写数据达到各个节点一致，并且在不小于一半节点挂掉，保证不影响数据正常存储。 AntQ NameServer 选主  ","date":-62135596800,"description":"","dir":"projects/sofa-jraft/user-stories/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b233be7d9eed33645945293e637e28ea","permalink":"/projects/sofa-jraft/user-stories/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-jraft/user-stories/","summary":"RheaKV：基于 JRaft 和 RocksDB 实现的嵌入式、分布式、高可用、强一致的 KV 存储类库。 AntQ Streams QCoordinator： 使用 JRaft 在 coordinator 集群内做选举、元信息存储等","tags":null,"title":"用户案例","type":"projects","url":"/projects/sofa-jraft/user-stories/","wordcount":140},{"author":null,"categories":null,"content":"SOFARPC 支持指定地址进行调用的场景。用 Java API 的使用方式如下，设置直连地址即可：\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumer = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig) .setDirectUrl(\u0026amp;quot;bolt://127.0.0.1:12201\u0026amp;quot;);  用 XML 的使用方式如下：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.alipay.sample.HelloService\u0026amp;quot; id=\u0026amp;quot;helloService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:route target-url=\u0026amp;quot;127.0.0.1:12200\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  用 Annotation 的使用方式如下：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, directUrl = \u0026amp;quot;127.0.0.1:12220\u0026amp;quot;)) private SampleService sampleService;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/peer-to-peer/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b1815c322f5dc9528f6429d1d5e38369","permalink":"/projects/sofa-rpc/peer-to-peer/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/peer-to-peer/","summary":"SOFARPC 支持指定地址进行调用的场景。用 Java API 的使用方式如下，设置直连地址即可： ConsumerConfig\u0026lt;HelloService\u0026gt; consumer = new ConsumerConfig\u0026lt;HelloService\u0026gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig) .setDirectUrl(\u0026quot;bolt://127.0.0.1:12201\u0026quot;); 用 XML 的使用方式如下： \u0026lt;sofa:reference interface=\u0026quot;com.alipay.sample.HelloService\u0026quot; id=\u0026quot;helloService\u0026quot;\u0026gt; \u0026lt;sofa:binding.bolt\u0026gt; \u0026lt;sofa:route target-url=\u0026quot;127.0.0.1:12200\u0026quot;/\u0026gt; \u0026lt;/sofa:binding.bolt\u0026gt; \u0026lt;/sofa:reference\u0026gt; 用 Annotation 的使用方式如下","tags":null,"title":"直连调用","type":"projects","url":"/projects/sofa-rpc/peer-to-peer/","wordcount":82},{"author":null,"categories":null,"content":"介绍几种 SOFARPC 在不同环境下的使用方式 * 非 Spring 环境 API 使用 * SOFABoot 环境 XML 配置使用 * SOFABoot 环境注解使用 * SOFABoot 环境动态 API 使用\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programming/","fuzzywordcount":100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"9a947dae761c84aa4d95121c076ac552","permalink":"/projects/sofa-rpc/programming/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/programming/","summary":"介绍几种 SOFARPC 在不同环境下的使用方式 * 非 Spring 环境 API 使用 * SOFABoot 环境 XML 配置使用 * SOFABoot 环境注解使用 * SOFABoot 环境动态 API 使用","tags":null,"title":"编程界面","type":"projects","url":"/projects/sofa-rpc/programming/","wordcount":50},{"author":null,"categories":null,"content":"自动故障剔除会自动监控 RPC 调用的情况，对故障节点进行权重降级，并在节点恢复健康时进行权重恢复。目前支持 bolt 协议。\n在 SOFABoot 中，只需要配置自动故障剔除的参数到 application.properties 即可。可以不完全配置，只配置自己关心的参数，其余参数会取默认值。需要注意的是 com.alipay.sofa.rpc.aft.regulation.effective 是该功能的全局开关，如果关闭则该功能不会运行，其他参数也都不生效。\n   属性 描述 默认值     timeWindow 时间窗口大小：对统计信息计算的周期。 10s   leastWindowCount 时间窗口内最少调用数：只有在时间窗口内达到了该最低值的数据才会被加入到计算和调控中。 10次   leastWindowExceptionRateMultiple 时间窗口内异常率与服务平均异常率的降级比值：在对统计信息进行计算的时候，会计算出该服务所有有效调用ip的平均异常率，如果某个ip的异常率大于等于了这个最低比值，则会被降级。 6倍   weightDegradeRate 降级比率：地址在进行权重降级时的降级比率。 1\u0026amp;frasl;20   weightRecoverRate 恢复比率：地址在进行权重恢复时的恢复比率。 2倍   degradeEffective 降级开关：如果应用打开了这个开关，则会对符合降级的地址进行降级，否则只会进行日志打印。 false(关闭)   degradeLeastWeight 降级最小权重：地址权重被降级后的值如果小于这个最小权重，则会以该最小权重作为降级后的值。 1   degradeMaxIpCount 降级的最大ip数：同一个服务被降级的ip数不能超过该值。 2   regulationEffective 全局开关：如果应用打开了这个开关，则会开启整个单点故障自动剔除摘除功能，否则完全不进入该功能的逻辑。 false(关闭)     示例\ncom.alipay.sofa.rpc.aft.time.window=20 com.alipay.sofa.rpc.aft.least.window.count=30 com.alipay.sofa.rpc.aft.least.window.exception.rate.multiple=1.4 com.alipay.sofa.rpc.aft.weight.degrade.rate=0.5 com.alipay.sofa.rpc.aft.weight.recover.rate=1.2 com.alipay.sofa.rpc.aft.degrade.effective=true com.alipay.sofa.rpc.aft.degrade.least.weight=1 com.alipay.sofa.rpc.aft.degrade.max.ip.count=2 com.alipay.sofa.rpc.aft.regulation.effective=true  如上配置，打开了自动故障剔除功能和降级开关，当节点出现故障时会被进行权重降级，在恢复时会被进行权重恢复。每隔 20s 进行一次节点健康状态的度量，20s 内调用次数超过 30 次的节点才被作为计算数据，如果单个节点的异常率超过了所有节点的平均异常率的 1.4 倍则对该节点进行权重降级，降级的比率为 0.5 。权重最小降级到 1 。如果单个节点的异常率低于了平均异常率的 1.4 倍则对该节点进行权重恢复，恢复的比率为1.2 。单个服务最多降级 2 个ip。\n  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/configuration-fault-tolerance/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a132b54b2398534d1773489e2b0db166","permalink":"/projects/sofa-rpc/configuration-fault-tolerance/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/configuration-fault-tolerance/","summary":"自动故障剔除会自动监控 RPC 调用的情况，对故障节点进行权重降级，并在节点恢复健康时进行权重恢复。目前支持 bolt 协议。 在 SOFABoot 中，只需要配置自动故障剔除的","tags":null,"title":"自动故障剔除","type":"projects","url":"/projects/sofa-rpc/configuration-fault-tolerance/","wordcount":759},{"author":null,"categories":null,"content":"集群中通常一个服务有多个服务提供者。其中部分服务提供者可能由于网络，配置，长时间 fullgc ，线程池满，硬件故障等导致长连接还存活但是程序已经无法正常响应。单机故障剔除功能会将这部分异常的服务提供者进行降级，使得客户端的请求更多地指向健康节点。当异常节点的表现正常后，单机故障剔除功能会对该节点进行恢复，使得客户端请求逐渐将流量分发到该节点。单机故障剔除功能解决了服务故障持续影响业务的问题，避免了雪崩效应。可以减少人工干预需要的较长的响应时间，提高系统可用率。\n运行机制：\n 单机故障剔除会统计一个时间窗口内的调用次数和异常次数，并计算每个服务对应ip的异常率和该服务的平均异常率。 当达到ip异常率大于服务平均异常率到一定比例时，会对该服务+ip的维度进行权重降级。 如果该服务+ip维度的权重并没有降为0，那么当该服务+ip维度的调用情况正常时，则会对其进行权重恢复。 整个计算和调控过程异步进行，不会阻塞调用。  单机故障剔除的使用方式如下：\nFaultToleranceConfig faultToleranceConfig = new FaultToleranceConfig(); faultToleranceConfig.setRegulationEffective(true); faultToleranceConfig.setDegradeEffective(true); faultToleranceConfig.setTimeWindow(20); faultToleranceConfig.setWeightDegradeRate(0.5); FaultToleranceConfigManager.putAppConfig(\u0026amp;quot;appName\u0026amp;quot;, faultToleranceConfig);  如上，该应用会在打开了单机故障剔除开关，每20s的时间窗口进行一次异常情况的计算，如果某个服务+ip的调用维度被判定为故障节点，则会进行将该服务+ip的权重降级为0.5倍。\n更加详细的参数请参考单机故障剔除参数。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/fault-tolerance/","fuzzywordcount":600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7501b0fac1d1d89c61de0d591e29e1d0","permalink":"/projects/sofa-rpc/fault-tolerance/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/fault-tolerance/","summary":"集群中通常一个服务有多个服务提供者。其中部分服务提供者可能由于网络，配置，长时间 fullgc ，线程池满，硬件故障等导致长连接还存活但是程序已经无法正常","tags":null,"title":"自动故障剔除","type":"projects","url":"/projects/sofa-rpc/fault-tolerance/","wordcount":528},{"author":null,"categories":null,"content":" 在使用自定义埋点组件的情况下，用户可以选择自定义 Reporter。\n自定义 Reporter 实现 public class MyReporter implements Reporter { @Override public String getReporterType() { return \u0026amp;quot;myReporter\u0026amp;quot;; } @Override public void report(SofaTracerSpan sofaTracerSpan) { // System.out 输出 System.out.println(\u0026amp;quot;this is my custom reporter\u0026amp;quot;); } @Override public void close() { // ignore } }  配置 com.alipay.sofa.tracer.reporter-name=com.glmapper.bridge.boot.flexible.MyReporter  自定义实现 Reporter 可以将业务埋点的日志输出到任何期望的地方。\n","date":-62135596800,"description":"","dir":"projects/sofa-tracer/reporter-custom/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"521206c7f4051c1cc8ec8232c20bab6d","permalink":"/projects/sofa-tracer/reporter-custom/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-tracer/reporter-custom/","summary":"在使用自定义埋点组件的情况下，用户可以选择自定义 Reporter。 自定义 Reporter 实现 public class MyReporter implements Reporter { @Override public String getReporterType() { return \u0026quot;myReporter\u0026quot;; } @Override public void report(SofaTracerSpan sofaTracerSpan) { // System.out 输出 System.out.println(\u0026quot;this is my custom reporter\u0026quot;); } @Override","tags":null,"title":"自定义 Reporter","type":"projects","url":"/projects/sofa-tracer/reporter-custom/","wordcount":108},{"author":null,"categories":null,"content":" SOFARPC 支持自定义业务线程池。可以为指定服务设置一个独立的业务线程池，和 SOFARPC 自身的业务线程池是隔离的。多个服务可以共用一个独立的线程池。\nSOFARPC 要求自定义线程池的类型必须是 com.alipay.sofa.rpc.server.UserThreadPool。\nXML 方式 如果采用 XML 的方式发布服务，可以先设定一个 class 为 com.alipay.sofa.rpc.server.UserThreadPool 的线程池的 Bean，然后设置到 \u0026amp;lt;sofa:global-attrs\u0026amp;gt; 标签的 thread-pool-ref 属性中：\n\u0026amp;lt;bean id=\u0026amp;quot;helloService\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.quickstart.HelloService\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;!-- 自定义一个线程池 --\u0026amp;gt; \u0026amp;lt;bean id=\u0026amp;quot;customExecutor\u0026amp;quot; class=\u0026amp;quot;com.alipay.sofa.rpc.server.UserThreadPool\u0026amp;quot; init-method=\u0026amp;quot;init\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;corePoolSize\u0026amp;quot; value=\u0026amp;quot;10\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;maximumPoolSize\u0026amp;quot; value=\u0026amp;quot;10\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;property name=\u0026amp;quot;queueSize\u0026amp;quot; value=\u0026amp;quot;0\u0026amp;quot; /\u0026amp;gt; \u0026amp;lt;/bean\u0026amp;gt; \u0026amp;lt;sofa:service ref=\u0026amp;quot;helloService\u0026amp;quot; interface=\u0026amp;quot;XXXService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;!-- 将线程池设置给一个 Service --\u0026amp;gt; \u0026amp;lt;sofa:global-attrs thread-pool-ref=\u0026amp;quot;customExecutor\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:service\u0026amp;gt;  Annotation 方式 如果是采用 Annotation 的方式发布服务，可以通过设置 @SofaServiceBinding 的 userThreadPool 属性来设置自定义线程池的 Bean：\n@SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, userThreadPool = \u0026amp;quot;customThreadPool\u0026amp;quot;)}) public class SampleServiceImpl implements SampleService { }  在 Spring 环境使用 API 方式 如果是在 Spring 环境下使用 API 的方式发布服务，可以通过调用 BoltBindingParam 的 setUserThreadPool 方法来设置自定义线程池：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setUserThreadPool(new UserThreadPool());  在非 Spring 环境下使用 API 方式 如果是在非 Spring 环境下使用 API 的方式，可以通过如下的方式来设置自定义线程池：\nUserThreadPool threadPool = new UserThreadPool(); threadPool.setCorePoolSize(10); threadPool.setMaximumPoolSize(100); threadPool.setKeepAliveTime(200); threadPool.setPrestartAllCoreThreads(false); threadPool.setAllowCoreThreadTimeOut(false); threadPool.setQueueSize(200); UserThreadPoolManager.registerUserThread(ConfigUniqueNameGenerator.getUniqueName(providerConfig), threadPool);  如上为 HelloService 服务设置了一个自定义线程池。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-threadpool/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"3f05f154bcb2b653ebeebb35b84d5ae1","permalink":"/projects/sofa-rpc/custom-threadpool/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/custom-threadpool/","summary":"SOFARPC 支持自定义业务线程池。可以为指定服务设置一个独立的业务线程池，和 SOFARPC 自身的业务线程池是隔离的。多个服务可以共用一个独立的线程池。 SOFARPC 要求自定义","tags":null,"title":"自定义线程池","type":"projects","url":"/projects/sofa-rpc/custom-threadpool/","wordcount":412},{"author":null,"categories":null,"content":"SOFARPC 中对服务地址的选择也抽象为了一条处理链，由每一个 Router 进行处理。同 Filter 一样， SOFARPC 对 Router 提供了同样的扩展能力。\n@Extension(value = \u0026amp;quot;customerRouter\u0026amp;quot;) @AutoActive(consumerSide = true) public class CustomerRouter extends Router { @Override public void init(ConsumerBootstrap consumerBootstrap) { } @Override public boolean needToLoad(ConsumerBootstrap consumerBootstrap) { return true; } @Override public List\u0026amp;lt;ProviderInfo\u0026amp;gt; route(SofaRequest request, List\u0026amp;lt;ProviderInfo\u0026amp;gt; providerInfos) { return providerInfos; }  新建扩展文件 META-INF/services/sofa-rpc/com.alipay.sofa.rpc.client.Router 。内容如下：\ncustomerRouter=com.alipay.sofa.rpc.custom.CustomRouter  如上自定义了一个 CustomerRouter ，生效于所有消费者。其中 init 参数 ConsumerBootstrap 是引用服务的包装类，能够拿到 ConsumerConfig ，代理类，服务地址池等对象。 needToLoad 表示是否生效该 Router ， route 方法即筛选地址的方法。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-router/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"236e8d4bda3e856267a3575853aa900c","permalink":"/projects/sofa-rpc/custom-router/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/custom-router/","summary":"SOFARPC 中对服务地址的选择也抽象为了一条处理链，由每一个 Router 进行处理。同 Filter 一样， SOFARPC 对 Router 提供了同样的扩展能力。 @Extension(value = \u0026quot;customerRouter\u0026quot;) @AutoActive(consumerSide = true) public class CustomerRouter extends Router { @Override public void init(ConsumerBootstrap consumerBootstrap) { } @Override","tags":null,"title":"自定义路由寻址","type":"projects","url":"/projects/sofa-rpc/custom-router/","wordcount":179},{"author":null,"categories":null,"content":" SOFARPC 提供了一套良好的可扩展性机制，为各个模块提供 SPI 的能力。 SOFARPC 对请求与响应的过滤链处理方式是通过多个过滤器 Filter 来进行具体的拦截处理，该部分可由用户自定义 Filter 扩展，自定义 Filter 的执行顺序在内置 Filter 之后。具体方式如下：\nBolt Filter  新建自定义 Filter 。\npublic class CustomFilter extends Filter { @Override public boolean needToLoad(FilterInvoker invoker) { return true; } @Override public SofaResponse invoke(FilterInvoker invoker, SofaRequest request) throws SofaRpcException { SofaResponse response = invoker.invoke(request); return response; } }   生效该自定义 Filter 到拦截器链中。这一步具体方式有三种。 方式1：API方式。该种方式能够生效到指定的 provider 或 consumer 。\n// 服务提供者 providerConfig.setFilterRef(Arrays.asList(new CustomFilter())); // 服务调用者 consumerConfig.setFilterRef(Arrays.asList(new CustomFilter()));  方式2：在类上加上 @Extension 注解+配置扩展文件方式。\n@Extension(\u0026amp;quot;customer\u0026amp;quot;) public class CustomFilter extends Filter { @Override public boolean needToLoad(FilterInvoker invoker) { return true; } @Override public SofaResponse invoke(FilterInvoker invoker, SofaRequest request) throws SofaRpcException { SofaResponse response = invoker.invoke(request); return response; } }  新建扩展文件 META-INF/services/sofa-rpc/com.alipay.sofa.rpc.filter.Filter 。内容如下：\ncustomer=com.alipay.sofa.rpc.custom.CustomFilter  编码注入。\n// 服务提供者 providerConfig.setFilter(Arrays.asList(\u0026amp;quot;customer\u0026amp;quot;)); // 服务调用者 consumerConfig.setFilter(Arrays.asList(\u0026amp;quot;customer\u0026amp;quot;));  方式三：在类上加上 @Extension 注解+ @AutoActive 注解方式+配扩展文件方式。该种方式利用 @AutoActive 注解代替了上述第二中方式的编码注入步骤，能够生效于所有 provider 或 consumer 。其中 providerSide 参数表示是否生效于服务端， consumerSide 参数表示是否生效于客户端。\n@Extension(\u0026amp;quot;customer\u0026amp;quot;) @AutoActive(providerSide = true, consumerSide = true) public class customerFilter extends Filter { // ... }    ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/custom-filter/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"30ff5937b52a7c2dd8028e878979a33d","permalink":"/projects/sofa-rpc/custom-filter/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/custom-filter/","summary":"SOFARPC 提供了一套良好的可扩展性机制，为各个模块提供 SPI 的能力。 SOFARPC 对请求与响应的过滤链处理方式是通过多个过滤器 Filter 来进行具体的拦截处理，该部分可由用户","tags":null,"title":"自定义过滤器","type":"projects","url":"/projects/sofa-rpc/custom-filter/","wordcount":409},{"author":null,"categories":null,"content":" 本文是对 MOSN 自定义配置的说明。\nDuration String  字符串，由一个十进制数字和一个时间单位后缀组成，有效的时间单位为 ns、us（或µs）、ms、s、m、h，例如 1h、3s、500ms。  metadata metadata 用于 MOSN 路由和 Cluster Host 之间的匹配。\n{ \u0026amp;quot;filter_metadata\u0026amp;quot;:{ \u0026amp;quot;mosn.lb\u0026amp;quot;:{} } }  mosn.lb 可对应任意的 string-string 的内容。\ntls_context { \u0026amp;quot;status\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;type\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;server_name\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;ca_cert\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;cert_chain\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;private_key\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;verify_client\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;require_client_cert\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;insecure_skip\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;cipher_suites\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;ecdh_curves\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;min_version\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;max_version\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;alpn\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;fall_back\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;extend_verify\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;, \u0026amp;quot;sds_source\u0026amp;quot;:{} }   status，bool类型，表示是否开启 TLS，默认是 false。 type，字符串类型，描述 tls_context 的类型。tls_context 支持扩展实现，不同的 type 对应不同的实现方式，默认实现方式对应的 type 是空字符串。 server_name，当没有配置 insecure_skip 时，用于校验服务端返回证书的 hostname。作为Cluster配置时有效。 ca_cert，证书签发的根 CA 证书。 cert_chain，TLS 证书链配置。 private_key，证书私钥配置。 verify_client，bool 类型，作为 Listener 配置时有效，表示是否要校验 Client 端证书 require_client_cert，bool 类型，表示是否强制 Client 端必须携带证书。 insecure_skip，bool 类型，作为 Cluster 配置时有效，表示是否要忽略 Server 端的证书校验。 cipher_suites，如果配置了该配置，那么 TLS 连接将只支持配置了的密码套件，并且会按照配置的顺序作为优先级使用，支持的套件类型如下：\nECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-ECDSA-WITH-CHACHA20-POLY1305 ECDHE-RSA-WITH-CHACHA20-POLY1305 ECDHE-RSA-AES256-CBC-SHA ECDHE-RSA-AES128-CBC-SHA ECDHE-ECDSA-AES256-CBC-SHA ECDHE-ECDSA-AES128-CBC-SHA RSA-AES256-CBC-SHA RSA-AES128-CBC-SHA ECDHE-RSA-3DES-EDE-CBC-SHA RSA-3DES-EDE-CBC-SHA ECDHE-RSA-SM4-SM3 ECDHE-ECDSA-SM4-SM3  ecdh_curves，如果配置了该配置，那么 TLS 连接将只支持配置了的曲线。\n  支持 x25519、p256、p384、p521。  min_version，最低的 TLS 协议版本，默认是 TLS1.0。\n  支持 TLS1.0、TLS1.1、TLS1.2。   默认会自动识别可用的 TLS 协议版本。  max_version，最高的 TLS 协议版本，默认是 TLS1.2。\n  支持 TLS1.0、TLS1.1、TLS1.2。   默认会自动识别可用的 TLS 协议版本。  alpn，TLS 的 ALPN 配置。\n  支持 h2、http/1.1、 sofa。  fall_back，bool类型，当配置为 true 时，如果证书解析失败，不会报错而是相当于没有开启 TLS。\n extend_verify，任意 json 类型，当 type 为非空时，作为扩展的配置参数。\n sds_source，访问 SDS API 的配置，如果配置了这个配置，ca_cert、cert_chain 和 private_key 都会被忽略，但是其余的配置依然有效。\n  sds_source { \u0026amp;quot;CertificateConfig\u0026amp;quot;:{}, \u0026amp;quot;ValidationConfig\u0026amp;quot;:{} }   CertificateConfig 描述了如何获取 cert_chain 和 private_key 的配置。 ValidationConfig 描述了如何获取 ca_cert 的配置。 详细的 Config 内容参考 envoy: sdssecretconfig。  ","date":-62135596800,"description":"","dir":"projects/mosn/configuration/custom/","fuzzywordcount":1100,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"cdad467fd3551e47a7585511278767cd","permalink":"/projects/mosn/configuration/custom/","publishdate":"0001-01-01T00:00:00Z","readingtime":3,"relpermalink":"/projects/mosn/configuration/custom/","summary":"本文是对 MOSN 自定义配置的说明。 Duration String 字符串，由一个十进制数字和一个时间单位后缀组成，有效的时间单位为 ns、us（或µs）、ms、s、m、h，例如","tags":null,"title":"自定义配置说明","type":"projects","url":"/projects/mosn/configuration/custom/","wordcount":1005},{"author":null,"categories":null,"content":" SOFARPC 支持进行框架层面的重试策略，前提是集群模式为 FailOver（SOFARPC 默认即为 FailOver 模式）。重试只有在发生服务端的框架层面异常或者是超时异常才会发起。如果是业务抛出异常，是不会重试的。默认情况下 SOFARPC 不进行任何重试。\n 请注意：超时异常虽然可以重试，但是需要服务端保证业务的幂等性，否则可能会有风险\n XML 方式 如果使用 XML 方式订阅服务，可以设置 sofa:global-attrs 的 retries 参数来设置重试次数：\n\u0026amp;lt;sofa:reference jvm-first=\u0026amp;quot;false\u0026amp;quot; id=\u0026amp;quot;retriesServiceReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.rpc.samples.retries.RetriesService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs retries=\u0026amp;quot;2\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 方式 如果是使用 Annotation 的方式，可以通过设置 @SofaReferenceBinding 注解的 retries 属性来设置：\n@SofaReference(binding = @SofaReferenceBinding(bindingType = \u0026amp;quot;bolt\u0026amp;quot;, retries = 2)) private SampleService sampleService;  Spring 环境下 API 方式 如果是在 Spring 环境下用 API 的方式，可以调用 BoltBindingParam 的 setRetries 方法来设置：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setRetries(2);  非 Spring 环境下 API 方式 如果是在非 Spring 环境下直接使用 SOFARPC 的裸 API 的方式，可以通过调用 ConsumerConfig 的 setRetries 方法来设置：\nConsumerConfig\u0026amp;lt;RetriesService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;RetriesService\u0026amp;gt;(); consumerConfig.setRetries(2);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/retry-invoke/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d60b44aa8f1b49ab6c1bbc55593a91da","permalink":"/projects/sofa-rpc/retry-invoke/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/retry-invoke/","summary":"SOFARPC 支持进行框架层面的重试策略，前提是集群模式为 FailOver（SOFARPC 默认即为 FailOver 模式）。重试只有在发生服务端的框架层面异常或者是超时","tags":null,"title":"调用重试","type":"projects","url":"/projects/sofa-rpc/retry-invoke/","wordcount":319},{"author":null,"categories":null,"content":" SOFARPC 提供多种负载均衡算法，目前支持以下五种：\n   类型 名称 描述     random 随机算法 默认负载均衡算法。   localPref 本地优先算法 优先发现是否本机发布了该服务，如果没有再采用随机算法。   roundRobin 轮询算法 方法级别的轮询，各个方法间各自轮询，互不影响。   consistentHash 一致性hash算法 同样的方法级别的请求会路由到同样的节点。   weightRoundRobin 按权重负载均衡轮询算法 按照权重对节点进行轮询。性能较差，不推荐使用。    要使用某种特定的负载均衡算法，可以按照以下的方式进行设置：\nXML 方式 如果使用 XML 的方式引用服务，可以通过设置 sofa:global-attrs 标签的 loadBalancer 属性来设置：\n\u0026amp;lt;sofa:reference interface=\u0026amp;quot;com.example.demo.SampleService\u0026amp;quot; id=\u0026amp;quot;sampleService\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs loadBalancer=\u0026amp;quot;roundRobin\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  Annotation 方式 Annotation 方式目前暂未提供设置某一个 Reference 的负载均衡算法的方式。将在后续的版本中提供。\n在 Spring 环境下 API 方式 如果在 Spring 或者 Spring Boot 的环境下使用 API，可以通过调用 BoltBindingParam 的 setLoadBalancer 方法来设置：\nBoltBindingParam boltBindingParam = new BoltBindingParam(); boltBindingParam.setLoadBalancer(\u0026amp;quot;roundRobin\u0026amp;quot;);  非 Spring 环境下 API 方式 如果在非 Spring 环境下直接使用 SOFARPC 提供的裸 API，可以通过调用 ConsumerConfig 的 setLoadBalancer 方法来设置：\nConsumerConfig consumerConfig = new ConsumerConfig(); consumerConfig.setLoadbalancer(\u0026amp;quot;random\u0026amp;quot;);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/load-balance/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"739984ca9a414429304f85010fd73ad0","permalink":"/projects/sofa-rpc/load-balance/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/load-balance/","summary":"SOFARPC 提供多种负载均衡算法，目前支持以下五种： 类型 名称 描述 random 随机算法 默认负载均衡算法。 localPref 本地优先算法 优先发现是否本机发布了该服务，如果没有再采用","tags":null,"title":"负载均衡","type":"projects","url":"/projects/sofa-rpc/load-balance/","wordcount":375},{"author":null,"categories":null,"content":" 任务列表 下面表格记录了还没有实现的功能特性，欢迎大家认领任务，参与贡献。\n   类型 任务 困难度 认领人及时间 计划完成时间 进度 相关 Issue     文档 SOFADashboard 配置参数文档 简单       代码 支持 SOFARegistry 中       代码 支持 Docker 中       代码 支持 Kubernetes 中       代码 支持 Apollo 中       代码 优化前端 中        ","date":-62135596800,"description":"","dir":"projects/sofa-dashboard/roadmap/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"a740c874742b504de9011b07f3a4ddb5","permalink":"/projects/sofa-dashboard/roadmap/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-dashboard/roadmap/","summary":"任务列表 下面表格记录了还没有实现的功能特性，欢迎大家认领任务，参与贡献。 类型 任务 困难度 认领人及时间 计划完成时间 进度 相关 Issue 文档 SOFADashboard 配置参数文档 简","tags":null,"title":"路线图及任务认领","type":"projects","url":"/projects/sofa-dashboard/roadmap/","wordcount":102},{"author":null,"categories":null,"content":" 1. registry-meta 1.1 推送开关 在注册中心新版本发布的过程中为了把对业务的影响减少到最小，避免服务端重启动引发大规模服务地址信息变更产生大量推送，我们提供运维层面暂时关闭推送的能力。在服务端完成发布后，可以打开推送恢复正常工作状态，在关闭期间的数据订阅和服务发布信息会再次进行全局推送进行补偿。\n打开推送：\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/stopPushDataSwitch/close\u0026amp;quot;  关闭推送：\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/stopPushDataSwitch/open\u0026amp;quot;  1.2 查询地址列表 查看 meta 集群的地址列表：\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/META/node/query\u0026amp;quot;  查看 data 集群的地址列表：\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/DATA/node/query\u0026amp;quot;  查看 session 集群的地址列表：\ncurl \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/digest/SESSION/node/query\u0026amp;quot;  1.3 meta 集群扩缩容 1.3.1 修改集群：changePeer 修改 raft 集群列表，当有扩容或缩容时，可以调用该 API，对集群列表进行修改，这样才能将扩容节点或缩容节点，正确地添加到或移除出集群：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/manage/changePeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;ip1\u0026amp;gt;,\u0026amp;lt;ip2\u0026amp;gt;,\u0026amp;lt;ip3\u0026amp;gt;\u0026amp;quot;  1.3.2 重置集群：resetPeer 当集群不可用时，例如 3 台机器，宕机 2 台，集群将无法选举。此时可以使用该 API 强制地重置集群列表，比如可以将集群重置为1台机器(唯一可用的那台)，这样可以恢复选举，集群可恢复可用：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;meta_ip\u0026amp;gt;:9615/manage/resetPeer\u0026amp;quot; -d \u0026amp;quot;ipAddressList=\u0026amp;lt;ip1\u0026amp;gt;,\u0026amp;lt;ip2\u0026amp;gt;,\u0026amp;lt;ip3\u0026amp;gt;\u0026amp;quot;  2. registry-data 2.1 查询数据 查看 pub 数量：\ncurl \u0026amp;quot;http://\u0026amp;lt;data_ip\u0026amp;gt;:9622/digest/datum/count\u0026amp;quot;  根据客户端的 ip\u0026amp;amp;port 查询其发布的数据：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;data_ip\u0026amp;gt;:9622/digest/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;{\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;\u0026amp;quot;:\u0026amp;quot;\u0026amp;lt;client端口\u0026amp;gt;\u0026amp;quot;}\u0026#39;  3. registry-session 3.1 查询数据 根据客户端的 ip\u0026amp;amp;port 查询其发布的数据：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;session_ip\u0026amp;gt;:9603/digest/pub/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;[\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;:\u0026amp;lt;client端口\u0026amp;gt;\u0026amp;quot;]\u0026#39;  根据客户端的 ip\u0026amp;amp;port 查询其订阅的数据：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;session_ip\u0026amp;gt;:9603/digest/sub/connect/query\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;[\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;:\u0026amp;lt;client端口\u0026amp;gt;\u0026amp;quot;]\u0026#39;  3.2 断开客户端链接：clientOff 根据客户端的 ip\u0026amp;amp;port 强制删除其所有 sub\u0026amp;amp;pub 数据（但不会断开连接）：\ncurl -X POST \u0026amp;quot;http://\u0026amp;lt;session_ip\u0026amp;gt;:9603/api/clients/off\u0026amp;quot; -H \u0026amp;quot;Content-Type: application/json\u0026amp;quot; -d \u0026#39;{\u0026amp;quot;connectIds\u0026amp;quot;: [\u0026amp;quot;\u0026amp;lt;clientIP\u0026amp;gt;:\u0026amp;lt;client端口\u0026amp;gt;\u0026amp;quot;]}\u0026#39;  ","date":-62135596800,"description":"","dir":"projects/sofa-registry/management-api/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"2cf59ac422c84c279d73c1f7f1cd0902","permalink":"/projects/sofa-registry/management-api/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-registry/management-api/","summary":"1. registry-meta 1.1 推送开关 在注册中心新版本发布的过程中为了把对业务的影响减少到最小，避免服务端重启动引发大规模服务地址信息变更产生大量推送，我们提供运维","tags":null,"title":"运维命令","type":"projects","url":"/projects/sofa-registry/management-api/","wordcount":755},{"author":null,"categories":null,"content":"SOFARPC 支持不同的通信协议，目前支持 Bolt, RESTful 和 Dubbo，详细的事情请参考各个协议对应的文档： * Bolt 协议 * 基本使用 * 调用方式 * 超时控制 * 泛化调用 * 序列化协议 * 自定义线程池 * RESTful * 基本使用 * 自定义 Filter * 集成 Swagger * Dubbo * 基本使用 * H2C * 基本使用\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/protocol/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"18f51cb12f7a0384a71ab22349292a08","permalink":"/projects/sofa-rpc/protocol/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/protocol/","summary":"SOFARPC 支持不同的通信协议，目前支持 Bolt, RESTful 和 Dubbo，详细的事情请参考各个协议对应的文档： * Bolt 协议 * 基本使用 * 调用方式 * 超时控制 * 泛化调用 * 序列化","tags":null,"title":"通信协议","type":"projects","url":"/projects/sofa-rpc/protocol/","wordcount":109},{"author":null,"categories":null,"content":" 环境准备 要使用 SOFARegistry，需要先准备好基础环境，SOFARegistry 依赖以下环境： * Linux/Unix/Mac/Windows * JDK8 * 需要采用 Apache Maven 3.2.5 或者以上的版本来编译\n两种部署模式  集成部署模式  将 meta/data/session 三个角色打包集成在一个 jvm 里运行，可单机或集群部署，部署简单。  独立部署模式  将 meta/data/session 三个角色分开部署，每个角色都可以单机或集群部署，可根据实际情况为每个角色部署不同的数量。 生产环境建议使用这种部署模式。   部署步骤 1. 下载源码和编译打包 1.1 下载源码 git clone https://github.com/sofastack/sofa-registry.git cd sofa-registry  1.2 编译打包 mvn clean package -DskipTests  2. 部署注册中心 2.1 集成部署模式 集成部署模式，是将 meta/data/session 三个角色打包集成在一个 JVM 里运行，可单机或集群部署。\n2.1.1 单机部署 集成部署的单机部署模式可以直接参考快速开始-服务端部署部分。\n2.1.2 集群部署  解压 registry-integration.tgz，并修改配置文件  集群部署，即搭建2台以上的集群，建议至少使用3台（注意：目前不支持在同一台机器部署多个 SOFARegistry，因此您必须有3台不同的机器）。在每一台机器上的部署方法同上：\ncp server/distribution/integration/target/registry-integration.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-integration tar -zxvf registry-integration.tgz -C registry-integration  区别是每台机器在部署时需要修改 conf/application.properties 配置：\n# 将3台机器的ip或hostname配置到下方(填入的hostname会被内部解析为ip地址) nodes.metaNode=DefaultDataCenter:\u0026amp;lt;hostname1\u0026amp;gt;,\u0026amp;lt;hostname2\u0026amp;gt;,\u0026amp;lt;hostname3\u0026amp;gt; nodes.localDataCenter=DefaultDataCenter nodes.localRegion=DefaultZone   启动 registry-integration  每台机器都修改以上配置文件后，按照“单机部署”的步骤去启动 registry-integration 即可。 * Linux/Unix/Mac：sh bin/startup.sh。 * Windows: 双击 bin 目录下的 startup.bat 运行文件。 * 确认运行状态：对每一台机器，都可访问三个角色提供的健康监测api，或查看日志 logs/registry-startup.log\n# 查看meta角色的健康检测接口：(3台机器，有1台是Leader，其他2台是Follower) $ curl http://localhost:9615/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... raftStatus:Leader\u0026amp;quot;} # 查看data角色的健康检测接口： $ curl http://localhost:9622/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... status:WORKING\u0026amp;quot;} # 查看session角色的健康检测接口： $ curl http://localhost:9603/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;...\u0026amp;quot;}  2.2 独立部署模式 独立部署模式，是将 meta/data/session 三个角色分开部署，每个角色都可以单机或集群部署，可根据实际情况为每个角色部署不同的数量，生产环境推荐使用这种部署模式。\n以下介绍 332 模式（即 3 台 meta + 3 台 data + 2 台 session）的部署步骤。\n2.2.1 部署meta  解压 registry-meta.tgz，并修改配置文件  在3台机器上部署 meta 角色。在每一台机器上的部署方法如下：\ncp server/distribution/meta/target/registry-meta.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-meta tar -zxvf registry-meta.tgz -C registry-meta  每台机器在部署时需要修改 conf/application.properties 配置：\n# 将3台meta机器的ip或hostname配置到下方(填入的hostname会被内部解析为ip地址) nodes.metaNode=DefaultDataCenter:\u0026amp;lt;meta_hostname1\u0026amp;gt;,\u0026amp;lt;meta_hostname2\u0026amp;gt;,\u0026amp;lt;meta_hostname3\u0026amp;gt; nodes.localDataCenter=DefaultDataCenter   启动 registry-meta  Linux/Unix/Mac：sh bin/startup.sh。 Windows: 双击 bin 目录下的 startup.bat 运行文件。   确认运行状态：对每一台机器，都可访问meta提供的健康监测api，或查看日志 logs/registry-startup.log  # 查看 meta 角色的健康检测接口：(3台机器，有1台是 Leader，其他2台是 Follower) $ curl http://localhost:9615/health/check {\u0026amp;quot;success\u0026amp;quot;:true,\u0026amp;quot;message\u0026amp;quot;:\u0026amp;quot;... raftStatus:Leader\u0026amp;quot;}  2.2.2 部署data  解压 registry-data.tgz，并修改配置文件  在3台机器上部署 data 角色。在每一台机器上的部署方法如下：\ncp server/distribution/data/target/registry-data.tgz \u0026amp;lt;somewhere\u0026amp;gt; cd \u0026amp;lt;somewhere\u0026amp;gt; \u0026amp;amp;\u0026amp;amp; mkdir registry-data tar -zxvf …","date":-62135596800,"description":"","dir":"projects/sofa-registry/deployment/","fuzzywordcount":1600,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"7e28583bc38be66af8d704d7fbcd9dd4","permalink":"/projects/sofa-registry/deployment/","publishdate":"0001-01-01T00:00:00Z","readingtime":4,"relpermalink":"/projects/sofa-registry/deployment/","summary":"环境准备 要使用 SOFARegistry，需要先准备好基础环境，SOFARegistry 依赖以下环境： * Linux/Unix/Mac/Windows * JDK8 * 需要采用 Apache Maven 3.2.5 或者以上的版本来","tags":null,"title":"部署","type":"projects","url":"/projects/sofa-registry/deployment/","wordcount":1584},{"author":null,"categories":null,"content":" MOSN 的配置文件可以分为以下四大部分：\n Servers 配置，目前仅支持最多 1 个 Server 的配置，Server 中包含一些基础配置以及对应的 Listener 配置 ClusterManager 配置，包含 MOSN 的 Upstream 详细信息 对接控制平面（Pilot）的 xDS 相关配置 其他配置  Trace、Metrics、Debug、Admin API 相关配置 扩展配置，提供自定义配置扩展需求   配置文件概览\nMOSN 的基本配置部分如下所示：\n{ \u0026amp;quot;servers\u0026amp;quot;: [], \u0026amp;quot;cluster_manager\u0026amp;quot;: {}, \u0026amp;quot;dynamic_resources\u0026amp;quot;: {}, \u0026amp;quot;static_resources\u0026amp;quot;: {}, \u0026amp;quot;admin\u0026amp;quot;:{}, \u0026amp;quot;pprof\u0026amp;quot;:{}, \u0026amp;quot;tracing\u0026amp;quot;:{}, \u0026amp;quot;metrics\u0026amp;quot;:{} }  配置类型 MOSN 的配置包括以下几种类型：\n 静态配置 动态配置 混合模式  静态配置  静态配置是指 MOSN 启动时，不对接控制平面 Pilot 的配置，用于一些相对固定的简单场景（如 MOSN 的示例）。 使用静态配置启动的 MOSN，也可以通过扩展代码，调用动态更新配置的接口实现动态修改。 静态配置启动时必须包含一个 Server 以及至少一个 Cluster。  动态配置  动态配置是指 MOSN 启动时，只有访问控制平面相关的配置，没有 MOSN 运行时所需要的配置。\n 使用动态配置启动的 MOSN，会向管控面请求获取运行时所需要的配置，管控面也可能在运行时推送更新 MOSN 运行配置。\n 动态配置启动时必须包含 DynamicResources 和 StaticResources 配置。\n  混合模式 MOSN 启动时的配置可以同时包含静态模式与动态模式，以混合模式启动的 MOSN 会先以静态配置完成初始化，随后可能由控制平面获取配置更新。\n配置示例 静态配置示例 动态配置的示例如下所示。\n{ \u0026amp;quot;servers\u0026amp;quot;: [ { \u0026amp;quot;default_log_path\u0026amp;quot;: \u0026amp;quot;/home/admin/logs/mosn/default.log\u0026amp;quot;, \u0026amp;quot;default_log_level\u0026amp;quot;: \u0026amp;quot;DEBUG\u0026amp;quot;, \u0026amp;quot;processor\u0026amp;quot;: 4, \u0026amp;quot;listeners\u0026amp;quot;: [ { \u0026amp;quot;address\u0026amp;quot;: \u0026amp;quot;0.0.0.0:12220\u0026amp;quot;, \u0026amp;quot;bind_port\u0026amp;quot;: true, \u0026amp;quot;filter_chains\u0026amp;quot;: [ { \u0026amp;quot;filters\u0026amp;quot;: [ { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;proxy\u0026amp;quot;, \u0026amp;quot;config\u0026amp;quot;: { \u0026amp;quot;downstream_protocol\u0026amp;quot;: \u0026amp;quot;SofaRpc\u0026amp;quot;, \u0026amp;quot;upstream_protocol\u0026amp;quot;: \u0026amp;quot;SofaRpc\u0026amp;quot;, \u0026amp;quot;router_config_name\u0026amp;quot;: \u0026amp;quot;test_router\u0026amp;quot; } }, { \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;connection_manager\u0026amp;quot;, \u0026amp;quot;config\u0026amp;quot;: { \u0026amp;quot;router_config_name\u0026amp;quot;: \u0026amp;quot;test_router\u0026amp;quot;, \u0026amp;quot;virtual_hosts\u0026amp;quot;: [] } } ] } ] } ] } ], \u0026amp;quot;cluster_manager\u0026amp;quot;: { \u0026amp;quot;clusters\u0026amp;quot;: [ { \u0026amp;quot;name\u0026amp;quot;:\u0026amp;quot;example\u0026amp;quot;, \u0026amp;quot;lb_type\u0026amp;quot;: \u0026amp;quot;LB_ROUNDROBIN\u0026amp;quot;, \u0026amp;quot;hosts\u0026amp;quot;: [ {\u0026amp;quot;address\u0026amp;quot;: \u0026amp;quot;127.0.0.1:12200\u0026amp;quot;} ] } ] } }  动态配置示例 静态配置的示例如下所示。\n{ \u0026amp;quot;dynamic_resources\u0026amp;quot;: { \u0026amp;quot;ads_config\u0026amp;quot;: { \u0026amp;quot;api_type\u0026amp;quot;: \u0026amp;quot;GRPC\u0026amp;quot;, \u0026amp;quot;grpc_services\u0026amp;quot;: [ { \u0026amp;quot;envoy_grpc\u0026amp;quot;: {\u0026amp;quot;cluster_name\u0026amp;quot;: \u0026amp;quot;xds-grpc\u0026amp;quot;} } ] } } }, \u0026amp;quot;static_resources\u0026amp;quot;: { \u0026amp;quot;clusters\u0026amp;quot;: [ { \u0026amp;quot;name\u0026amp;quot;: \u0026amp;quot;xds-grpc\u0026amp;quot;, \u0026amp;quot;type\u0026amp;quot;: \u0026amp;quot;STRICT_DNS\u0026amp;quot;, \u0026amp;quot;connect_timeout\u0026amp;quot;: \u0026amp;quot;10s\u0026amp;quot;, \u0026amp;quot;lb_policy\u0026amp;quot;: \u0026amp;quot;ROUND_ROBIN\u0026amp;quot;, \u0026amp;quot;hosts\u0026amp;quot;: [ { \u0026amp;quot;socket_address\u0026amp;quot;: {\u0026amp;quot;address\u0026amp;quot;: \u0026amp;quot;pilot\u0026amp;quot;, \u0026amp;quot;port_value\u0026amp;quot;: 15010} } ], \u0026amp;quot;upstream_connection_options\u0026amp;quot;: { \u0026amp;quot;tcp_keepalive\u0026amp;quot;: { \u0026amp;quot;keepalive_time\u0026amp;quot;: 300 } }, \u0026amp;quot;http2_protocol_options\u0026amp;quot;: { } } ] } }  ","date":-62135596800,"description":"","dir":"projects/mosn/configuration/overview/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"0aa65950d2c24e8ce86d265bea275e2a","permalink":"/projects/mosn/configuration/overview/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/mosn/configuration/overview/","summary":"MOSN 的配置文件可以分为以下四大部分： Servers 配置，目前仅支持最多 1 个 Server 的配置，Server 中包含一些基础配置以及对应的 Listener 配置 ClusterManager 配置，包含 MOSN 的 Upstream 详细信","tags":null,"title":"配置概览","type":"projects","url":"/projects/mosn/configuration/overview/","wordcount":663},{"author":null,"categories":null,"content":" 目前 SOFATracer 提供了两种采样模式，一种是基于 BitSet 实现的基于固定采样率的采样模式；另外一种是提供给用户自定义实现采样的采样模式。下面通过案例来演示如何使用。\n本示例基于 tracer-sample-with-springmvc 工程；除 application.properties 之外，其他均相同。\n基于固定采样率的采样模式 在 application.properties 中增加采样相关配置项 #采样率 0~100 com.alipay.sofa.tracer.samplerPercentage=100 #采样模式类型名称 com.alipay.sofa.tracer.samplerName=PercentageBasedSampler  验证方式  当采样率设置为100时，每次都会打印摘要日志。 当采样率设置为0时，不打印 当采样率设置为0~100之间时，按概率打印  以请求 10 次来验证下结果。\n1、当采样率设置为100时，每次都会打印摘要日志\n启动工程，浏览器中输入：http://localhost:8080/springmvc ；并且刷新地址10次，查看日志如下：\n{\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:47.643\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173568757510019269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:68,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-1\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:50.980\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173569097710029269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:3,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-2\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:51.542\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173569153910049269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:3,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-4\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 11:54:52.061\u0026amp;quot;,\u0026amp;quot;local.app\u0026amp;quot;:\u0026amp;quot;SOFATracerSpringMVC\u0026amp;quot;,\u0026amp;quot;traceId\u0026amp;quot;:\u0026amp;quot;0a0fe8ec154173569205910069269\u0026amp;quot;,\u0026amp;quot;spanId\u0026amp;quot;:\u0026amp;quot;0.1\u0026amp;quot;,\u0026amp;quot;request.url\u0026amp;quot;:\u0026amp;quot;http://localhost:8080/springmvc\u0026amp;quot;,\u0026amp;quot;method\u0026amp;quot;:\u0026amp;quot;GET\u0026amp;quot;,\u0026amp;quot;result.code\u0026amp;quot;:\u0026amp;quot;200\u0026amp;quot;,\u0026amp;quot;req.size.bytes\u0026amp;quot;:-1,\u0026amp;quot;resp.size.bytes\u0026amp;quot;:0,\u0026amp;quot;time.cost.milliseconds\u0026amp;quot;:2,\u0026amp;quot;current.thread.name\u0026amp;quot;:\u0026amp;quot;http-nio-8080-exec-6\u0026amp;quot;,\u0026amp;quot;baggage\u0026amp;quot;:\u0026amp;quot;\u0026amp;quot;} {\u0026amp;quot;time\u0026amp;quot;:\u0026amp;quot;2018-11-09 …","date":-62135596800,"description":"","dir":"projects/sofa-tracer/sampler/","fuzzywordcount":800,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"48856a040da01abc84213934c1c5fce4","permalink":"/projects/sofa-tracer/sampler/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-tracer/sampler/","summary":"目前 SOFATracer 提供了两种采样模式，一种是基于 BitSet 实现的基于固定采样率的采样模式；另外一种是提供给用户自定义实现采样的采样模式。下面通过案例来演示如何使","tags":null,"title":"采样模式","type":"projects","url":"/projects/sofa-tracer/sampler/","wordcount":719},{"author":null,"categories":null,"content":" 链路数据透传 链路数据透传功能支持应用向调用上下文中存放数据，达到整个链路上的应用都可以操作该数据。 使用方式如下，可分别向链路的 request 和 response 中放入数据进行透传，并可获取到链路中相应的数据。\nRpcInvokeContext.getContext().putRequestBaggage(\u0026amp;quot;key_request\u0026amp;quot;,\u0026amp;quot;value_request\u0026amp;quot;); RpcInvokeContext.getContext().putResponseBaggage(\u0026amp;quot;key_response\u0026amp;quot;,\u0026amp;quot;value_response\u0026amp;quot;); String requestValue=RpcInvokeContext.getContext().getRequestBaggage(\u0026amp;quot;key_request\u0026amp;quot;); String responseValue=RpcInvokeContext.getContext().getResponseBaggage(\u0026amp;quot;key_response\u0026amp;quot;);  使用示例 例如 A -\u0026amp;gt; B -\u0026amp;gt; C 的场景中，将 A 设置的请求隐式传参数据传递给 B 和 C。在返回的时候，将 C 和 B 的响应隐式传参数据传递给 A。\nA 请求方设置的时候：\n// 调用前设置请求透传的值 RpcInvokeContext context = RpcInvokeContext.getContext(); context.putRequestBaggage(\u0026amp;quot;reqBaggageB\u0026amp;quot;, \u0026amp;quot;a2bbb\u0026amp;quot;); // 调用 String result = service.hello(); // 拿到结果透传的值 context.getResponseBaggage(\u0026amp;quot;respBaggageB\u0026amp;quot;);  B 业务代码中：\npublic String hello() { // 拿到请求透传的值 RpcInvokeContext context = RpcInvokeContext.getContext(); String reqBaggage = context.getRequestBaggage(\u0026amp;quot;reqBaggageB\u0026amp;quot;); // doSomthing(); // 结果透传一个值 context.putResponseBaggage(\u0026amp;quot;respBaggageB\u0026amp;quot;, \u0026amp;quot;b2aaa\u0026amp;quot;); return result; }  如果中途自己启动了子线程，则需要设置子线程的上下文：\nCountDownLatch latch = new CountDownLatch(1); final RpcInvokeContext parentContext = RpcInvokeContext.peekContext(); Thread thread = new Thread(new Runnable(){ public void run(){ try { RpcInvokeContext.setContext(parentContext); // 调一个远程服务 xxxService.sayHello(); latch.countDown(); } finally { RpcInvokeContext.removeContext(); } } }, \u0026amp;quot;new-thread\u0026amp;quot;); thread.start(); // 此时拿不到返回值透传的数据的 latch.await(); //等待 // 此时返回结束，能拿到返回透传的值  和 SOFATracer 的比较 SOFATracer 是蚂蚁开源的一个分布式链路追踪系统,RPC 目前已经和 Tracer 做了集成,默认开启. 和 Tracer 进行数据传递不同的是\n RPC的数据透传更偏向业务使用,而且可以在全链路中进行双向传递,调用方可以传给服务方,服务方也可以传递信息给调用方,SOFATracer 更加偏向于中间件和业务无感知的数据的传递,只能进行单向传递. RPC的透传可以选择性地不在全链路中透传,而Tracer 中如果传递大量信息,会在整个链路中传递.可能对下游业务会有影响.  所以整体来看,两个信息各有利弊,在有一些和业务相关的透传数据的情况下,可以选择 RPC 的透传.\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/invoke-chain-pass-data/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"96cfb41f07a6a2ad979b53093ff5eee9","permalink":"/projects/sofa-rpc/invoke-chain-pass-data/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/invoke-chain-pass-data/","summary":"链路数据透传 链路数据透传功能支持应用向调用上下文中存放数据，达到整个链路上的应用都可以操作该数据。 使用方式如下，可分别向链路的 request 和 response 中放入数","tags":null,"title":"链路数据透传","type":"projects","url":"/projects/sofa-rpc/invoke-chain-pass-data/","wordcount":606},{"author":null,"categories":null,"content":"默认 SOFARPC 已经集成了 SOFATracer，用户也可以使用其他的 APM 产品，如 Skywalking来实现相应的功能。详见文档：\n SOFATracer Skywalking  如果想要关闭 SOFARPC 的链路追踪能力的话，在使用了 rpc-sofa-boot-starter 的情况下，可以在 application.properties 配置文件中配置 com.alipay.sofa.rpc.defaultTracer=。\n在直接使用 sofa-rpc-all 的情况下，可以在 main 函数里面加上如下的代码来关闭 SOFARPC 的链路追踪的能力（在发布任何 SOFARPC 的服务或者引用任何 SOFARPC 的服务之前）：\nRpcConfigs.putValue(RpcOptions.DEFAULT_TRACER, \u0026amp;quot;\u0026amp;quot;);  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/tracing-usage/","fuzzywordcount":200,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"5f944f87d827ae060fb0528f6715af97","permalink":"/projects/sofa-rpc/tracing-usage/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/tracing-usage/","summary":"默认 SOFARPC 已经集成了 SOFATracer，用户也可以使用其他的 APM 产品，如 Skywalking来实现相应的功能。详见文档： SOFATracer Skywalking 如果想要关闭 SOFARPC 的链路","tags":null,"title":"链路追踪","type":"projects","url":"/projects/sofa-rpc/tracing-usage/","wordcount":197},{"author":null,"categories":null,"content":" 我引用了jar包，发现启动不了，报错怎么办？  答：这种需要你自己去定位问题，查看是否按照文档来进行配置，环境是否正确，是否有依赖冲突问题，实在解决不了，可以在github上提供issue， 我们团队会进行技术支持。  微服务异常，事务没回滚怎么办？  答：首先，你可以去查看日志记录，如果日志记录存在，那在你配置的调度时间之后，会执行回滚。  编译源码，发现缺少get，set方法怎么办？  答：源码使用了lombok，你可能需要在你的开发工具，安装对应的插件。（没有set get方法并不影响运行的）。  ","date":-62135596800,"description":"问题描述","dir":"projects/hmily/faq/","fuzzywordcount":300,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"291ff6f051e39fe15edb0c08d62aef12","permalink":"/projects/hmily/faq/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/hmily/faq/","summary":"我引用了jar包，发现启动不了，报错怎么办？ 答：这种需要你自己去定位问题，查看是否按照文档来进行配置，环境是否正确，是否有依赖冲突问题，实在","tags":null,"title":"问题","type":"projects","url":"/projects/hmily/faq/","wordcount":241},{"author":null,"categories":null,"content":" 从 rpc-sofa-boot-starter 6.0.1 版本开始，SOFARPC 提供了 RESTful 服务和 Swagger 的一键集成的能力。\n在使用了 rpc-sofa-boot-starter 的情况下，如果想要开启 swagger 的能力，首先需要在 pom.xml 中增加 Swagger 的依赖：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.swagger.core.v3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;swagger-jaxrs2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.guava\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;guava\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;20.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  然后在 application.properties 里面增加 com.alipay.sofa.rpc.restSwagger=true。\n最后，访问 http://localhost:8341/swagger/openapi 就可以拿到 SOFARPC 的 RESTful 的 Swagger OpenAPI 内容。\n如果没有使用 rpc-sofa-boot-starter 或者在 rpc-sofa-boot-starter 的版本低于 6.0.1，可以采用如下的方式集成 Swagger。\n首先，需要在应用中引入 Swagger 相关的依赖，由于 SOFARPC 的 RESTful 协议走的是 JAXRS 标准，因此我们引入 Swagger 的 JAXRS 依赖即可：\n\u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;io.swagger.core.v3\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;swagger-jaxrs2\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;2.0.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt; \u0026amp;lt;dependency\u0026amp;gt; \u0026amp;lt;groupId\u0026amp;gt;com.google.guava\u0026amp;lt;/groupId\u0026amp;gt; \u0026amp;lt;artifactId\u0026amp;gt;guava\u0026amp;lt;/artifactId\u0026amp;gt; \u0026amp;lt;version\u0026amp;gt;20.0\u0026amp;lt;/version\u0026amp;gt; \u0026amp;lt;/dependency\u0026amp;gt;  上面引入 Guava 的 20.0 的版本是为了解决 Guava 的版本冲突。\n增加一个 Swagger 的 RESTful 服务 为了能够让 Swagger 将 SOFARPC 的 RESTful 的服务通过 Swagger OpenAPI 暴露出去，我们可以反过来用 SOFARPC 的 RESTful 的服务提供 Swagger 的 OpenAPI 服务。首先，需要新建一个接口：\n@Path(\u0026amp;quot;swagger\u0026amp;quot;) public interface OpenApiService { @GET @Path(\u0026amp;quot;openapi\u0026amp;quot;) @Produces(\u0026amp;quot;application/json\u0026amp;quot;) String openApi(); }  然后提供一个实现类，并且发布成 SOFARPC 的 RESTful 的服务：\n@Service @SofaService(bindings = {@SofaServiceBinding(bindingType = \u0026amp;quot;rest\u0026amp;quot;)}, interfaceType = OpenApiService.class) public class OpenApiServiceImpl implements OpenApiService, InitializingBean { private OpenAPI openAPI; @Override public String openApi() { return Json.pretty(openAPI); } @Override public void afterPropertiesSet() { List\u0026amp;lt;Package\u0026amp;gt; resources = new ArrayList\u0026amp;lt;\u0026amp;gt;(); resources.add(this.getClass().getPackage()); // 扫描当前类所在的 Package，也可以扫描其他的 SOFARPC RESTful 服务接口所在的 Package if (!resources.isEmpty()) { // init context try { SwaggerConfiguration oasConfig = new SwaggerConfiguration() .resourcePackages(resources.stream().map(Package::getName).collect(Collectors.toSet())); OpenApiContext oac = new JaxrsOpenApiContextBuilder() .openApiConfiguration(oasConfig) .buildContext(true); openAPI = oac.read(); } catch (OpenApiConfigurationException e) { throw new RuntimeException(e.getMessage(), e); } } } }  这样，应用启动后，访问 http://localhost:8341/swagger/openapi 即可得到当前的应用发布的所有的 RESTful 的服务的信息。\n解决跨域问题 如果用户在另外一个端口中启动了一个 Swagger UI，并且希望通过 Swagger UI 来访问 http://localhost:8341/swagger/openapi 查看 API 定义，发起调用，那么可能需要解决访问跨域的问题，要解决 SOFARPC RESTful 服务访问跨域的问题，可以在应用启动前增加如下的代码：\nimport org.jboss.resteasy.plugins.interceptors.CorsFilter; public static void main(String[] args) { CorsFilter corsFilter = new CorsFilter(); corsFilter.getAllowedOrigins().add(\u0026amp;quot;*\u0026amp;quot;); …","date":-62135596800,"description":"","dir":"projects/sofa-rpc/restful-swagger/","fuzzywordcount":700,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"d068767fe0dd2922eecef69736684be8","permalink":"/projects/sofa-rpc/restful-swagger/","publishdate":"0001-01-01T00:00:00Z","readingtime":2,"relpermalink":"/projects/sofa-rpc/restful-swagger/","summary":"从 rpc-sofa-boot-starter 6.0.1 版本开始，SOFARPC 提供了 RESTful 服务和 Swagger 的一键集成的能力。 在使用了 rpc-sofa-boot-starter 的情况下，如果想要开启 swagger 的能力，首先需要在 pom.xml 中增加 Swagger 的依赖： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.swagger.core.v3\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;swagger-jaxrs2\u0026lt;/artifactId\u0026gt;","tags":null,"title":"集成 SOFARPC RESTful 服务和 Swagger","type":"projects","url":"/projects/sofa-rpc/restful-swagger/","wordcount":646},{"author":null,"categories":null,"content":" 服务发布 服务发布过程涉及到三个类 RegistryConfig ，ServerConfig ，ProviderConfig 。\n1. RegistryConfig\nRegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026amp;quot;zookeeper\u0026amp;quot;) .setAddress(\u0026amp;quot;127.0.0.1:2181\u0026amp;quot;)  RegistryConfig 表示注册中心。如上声明了服务注册中心的地址和端口是127.0.0.1:2181，协议是 Zookeeper。\n2. ServerConfig\nServerConfig serverConfig = new ServerConfig() .setPort(8803) .setProtocol(\u0026amp;quot;bolt\u0026amp;quot;);  ServerConfig 表示服务运行容器。如上声明了一个使用8803端口和 bolt 协议的 server 。\n3. ProviderConfig\nProviderConfig\u0026amp;lt;HelloWorldService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloWorldService\u0026amp;gt;() .setInterfaceId(HelloWorldService.class.getName()) .setRef(new HelloWorldServiceImpl()) .setServer(serverConfig) .setRegistry(registryConfig); providerConfig.export();  ProviderConfig 表示服务发布。如上声明了服务的接口，实现和该服务运行的 server 。 最终通过 export 方法将这个服务发布出去了。\n服务引用 服务引用涉及到两个类， RegistryConfig 和 ConsumerConfig 。\nConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt; consumerConfig = new ConsumerConfig\u0026amp;lt;HelloService\u0026amp;gt;() .setInterfaceId(HelloService.class.getName()) .setRegistry(registryConfig); HelloService helloService = consumerConfig.refer();  ConsumerConfig 表示服务引用，如上声明了所引用服务的接口和服务注册中心。 最终通过 refer 方法将这个服务引用，获取到该服务的远程调用的代理。\n","date":-62135596800,"description":"","dir":"projects/sofa-rpc/programing-rpc/","fuzzywordcount":400,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"ee6f74a4974c7abf72322cef108d5ef0","permalink":"/projects/sofa-rpc/programing-rpc/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/programing-rpc/","summary":"服务发布 服务发布过程涉及到三个类 RegistryConfig ，ServerConfig ，ProviderConfig 。 1. RegistryConfig RegistryConfig registryConfig = new RegistryConfig() .setProtocol(\u0026quot;zookeeper\u0026quot;) .setAddress(\u0026quot;127.0.0.1:2181\u0026quot;) RegistryConfig 表示注册中心。如上声明了服务","tags":null,"title":"非 Spring 环境 API 使用","type":"projects","url":"/projects/sofa-rpc/programing-rpc/","wordcount":300},{"author":null,"categories":null,"content":"预热权重功能让客户端机器能够根据服务端的相应权重进行流量的分发。该功能也常被用于集群内少数机器的启动场景。利用流量权重功能在短时间内对服务端机器进行预热，然后再接收正常的流量比重。 运行机制如下： 1.服务端服务在启动时会将自身的预热时间，预热期内权重，预热完成后的正常权重推送给服务注册中心。如上图 ServiceB 指向 Service Registry 。\n2.客户端在引用服务的时候会获得每个服务实例的预热权重信息。如上图 Service Registry 指向 client 。\n3.客户端在进行调用的时候会根据服务所在地址的预热时期所对应的权重进行流量分发。如上图 client 指向 ServiceA 和 ServiceB 。 ServiceA 预热完毕，权重默认 100 ， ServiceB 处于预热期，权重为 10，因此所承受流量分别为 100%110 和 10%110 。\n该功能使用方式如下。\nProviderConfig\u0026amp;lt;HelloWordService\u0026amp;gt; providerConfig = new ProviderConfig\u0026amp;lt;HelloWordService\u0026amp;gt;() .setWeight(100) .setParameter(ProviderInfoAttrs.ATTR_WARMUP_WEIGHT,\u0026amp;quot;10\u0026amp;quot;) .setParameter(ProviderInfoAttrs.ATTR_WARM_UP_END_TIME,\u0026amp;quot;12000\u0026amp;quot;);  如上，该服务的预热期为12s，在预热期内权重为10，预热期结束后的正常权重为100。如果该服务一共发布在两个机器A,B上，A机器正处于预热期内，并使用上述配置，B已经完成预热，正常权重为200。那么客户端在调用的时候，此时流量分发的比重为10：200，A机器预热结束后，流量分发比重为100：200。 在SOFABoot中，如下配置预热时间，预热期间权重和预热完后的权重即可。\n\u0026amp;lt;sofa:reference id=\u0026amp;quot;sampleRestFacadeReferenceBolt\u0026amp;quot; interface=\u0026amp;quot;com.alipay.sofa.endpoint.facade.SampleFacade\u0026amp;quot;\u0026amp;gt; \u0026amp;lt;sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;sofa:global-attrs weight=\u0026amp;quot;100\u0026amp;quot; warm-up-time=\u0026amp;quot;10000\u0026amp;quot; warm-up-weight=\u0026amp;quot;1000\u0026amp;quot;/\u0026amp;gt; \u0026amp;lt;/sofa:binding.bolt\u0026amp;gt; \u0026amp;lt;/sofa:reference\u0026amp;gt;  ","date":-62135596800,"description":"","dir":"projects/sofa-rpc/provider-warmup-weight/","fuzzywordcount":500,"kind":"page","lang":"zh","lastmod":1611451625,"objectID":"b9e320dfaa4f9700ecdca67d76e07d54","permalink":"/projects/sofa-rpc/provider-warmup-weight/","publishdate":"0001-01-01T00:00:00Z","readingtime":1,"relpermalink":"/projects/sofa-rpc/provider-warmup-weight/","summary":"预热权重功能让客户端机器能够根据服务端的相应权重进行流量的分发。该功能也常被用于集群内少数机器的启动场景。利用流量权重功能在短时间内对服务端","tags":null,"title":"预热权重","type":"projects","url":"/projects/sofa-rpc/provider-warmup-weight/","wordcount":497}]